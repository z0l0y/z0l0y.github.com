<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>为什么要函数式编程？</title>
    <url>/2024/09/19/CPP01/</url>
    <content><![CDATA[<h2 id="为什么需要函数？"><a href="#为什么需要函数？" class="headerlink" title="为什么需要函数？"></a>为什么需要函数？</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="type">int</span> s = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; a.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        s += a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;sum = &#123;&#125;&quot;</span>, s);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这是一个计算数组求和的简单程序。</p>
<p>但是，他只能计算数组 a 的求和，无法复用。</p>
<p>如果我们有另一个数组 b 也需要求和的话，就得把整个求和的 for 循环重新写一遍：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="type">int</span> s = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; a.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        s += a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;sum of a = &#123;&#125;&quot;</span>, s);</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; b = &#123;<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>&#125;;</span><br><span class="line">    s = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; a.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        s += b[i];</span><br><span class="line">    &#125;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;sum of b = &#123;&#125;&quot;</span>, s);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这就出现了程序设计的大忌：<strong>代码重复</strong>。</p>
<p><strong>在软件设计中，也有一个类似的概念：DRY(Don’t Repeat Yourself)。</strong></p>
<blockquote>
<p>例如，你有吹空调的需求，和充手机的需求。你为了满足这两个需求，购买了两台发电机，分别为空调和手机供电。第二天，你又产生了玩电脑需求，于是你又购买一台发电机，专为电脑供电……真是浪费！</p>
</blockquote>
<p>重复的代码不仅影响代码的<strong>可读性</strong>，也增加了<strong>维护</strong>代码的成本。</p>
<ul>
<li>看起来乱糟糟的，<strong>信息密度低</strong>，让人一眼看不出代码在干什么的功能</li>
<li>很容易写错，看走眼，难调试</li>
<li>复制粘贴过程中，<strong>容易漏改</strong>，比如这里的 <code>s += b[i]</code> 可能写成 <code>s += a[i]</code> 而自己不发现</li>
<li>改起来不方便，<strong>当我们的需求变更时，需要多处修改</strong>，比如当我需要改为计算乘积时，需要把两个地方都改成 <code>s *=</code></li>
<li>改了以后<strong>可能漏改</strong>一部分，留下 Bug 隐患</li>
<li>敏捷开发需要<strong>反复修改代码</strong>，比如你正在调试 <code>+=</code> 和 <code>-=</code> 的区别，看结果变化，如果一次切换需要改多处，就影响了调试速度</li>
</ul>
<h3 id="狂想：没有函数的世界？"><a href="#狂想：没有函数的世界？" class="headerlink" title="狂想：没有函数的世界？"></a>狂想：没有函数的世界？</h3><blockquote>
<p>如果你还是喜欢“一本道”写法的话，不妨想想看，完全不用任何标准库和第三方库的函数和类，把 <code>fmt::println</code> 和 <code>std::vector</code> 这些函数全部拆解成一个个系统调用。那这整个程序会有多难写？</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> _WIN32</span></span><br><span class="line">    <span class="type">int</span> *a = (<span class="type">int</span> *)<span class="built_in">VirtualAlloc</span>(<span class="literal">NULL</span>, <span class="number">4096</span>, MEM_COMMIT, PAGE_EXECUTE_READWRITE);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="type">int</span> *a = (<span class="type">int</span> *)<span class="built_in">mmap</span>(<span class="literal">NULL</span>, <span class="number">4</span> * <span class="built_in">sizeof</span>(<span class="type">int</span>), PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_PRIVATE, <span class="number">-1</span>, <span class="number">0</span>);</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">    a[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    a[<span class="number">1</span>] = <span class="number">2</span>;</span><br><span class="line">    a[<span class="number">2</span>] = <span class="number">3</span>;</span><br><span class="line">    a[<span class="number">3</span>] = <span class="number">4</span>;</span><br><span class="line">    <span class="type">int</span> s = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">        s += a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">char</span> buffer[<span class="number">64</span>];</span><br><span class="line">    buffer[<span class="number">0</span>] = <span class="string">&#x27;s&#x27;</span>;</span><br><span class="line">    buffer[<span class="number">1</span>] = <span class="string">&#x27;u&#x27;</span>;</span><br><span class="line">    buffer[<span class="number">2</span>] = <span class="string">&#x27;m&#x27;</span>;</span><br><span class="line">    buffer[<span class="number">3</span>] = <span class="string">&#x27; &#x27;</span>;</span><br><span class="line">    buffer[<span class="number">4</span>] = <span class="string">&#x27;=&#x27;</span>;</span><br><span class="line">    buffer[<span class="number">5</span>] = <span class="string">&#x27; &#x27;</span>; <span class="comment">// 例如，如果要修改此处的提示文本，甚至需要修改后面的 len 变量...</span></span><br><span class="line">    <span class="type">int</span> len = <span class="number">6</span>;</span><br><span class="line">    <span class="type">int</span> x = s;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        buffer[len++] = <span class="string">&#x27;0&#x27;</span> + x % <span class="number">10</span>;</span><br><span class="line">        x /= <span class="number">10</span>;</span><br><span class="line">    &#125; <span class="keyword">while</span> (x);</span><br><span class="line">    buffer[len++] = <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> _WIN32</span></span><br><span class="line">    <span class="built_in">WriteFile</span>(<span class="built_in">GetStdHandle</span>(STD_OUTPUT_HANDLE), buffer, len, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="built_in">write</span>(<span class="number">1</span>, buffer, len);</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">    <span class="type">int</span> *b = (<span class="type">int</span> *)a;</span><br><span class="line">    b[<span class="number">0</span>] = <span class="number">4</span>;</span><br><span class="line">    b[<span class="number">1</span>] = <span class="number">5</span>;</span><br><span class="line">    b[<span class="number">2</span>] = <span class="number">6</span>;</span><br><span class="line">    b[<span class="number">3</span>] = <span class="number">7</span>;</span><br><span class="line">    <span class="type">int</span> s = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; i++) &#123;</span><br><span class="line">        s += b[i];</span><br><span class="line">    &#125;</span><br><span class="line">    len = <span class="number">6</span>;</span><br><span class="line">    x = s;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        buffer[len++] = <span class="string">&#x27;0&#x27;</span> + x % <span class="number">10</span>;</span><br><span class="line">        x /= <span class="number">10</span>;</span><br><span class="line">    &#125; <span class="keyword">while</span> (x);</span><br><span class="line">    buffer[len++] = <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> _WIN32</span></span><br><span class="line">    <span class="built_in">WriteFile</span>(<span class="built_in">GetStdHandle</span>(STD_OUTPUT_HANDLE), buffer, len, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="built_in">write</span>(<span class="number">1</span>, buffer, len);</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> _WIN32</span></span><br><span class="line">    <span class="built_in">VirtualFree</span>(a, <span class="number">0</span>, MEM_RELEASE);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="built_in">munmap</span>(a);</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>不仅<strong>完全没有可读性、可维护性</strong>，甚至都没有可移植性。</p>
<p>除非你只写应付导师的“一次性”程序，一旦要实现复杂的业务需求，<strong>不可避免的要自己封装函数或类</strong>。网上所有鼓吹“不封装”“设计模式是面子工程”的反智言论，都是没有做过大型项目的。</p>
<h3 id="设计模式追求的是“可改”而不是“可读”！"><a href="#设计模式追求的是“可改”而不是“可读”！" class="headerlink" title="设计模式追求的是“可改”而不是“可读”！"></a>设计模式追求的是“可改”而不是“可读”！</h3><p>很多设计模式教材片面强调<strong>可读性</strong>，仿佛设计模式就是为了“优雅”“高大上”“美学”？使得很多人认为，“我这个是自己的项目，<strong>不用美化</strong>给领导看”而拒绝设计模式。实际上设计模式的主要价值在于<strong>方便后续修改</strong>！</p>
<blockquote>
<p>例如 B 站以前只支持上传普通视频，现在叔叔突然提出：要支持互动视频，充电视频，视频合集，还废除了视频分 p，还要支持上传短视频，竖屏开关等……每一个叔叔的要求，<strong>都需要大量程序员修改代码，无论涉及前端还是后端</strong>。</p>
</blockquote>
<p>与建筑、绘画等领域不同，一次交付完毕就可以几乎永久使用。<strong>而软件开发是一个持续的过程，每次需求变更，都导致代码需要修改</strong>。开发人员几乎需要一直围绕着软件代码，不断的修改。调查表明，程序员 90% 的时间花在<strong>改代码</strong>上，<strong>写代码</strong>只占 10%。</p>
<blockquote>
<p>软件就像生物，要不断进化，软件不更新不维护了等于死。如果一个软件逐渐变得臃肿难以修改，无法适应新需求，那他就像已经失去进化能力的生物种群，如《三体》世界观中“安顿”到澳大利亚保留区里“绝育”的人类，被淘汰只是时间问题。</p>
</blockquote>
<p>如果我们能在<strong>写代码</strong>阶段，就把程序准备得<strong>易于后续修改</strong>，那就可以在后续 90% 的<strong>改代码</strong>阶段省下无数时间。</p>
<p>如何让代码易于修改？前人总结出一系列常用的写法，这类写法有助于让后续修改更容易，各自适用于不同的场合，这就是设计模式。</p>
<p><strong>提升可维护性最基础的一点，就是避免重复！</strong></p>
<p>当你有很多地方出现重复的代码时，一旦需要涉及修改这部分逻辑时，就需要到每一个出现了这个逻辑的代码中，去逐一修改。</p>
<blockquote>
<p>例如你的名字，在出生证，身份证，学生证，毕业证，房产证，驾驶证，各种地方都出现了。<strong>那么你要改名的话，所有这些证件都需要重新印刷！如果能把他们合并成一个“统一证”，那么只需要修改“统一证”上的名字就行了。</strong></p>
<p>可以理解为这些证在使用的时候是到一个固定的点位去获取名字，而不是硬编码写死，我们微服务的Nacos这点实现的就非常好。通过这样的操作，我们就不需要全部重新修改一遍了。</p>
</blockquote>
<p>不过，现实中并没有频繁改名字的需求，这说明：</p>
<ul>
<li><strong>对于不常修改的东西，可以容忍一定的重复。</strong></li>
<li><strong>越是未来有可能修改的，就越需要设计模式降重！</strong></li>
</ul>
<p>例如数学常数 PI &#x3D; 3.1415926535897，这辈子都不可能出现修改的需求，那写死也没关系。如果要把 PI 定义成宏，只是出于“记不住”“写起来太长了”“复制粘贴麻烦”。所以对于 PI 这种不会修改的东西，降重只是增加<strong>可读性</strong>，而不是<strong>可修改性</strong>。</p>
<blockquote>
<p>但是，不要想当然！需求的千变万化总是超出你的想象。</p>
</blockquote>
<p>例如你做了一个“愤怒的小鸟”游戏，需要用到重力加速度 g &#x3D; 9.8，你想当然认为 g 以后不可能修改。老板也信誓旦旦向你保证：“没事，重力加速度不会改变。”你就写死在代码里了。</p>
<p>没想到，“愤怒的小鸟”老板突然要求你加入“月球章”关卡，在这些关卡中，重力加速度是 g &#x3D; 1.6。</p>
<p>如果你一开始就已经把 g 提取出来，定义为常量：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Level</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">double</span> g = <span class="number">9.8</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">physics_sim</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        bird.v = g * t; <span class="comment">// 假装这里是物理仿真程序</span></span><br><span class="line">        pig.v = g * t;  <span class="comment">// 假装这里是物理仿真程序</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>那么要支持月球关卡，只需修改一处就可以了。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Level</span> &#123;</span><br><span class="line">    <span class="type">double</span> g;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">Level</span>(Chapter chapter) &#123;</span><br><span class="line">        <span class="keyword">if</span> (chapter == ChapterMoon) &#123;</span><br><span class="line">            g = <span class="number">1.6</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            g = <span class="number">9.8</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">physics_sim</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        bird.v = g * t; <span class="comment">// 无需任何修改，自动适应了新的非常数 g</span></span><br><span class="line">        pig.v = g * t;  <span class="comment">// 无需任何修改，自动适应了新的非常数 g</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<blockquote>
<p> 小彭老师之前做 zeno 时，询问要不要把渲染管线节点化，方便用户动态编程？张猩猩就是信誓旦旦道：“渲染是一个高度成熟领域，不会有多少修改需求的。”小彭老师遂写死了渲染管线，专为性能极度优化，几个月后，张猩猩羞答答找到小彭老师：“小彭老师，那个，渲染，能不能改成节点啊……”。这个故事告诉我们，甲方的信誓旦旦放的一个屁都不能信。</p>
</blockquote>
<h3 id="用函数封装"><a href="#用函数封装" class="headerlink" title="用函数封装"></a>用函数封装</h3><p>函数就是来帮你解决代码重复问题的！要领：</p>
<p><strong>把共同的部分提取出来，把不同的部分作为参数传入。</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">sum</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> s = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; v.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        s += v[i];</span><br><span class="line">    &#125;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;sum of v = &#123;&#125;&quot;</span>, s);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="built_in">sum</span>(a);</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; b = &#123;<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>&#125;;</span><br><span class="line">    <span class="built_in">sum</span>(b);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这样 main 函数里就可以只关心要求和的数组，而不用关心求和具体是如何实现的了。事后我们可以随时把 sum 的内容偷偷换掉，换成并行的算法，main 也不用知道。这就是<strong>封装</strong>，可以<strong>把重复的公共部分抽取出来，方便以后修改代码</strong>。</p>
<blockquote>
<p>sum 函数相当于，当需要吹空调时，插上空调插座。当需要给手机充电时，插上手机充电器。你不需要关心插座里的电哪里来，“国家电网”会替你想办法解决，想办法优化，想办法升级到绿色能源。你只需要吹着空调给你正在开发的手机 App 优化就行了，大大减轻程序员心智负担。</p>
</blockquote>
<h3 id="要封装，但不要耦合"><a href="#要封装，但不要耦合" class="headerlink" title="要封装，但不要耦合"></a>要封装，但不要耦合</h3><p>但是！这段代码仍然有个问题，<strong>我们把 sum 求和的结果，直接在 sum 里打印了出来。sum 里写死了，求完和之后只能直接打印，调用者 main 根本无法控制。</strong></p>
<p>这是一种错误的封装，或者说，封装过头了。</p>
<blockquote>
<p>你把手机充电器 (fmt::println) 焊死在了插座 (sum) 上，<strong>现在这个插座只能给手机充电 (用于直接打印) 了，不能给笔记本电脑充电 (求和结果不直接用于打印) 了</strong>！尽管通过更换充电线 (参数 v)，还可以支持支持安卓 (a) 和苹果 (b) 两种手机的充电，但这样焊死的插座已经和笔记本电脑无缘了。</p>
</blockquote>
<h3 id="每个函数应该职责单一，别一心多用"><a href="#每个函数应该职责单一，别一心多用" class="headerlink" title="每个函数应该职责单一，别一心多用"></a>每个函数应该职责单一，别一心多用</h3><p>很明显，<strong>“打印”和“求和”是两个独立的操作，不应该焊死在一块。</strong></p>
<p>sum 函数的本职工作是“数组求和”，<strong>不应该附赠打印功能</strong>。</p>
<p>sum 计算出求和结果后，<strong>直接 return 即可</strong>。</p>
<blockquote>
<p><strong>如何处理这个结果，是调用者 main 的事</strong>，正如“国家电网”不会管你用他提供的电来吹空调还是玩游戏一样，只要不妨碍到其他居民的正常用电。</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">sum</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> s = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; v.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        s += v[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;sum of a = &#123;&#125;&quot;</span>, <span class="built_in">sum</span>(a));</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; b = &#123;<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>&#125;;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;sum of b = &#123;&#125;&quot;</span>, <span class="built_in">sum</span>(b));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这就是设计模式所说的<strong>职责单一原则</strong>。</p>
<h3 id="二次封装"><a href="#二次封装" class="headerlink" title="二次封装"></a>二次封装</h3><p>假设我们要计算一个数组的平均值，可以再定义个函数 average，他可以基于 sum 实现：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">sum</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> s = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; v.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        s += v[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">average</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="type">double</span>)<span class="built_in">sum</span>(v) / v.<span class="built_in">size</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;average of a = &#123;&#125;&quot;</span>, <span class="built_in">average</span>(a));</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; b = &#123;<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>&#125;;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;average of b = &#123;&#125;&quot;</span>, <span class="built_in">average</span>(b));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>进一步封装一个打印数组所有统计学信息的函数：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">print_statistics</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (v.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">        fmt::<span class="built_in">println</span>(<span class="string">&quot;this is empty...&quot;</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        fmt::<span class="built_in">println</span>(<span class="string">&quot;sum: &#123;&#125;&quot;</span>, <span class="built_in">sum</span>(v));</span><br><span class="line">        fmt::<span class="built_in">println</span>(<span class="string">&quot;average: &#123;&#125;&quot;</span>, <span class="built_in">average</span>(v));</span><br><span class="line">        fmt::<span class="built_in">println</span>(<span class="string">&quot;min: &#123;&#125;&quot;</span>, <span class="built_in">min</span>(v));</span><br><span class="line">        fmt::<span class="built_in">println</span>(<span class="string">&quot;max: &#123;&#125;&quot;</span>, <span class="built_in">max</span>(v));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="built_in">print_statistics</span>(a);</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; b = &#123;<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>&#125;;</span><br><span class="line">    <span class="built_in">print_statistics</span>(b);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>暴露 API 时，要同时提供底层的 API 和高层封装的 API。<strong>用户如果想要控制更多细节可以调用底层 API，想要省事的用户可以调用高层封装好的 API。</strong></p>
<blockquote>
<p>高层封装 API 应当可以完全通过调用底层 API 实现，提供高层 API 只是<strong>方便初级用户使用和理解</strong>。</p>
<p>例如 <code>libcurl</code> 就提供了 <code>curl_easy</code> 和 <code>curl_multi</code> 两套 API。</p>
</blockquote>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="code">`curl_multi`</span> 提供了超详细的参数，把每个操作分拆成多步，方便用户插手细节，满足高级用户的定制化需求，但太过复杂，难以学习。</span><br><span class="line"><span class="bullet">-</span> <span class="code">`curl_easy`</span> 是对 <span class="code">`curl_multi`</span> 的再封装，提供了更简单的 API，但是对具体细节就难以操控了，适合初学者上手。</span><br></pre></td></tr></table></figure>

<h3 id="Linus-的最佳实践：每个函数不要超过-3-层嵌套，一行不要超过-80-字符，每个函数体不要超过-24-行"><a href="#Linus-的最佳实践：每个函数不要超过-3-层嵌套，一行不要超过-80-字符，每个函数体不要超过-24-行" class="headerlink" title="Linus 的最佳实践：每个函数不要超过 3 层嵌套，一行不要超过 80 字符，每个函数体不要超过 24 行"></a>Linus 的最佳实践：每个函数不要超过 3 层嵌套，一行不要超过 80 字符，每个函数体不要超过 24 行</h3><p>Linux 内核为什么坚持使用 <strong>8 缩进为代码风格</strong>？</p>
<p><strong>因为高缩进可以避免程序员写出嵌套层数太深的代码，当他写出太深嵌套时，巨大的 8 缩进会让代码变得非常偏右，写不下多少空间。</strong>从而让程序员自己红着脸“对不起，我把单个函数写太深了”然后赶紧拆分出多个函数来。</p>
<p>此外，他还规定了<strong>单一一个函数必须在终端宽度 80 x 24 中显示得下，否则就需要拆分成多个函数重写，这配合 8 缩进，有效的限制了嵌套的层数，迫使程序员不得不重新思考，更解耦的写法出来。</strong></p>
<h2 id="为什么需要函数式？"><a href="#为什么需要函数式？" class="headerlink" title="为什么需要函数式？"></a>为什么需要函数式？</h2><p>你产生了两个需求，分别封装了两个函数：</p>
<ul>
<li><code>sum</code> 求所有元素的和</li>
<li><code>product</code> 求所有元素的积</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">sum</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> ret = v[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; v.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        ret += v[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">product</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> ret = v[<span class="number">0</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; v.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        ret *= v[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;sum: &#123;&#125;&quot;</span>, <span class="built_in">sum</span>(a));</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;product: &#123;&#125;&quot;</span>, <span class="built_in">product</span>(a));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意到 <code>sum</code> 和 <code>product</code> 的内容几乎如出一辙，唯一的区别在于：</p>
<ul>
<li><code>sum</code> 的循环体为 <code>+=</code>；</li>
<li><code>product</code> 的循环体为 <code>*=</code>。</li>
</ul>
<p><strong>这种函数体内有部分代码重复，但又有特定部分不同，难以抽离。</strong></p>
<p>该怎么复用这重复的部分代码呢？</p>
<p>我们要把 <code>sum</code> 和 <code>product</code> 合并成一个函数 <code>generic_sum</code>。然后通过函数参数，把差异部分（0、<code>+=</code>）<strong>“注入”到两个函数原本不同地方。</strong></p>
<h3 id="枚举的糟糕用法"><a href="#枚举的糟糕用法" class="headerlink" title="枚举的糟糕用法"></a>枚举的糟糕用法</h3><p>如何表示我这个函数是要做求和 <code>+=</code> 还是求积 <code>*=</code>？</p>
<p>让我们定义枚举：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="title class_">Mode</span> &#123;</span><br><span class="line">    ADD, <span class="comment">// 求和操作</span></span><br><span class="line">    MUL, <span class="comment">// 求积操作</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">generic_sum</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v, Mode mode)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> ret = v[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; v.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (mode == ADD) &#123; <span class="comment">// 函数内判断枚举，决定要做什么操作</span></span><br><span class="line">            ret += v[i];</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (mode == MUL) &#123;</span><br><span class="line">            ret *= v[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;sum: &#123;&#125;&quot;</span>, <span class="built_in">generic_sum</span>(a, ADD)); <span class="comment">// 用户指定他想要的操作</span></span><br><span class="line">    fmt::<span class="built_in">println</span>(<span class="string">&quot;product: &#123;&#125;&quot;</span>, <span class="built_in">generic_sum</span>(a, MUL));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然而，如果用户现在想要求数组的<strong>最大值</strong>呢？</p>
<p>枚举中还没有实现最大值的操作……要支持，就得手忙脚乱地去修改 <code>generic_sum</code> 函数和 <code>Mode</code> 枚举原本的定义，真麻烦！</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="title class_">Mode</span> &#123;</span><br><span class="line">    ADD,</span><br><span class="line">    MUL,</span><br><span class="line">    MAX, <span class="comment">// ***改***</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">generic_sum</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v, Mode mode)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> ret = v[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; v.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (mode == ADD) &#123;</span><br><span class="line">            ret += v[i];</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (mode == MUL) &#123;</span><br><span class="line">            ret *= v[i];</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (mode == MAX) &#123; <span class="comment">// ***改***</span></span><br><span class="line">            ret = std::<span class="built_in">max</span>(ret, v[i]); <span class="comment">// ***改***</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="built_in">generic_sum</span>(a, MAX); <span class="comment">// ***改***</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p> 我用 <code>// ***改***</code> 指示了所有需要改动的地方。</p>
</blockquote>
<p><strong>为了增加一个求最大值的操作，就需要三处分散在各地的改动！</strong></p>
<p>不仅如此，还容易抄漏，抄错，比如 <code>MAX</code> 不小心打错成 <code>MUL</code> 了，自己却没发现，留下 BUG 隐患。</p>
<p>这样写代码的方式，心智负担极大，整天就提心吊胆着东一块，西一块的散装代码，担心着有没有哪个地方写错写漏，严重妨碍了开发效率。</p>
<p>并且写出来的代码也不能适应需求的变化：假如我需要支持 <code>MIN</code> 呢？又得改三个地方！这违背了设计模式的<strong>开闭原则</strong>。</p>
<ul>
<li>开闭原则: <strong>对扩展开放，对修改封闭</strong>。指的是软件在适应需求变化时，应尽量通过<strong>扩展代码来实现变化，而不是通过修改已有代码</strong>来实现变化。</li>
</ul>
<p>使用枚举和 if-else 实现多态，难以扩展，还要一直去修改原函数的底层实现，就违背了<strong>开闭原则</strong>。</p>
<h3 id="函数式编程光荣救场"><a href="#函数式编程光荣救场" class="headerlink" title="函数式编程光荣救场"></a>函数式编程光荣救场</h3><p>如果我们可以<strong>“注入”代码</strong>就好了！能否把一段“代码”作为 <code>generic_sum</code> 函数的参数呢？</p>
<p><strong>代码，实际上就是函数，注入代码就是注入函数</strong>。我们先定义出三个不同操作对应的函数：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">add</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">mul</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a * b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">max</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> std::<span class="built_in">max</span>(a, b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后，把这三个小函数，作为另一个大函数 <code>generic_sum</code> 的参数就行！</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">generic_sum</span><span class="params">(std::vector&lt;<span class="type">int</span>&gt; <span class="type">const</span> &amp;v, <span class="keyword">auto</span> op)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> ret = v[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; v.<span class="built_in">size</span>(); i++) &#123;</span><br><span class="line">        <span class="comment">// 函数作者无需了解用户指定的“操作”具体是什么</span></span><br><span class="line">        <span class="comment">// 只需要调用这一“操作”，得到结果就行</span></span><br><span class="line">        ret = <span class="built_in">op</span>(ret, v[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">int</span>&gt; a = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">    <span class="comment">// 用户无需关心函数的具体实现是什么</span></span><br><span class="line">    <span class="comment">// 只需随心所欲指定他的“操作”作为参数</span></span><br><span class="line">    <span class="built_in">generic_sum</span>(a, add);</span><br><span class="line">    <span class="built_in">generic_sum</span>(a, product);</span><br><span class="line">    <span class="built_in">generic_sum</span>(a, max);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>责任明确了，我们成功把一部分细节从 <code>generic_sum</code> 中进一步抽离。</p>
<ul>
<li>库作者 <code>generic_sum</code> 不必了解 <code>main</code> 的操作具体是什么，他只负责利用这个操作求“和”。</li>
<li>库用户 <code>main</code> 不必了解 <code>generic_sum</code> 如何实现操作累加，<strong>他只管注入“如何操作”的代码，以函数的形式</strong>。</li>
</ul>
<h3 id="依赖注入原则"><a href="#依赖注入原则" class="headerlink" title="依赖注入原则"></a>依赖注入原则</h3><p>函数对象 <code>op</code> 作为参数传入，让 <code>generic_sum</code> 内部去调用，就像往 <code>generic_sum</code> 体内“注入”了一段自定义代码一样。</p>
<p>这可以让 <code>generic_sum</code> 在不修改本体的情况下，通过修改“注入”部分，轻松扩展，满足<strong>开闭原则</strong>。</p>
<p>更准确的说，这体现的是设计模式所要求的<strong>依赖注入原则</strong>。</p>
<ul>
<li>依赖注入原则: <strong>一个封装好的函数或类，应该尽量依赖于抽象接口，而不是依赖于具体实现</strong>。这可以提高程序的灵活性和可扩展性。</li>
</ul>
<p>四大编程范式都各自发展出了<strong>依赖注入原则</strong>的解决方案：</p>
<ul>
<li>面向过程编程范式中，<strong>函数指针</strong>就是那个抽象接口。</li>
<li><strong>面向对象</strong>编程范式中，<strong>虚函数</strong>就是那个抽象接口。</li>
<li>函数式编程范式中，<strong>函数对象</strong>就是那个抽象接口。</li>
<li>模板元编程范式中，<strong>模板参数</strong>就是那个抽象接口。</li>
</ul>
<p>同样是把抽象接口作为参数，同样解决可扩展问题。</p>
<p>函数指针贴近底层硬件，虚函数方便整合多个接口，函数对象轻量级、随地取用，模板元有助高性能优化，不同的编程范式殊途同归。</p>
<h3 id="低耦合，高内聚"><a href="#低耦合，高内聚" class="headerlink" title="低耦合，高内聚"></a>低耦合，高内聚</h3><p><strong>依赖注入原则可以减少代码之间的耦合度</strong>，大大提高代码的灵活性和可扩展性。</p>
<ul>
<li>耦合度: 指的是一个模块、类、函数和其他模块、类、函数之间的关联程度。<strong>耦合度越低，越容易进行单元测试、重构、复用和扩展</strong>。</li>
</ul>
<blockquote>
<p> 高耦合度的典型是“牵一发而动全身”。低耦合的典范是蚯蚓，因为蚯蚓可以在任意断面切开，还能活下来，看来蚯蚓的身体设计非常“模块化”呢。</p>
</blockquote>
<p>通常来说，<strong>软件应当追求低耦合度，适度解耦的软件能更快适应需求变化。但过度的低耦合也会导致代码过于分散，不易阅读和修改，甚至可能起到反效果。</strong></p>
<blockquote>
<p>若你解耦后，<strong>每次需求变化要改动的地方变少了，那就是合理的解耦。若你过分解耦，代码东一块西一块，以至于需求变化时需要到处改，比不解耦时浪费的时间还要多，那就是解耦过度。</strong></p>
<p>完全零耦合的程序每个函数互不联系，<strong>就像把蚯蚓拆散成一个个独立的细胞一样</strong>。连初始需求“活着”都实现不了，谈何适应需求变化？所以解耦也切勿矫枉过正。</p>
</blockquote>
<p>为了避免解耦矫枉过正，<strong>人们又提出了内聚的概念，并规定解耦的前提是：不耽误内聚</strong>。耽误到内聚的解耦，就只会起到降低可维护性的反效果了。</p>
<ul>
<li>内聚: 指的是同一个模块、类、函数内部各个元素之间的关联程度。内聚度越高，功能越独立，越方便集中维护。</li>
</ul>
<blockquote>
<p>例如，人的心脏专门负责泵血，肝脏只负责解毒，这就是<strong>高内聚</strong>的人体器官。若人的心脏还要兼职解毒，肝脏还兼职泵血，看似好像是增加了“万一心脏坏掉”的冗余性，实际上把“泵血”这一功能拆散到各地，无法“集中力量泵大血”了。</p>
<p> 人类的大脑和 CPU 一样，也有“缓存局域性 (cache-locality)”的限制：**不能同时在很多个主题之间快速切换，无论是时间上的还是空间上的割裂 (cache-miss)**，都会干扰程序员思维的连贯性，从而增大心智负担。</p>
</blockquote>
<p>好的软件要保持低耦合，同时高内聚。</p>
<blockquote>
<p>就像“民主集中制”一样，既要监督防止大权独揽，又要集中力量办一个人办不成的大事。</p>
</blockquote>
<p>节选自：<a href="https://parallel101.github.io/cppguidebook/lambda/#_2">小彭老师带你学函数式编程 - ✝️小彭大典✝️ (parallel101.github.io)</a></p>
]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>现代化的 API 设计指南</title>
    <url>/2024/09/19/CPP02/</url>
    <content><![CDATA[<p>如何写出易于维护的代码，阻止犯错？</p>
<p><strong>类型就是最好的注释！</strong></p>
<p><strong>Type is all you need</strong></p>
<h2 id="结构体传参"><a href="#结构体传参" class="headerlink" title="结构体传参"></a>结构体传参</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(string name, <span class="type">int</span> age, <span class="type">int</span> phone, <span class="type">int</span> address)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">foo</span>(<span class="string">&quot;小彭老师&quot;</span>, <span class="number">24</span>, <span class="number">12345</span>, <span class="number">67890</span>);</span><br></pre></td></tr></table></figure>

<ul>
<li>痛点：<strong>参数多，类型相似，容易顺序写错而自己不察觉</strong></li>
<li>天书：<strong>阅读代码时看不见参数名，不清楚每个参数分别代表什么</strong></li>
</ul>
<blockquote>
<p>怎么办？</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">FooOptions</span> &#123;</span><br><span class="line">    string name;</span><br><span class="line">    <span class="type">int</span> age;</span><br><span class="line">    <span class="type">int</span> phone;</span><br><span class="line">    <span class="type">int</span> address;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(FooOptions opts)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">foo</span>(&#123;.name = <span class="string">&quot;小彭老师&quot;</span>, .age = <span class="number">24</span>, .phone = <span class="number">12345</span>, .address = <span class="number">67890</span>&#125;);</span><br></pre></td></tr></table></figure>

<p>✔️ 优雅，每个参数负责做什么一目了然</p>
<p><strong>也有某些大厂推崇注释参数名来增强可读性：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">foo</span>(<span class="comment">/*name=*/</span><span class="string">&quot;小彭老师&quot;</span>, <span class="comment">/*age=*/</span><span class="number">24</span>, <span class="comment">/*phone=*/</span><span class="number">12345</span>, <span class="comment">/*address=*/</span><span class="number">67890</span>);</span><br></pre></td></tr></table></figure>

<p>但注释可以骗人：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">foo</span>(<span class="comment">/*name=*/</span><span class="string">&quot;小彭老师&quot;</span>, <span class="comment">/*phone=*/</span><span class="number">12345</span>, <span class="comment">/*age=*/</span><span class="number">24</span>, <span class="comment">/*address=*/</span><span class="number">67890</span>);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里 age 和 phone 参数写反了！<strong>阅读者如果不看下 foo 的定义，根本发现不了</strong></p>
</blockquote>
<p>而代码不会：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 即使顺序写错，只要名字写对依然可以正常运行</span></span><br><span class="line"><span class="built_in">foo</span>(&#123;.name = <span class="string">&quot;小彭老师&quot;</span>, .phone = <span class="number">12345</span>, .age = <span class="number">24</span>, .address = <span class="number">67890</span>&#125;);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>总之，好的 API 设计绝不会给人留下犯错的机会！</p>
</blockquote>
<p>再来看一个场景，<strong>假设foo内部需要把所有参数转发给另一个函数bar</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">int</span> index, string name, <span class="type">int</span> age, <span class="type">int</span> phone, <span class="type">int</span> address)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(string name, <span class="type">int</span> age, <span class="type">int</span> phone, <span class="type">int</span> address)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">bar</span>(<span class="built_in">get_hash_index</span>(name), name, age, phone, address);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>痛点：<strong>你需要不断地复制粘贴所有这些参数，非常容易抄错</strong></li>
<li>痛点：<strong>一旦参数类型有所修改，或者要加新参数，需要每个地方都改一下</strong></li>
</ul>
<blockquote>
<p>怎么办？</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">FooOptions</span> &#123;</span><br><span class="line">    string name;</span><br><span class="line">    <span class="type">int</span> age;</span><br><span class="line">    <span class="type">int</span> phone;</span><br><span class="line">    <span class="type">int</span> address;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">int</span> index, FooOptions opts)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(FooOptions opts)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 所有逻辑上相关的参数全合并成一个结构体，方便使用更方便阅读</span></span><br><span class="line">    <span class="built_in">bar</span>(<span class="built_in">get_hash_index</span>(opts.name), opts);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>✔️ 优雅</p>
<p>当老板要求你增加一个参数 sex，加在 age 后面：</p>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line"><span class="deletion">-void foo(string name, int age, int phone, int address);</span></span><br><span class="line"><span class="addition">+void foo(string name, int age, int sex, int phone, int address);</span></span><br></pre></td></tr></table></figure>

<p>你手忙脚乱地打开所有调用了 foo 的文件，发现有大量地方需要修改…</p>
<p>而优雅的 API 总设计师小彭老师只需轻轻修改一处：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">FooOptions</span> &#123;</span><br><span class="line">    string name;</span><br><span class="line">    <span class="type">int</span> age;</span><br><span class="line">    <span class="type">int</span> sex = <span class="number">0</span>; <span class="comment">// 令 sex 默认为 0</span></span><br><span class="line">    <span class="type">int</span> phone;</span><br><span class="line">    <span class="type">int</span> address;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>所有的老代码依然照常调用新的 foo 函数，未指定的 sex 会具有结构体里定义的默认值 0：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">foo</span>(&#123;.name = <span class="string">&quot;小彭老师&quot;</span>, .phone = <span class="number">12345</span>, .age = <span class="number">24</span>, .address = <span class="number">67890</span>&#125;);</span><br></pre></td></tr></table></figure>

<h2 id="返回一个结构体"><a href="#返回一个结构体" class="headerlink" title="返回一个结构体"></a>返回一个结构体</h2><p>当你需要多个返回值时：<strong>不要返回 pair 或 tuple</strong>！</p>
<p>一些 STL 容器的 API 设计是反面典型，例如：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">std::pair&lt;<span class="type">bool</span>, iterator&gt; <span class="title">insert</span><span class="params">(std::pair&lt;K, V&gt; entry)</span></span>;</span><br></pre></td></tr></table></figure>

<p>用的时候每次都要想一下，<strong>到底第一个是 bool 还是第二个是 bool 来着？然后看一眼 IDE 提示，才反应过来</strong>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> result = map.<span class="built_in">insert</span>(&#123;<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>&#125;);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; <span class="string">&quot;是否成功: &quot;</span> &lt;&lt; result.first &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;插入到位置: &quot;</span> &lt;&lt; result.second &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>first？second？这算什么鬼？</p>
<p>更好的做法是返回一个<strong>定制的结构体</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">insert_result_t</span> &#123;</span><br><span class="line">    <span class="type">bool</span> success;</span><br><span class="line">    iterator position;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">insert_result_t</span> <span class="title">insert</span><span class="params">(std::pair&lt;K, V&gt; entry)</span></span>;</span><br></pre></td></tr></table></figure>

<p><strong>直接通过名字访问成员，语义清晰明确</strong>，我管你是第一个第二个，我只想要表示“是否成功(success)”的那个变量。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> result = map.<span class="built_in">insert</span>(&#123;<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>&#125;);</span><br><span class="line"></span><br><span class="line">cout &lt;&lt; <span class="string">&quot;是否成功: &quot;</span> &lt;&lt; result.success &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;插入到位置: &quot;</span> &lt;&lt; result.position &lt;&lt; <span class="string">&#x27;\n&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>最好当然是<strong>返回和参数类型都是结构体（这在我们的Java中其实体现的非常充分了，返回Result，传入DTO）</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">insert_result_t</span> &#123;</span><br><span class="line">    <span class="type">bool</span> success;</span><br><span class="line">    iterator position;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">map_entry_t</span> &#123;</span><br><span class="line">    K key;</span><br><span class="line">    V value;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">insert_result_t</span> <span class="title">insert</span><span class="params">(<span class="type">map_entry_t</span> entry)</span></span>;</span><br></pre></td></tr></table></figure>

<p>这里说的都比较激进，你可能暂时不会认同，等你大手大脚犯了几个错以后，你自然会心服口服。 <strong>小彭老师以前也和你一样是指针仙人，不喜欢强类型，喜欢 <code>void *</code> 满天飞，然后随便改两行就蹦出个 Segmentation Fault，指针一时爽，调试火葬场，然后才开始反思。</strong></p>
<p>STL 中依然在大量用 pair 是因为 map 容器出现的很早，历史原因。 <strong>我们自己项目的 API 就不要设计成这熊样了。</strong></p>
<blockquote>
<p>当然，和某些二级指针返回仙人相比 <code>cudaError_t cudaMalloc(void **pret);</code>，返回 pair 已经算先进的了</p>
</blockquote>
<p>例如 C++17 中的 <code>from_chars</code> 函数，<strong>他的返回类型就是一个定制的结构体：（嗯，很Java）</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">from_chars_result</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span> *ptr;</span><br><span class="line">    errc ec;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">from_chars_result <span class="title">from_chars</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *first, <span class="type">const</span> <span class="type">char</span> *last, <span class="type">int</span> &amp;value)</span></span>;</span><br></pre></td></tr></table></figure>

<p>这说明他们也已经意识到了以前动不动返回 pair 的设计是有问题的，已经在新标准中开始改用更好的设计。</p>
<h2 id="类型即注释"><a href="#类型即注释" class="headerlink" title="类型即注释"></a>类型即注释</h2><p>你是一个新来的员工，看到下面这个函数：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">char</span> *x)</span></span>;</span><br></pre></td></tr></table></figure>

<p>这里的 x 有可能是：</p>
<ol>
<li>0结尾字符串，只读，但是作者忘了加 const</li>
<li>指向单个字符，用于返回单个 char（指针返回仙人）</li>
<li>指向一个字符数组缓冲区，用于返回字符串，但缓冲区大小的确定方式未知</li>
</ol>
<p><strong>如果作者没写文档，变量名又非常含糊，根本不知道这个 x 参数要怎么用。</strong></p>
<blockquote>
<p><strong>类型写的好，能起到注释的作用！</strong></p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(string x)</span></span>;</span><br></pre></td></tr></table></figure>

<p>这样就一目了然了，很明显，是字符串类型的参数。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(string &amp;x)</span></span>;</span><br></pre></td></tr></table></figure>

<p>看起来是返回一个字符串，但是通过引用传参的方式来返回的</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">string <span class="title">foo</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>

<p>通过常规方式直接返回一个字符串。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(vector&lt;<span class="type">uint8_t</span>&gt; x)</span></span>;</span><br></pre></td></tr></table></figure>

<p>是一个 8 位无符号整数组成的数组！</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(span&lt;<span class="type">uint8_t</span>&gt; x)</span></span>;</span><br></pre></td></tr></table></figure>

<p>是一个 8 位无符号整数的数组切片。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(string_view x)</span></span>;</span><br></pre></td></tr></table></figure>

<p>是一个字符串的切片，可能是作者想要避免拷贝开销。</p>
<p>还可以使用类型别名：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> ISBN = string;</span><br><span class="line"></span><br><span class="line"><span class="function">BookInfo <span class="title">foo</span><span class="params">(ISBN isbn)</span></span>;</span><br></pre></td></tr></table></figure>

<p><strong>这样用户一看就明白，这个函数是接收一个 ISBN 编号（出版刊物都有一个这种编号），返回关于这本书的详细信息。</strong></p>
<p><strong>尽管函数名 foo 让人摸不着头脑，但仅凭直观的类型标识，我们就能函数功能把猜的七七八八。</strong></p>
<h2 id="强类型封装"><a href="#强类型封装" class="headerlink" title="强类型封装"></a>强类型封装</h2><p>假设你正在学习这个 Linux 系统 API 函数：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">ssize_t</span> <span class="title">read</span><span class="params">(<span class="type">int</span> fd, <span class="type">char</span> *buf, <span class="type">size_t</span> len)</span></span>;</span><br><span class="line"><span class="comment">// fd - 文件句柄，int 类型</span></span><br></pre></td></tr></table></figure>

<p>但是你没有看他的函数参数类型和名字。你是这样调用的：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> fd = <span class="built_in">open</span>(...);</span><br><span class="line"><span class="type">char</span> buf[<span class="number">32</span>];</span><br><span class="line"><span class="built_in">read</span>(<span class="number">32</span>, buf, fd);</span><br><span class="line"><span class="type">char</span> buf[<span class="number">32</span>];</span><br><span class="line"><span class="built_in">read</span>(<span class="number">32</span>, buf, fd);</span><br></pre></td></tr></table></figure>

<p>你这里的 32 本意是缓冲区的大小，却不幸地和 fd 参数写错了位置，而编译器毫无报错，你浑然不知。</p>
<p><strong>仅仅只是装模作样的用 typedef 定义个好看的类型别名，并没有任何意义！ 他连你的参数名 fd 都能看不见，你觉得他会看到你的参数类型是个别名？</strong></p>
<p>用户一样可以用一个根本不是文件句柄的臭整数来调用你，而得不到任何警告或报错：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> FileHandle;</span><br><span class="line"><span class="function"><span class="type">ssize_t</span> <span class="title">read</span><span class="params">(FileHandle fd, <span class="type">char</span> *buf, <span class="type">size_t</span> len)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">read</span>(<span class="number">32</span>, buf, fd); <span class="comment">// 照样编译通过！</span></span><br></pre></td></tr></table></figure>

<p>如果我们把文件句柄定义为一个结构体：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">FileHandle</span> &#123;</span><br><span class="line">    <span class="type">int</span> handle;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">FileHandle</span><span class="params">(<span class="type">int</span> handle)</span> : handle(handle) &#123;</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">ssize_t</span> <span class="title">read</span><span class="params">(FileHandle handle, <span class="type">char</span> *buf, <span class="type">size_t</span> len)</span></span>;</span><br></pre></td></tr></table></figure>

<p>就能在用户犯马虎的时候，给他弹出一个编译错误：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">read</span>(<span class="number">32</span>, buf, fd);  <span class="comment">// 编译报错：无法将 int 类型的 32 隐式转换为 FileHandle！</span></span><br></pre></td></tr></table></figure>

<p>对于整数类型，也有的人喜欢用 C++11 的<strong>强类型</strong>枚举：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum class</span> <span class="title class_">FileHandle</span> : <span class="type">int</span> &#123;&#125;;</span><br></pre></td></tr></table></figure>

<p>这样一来，如果用户真的是想要读取“32号句柄”的文件，他就必须显式地写出完整类型才能编译通过：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">read</span>(<span class="built_in">FileHandle</span>(<span class="number">32</span>), buf, fd);  <span class="comment">// 编译通过了</span></span><br></pre></td></tr></table></figure>

<p><strong>强迫你写上类型名，就给了你一次再思考的机会，让你突然惊醒： 哦天哪，我怎么把缓冲区大小当成句柄来传递了！ 从而减少睁着眼睛还犯错的可能。</strong></p>
<p>然后，你的 open 函数也返回 FileHandle，整个代码中就不用强制类型转换了。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">FileHandle fd = <span class="built_in">open</span>(std::filesystem::<span class="built_in">path</span>(<span class="string">&quot;路径&quot;</span>), OpenFlag::Read);</span><br><span class="line"><span class="type">char</span> buf[<span class="number">32</span>];</span><br><span class="line"><span class="built_in">read</span>(fd, buf, <span class="number">32</span>);</span><br></pre></td></tr></table></figure>

<h2 id="点名批评的-STL-设计"><a href="#点名批评的-STL-设计" class="headerlink" title="点名批评的 STL 设计"></a>点名批评的 STL 设计</h2><p>例如 std::stack 的设计就非常失败：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (!stack.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    <span class="keyword">auto</span> val = std::<span class="built_in">move</span>(stack.<span class="built_in">top</span>());</span><br><span class="line">    stack.<span class="built_in">pop</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>我们必须判断 stack 不为空，才能弹出栈顶元素</strong>。对着一个空的栈 pop 是未定义行为。 而 pop() 又是一个返回 void 的函数，他只是删除栈顶元素，并不会返回元素。 <strong>我们必须先调用 top() 把栈顶取出来，然后才能 pop！</strong></p>
<p>明明是同一个操作，却要拆成三个函数来完成，很烂。如果你不慎把判断条件写反：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (stack.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    <span class="keyword">auto</span> val = std::<span class="built_in">move</span>(stack.<span class="built_in">top</span>());</span><br><span class="line">    stack.<span class="built_in">pop</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>就一个 Segmentation Fault 蹦你脸上，你找半天都找不到自己哪错了！</p>
<p>小彭老师重新设计，整合成一个函数：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">std::optional&lt;<span class="type">int</span>&gt; <span class="title">pop</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>

<p>语义明确，用起来也方便，用户不容易犯错。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="keyword">auto</span> val = stack.<span class="built_in">pop</span>()) &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>把多个本就属于同一件事的函数，整合成一个，避免用户中间出纰漏。 从参数和返回值的类型上，限定自由度，减轻用户思考负担。</strong></p>
<p>众所周知，vector 有两个函数用于访问指定位置的元素。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> &amp;<span class="keyword">operator</span>[](<span class="type">size_t</span> index);</span><br><span class="line"><span class="function"><span class="type">int</span> &amp;<span class="title">at</span><span class="params">(<span class="type">size_t</span> index)</span></span>;</span><br><span class="line"></span><br><span class="line">vec[<span class="number">3</span>];  <span class="comment">// 如果 vec 的大小不足 3，会发生数组越界！这是未定义行为</span></span><br><span class="line">vec.<span class="built_in">at</span>(<span class="number">3</span>);  <span class="comment">// 如果 vec 的大小不足 3，会抛出 out_of_range 异常</span></span><br></pre></td></tr></table></figure>

<p>用户通常会根据自己的需要，<strong>如果他们非常自信自己的索引不会越界，可以用高效的 []，不做检测。 如果不确定，可以用更安全的 at()，一旦越界自动抛出异常，方便调试。</strong></p>
<p>我们可以重新设计一个 .get() 函数：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">std::optional&lt;<span class="type">int</span>&gt; <span class="title">get</span><span class="params">(<span class="type">size_t</span> index)</span></span>;</span><br></pre></td></tr></table></figure>

<p>当检测到数组越界时，返回 nullopt。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">*vec.<span class="built_in">get</span>(<span class="number">3</span>);             <span class="comment">// 如果用户追求性能，可以把数组越界转化为未定义行为，从而让编译器自动优化掉越界的路径</span></span><br><span class="line">vec.<span class="built_in">get</span>(<span class="number">3</span>).<span class="built_in">value</span>();      <span class="comment">// 如果用户追求安全，可以把数组越界转化为一个异常</span></span><br><span class="line">vec.<span class="built_in">get</span>(<span class="number">3</span>).<span class="built_in">value_or</span>(<span class="number">0</span>);  <span class="comment">// 如果用户想要在越界时获得默认值 0</span></span><br></pre></td></tr></table></figure>

<p><strong>这样就只需要一个函数，不论用户想要的是什么，都只需要这一个统一的 get() 函数。</strong></p>
<p>小彭老师，你这个只能 get，要如何 set 呀？</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">std::optional&lt;<span class="type">int</span>&gt; <span class="title">get</span><span class="params">(<span class="type">size_t</span> index)</span></span>;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">set</span><span class="params">(<span class="type">size_t</span> index, <span class="type">int</span> value)</span></span>;  <span class="comment">// 如果越界，返回 false</span></span><br></pre></td></tr></table></figure>

<ul>
<li>缺点1：返回 bool 无法运用 optional 的小技巧：通过 value() 转化为异常，且用户容易忘记检查返回值。</li>
<li>缺点2：两个参数，一个是 size_t 一个是 int，还是很容易顺序搞混。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">std::optional&lt;std::reference_wrapper&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">get</span>(<span class="type">size_t</span> index);</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> x = **vec.<span class="built_in">get</span>(<span class="number">3</span>);         <span class="comment">// 性能读</span></span><br><span class="line"><span class="keyword">auto</span> x = *vec.<span class="built_in">get</span>(<span class="number">3</span>).<span class="built_in">value</span>();  <span class="comment">// 安全读</span></span><br><span class="line">*vec.<span class="built_in">get</span>(<span class="number">3</span>) = <span class="number">42</span>;              <span class="comment">// 性能写</span></span><br><span class="line">vec.<span class="built_in">get</span>(<span class="number">3</span>).<span class="built_in">value</span>() = <span class="number">42</span>;       <span class="comment">// 安全写</span></span><br></pre></td></tr></table></figure>

<h2 id="点名表扬的-STL-部分"><a href="#点名表扬的-STL-部分" class="headerlink" title="点名表扬的 STL 部分"></a>点名表扬的 STL 部分</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Sleep</span><span class="params">(<span class="type">int</span> delay)</span></span>;</span><br></pre></td></tr></table></figure>

<p>谁知道这个 delay 的单位是什么？秒？毫秒？</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Sleep</span><span class="params">(<span class="type">int</span> ms)</span></span>;</span><br></pre></td></tr></table></figure>

<p>好吧，是毫秒。可是除非看一眼函数定义或文档，谁想得到这是个毫秒？</p>
<p>一个用户想要睡 3 秒，他写道：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Sleep</span>(<span class="number">3</span>);</span><br></pre></td></tr></table></figure>

<p>编译器没有任何报错，一运行只睡了 3 毫秒。 用户大发雷霆以为你的 Sleep 函数有 BUG，我让他睡 3 秒怎么好像根本没睡啊。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">SleepMilliSeconds</span><span class="params">(<span class="type">int</span> ms)</span></span>;</span><br></pre></td></tr></table></figure>

<p><strong>改个函数名可以解决一部分问题</strong>，当用户调用时，他需要手动打出 <code>MilliSeconds</code>，从而强迫他清醒一下，自己给的 3 到底是不是自己想要的。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">MilliSeconds</span> &#123;</span><br><span class="line">    <span class="type">int</span> count;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">MilliSeconds</span><span class="params">(<span class="type">int</span> count)</span> : count(count) &#123;</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Sleep</span><span class="params">(MilliSeconds delay)</span></span>;</span><br></pre></td></tr></table></figure>

<p>现在，如果用户写出</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Sleep</span>(<span class="number">3</span>);</span><br></pre></td></tr></table></figure>

<p>编译器会报错。 <strong>他必须明确写出</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Sleep</span>(<span class="built_in">MilliSeconds</span>(<span class="number">3</span>));</span><br></pre></td></tr></table></figure>

<p>才能通过编译。</p>
<p>标准库的 chrono 模块就大量运用了这种<strong>强类型封装</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">this_thread::<span class="built_in">sleep_for</span>(chrono::<span class="built_in">seconds</span>(<span class="number">3</span>));</span><br></pre></td></tr></table></figure>

<p>如果你 <code>using namespace std::literials;</code> 还可以这样快捷地创建字面量：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">this_thread::<span class="built_in">sleep_for</span>(<span class="number">3</span>ms);  <span class="comment">// 3 毫秒</span></span><br><span class="line">this_thread::<span class="built_in">sleep_for</span>(<span class="number">3</span>s);  <span class="comment">// 3 秒</span></span><br><span class="line">this_thread::<span class="built_in">sleep_for</span>(<span class="number">3</span>m);  <span class="comment">// 3 分钟</span></span><br><span class="line">this_thread::<span class="built_in">sleep_for</span>(<span class="number">3</span>h);  <span class="comment">// 3 小时</span></span><br></pre></td></tr></table></figure>

<p>且支持运算符重载，不同单位之间还可以互相转换：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">this_thread::<span class="built_in">sleep_for</span>(<span class="number">1</span>s + <span class="number">200</span>ms);</span><br><span class="line">chrono::minutes three_minutes = <span class="number">180</span>s;</span><br></pre></td></tr></table></figure>

<p>chrono 是一个优秀的类型封装案例，把 time_t 类型封装成了强类型的 duration 和 time_point。</p>
<p>时间点（time_point）表示某个具体的时间，例如 2024 年 5 月 16 日 18:06:28。 时间段（duration）表示一段时间的长度，例如 1 天，2 小时，3 分钟，4 秒。</p>
<p>时间段很容易表示，只需要指定一个单位，比如秒，然后用一个数字就可以表示多少秒的时间段。 </p>
<p>Unix 时间戳用一个数字来表示时间点，数字的含义是从当前时间到 1970 年 1 月 1 日 00:00:00 的秒数。 例如写作这篇文章的时间戳是 1715853968 (2024&#x2F;5&#x2F;16 18:06)。 C 语言用一个 <code>time_t</code>，实际上是 <code>long</code> 的类型别名来表示时间戳，但它有一个严重的问题： 它可以被当成时间点，也可以被当成时间段，这就造成了巨大的混乱。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">time_t</span> t0 = <span class="built_in">time</span>(<span class="literal">NULL</span>);  <span class="comment">// 时间点</span></span><br><span class="line">...</span><br><span class="line"><span class="type">time_t</span> t1 = <span class="built_in">time</span>(<span class="literal">NULL</span>);  <span class="comment">// 时间点</span></span><br><span class="line"><span class="type">time_t</span> dt = t1 - t0;     <span class="comment">// 时间段</span></span><br></pre></td></tr></table></figure>

<ul>
<li>痛点：如果这里的负号写错，写成 <code>t1 + t0</code>，编译器不会报错，你可能根本没发现，浪费大量时间调试最后只发现一个低级错误。</li>
<li>模糊：<strong>时间点（t0、t1）和时间段（dt）都是 time_t，初次阅读代码很容易分不清哪个是时间点，哪个是时间段。</strong></li>
</ul>
<p>如果不慎把“时间点”的 time_t 传入到本应只支持“时间段”的 sleep 函数，会出现“睡美人”的奇观：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">time_t</span> t = <span class="built_in">time</span>(<span class="literal">NULL</span>);  <span class="comment">// 返回 1715853968 表示当前时间点</span></span><br><span class="line"><span class="built_in">sleep</span>(t);               <span class="comment">// 不小心把时间点当成时间段来用了！</span></span><br></pre></td></tr></table></figure>

<p>这个程序会睡 1715853968 秒后才醒，即 54 年后！</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">chrono::system_clock::time_point last = chrono::system_clock::<span class="built_in">now</span>();</span><br><span class="line">...</span><br><span class="line">chrono::system_clock::time_point now = chrono::system_clock::<span class="built_in">now</span>();</span><br><span class="line">chrono::system_clock::duration dt = now - last;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;用了 &quot;</span> &lt;&lt; <span class="built_in">duration_cast</span>&lt;chrono::seconds&gt;(dt).<span class="built_in">count</span>() &lt;&lt; <span class="string">&quot; 秒\n&quot;</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>一看就知道哪个是时间点，哪个是时间段</li>
<li>用错了编译器会报错</li>
<li>单位转换不会混淆</li>
<li>时间点 + 时间点 &#x3D; 编译出错！因为时间点之间不允许相加，2024 + 2024，你是想加到 4048 年去吗？</li>
<li>时间点 - 时间点 &#x3D; 时间段</li>
<li>时间点 + 时间段 &#x3D; 时间点</li>
<li>时间点 - 时间段 &#x3D; 时间点</li>
<li>时间段 + 时间段 &#x3D; 时间段</li>
<li>时间段 - 时间段 &#x3D; 时间段</li>
<li>时间段 × 常数 &#x3D; 时间段</li>
<li>时间段 &#x2F; 常数 &#x3D; 时间段</li>
</ul>
<p>这就是本期课程的主题，<strong>通过强大的类型系统，对可能的用法加以严格的限制，最大限度阻止用户不经意间写出错误的代码</strong>。</p>
<h2 id="枚举类型"><a href="#枚举类型" class="headerlink" title="枚举类型"></a>枚举类型</h2><p>你的老板要求一个设定客户性别的函数：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">int</span> sex)</span></span>;</span><br></pre></td></tr></table></figure>

<p>老板口头和员工约定说，0表示女，1表示男，2表示自定义。</p>
<p>这谁记得住？设想你是一个新来的员工，看到下面的代码：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">foo</span>(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>你能猜到这个 1 是什么意思吗？</p>
<p>解决方法是使用枚举类型，<strong>给每个数值一个唯一的名字</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="title class_">Sex</span> &#123;</span><br><span class="line">    Female = <span class="number">0</span>,</span><br><span class="line">    Male = <span class="number">1</span>,</span><br><span class="line">    Custom = <span class="number">2</span>,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(Sex sex)</span></span>;</span><br></pre></td></tr></table></figure>

<p>再假设你是一个新来的员工，看到：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">foo</span>(Male);</span><br></pre></td></tr></table></figure>

<p>是不是就一目了然啦？</p>
<p>枚举的值也可以不用写，让编译器自动按 0、1、2 的顺序分配值：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="title class_">Sex</span> &#123;</span><br><span class="line">    Female,   <span class="comment">// 0</span></span><br><span class="line">    Male,     <span class="comment">// 1</span></span><br><span class="line">    Custom,   <span class="comment">// 2</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>可以指定从 1 开始计数：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="title class_">Sex</span> &#123;</span><br><span class="line">    Female = <span class="number">1</span>,</span><br><span class="line">    Male,      <span class="comment">// 2</span></span><br><span class="line">    Custom,    <span class="comment">// 3</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>但枚举类型还是可以骗人，再假设你是新来的，看到：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">foo</span>(Male, <span class="number">24</span>);</span><br></pre></td></tr></table></figure>

<p>是不是想当然的感觉这个代码没问题？</p>
<p>但当你看到 foo 准确的函数定义时，傻眼了：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">int</span> age, Sex sex)</span></span>;</span><br></pre></td></tr></table></figure>

<p>相当于注册了一个 1 岁，性别是 24 的伪人。<strong>且程序员很容易看不出问题，编译器也不报错。</strong></p>
<p>为此，C++11 引入了<strong>强类型枚举</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum class</span> <span class="title class_">Sex</span> &#123;</span><br><span class="line">    Female = <span class="number">0</span>,</span><br><span class="line">    Male = <span class="number">1</span>,</span><br><span class="line">    Custom = <span class="number">2</span>,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>现在，如果你再不小心把 sex 传入 age 的话，编译器会报错！<strong>因为强类型枚举不允许与 int 隐式转换。</strong></p>
<p>而且强类型枚举会需要显式写出 <code>Sex::</code> 类型前缀，<strong>当你有很多枚举类型时不容易混淆：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">foo</span>(<span class="number">24</span>, Sex::Male);</span><br></pre></td></tr></table></figure>

<p>如果你的 Sex 范围很小，只需要 uint8_t 的内存就够，可以用这个语法指定枚举的“后台类型”：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum class</span> <span class="title class_">Sex</span> : <span class="type">uint8_t</span> &#123;</span><br><span class="line">    Female = <span class="number">0</span>,</span><br><span class="line">    Male = <span class="number">1</span>,</span><br><span class="line">    Custom = <span class="number">2</span>,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">static_assert</span>(<span class="built_in">sizeof</span>(Sex) == <span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>假如你的所有 age 都是 int 类型的，但是现在，老板突然心血来潮：</p>
<p>说为了“优化存储空间”，想要把所有 age 改成 uint8_t 类型的！</p>
<p><strong>为了预防未来可能需要改变类型的需求，也是为了可读性，我们可以使用类型别名：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> Age = <span class="type">int</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(Age age, Sex sex)</span></span>;</span><br></pre></td></tr></table></figure>

<p>这样当老板需要改变底层类型时，只需要改动一行：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> Age = <span class="type">uint8_t</span>;</span><br></pre></td></tr></table></figure>

<p>就能自动让所有代码都使用 uint8_t 作为 age 了。</p>
<p><strong>但是类型别名毕竟只是别名，并没有强制保障：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> Age = <span class="type">int</span>;</span><br><span class="line"><span class="keyword">using</span> Phone = <span class="type">int</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">foo</span>(Age age, Phone phone);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Age age = <span class="number">42</span>;</span><br><span class="line">    Phone phone = <span class="number">12345</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">foo</span>(phone, age); <span class="comment">// 不小心写反了！而编译器不会提醒你！</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>因为 Age 和 Phone 只是类型别名，实际上还是同样的 int 类型…所以编译器甚至不会有任何警告。</p>
<p><strong>有一种很极端的做法是把 Age 和 Phone 也做成枚举，但没有定义任何值：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum class</span> <span class="title class_">Age</span> : <span class="type">int</span> &#123;&#125;;</span><br><span class="line"><span class="keyword">enum class</span> <span class="title class_">Phone</span> : <span class="type">int</span> &#123;&#125;;</span><br></pre></td></tr></table></figure>

<p>这样用到的时候就<strong>只能通过强制转换的语法</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">foo</span>(<span class="built_in">Age</span>(<span class="number">42</span>), <span class="built_in">Phone</span>(<span class="number">12345</span>));</span><br></pre></td></tr></table></figure>

<p>并且如果写错顺序，<strong>尝试把 Phone 传入 Age 类型的参数，编译器会立即报错，阻止你埋下 BUG 隐患。</strong></p>
<p>小彭老师，我用了你的方法以后，不能做加法了怎么办？</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Age</span>(<span class="number">42</span>) + <span class="built_in">Age</span>(<span class="number">1</span>) <span class="comment">// 编译器错误！</span></span><br></pre></td></tr></table></figure>

<p>这是因为 Age 是强类型枚举，不能隐式转换为 int 后做加法。</p>
<p><strong>可以定义一个运算符重载：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum class</span> <span class="title class_">Age</span> : <span class="type">int</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> Age <span class="keyword">operator</span>+(Age a, Age b) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">Age</span>((<span class="type">int</span>)a + (<span class="type">int</span>)b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>或者运用模板元编程，直接让加法运算符对于所有枚举类型都默认生效：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt; <span class="keyword">requires</span> std::is_enum_v&lt;T&gt;</span><br><span class="line">T <span class="keyword">operator</span>+(T a, T b) &#123;</span><br><span class="line">    <span class="keyword">using</span> U = std::<span class="type">underlying_type_t</span>&lt;T&gt;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">T</span>((U)a + (U)b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>有时这反而是个优点，比如你可以只定义加法运算符，就可以让 Age 不支持乘法，需要手动转换后才能乘，避免无意中犯错的可能。</p>
<p>小彭老师，我用了你推荐的<strong>强类型枚举</strong>，不支持我最爱的或运算 <code>|</code> 了怎么办？</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum class</span> <span class="title class_">OpenFlag</span> &#123;</span><br><span class="line">    Create = <span class="number">1</span>,</span><br><span class="line">    Read = <span class="number">2</span>,</span><br><span class="line">    Write = <span class="number">4</span>,</span><br><span class="line">    Truncate = <span class="number">8</span>,</span><br><span class="line">    Append = <span class="number">16</span>,</span><br><span class="line">    Binary = <span class="number">32</span>,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> OpenFlag <span class="keyword">operator</span>|(OpenFlag a, OpenFlag b) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">OpenFlag</span>((<span class="type">int</span>)a | (<span class="type">int</span>)b);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> OpenFlag <span class="keyword">operator</span>&amp;(OpenFlag a, OpenFlag b) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">OpenFlag</span>((<span class="type">int</span>)a &amp; (<span class="type">int</span>)b);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> OpenFlag <span class="keyword">operator</span>~(OpenFlag a) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">OpenFlag</span>(~(<span class="type">int</span>)a);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>节选自：<a href="https://parallel101.github.io/cppguidebook/type_rich_api">现代化的 API 设计指南 - ✝️小彭大典✝️ (parallel101.github.io)</a></p>
]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>工作区和GOPATH</title>
    <url>/2024/09/19/Golang01/</url>
    <content><![CDATA[<p>我们学习 Go 语言时，要做的第一件事，都是根据自己电脑的计算架构（比如，是 32 位的计算机还是 64 位的计算机）以及操作系统（比如，是 Windows 还是 Linux），从<a href="https://golang.google.cn/">Go 语言官网</a>下载对应的二进制包，也就是可以拿来即用的安装包。</p>
<p>随后，我们会解压缩安装包、放置到某个目录、配置环境变量，并通过在命令行中输入<code>go version</code>来验证是否安装成功。</p>
<p>在这个过程中，我们还需要配置 3 个环境变量，也就是 GOROOT、GOPATH 和 GOBIN。这里我可以简单介绍一下。</p>
<ul>
<li>GOROOT：Go 语言安装根目录的路径，也就是 GO 语言的安装路径。</li>
<li>GOPATH：若干工作区目录的路径。是我们自己定义的工作空间。</li>
<li>GOBIN：GO 程序生成的可执行文件（executable file）的路径。</li>
</ul>
<p>我们电脑上的配置如下图所示：</p>
<p><img src="/2024/09/19/Golang01/image-20240919175500145.png" alt="image-20240919175500145"></p>
<p>其中，GOPATH 背后的概念是最多的，也是最重要的。那么，<strong>今天我们的面试问题是：你知道设置 GOPATH 有什么意义吗？</strong></p>
<p>关于这个问题，它的<strong>典型回答</strong>是这样的：</p>
<p>你可以把 GOPATH 简单理解成 Go 语言的工作目录，它的值是一个目录的路径，也可以是多个目录路径，每个目录都代表 Go 语言的一个工作区（workspace）。</p>
<p>我们需要利于这些工作区，<strong>去放置 Go 语言的源码文件（source file），以及安装（install）后的归档文件（archive file，也就是以“.a”为扩展名的文件）和可执行文件（executable file）</strong>。</p>
<p>事实上，由于 <strong>Go 语言项目在其生命周期内的所有操作（编码、依赖管理、构建、测试、安装等）基本上都是围绕着 GOPATH 和工作区进行的</strong>。所以，它的背后至少有 3 个知识点，分别是：</p>
<ul>
<li><strong>1. Go 语言源码的组织方式是怎样的；</strong></li>
<li><strong>2. 你是否了解源码安装后的结果（只有在安装后，Go 语言源码才能被我们或其他代码使用）；</strong></li>
<li><strong>3. 你是否理解构建和安装 Go 程序的过程（这在开发程序以及查找程序问题的时候都很有用，否则你很可能会走弯路）。</strong></li>
</ul>
<p>下面我就重点来聊一聊这些内容。</p>
<h3 id="知识扩展"><a href="#知识扩展" class="headerlink" title="知识扩展"></a>知识扩展</h3><h3 id="1-Go-语言源码的组织方式"><a href="#1-Go-语言源码的组织方式" class="headerlink" title="1. Go 语言源码的组织方式"></a>1. Go 语言源码的组织方式</h3><p>与许多编程语言一样，<strong>Go 语言的源码也是以代码包为基本组织单位的。在文件系统中，这些代码包其实是与目录一一对应的。由于目录可以有子目录，所以代码包也可以有子包</strong>。</p>
<p><strong>一个代码包中可以包含任意个以.go 为扩展名的源码文件，这些源码文件都需要被声明属于同一个代码包。</strong></p>
<p><strong>代码包的名称一般会与源码文件所在的目录同名。如果不同名，那么在构建、安装的过程中会以代码包名称为准。</strong></p>
<p>每个代码包都会有导入路径。<strong>代码包的导入路径是其他代码在使用该包中的程序实体时，需要引入的路径。在实际使用程序实体之前，我们必须先导入其所在的代码包</strong>。具体的方式就是<code>import</code>该代码包的导入路径。就像这样：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> <span class="string">&quot;github.com/labstack/echo&quot;</span></span><br></pre></td></tr></table></figure>

<p>在工作区中，<strong>一个代码包的导入路径实际上就是从 src 子目录，到该包的实际存储位置的相对路径</strong>。</p>
<p>所以说，Go 语言源码的组织方式就是以环境变量 GOPATH、工作区、src 目录和代码包为主线的。<strong>一般情况下，Go 语言的源码文件都需要被存放在环境变量 GOPATH 包含的某个工作区（目录）中的 src 目录下的某个代码包（目录）中</strong>。</p>
<h3 id="2-了解源码安装后的结果"><a href="#2-了解源码安装后的结果" class="headerlink" title="2. 了解源码安装后的结果"></a>2. 了解源码安装后的结果</h3><p>了解了 Go 语言源码的组织方式后，我们很有必要知道 Go 语言源码在安装后会产生怎样的结果。</p>
<p>源码文件以及安装后的结果文件都会放到哪里呢？<strong>我们都知道，源码文件通常会被放在某个工作区的 src 子目录下。</strong></p>
<p><strong>那么在安装后如果产生了归档文件（以“.a”为扩展名的文件），就会放进该工作区的 pkg 子目录；如果产生了可执行文件，就可能会放进该工作区的 bin 子目录。</strong></p>
<p>我再讲一下归档文件存放的具体位置和规则。</p>
<p>源码文件会以代码包的形式组织起来，一个代码包其实就对应一个目录。安装某个代码包而产生的归档文件是与这个代码包同名的。</p>
<p>放置它的相对目录就是该代码包的导入路径的直接父级。比如，一个已存在的代码包的导入路径是</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">github.com/labstack/echo，</span><br></pre></td></tr></table></figure>

<p>那么执行命令</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">go</span> install github.com/labstack/echo</span><br></pre></td></tr></table></figure>

<p>生成的归档文件的相对目录就是 <a href="http://github.com/labstack%EF%BC%8C">github.com&#x2F;labstack，</a> 文件名为 echo.a。</p>
<p>顺便说一下，<strong>上面这个代码包导入路径还有另外一层含义，那就是：该代码包的源码文件存在于 GitHub 网站的 labstack 组的代码仓库 echo 中</strong>。</p>
<p>再说回来，归档文件的相对目录与 pkg 目录之间还有一级目录，叫做平台相关目录。平台相关目录的名称是由 build（也称“构建”）的目标操作系统、下划线和目标计算架构的代号组成的。</p>
<p>比如，构建某个代码包时的目标操作系统是 Linux，目标计算架构是 64 位的，那么对应的平台相关目录就是 linux_amd64。</p>
<p>因此，上述代码包的归档文件就会被放置在当前工作区的子目录 pkg&#x2F;linux_amd64&#x2F;github.com&#x2F;labstack 中。</p>
<p>（GOPATH 与工作区）</p>
<p><img src="/2024/09/19/Golang01/image" alt="img"></p>
<p>总之，你需要记住的是，<strong>某个工作区的 src 子目录下的源码文件在安装后一般会被放置到当前工作区的 pkg 子目录下对应的目录中，或者被直接放置到该工作区的 bin 子目录中</strong>。</p>
<h3 id="3-理解构建和安装-Go-程序的过程"><a href="#3-理解构建和安装-Go-程序的过程" class="headerlink" title="3. 理解构建和安装 Go 程序的过程"></a>3. 理解构建和安装 Go 程序的过程</h3><p>我们再来说说构建和安装 Go 程序的过程都是怎样的，以及它们的异同点。</p>
<p><strong>构建</strong>使用命令<code>go build</code>，<strong>安装</strong>使用命令<code>go install</code>。<strong>构建和安装代码包的时候都会执行编译、打包等操作，并且，这些操作生成的任何文件都会先被保存到某个临时的目录中</strong>。</p>
<p>如果构建的是库源码文件，那么操作的结果文件只会存在于临时目录中。这里的构建的主要意义在于检查和验证。</p>
<p>如果构建的是命令源码文件，那么操作的结果文件会被搬运到源码文件所在的目录中。</p>
<p><strong>安装操作会先执行构建，然后还会进行链接操作，并且把结果文件搬运到指定目录</strong>。进一步说，如果安装的是库源码文件，那么结果文件会被搬运到它所在工作区的 pkg 目录下的某个子目录中。</p>
<p>如果安装的是命令源码文件，那么结果文件会被搬运到它所在工作区的 bin 目录中，或者环境变量<code>GOBIN</code>指向的目录中。</p>
<p>这里你需要记住的是，构建和安装的不同之处，以及执行相应命令后得到的结果文件都会出现在哪里。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>工作区和 GOPATH 的概念和含义是每个 Go 工程师都需要了解的。虽然它们都比较简单，但是说它们是 Go 程序开发的核心知识并不为过。</p>
<p>然而，我在招聘面试的过程中仍然发现有人忽略掉了它们。<strong>Go 语言提供的很多工具都是在 GOPATH 和工作区的基础上运行的</strong>，比如上面提到的<code>go build</code>、<code>go install</code>和<code>go get</code>，这三个命令也是我们最常用到的。</p>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><p>说到 Go 程序中的依赖管理，其实还有很多问题值得我们探索。我在这里留下两个问题供你进一步思考。</p>
<ol>
<li>Go 语言在多个工作区中查找依赖包的时候是以怎样的顺序进行的？</li>
<li>如果在多个工作区中都存在导入路径相同的代码包会产生冲突吗？</li>
</ol>
<p>这两个问题之间其实是有一些关联的。答案并不复杂，你做几个试验几乎就可以找到它了。你也可以看一下 Go 语言标准库中<code>go build</code>包及其子包的源码。那里面的宝藏也很多，可以助你深刻理解 Go 程序的构建过程。</p>
<hr>
<h3 id="补充阅读"><a href="#补充阅读" class="headerlink" title="补充阅读"></a>补充阅读</h3><h3 id="go-build-命令一些可选项的用途和用法"><a href="#go-build-命令一些可选项的用途和用法" class="headerlink" title="go build 命令一些可选项的用途和用法"></a>go build 命令一些可选项的用途和用法</h3><p>在运行<code>go build</code>命令的时候，默认不会编译目标代码包所依赖的那些代码包。当然，如果被依赖的代码包的归档文件不存在，或者源码文件有了变化，那它还是会被编译。</p>
<p>如果要强制编译它们，可以在执行命令的时候加入标记<code>-a</code>。此时，不但目标代码包总是会被编译，它依赖的代码包也总会被编译，即使依赖的是标准库中的代码包也是如此。</p>
<p>另外，如果不但要编译依赖的代码包，还要安装它们的归档文件，那么可以加入标记<code>-i</code>。</p>
<p>那么我们怎么确定哪些代码包被编译了呢？有两种方法。</p>
<ol>
<li>运行<code>go build</code>命令时加入标记<code>-x</code>，这样可以看到<code>go build</code>命令具体都执行了哪些操作。另外也可以加入标记<code>-n</code>，这样可以只查看具体操作而不执行它们。</li>
<li>运行<code>go build</code>命令时加入标记<code>-v</code>，这样可以看到<code>go build</code>命令编译的代码包的名称。它在与<code>-a</code>标记搭配使用时很有用。</li>
</ol>
<p>下面再说一说与 Go 源码的安装联系很紧密的一个命令：<code>go get</code>。</p>
<p>命令<code>go get</code>会自动从一些主流公用代码仓库（比如 GitHub）下载目标代码包，并把它们安装到环境变量<code>GOPATH</code>包含的第 1 工作区的相应目录中。如果存在环境变量<code>GOBIN</code>，那么仅包含命令源码文件的代码包会被安装到<code>GOBIN</code>指向的那个目录。</p>
<p>最常用的几个标记有下面几种。</p>
<ul>
<li><code>-u</code>：下载并安装代码包，不论工作区中是否已存在它们。</li>
<li><code>-d</code>：只下载代码包，不安装代码包。</li>
<li><code>-fix</code>：在下载代码包后先运行一个用于根据当前 Go 语言版本修正代码的工具，然后再安装代码包。</li>
<li><code>-t</code>：同时下载测试所需的代码包。</li>
<li><code>-insecure</code>：允许通过非安全的网络协议下载和安装代码包。HTTP 就是这样的协议。</li>
</ul>
<p>Go 语言官方提供的<code>go get</code>命令是比较基础的，其中并没有提供依赖管理的功能。目前 GitHub 上有很多提供这类功能的第三方工具，比如<code>glide</code>、<code>gb</code>以及官方出品的<code>dep</code>、<code>vgo</code>等等，它们在内部大都会直接使用<code>go get</code>。</p>
<p>有时候，我们可能会出于某种目的变更存储源码的代码仓库或者代码包的相对路径。这时，为了让代码包的远程导入路径不受此类变更的影响，我们会使用自定义的代码包导入路径。</p>
<p><strong>对代码包的远程导入路径进行自定义的方法是：在该代码包中的库源码文件的包声明语句的右边加入导入注释</strong>，像这样：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> semaphore <span class="comment">// import &quot;golang.org/x/sync/semaphore&quot;</span></span><br></pre></td></tr></table></figure>

<p>这个代码包原本的完整导入路径是<code>github.com/golang/sync/semaphore</code>。这与实际存储它的网络地址对应的。该代码包的源码实际存在 GitHub 网站的 golang 组的 sync 代码仓库的 semaphore 目录下。而加入导入注释之后，用以下命令即可下载并安装该代码包了：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">go</span> get golang.org/x/sync/semaphore</span><br></pre></td></tr></table></figure>

<p>而 Go 语言官网 golang.org 下的路径 &#x2F;x&#x2F;sync&#x2F;semaphore 并不是存放<code>semaphore</code>包的真实地址。我们称之为代码包的自定义导入路径。</p>
<p>不过，这还需要在 golang.org 这个域名背后的服务端程序上，添加一些支持才能使这条命令成功。</p>
<p>关于自定义代码包导入路径的完整说明可以参看<a href="https://github.com/hyper0x/go_command_tutorial/blob/master/0.3.md">这里</a>。</p>
<p>好了，对于<code>go build</code>命令和<code>go get</code>命令的简短介绍就到这里。如果你想查阅更详细的文档，那么可以访问 Go 语言官方的<a href="https://golang.google.cn/cmd/go">命令文档页面</a>，或者在命令行下输入诸如<code>go help build</code>这类的命令。</p>
<p>参考文章：<a href="https://jums.gitbook.io/36-lectures-on-golang/01-gong-zuo-qu-he-gopath">01 | 工作区和GOPATH | Go语言核心36讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>库源码文件</title>
    <url>/2024/09/19/Golang03/</url>
    <content><![CDATA[<p>你已经使用过 Go 语言编写了小命令（或者说微型程序）吗？</p>
<p>当你在编写“Hello, world”的时候，一个源码文件就足够了，虽然这种小玩意儿没什么用，最多能给你一点点莫名的成就感。如果你对这一点点并不满足，别着急，跟着学，我肯定你也可以写出很厉害的程序。</p>
<hr>
<p>我们在上一篇的文章中学到了<strong>命令源码文件</strong>的相关知识，那么除了命令源码文件，<strong>你还能用 Go 语言编写库源码文件。那么什么是库源码文件呢？</strong></p>
<p>在我的定义中，<strong>库源码文件是不能被直接运行的源码文件，它仅用于存放程序实体，这些程序实体可以被其他代码使用（只要遵从 Go 语言规范的话）。</strong></p>
<p>这里的“其他代码”<strong>可以与被使用的程序实体在同一个源码文件内，也可以在其他源码文件，甚至其他代码包中</strong>。</p>
<blockquote>
<p>那么程序实体是什么呢？<strong>在 Go 语言中，程序实体是变量、常量、函数、结构体和接口的统称。</strong></p>
<p>我们总是会先声明（或者说定义）程序实体，然后再去使用。比如在上一篇的例子中，我们先定义了变量<code>name</code>，然后在<code>main</code>函数中调用<code>fmt.Printf</code>函数的时候用到了它。</p>
<p>再多说一点，<strong>程序实体的名字被统称为标识符。标识符可以是任何 Unicode 编码可以表示的字母字符、数字以及下划线“_”，但是其首字母不能是数字。</strong></p>
<p>从规则上说，我们可以用中文作为变量的名字。<strong>但是，我觉得这种命名方式非常不好，自己也会在开发团队中明令禁止这种做法。</strong>作为一名合格的程序员，我们应该向着编写国际水准的程序无限逼近。</p>
</blockquote>
<p>回到正题。</p>
<p>我们今天的<strong>问题是：怎样把命令源码文件中的代码拆分到其他库源码文件？</strong></p>
<p>我们用代码演示，把这个问题说得更具体一些。</p>
<p>如果在某个目录下有一个命令源码文件 demo4.go，如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;flag&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> name <span class="type">string</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</span><br><span class="line">flag.StringVar(&amp;name, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;everyone&quot;</span>, <span class="string">&quot;The greeting object.&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">flag.Parse()</span><br><span class="line">hello(name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中的代码你应该比较眼熟了。我在讲命令源码文件的时候贴过很相似的代码，那个源码文件名为 demo2.go。</p>
<p>这两个文件的不同之处在于，demo2.go 直接通过调用<code>fmt.Printf</code>函数打印问候语，而当前的 demo4.go 在同样位置调用了一个叫作<code>hello</code>的函数。</p>
<p>函数<code>hello</code>被声明在了另外一个源码文件中，我把它命名为 demo4_lib.go，并且放在与 demo4.go 相同的目录下。如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 需在此处添加代码。[1]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">hello</span><span class="params">(name <span class="type">string</span>)</span></span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">&quot;Hello, %s!\n&quot;</span>, name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>那么问题来了：注释 1 处应该填入什么代码？</p>
<h3 id="典型回答"><a href="#典型回答" class="headerlink" title="典型回答"></a><strong>典型回答</strong></h3><p>答案很简单，填入代码包声明语句<code>package main</code>。为什么？我之前说过，<strong>在同一个目录下的源码文件都需要被声明为属于同一个代码包</strong>。</p>
<p>如果该目录下有一个命令源码文件，<strong>那么为了让同在一个目录下的文件都通过编译，其他源码文件应该也声明属于<code>main</code>包。</strong></p>
<p>如此一来，我们就可以运行它们了。<strong>比如，我们可以在这些文件所在的目录下运行如下命令并得到相应的结果。</strong></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">go</span> run demo4.<span class="keyword">go</span> demo4_lib.<span class="keyword">go</span> </span><br><span class="line">Hello, everyone!</span><br></pre></td></tr></table></figure>

<p>或者，像下面这样先构建当前的代码包再运行。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">go</span> build puzzlers/article3/q1</span><br><span class="line">$ ./q1            </span><br><span class="line">Hello, everyone!</span><br></pre></td></tr></table></figure>

<p>在这里，我把 demo4.go 和 demo4_lib.go 都放在了一个相对路径为<code>puzzlers/article3/q1</code>的目录中。</p>
<p>在默认情况下，<strong>相应的代码包的导入路径会与此一致。我们可以通过代码包的导入路径引用其中声明的程序实体。但是，这里的情况是不同的</strong>。</p>
<p>注意，demo4.go 和 demo4_lib.go <strong>都声明自己属于<code>main</code>包。我在前面讲 Go 语言源码的组织方式的时候提到过这种用法，即：源码文件声明的包名可以与其所在目录的名称不同，只要这些文件声明的包名一致就可以。</strong></p>
<p>顺便说一下，我为本专栏创建了一个名为“Golang_Puzzlers”的项目。该项目的 src 子目录下会存有我们涉及的所有代码和相关文件。</p>
<p>也就是说，正确的用法是，你需要把该项目的打包文件下载到本地的任意目录下，然后经解压缩后把“Golang_Puzzlers”目录加入到环境变量<code>GOPATH</code>中。还记得吗？这会使“Golang_Puzzlers”目录成为工作区之一。</p>
<h3 id="问题解析"><a href="#问题解析" class="headerlink" title="问题解析"></a><strong>问题解析</strong></h3><p>这个问题考察的是代码包声明的基本规则。这里再总结一下。</p>
<p>第一条规则，<strong>同目录下的源码文件的代码包声明语句要一致。也就是说，它们要同属于一个代码包。这对于所有源码文件都是适用的</strong>。</p>
<p>如果目录中有命令源码文件，<strong>那么其他种类的源码文件也应该声明属于<code>main</code>包。这也是我们能够成功构建和运行它们的前提</strong>。</p>
<p>第二条规则，<strong>源码文件声明的代码包的名称可以与其所在的目录的名称不同。在针对代码包进行构建时，生成的结果文件的主名称与其父目录的名称一致</strong>。</p>
<p>对于命令源码文件而言，<strong>构建生成的可执行文件的主名称会与其父目录的名称相同</strong>，这在我前面的回答中也验证过了。</p>
<p>好了，经过我的反复强调，相信你已经记住这些规则了。下面的内容也将会与它们相关。</p>
<p>在编写真正的程序时，我们仅仅把代码拆分到几个源码文件中是不够的。我们往往会用模块化编程的方式，<strong>根据代码的功能和用途把它们放置到不同的代码包中</strong>。不过，这又会牵扯进一些 Go 语言的代码组织规则。我们一起来往下看。</p>
<h3 id="知识精讲"><a href="#知识精讲" class="headerlink" title="知识精讲"></a><strong>知识精讲</strong></h3><h4 id="1-怎样把命令源码文件中的代码拆分到其他代码包？"><a href="#1-怎样把命令源码文件中的代码拆分到其他代码包？" class="headerlink" title="1. 怎样把命令源码文件中的代码拆分到其他代码包？"></a>1. 怎样把命令源码文件中的代码拆分到其他代码包？</h4><p>我们先不用关注拆分代码的技巧。我在这里仍然依从前面的拆分方法。我把 demo4.go 另存为 demo5.go，并放到一个相对路径为<code>puzzlers/article3/q2</code>的目录中。</p>
<p>然后我再创建一个相对路径为<code>puzzlers/article3/q2/lib</code>的目录，再把 demo4_lib.go 复制一份并改名为 demo5_lib.go 放到该目录中。</p>
<p>现在，为了让它们通过编译，我们应该怎样修改代码？你可以先思考一下。我在这里给出一部分答案，我们一起来看看已经过修改的 demo5_lib.go 文件。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> lib5</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Hello</span><span class="params">(name <span class="type">string</span>)</span></span> &#123;</span><br><span class="line">fmt.Printf(<span class="string">&quot;Hello, %s!\n&quot;</span>, name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，我在这里修改了两个地方。</p>
<p>第一个改动是，我把代码包声明语句由<code>package main</code>改为了<code>package lib5</code>。注意，<strong>我故意让声明的包名与其所在的目录的名称不同</strong>。第二个改动是，<strong>我把全小写的函数名<code>hello</code>改为首字母大写的<code>Hello</code>。</strong></p>
<p>基于以上改动，我们再来看下面的几个问题。</p>
<h4 id="2-代码包的导入路径总会与其所在目录的相对路径一致吗？"><a href="#2-代码包的导入路径总会与其所在目录的相对路径一致吗？" class="headerlink" title="2. 代码包的导入路径总会与其所在目录的相对路径一致吗？"></a><strong>2. 代码包的导入路径总会与其所在目录的相对路径一致吗？</strong></h4><p>库源码文件 demo5_lib.go 所在目录的相对路径是<code>puzzlers/article3/q2/lib</code>，而它却声明自己属于<code>lib5</code>包。在这种情况下，该包的导入路径是<code>puzzlers/article3/q2/lib</code>，还是<code>puzzlers/article3/q2/lib5</code>？</p>
<p>这个问题往往会让 Go 语言的初学者们困惑，就算是用 Go 开发过程序的人也不一定清楚。我们一起来看看。</p>
<p>首先，<strong>我们在构建或者安装这个代码包的时候，提供给<code>go</code>命令的路径应该是目录的相对路径</strong>，就像这样：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">go</span> install puzzlers/article3/q2/lib </span><br></pre></td></tr></table></figure>

<p>该命令会成功完成。<strong>之后，当前工作区的 pkg 子目录下会产生相应的归档文件，具体的相对路径是:</strong></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">pkg/darwin_amd64/puzzlers/article3/q2/lib.a</span><br></pre></td></tr></table></figure>

<p>其中的<code>darwin_amd64</code>就是我在讲工作区时提到的平台相关目录。<strong>可以看到，这里与源码文件所在目录的相对路径是对应的。</strong></p>
<p>为了进一步说明问题，我需要先对 demo5.go 做两个改动。第一个改动是，在以<code>import</code>为前导的代码包导入语句中加入<code>puzzlers/article3/q2/lib</code>，也就是试图导入这个代码包。</p>
<p>第二个改动是，把对<code>hello</code>函数的调用改为对<code>lib.Hello</code>函数的调用。其中的<code>lib.</code>叫做限定符，旨在指明右边的程序实体所在的代码包。不过这里与代码包导入路径的完整写法不同，只包含了路径中的最后一级<code>lib</code>，这与代码包声明语句中的规则一致。</p>
<p>现在，我们可以通过运行<code>go run demo5.go</code>命令试一试。错误提示会类似于下面这种。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">./demo5.<span class="keyword">go</span>:<span class="number">5</span>:<span class="number">2</span>: imported and not used: <span class="string">&quot;puzzlers/article3/q2/lib&quot;</span> as lib5</span><br><span class="line">./demo5.<span class="keyword">go</span>:<span class="number">16</span>:<span class="number">2</span>: undefined: lib</span><br></pre></td></tr></table></figure>

<p>第一个错误提示的意思是，<strong>我们导入了<code>puzzlers/article3/q2/lib</code>包，但没有实际使用其中的任何程序实体。这在 Go 语言中是不被允许的，在编译时就会导致失败。</strong></p>
<p>注意，这里还有另外一个线索，那就是“as lib5”。<strong>这说明虽然导入了代码包<code>puzzlers/article3/q2/lib</code>，但是使用其中的程序实体的时候应该以<code>lib5.</code>为限定符。这也就是第二个错误提示的原因了。Go 命令找不到<code>lib.</code>这个限定符对应的代码包。</strong></p>
<p>为什么会是这样？根本原因就是，<strong>我们在源码文件中声明所属的代码包与其所在目录的名称不同</strong>。请记住，<strong>源码文件所在的目录相对于 src 目录的相对路径就是它的代码包导入路径</strong>，而实际使用其程序实体时给定的限定符要与它声明所属的代码包名称对应。</p>
<p>有两个方式可以使上述构建成功完成。<strong>我在这里选择把 demo5_lib.go 文件中的代码包声明语句改为<code>package lib</code>。理由是，为了不让该代码包的使用者产生困惑，我们总是应该让声明的包名与其父目录的名称一致</strong>。</p>
<h4 id="3-什么样的程序实体才可以被当前包外的代码引用？"><a href="#3-什么样的程序实体才可以被当前包外的代码引用？" class="headerlink" title="3. 什么样的程序实体才可以被当前包外的代码引用？"></a><strong>3. 什么样的程序实体才可以被当前包外的代码引用？</strong></h4><p>你可能会有疑问，我为什么要把 demo5_lib.go 文件中的那个函数名称<code>hello</code>的首字母大写？实际上这涉及了 Go 语言中对于程序实体访问权限的规则。</p>
<p>超级简单，<strong>名称的首字母为大写的程序实体才可以被当前包外的代码引用，否则它就只能被当前包内的其他代码引用。</strong></p>
<p><strong>通过名称，Go 语言自然地把程序实体的访问权限划分为了包级私有的和公开的。对于包级私有的程序实体，即使你导入了它所在的代码包也无法引用到它</strong>。</p>
<h4 id="4-对于程序实体，还有其他的访问权限规则吗？"><a href="#4-对于程序实体，还有其他的访问权限规则吗？" class="headerlink" title="4. 对于程序实体，还有其他的访问权限规则吗？"></a><strong>4. 对于程序实体，还有其他的访问权限规则吗？</strong></h4><p>答案是肯定的。<strong>在 Go 1.5 及后续版本中，我们可以通过创建<code>internal</code>代码包让一些程序实体仅仅能被当前模块中的其他代码引用</strong>。这被称为 Go 程序实体的第三种访问权限：模块级私有。</p>
<p>具体规则是，**<code>internal</code>代码包中声明的公开程序实体仅能被该代码包的直接父包及其子包中的代码引用<strong>。当然，引用前需要先导入这个<code>internal</code>包。</strong>对于其他代码包，导入该<code>internal</code>包都是非法的，无法通过编译**。</p>
<p>“Golang_Puzzlers”项目的<code>puzzlers/article3/q4</code>包中有一个简单的示例，可供你查看。你可以改动其中的代码并体会<code>internal</code>包的作用。</p>
<p>注意使用的方式是这样的：</p>
<p><img src="/2024/09/19/Golang03/image-20240919202612385.png" alt="image-20240919202612385"></p>
<p>不能在非直接父包中使用：</p>
<p><img src="/2024/09/19/Golang03/image-20240919202743098.png" alt="image-20240919202743098"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>我们在本篇文章中详细讨论了<strong>把代码从命令源码文件中拆分出来的方法，这包括拆分到其他库源码文件，以及拆分到其他代码包</strong>。</p>
<p>这里涉及了几条重要的 Go 语言基本编码规则，即：<strong>代码包声明规则、代码包导入规则以及程序实体的访问权限规则</strong>。在进行模块化编程时，你必须记住这些规则，否则你的代码很可能无法通过编译。（不过一般这些东西你使用IDE的话一般也没有什么问题）</p>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a><strong>思考题</strong></h3><p>这次的思考题都是关于代码包导入的，如下。</p>
<ol>
<li>如果你需要导入两个代码包，而这两个代码包的导入路径的最后一级是相同的，比如：<code>dep/lib/flag</code>和<code>flag</code>，那么会产生冲突吗？</li>
<li>如果会产生冲突，那么怎样解决这种冲突，有几种方式？</li>
</ol>
<p>第一个问题比较简单，你一试便知。强烈建议你编写个例子，然后运行<code>go</code>命令构建它，并看看会有什么样的提示。</p>
<p>而第二个问题涉及了代码包导入语句的高级写法，你可能需要去查阅一下 Go 语言规范。不过也不难。你最多能想出几种解决办法呢？</p>
<p>参考文章：<a href="https://jums.gitbook.io/36-lectures-on-golang/03-ku-yuan-ma-wen-jian">03 | 库源码文件 | Go语言核心36讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>命令源码文件</title>
    <url>/2024/09/19/Golang02/</url>
    <content><![CDATA[<p>我们已经知道，环境变量 GOPATH 指向的是一个或多个工作区，每个工作区中都会有以代码包为基本组织形式的源码文件。</p>
<p><strong>这里的源码文件又分为三种，即：命令源码文件、库源码文件和测试源码文件，它们都有着不同的用途和编写规则。</strong> 我在<a href="https://time.geekbang.org/column/article/13540?utm_source=weibo&utm_medium=xuxiaoping&utm_campaign=promotion&utm_content=columns">“预习篇”的基础知识图</a>介绍过这三种文件的基本情况。今天，我们就沿着<strong>命令源码文件</strong>的知识点，展开更深层级的学习。</p>
<hr>
<p>一旦开始学习用编程语言编写程序，我们就一定希望在编码的过程中及时地得到反馈，只有这样才能清楚对错。实际上，我们的有效学习和进步，都是通过不断地接受反馈和执行修正实现的。</p>
<p>对于 Go 语言学习者来说，你在学习阶段中，也一定会经常编写可以直接运行的程序。这样的程序肯定会涉及命令源码文件的编写，而且，命令源码文件也可以很方便地用<code>go run</code>命令启动。</p>
<p>那么，<strong>我今天的问题就是：命令源码文件的用途是什么，怎样编写它？</strong></p>
<p>这里，我给出你一个<strong>参考的回答</strong>：命令源码文件是程序的运行入口，是每个可独立运行的程序必须拥有的。我们可以通过构建或安装，生成与其对应的可执行文件，后者一般会与该命令源码文件的直接父目录同名。</p>
<p><strong>如果一个源码文件声明属于</strong><code>**main**</code><strong>包，并且包含一个无参数声明且无结果声明的</strong><code>**main**</code><strong>函数，那么它就是命令源码文件。</strong> 就像下面这段代码：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    fmt.Println(<span class="string">&quot;Hello, world!&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果你把这段代码存成 demo1.go 文件，那么运行<code>go run demo1.go</code>命令后就会在屏幕（标准输出）中看到<code>Hello, world!</code></p>
<blockquote>
<p><strong>当需要模块化编程时，我们往往会将代码拆分到多个文件，甚至拆分到不同的代码包中。但无论怎样，对于一个独立的程序来说，命令源码文件永远只会也只能有一个</strong>。如果有与命令源码文件同包的源码文件，那么它们也应该声明属于<code>main</code>包。</p>
</blockquote>
<h3 id="问题解析"><a href="#问题解析" class="headerlink" title="问题解析"></a>问题解析</h3><p>命令源码文件如此重要，以至于它毫无疑问地成为了我们学习 Go 语言的第一助手。不过，只会打印<code>Hello, world</code>是远远不够的，咱们千万不要成为“Hello, world”党。既然决定学习 Go 语言，你就应该从每一个知识点深入下去。</p>
<p>无论是 Linux 还是 Windows，如果你用过命令行（command line）的话，肯定就会知道几乎所有命令（command）都是可以接收参数（argument）的。<strong>通过构建或安装命令源码文件，生成的可执行文件就可以被视为“命令”，既然是命令，那么就应该具备接收参数的能力。</strong></p>
<p>下面，我就带你深入了解一下与命令参数的接收和解析有关的一系列问题。</p>
<h3 id="知识精讲"><a href="#知识精讲" class="headerlink" title="知识精讲"></a>知识精讲</h3><h4 id="1-命令源码文件怎样接收参数"><a href="#1-命令源码文件怎样接收参数" class="headerlink" title="1. 命令源码文件怎样接收参数"></a>1. 命令源码文件怎样接收参数</h4><p>我们先看一段不完整的代码：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="comment">// 需在此处添加代码。[1]</span></span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> name <span class="type">string</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">init</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 需在此处添加代码。[2]</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 需在此处添加代码。[3]</span></span><br><span class="line">    fmt.Printf(<span class="string">&quot;Hello, %s!\n&quot;</span>, name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果邀请你帮助我，在注释处添加相应的代码，并让程序实现”根据运行程序时给定的参数问候某人”的功能，你会打算怎样做？</p>
<p>如果你知道做法，请现在就动手实现它。如果不知道也不要着急，咱们一起来搞定。</p>
<p>首先，<strong>Go 语言标准库中有一个代码包专门用于接收和解析命令参数</strong>。这个代码包的名字叫<code>flag</code>。</p>
<p>我之前说过，如果想要在代码中使用某个包中的程序实体，那么应该先导入这个包。因此，我们需要在<code>[1]</code>处添加代码<code>&quot;flag&quot;</code>。注意，这里应该在代码包导入路径的前后加上英文半角的引号。如此一来，上述代码导入了<code>flag</code>和<code>fmt</code>这两个包。</p>
<p>其次，人名肯定是由字符串代表的。所以我们要在<code>[2]</code>处添加调用<code>flag</code>包的<code>StringVar</code>函数的代码。就像这样：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">flag.StringVar(&amp;name, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;everyone&quot;</span>, <span class="string">&quot;The greeting object.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>函数<code>flag.StringVar</code>接受 4 个参数。第 1 个参数是用于存储该命令参数的值的地址，具体到这里就是在前面声明的变量<code>name</code>的地址了，由表达式<code>&amp;name</code>表示。</p>
<p>第 2 个参数是为了<strong>指定该命令参数的名称</strong>，这里是<code>name</code>。第 3 个参数是为了<strong>指定在未追加该命令参数时的默认值</strong>，这里是<code>everyone</code>。</p>
<p>至于第 4 个函数参数，<strong>即是该命令参数的简短说明了，这在打印命令说明时会用到</strong>。</p>
<p>顺便说一下，还有一个与<code>flag.StringVar</code>函数类似的函数，叫<code>flag.String</code>。这两个函数的区别是，后者会直接返回一个已经分配好的用于存储命令参数值的地址。如果使用它的话，我们就需要把</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> name <span class="type">string</span></span><br></pre></td></tr></table></figure>

<p>改为</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> name = flag.String(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;everyone&quot;</span>, <span class="string">&quot;The greeting object.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>所以，如果我们使用<code>flag.String</code>函数就需要改动原有的代码。这样并不符合上述问题的要求。</p>
<p>再说最后一个填空。我们需要在<code>[3]</code>处添加代码<code>flag.Parse()</code>。函数<code>flag.Parse</code>用于<strong>真正解析命令参数，并把它们的值赋给相应的变量</strong>。</p>
<p>对该函数的调用必须在所有命令参数存储载体的声明（这里是对变量<code>name</code>的声明）和设置（这里是在<code>[2]</code>处对<code>flag.StringVar</code>函数的调用）之后，并且在读取任何命令参数值之前进行。</p>
<p>正因为如此，我们最好把<code>flag.Parse()</code>放在<code>main</code>函数的函数体的第一行。</p>
<h4 id="2-怎样在运行命令源码文件的时候传入参数，又怎样查看参数的使用说明"><a href="#2-怎样在运行命令源码文件的时候传入参数，又怎样查看参数的使用说明" class="headerlink" title="2. 怎样在运行命令源码文件的时候传入参数，又怎样查看参数的使用说明"></a>2. 怎样在运行命令源码文件的时候传入参数，又怎样查看参数的使用说明</h4><p>如果我们把上述代码存成名为 demo2.go 的文件，那么运行如下命令就可以为参数<code>name</code>传值：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">go</span> run demo2.<span class="keyword">go</span> -name=<span class="string">&quot;Robert&quot;</span></span><br></pre></td></tr></table></figure>

<p>运行后，打印到标准输出（stdout）的内容会是：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">Hello, Robert!</span><br></pre></td></tr></table></figure>

<p>另外，如果想查看该命令源码文件的参数说明，可以这样做：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">go</span> run demo2.<span class="keyword">go</span> --help</span><br></pre></td></tr></table></figure>

<p>其中的</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">MY_ZUES_CHAR</span><br><span class="line"><span class="keyword">go</span> run</span><br></pre></td></tr></table></figure>

<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">Usage of /<span class="keyword">var</span>/folders/ts/<span class="number">7</span>lg_tl_x2gd_k1lm5g_48c7w0000gn/T/<span class="keyword">go</span>-build155438482/b001/exe/demo2:</span><br><span class="line"> -name <span class="type">string</span></span><br><span class="line">    The greeting object. (<span class="keyword">default</span> <span class="string">&quot;everyone&quot;</span>)</span><br><span class="line">exit status <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>你可能不明白下面这段输出代码的意思。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">/<span class="keyword">var</span>/folders/ts/<span class="number">7</span>lg_tl_x2gd_k1lm5g_48c7w0000gn/T/<span class="keyword">go</span>-build155438482/b001/exe/demo2</span><br></pre></td></tr></table></figure>

<p>这其实是<code>go run</code>命令构建上述命令源码文件时<strong>临时生成的可执行文件的完整路径</strong>。</p>
<p>如果我们先构建这个命令源码文件再运行生成的可执行文件，像这样：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">go</span> build demo2.<span class="keyword">go</span></span><br><span class="line">$ ./demo2 --help</span><br></pre></td></tr></table></figure>

<p>那么输出就会是</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">Usage of ./demo2:</span><br><span class="line"> -name <span class="type">string</span></span><br><span class="line">    The greeting object. (<span class="keyword">default</span> <span class="string">&quot;everyone&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-怎样自定义命令源码文件的参数使用说明"><a href="#3-怎样自定义命令源码文件的参数使用说明" class="headerlink" title="3. 怎样自定义命令源码文件的参数使用说明"></a>3. 怎样自定义命令源码文件的参数使用说明</h4><p>这有很多种方式，最简单的一种方式就是对变量<code>flag.Usage</code>重新赋值。<code>flag.Usage</code>的类型是<code>func()</code>，<strong>即一种无参数声明且无结果声明的函数类型</strong>。</p>
<p><code>flag.Usage</code>变量在声明时就已经被赋值了，所以我们才能够在运行命令<code>go run demo2.go --help</code>时看到正确的结果。</p>
<p>注意，对<code>flag.Usage</code>的赋值必须在调用<code>flag.Parse</code>函数之前。</p>
<p>现在，我们把 demo2.go 另存为 demo3.go，然后在<code>main</code>函数体的开始处加入如下代码。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">flag.Usage = <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"> fmt.Fprintf(os.Stderr, <span class="string">&quot;Usage of %s:\n&quot;</span>, <span class="string">&quot;question&quot;</span>)</span><br><span class="line"> flag.PrintDefaults()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>那么当运行</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">go</span> run demo3.<span class="keyword">go</span> --help</span><br></pre></td></tr></table></figure>

<p>后，就会看到</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">Usage of question:</span><br><span class="line"> -name <span class="type">string</span></span><br><span class="line">    The greeting object. (<span class="keyword">default</span> <span class="string">&quot;everyone&quot;</span>)</span><br><span class="line">exit status <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>现在再深入一层，我们在调用<code>flag</code>包中的一些函数（比如<code>StringVar</code>、<code>Parse</code>等等）的时候，实际上是在调用<code>flag.CommandLine</code>变量的对应方法。</p>
<p><code>flag.CommandLine</code>相当于默认情况下的命令参数容器。所以，通过对<code>flag.CommandLine</code>重新赋值，我们可以更深层次地定制当前命令源码文件的参数使用说明。</p>
<p>现在我们把<code>main</code>函数体中的那条对<code>flag.Usage</code>变量的赋值语句注销掉，然后在<code>init</code>函数体的开始处添加如下代码：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">flag.CommandLine = flag.NewFlagSet(<span class="string">&quot;&quot;</span>, flag.ExitOnError)</span><br><span class="line">flag.CommandLine.Usage = <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">fmt.Fprintf(os.Stderr, <span class="string">&quot;Usage of %s:\n&quot;</span>, <span class="string">&quot;question&quot;</span>)</span><br><span class="line">flag.PrintDefaults()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再运行命令<code>go run demo3.go --help</code>后，其输出会与上一次的输出的一致。不过后面这种定制的方法更加灵活。比如，当我们把为<code>flag.CommandLine</code>赋值的那条语句改为</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">flag.CommandLine = flag.NewFlagSet(<span class="string">&quot;&quot;</span>, flag.PanicOnError)</span><br></pre></td></tr></table></figure>

<p>后，再运行<code>go run demo3.go --help</code>命令就会产生另一种输出效果。这是由于我们在这里传给<code>flag.NewFlagSet</code>函数的第二个参数值是<code>flag.PanicOnError</code>。<code>flag.PanicOnError</code>和<code>flag.ExitOnError</code>都是预定义在<code>flag</code>包中的常量。</p>
<p><code>flag.ExitOnError</code>的含义是，告诉命令参数容器，当命令后跟<code>--help</code>或者参数设置的不正确的时候，在打印命令参数使用说明后以状态码<code>2</code>结束当前程序。</p>
<p><strong>状态码<code>2</code>代表用户错误地使用了命令，而<code>flag.PanicOnError</code>与之的区别是在最后抛出“运行时恐慌（panic）”。</strong></p>
<p>上述两种情况都会在我们调用<code>flag.Parse</code>函数时被触发。顺便提一句，<strong>“运行时恐慌”是 Go 程序错误处理方面的概念。关于它的抛出和恢复方法，我在本专栏的后续部分中会讲到</strong>。</p>
<p>下面再进一步，我们索性不用全局的<code>flag.CommandLine</code>变量，转而自己创建一个私有的命令参数容器。我们在函数外再添加一个变量声明：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> cmdLine = flag.NewFlagSet(<span class="string">&quot;question&quot;</span>, flag.ExitOnError)</span><br></pre></td></tr></table></figure>

<p>然后，我们把对<code>flag.StringVar</code>的调用替换为对<code>cmdLine.StringVar</code>调用，再把<code>flag.Parse()</code>替换为<code>cmdLine.Parse(os.Args[1:])</code>。</p>
<p>其中的<code>os.Args[1:]</code>指的就是我们给定的那些命令参数。这样做就完全脱离了<code>flag.CommandLine</code>。<code>*flag.FlagSet</code>类型的变量<code>cmdLine</code>拥有很多有意思的方法。你可以去探索一下。我就不在这里一一讲述了。</p>
<p>这样做的好处依然是更灵活地定制命令参数容器。但更重要的是，你的定制完全不会影响到那个全局变量<code>flag.CommandLine</code>。</p>
<p><strong>总结</strong></p>
<p>恭喜你！你现在已经走出了 Go 语言编程的第一步。你可以用 Go 编写命令，并可以让它们像众多操作系统命令那样被使用，甚至可以把它们嵌入到各种脚本中。</p>
<p>虽然我为你讲解了命令源码文件的基本编写方法，并且也谈到了为了让它接受参数而需要做的各种准备工作，但这并不是全部。</p>
<p>别担心，我在后面会经常提到它的。另外，如果你想详细了解<code>flag</code>包的用法，可以到<a href="https://golang.google.cn/pkg/flag/">这个网址</a>查看文档。或者直接使用<code>godoc</code>命令在本地启动一个 Go 语言文档服务器。怎样使用<code>godoc</code>命令？你可以参看<a href="https://github.com/hyper0x/go_command_tutorial/blob/master/0.5.md">这里</a>。</p>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><p>我们已经见识过为<strong>命令源码文件传入字符串类型的参数值的方法</strong>，那还可以传入别的吗？这就是今天我留下的思考题。</p>
<ol>
<li>默认情况下，我们可以让命令源码文件接受哪些类型的参数值？</li>
<li>我们可以把自定义的数据类型作为参数值的类型吗？如果可以，怎样做？</li>
</ol>
<p>你可以通过查阅文档获得第一个问题的答案。<strong>记住，快速查看和理解文档是一项必备的技能。</strong></p>
<p>至于第二个问题，你回答起来可能会有些困难，因为这涉及了另一个问题：“怎样声明自己的数据类型？”这个问题我在专栏的后续部分中也会讲到。如果是这样，我希望你记下它和这里说的另一问题，并在能解决后者之后再来回答前者。</p>
<p>参考文章：<a href="https://jums.gitbook.io/36-lectures-on-golang/02-ming-ling-yuan-ma-wen-jian">02 | 命令源码文件 | Go语言核心36讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>程序实体的那些事儿（上）</title>
    <url>/2024/09/19/Golang04/</url>
    <content><![CDATA[<p>我已经为你打开了 Go 语言编程之门，并向你展示了“<strong>程序从初建到拆分，再到模块化</strong>”的基本演化路径。</p>
<p>一个编程老手让程序完成基本演化，可能也就需要几十分钟甚至十几分钟，因为他们一开始就会把车开到模块化编程的道路上。我相信，等你真正理解了这个过程之后，也会驾轻就熟的。</p>
<p>上述套路是通用的，不是只适用于 Go 语言。但从本篇开始，<strong>我会开始向你介绍 Go 语言中的各种特性以及相应的编程方法和思想。</strong></p>
<hr>
<p>我在讲解那两种源码文件基本编写方法的时候，声明和使用了一些程序实体。你也许已经若有所觉，也许还在云里雾里。没关系，我现在就与你一起梳理这方面的重点。</p>
<p>还记得吗？<strong>Go 语言中的程序实体包括变量、常量、函数、结构体和接口。</strong> Go 语言是<strong>静态类型的编程语言</strong>，所以我们在声明变量或常量的时候，<strong>都需要指定它们的类型，或者给予足够的信息，这样才可以让 Go 语言能够推导出它们的类型</strong>。</p>
<blockquote>
<p>在 Go 语言中，变量的类型可以是其预定义的那些类型，也可以是程序自定义的函数、结构体或接口。常量的合法类型不多，只能是那些 Go 语言预定义的基本类型。它的声明方式也更简单一些。</p>
</blockquote>
<p>好了，下面这个简单的问题你需要了解一下。</p>
<h3 id="问题：声明变量有几种方式？"><a href="#问题：声明变量有几种方式？" class="headerlink" title="问题：声明变量有几种方式？"></a><strong>问题：声明变量有几种方式？</strong></h3><p>先看段代码。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;flag&quot;</span></span><br><span class="line"><span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> name <span class="type">string</span> <span class="comment">// [1]</span></span><br><span class="line">flag.StringVar(&amp;name, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;everyone&quot;</span>, <span class="string">&quot;The greeting object.&quot;</span>) <span class="comment">// [2]</span></span><br><span class="line">flag.Parse()</span><br><span class="line">fmt.Printf(<span class="string">&quot;Hello, %v!\n&quot;</span>, name)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这是一个很简单的命令源码文件，我把它命名为 demo7.go。它是 demo2.go 的微调版。我只是把变量<code>name</code>的声明和对<code>flag.StringVar</code>函数的调用，都移动到了<code>main</code>函数中，这分别对应代码中的注释<code>[1]</code>和<code>[2]</code>。</p>
<p>具体的问题是，除了<code>var name string</code>这种声明变量<code>name</code>的方式，还有其他方式吗？你可以选择性地改动注释<code>[1]</code>和<code>[2]</code>处的代码。</p>
<h3 id="典型回答"><a href="#典型回答" class="headerlink" title="典型回答"></a><strong>典型回答</strong></h3><p>这有几种做法，我在这里只说最典型的两种。</p>
<p><strong>第一种方式</strong>需要先对注释<code>[2]</code>处的代码稍作改动，把被调用的函数由<code>flag.StringVar</code>改为<code>flag.String</code>，传参的列表也需要随之修改，这是为了<code>[1]</code>和<code>[2]</code>处代码合并的准备工作。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> name = *flag.String(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;everyone&quot;</span>, <span class="string">&quot;The greeting object.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>合并后的代码看起来更简洁一些。我把注释<code>[1]</code>处的代码中的<code>string</code>去掉了，右边添加了一个<code>=</code>，然后再拼接上经过修改的<code>[2]</code>处代码。</p>
<p><strong>我在调用<code>flag.String</code>函数的代码左边加了一个<code>*</code>，这是因为该函数返回的结果值类型是<code>*string</code>而不是<code>string</code>。类型<code>*string</code>代表的是字符串的指针类型，而不是字符串类型。</strong></p>
<p>关于 Go 语言中的指针，我在后面会有专门的介绍。<strong>你在这里只需要知道，我通过一个“<code>*</code>”把这个字符串指针值指向的字符串值取出来了，然后通过赋值符号“<code>=</code>”把后者赋给了<code>name</code>变量。</strong></p>
<p>好了，我想你已经基本理解了这行代码中的每一个部分。</p>
<p><strong>下面我接着说第二种方式。</strong>第二种方式与第一种方式非常类似，<strong>它基于第一种方式的代码，赋值符号<code>=</code>右边的代码不动，左边只留下<code>name</code>，再把<code>=</code>变成<code>:=</code>。</strong></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">name := *flag.String(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;everyone&quot;</span>, <span class="string">&quot;The greeting object.&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="问题解析"><a href="#问题解析" class="headerlink" title="问题解析"></a><strong>问题解析</strong></h3><p>这个问题的基本考点有两个。<strong>一个是你要知道 Go 语言中的类型推断，以及它在代码中的基本体现，另一个是短变量声明的用法。</strong></p>
<p>第一种方式中的代码在声明变量<code>name</code>的同时，还为它赋了值，而这时声明中并没有显式指定<code>name</code>的类型。</p>
<p>还记得吗？之前的变量声明语句是<code>var name string</code>。这里利用了 Go 语言自身的类型推断而省去了<code>string</code>。</p>
<blockquote>
<p>简单地说，<strong>类型推断是一种编程语言在编译期自动解释表达式类型的能力</strong>。什么是表达式？详细的解释你可以参看 Go 语言规范中的<a href="https://golang.google.cn/ref/spec#Expressions">表达式</a>和<a href="https://golang.google.cn/ref/spec#Expression_statements">表达式语句</a>章节。我在这里就不赘述了。</p>
</blockquote>
<p>你可以认为，<strong>表达式类型就是对表达式进行求值后得到结果的类型。Go 语言中的类型推断是很简约的，这也是 Go 语言整体的风格</strong>。</p>
<p>它只能用于对变量或常量的初始化，就像上述回答中描述的那样。对<code>flag.String</code>函数的调用其实就是一个调用表达式，而这个表达式的类型是<code>*string</code>，即字符串的指针类型。</p>
<p>这也是调用<code>flag.String</code>函数后得到结果的类型。<strong>随后，Go 语言把这个调用了<code>flag.String</code>函数的表达式类型，直接作为了变量<code>name</code>的类型，这就是“推断”一词所指代的操作了</strong>。</p>
<p>至于第二种方式所用的短变量声明，<strong>实际上就是 Go 语言的类型推断再加上一点点语法糖</strong>。</p>
<p>我们只能在<strong>函数体内部使用短变量声明</strong>。在编写<code>if</code>、<code>for</code>或<code>switch</code>语句的时候，<strong>我们经常把它安插在初始化子句中，并用来声明一些临时的变量</strong>。而相比之下，第一种方式更加通用，它可以被用在任何地方。</p>
<p>比如在 if 中的使用：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="comment">// 短变量声明</span></span><br><span class="line">    <span class="keyword">if</span> x := <span class="number">10</span>; x &gt; <span class="number">5</span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">&quot;x is greater than 5&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// x 在这里不可用，因为它只在 if 语句的作用域内可见</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 for 中的使用：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">5</span>; i++ &#123;</span><br><span class="line">        fmt.Println(i) <span class="comment">// i 在这里有效</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// i 在这里不可用，因为它只在 for 循环的作用域内可见</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>短变量声明还有其他的玩法，我稍后就会讲到。</p>
<h3 id="知识扩展"><a href="#知识扩展" class="headerlink" title="知识扩展"></a><strong>知识扩展</strong></h3><h4 id="1-Go-语言的类型推断可以带来哪些好处？"><a href="#1-Go-语言的类型推断可以带来哪些好处？" class="headerlink" title="1. Go 语言的类型推断可以带来哪些好处？"></a><strong>1. Go 语言的类型推断可以带来哪些好处？</strong></h4><p>如果面试官问你这个问题，你应该怎样回答？</p>
<p>当然，在写代码时，<strong>我们通过使用 Go 语言的类型推断，而节省下来的键盘敲击次数几乎可以忽略不计。但它真正的好处，往往会体现在我们写代码之后的那些事情上，比如代码重构</strong>。</p>
<p>为了更好的演示，我们先要做一点准备工作。我们依然通过调用一个函数在声明<code>name</code>变量的同时为它赋值，但是这个函数不是<code>flag.String</code>，而是由我们自己定义的某个函数，比如叫<code>getTheFlag</code>。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">&quot;flag&quot;</span></span><br><span class="line"><span class="string">&quot;fmt&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> name = getTheFlag()</span><br><span class="line">flag.Parse()</span><br><span class="line">fmt.Printf(<span class="string">&quot;Hello, %v!\n&quot;</span>, *name)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getTheFlag</span><span class="params">()</span></span> *<span class="type">string</span> &#123;</span><br><span class="line"><span class="keyword">return</span> flag.String(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;everyone&quot;</span>, <span class="string">&quot;The greeting object.&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们可以用<code>getTheFlag</code>函数包裹（或者说包装）那个对<code>flag.String</code>函数的调用，并把其结果直接作为<code>getTheFlag</code>函数的结果，结果的类型是<code>*string</code>。</p>
<p>这样一来，<code>var name =</code>右边的表达式，可以变为针对<code>getTheFlag</code>函数的调用表达式了。<strong>这实际上是对“声明并赋值<code>name</code>变量的那行代码”的重构。</strong></p>
<blockquote>
<p><strong>我们通常把不改变某个程序与外界的任何交互方式和规则，而只改变其内部实现”的代码修改方式，叫做对该程序的重构</strong>。重构的对象可以是<strong>一行代码、一个函数、一个功能模块，甚至一个软件系统</strong>。</p>
</blockquote>
<p>好了，在准备工作做完之后，你会发现，你可以随意改变<code>getTheFlag</code>函数的内部实现，及其返回结果的类型，而不用修改<code>main</code>函数中的任何代码。</p>
<p>这个命令源码文件依然可以通过编译，并且构建和运行也都不会有问题。<strong>也许你能感觉得到，这是一个关于程序灵活性的质变</strong>。</p>
<p>我们不显式地指定变量<code>name</code>的类型，<strong>使得它可以被赋予任何类型的值。也就是说，变量<code>name</code>的类型可以在其初始化时，由其他程序动态地确定</strong>。</p>
<p>在你改变<code>getTheFlag</code>函数的结果类型之后，Go 语言的编译器会在你再次构建该程序的时候，自动地更新变量<code>name</code>的类型。<strong>如果你使用过<code>Python</code>或<code>Ruby</code>这种动态类型的编程语言的话，一定会觉得这情景似曾相识</strong>。</p>
<p>没错，通过这种类型推断，<strong>你可以体验到动态类型编程语言所带来的一部分优势，即程序灵活性的明显提升</strong>。但在那些编程语言中，这种提升可以说是用程序的可维护性和运行效率换来的。</p>
<p>Go 语言是静态类型的，<strong>所以一旦在初始化变量时确定了它的类型，之后就不可能再改变。这就避免了在后面维护程序时的一些问题</strong>。另外，请记住，<strong>这种类型的确定是在编译期完成的，因此不会对程序的运行效率产生任何影响</strong>。</p>
<p>现在，你应该已经对这个问题有一个比较深刻的理解了。</p>
<p>如果只用一两句话回答这个问题的话，我想可以是这样的：<strong>Go 语言的类型推断可以明显提升程序的灵活性，使得代码重构变得更加容易，同时又不会给代码的维护带来额外负担</strong>（实际上，它恰恰可以避免散弹式的代码修改），<strong>更不会损失程序的运行效率</strong>。</p>
<h4 id="2-变量的重声明是什么意思？"><a href="#2-变量的重声明是什么意思？" class="headerlink" title="2. 变量的重声明是什么意思？"></a><strong>2. 变量的重声明是什么意思？</strong></h4><p>这涉及了短变量声明。通过使用它，我们可以对同一个代码块中的变量进行重声明。</p>
<p>既然说到了代码块，我先来解释一下它。在 Go 语言中，代码块一般就是一个由花括号括起来的区域，里面可以包含表达式和语句。<strong>Go 语言本身以及我们编写的代码共同形成了一个非常大的代码块，也叫全域代码块。</strong></p>
<p>这主要体现在，<strong>只要是公开的全局变量，都可以被任何代码所使用。相对小一些的代码块是代码包，一个代码包可以包含许多子代码包</strong>，所以这样的代码块也可以很大。</p>
<p>接下来，每个源码文件也都是一个代码块，每个函数也是一个代码块，每个<code>if</code>语句、<code>for</code>语句、<code>switch</code>语句和<code>select</code>语句都是一个代码块。甚至，<code>switch</code>或<code>select</code>语句中的<code>case</code>子句也都是独立的代码块。</p>
<p>走个极端，我就在<code>main</code>函数中写一对紧挨着的花括号算不算一个代码块？当然也算，这甚至还有个名词，叫“空代码块”。</p>
<p>回到变量重声明的问题上。<strong>其含义是对已经声明过的变量再次声明。变量重声明的前提条件如下。</strong></p>
<ol>
<li><p><strong>由于变量的类型在其初始化时就已经确定了，所以对它再次声明时赋予的类型必须与其原本的类型相同，否则会产生编译错误。</strong></p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    a := <span class="number">10</span>        <span class="comment">// a 是 int 类型</span></span><br><span class="line">    a, b := <span class="number">20</span>, <span class="number">30</span> <span class="comment">// 这里 a 被重声明为 int 类型，b 是新的 int 类型变量</span></span><br><span class="line">    fmt.Println(a, b) <span class="comment">// 输出: 20 30</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果尝试将 <code>a</code> 的类型更改为不同的类型，将导致编译错误</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="comment">// a, b := &quot;hello&quot;, 30 // 错误：无法重声明 a，类型不匹配</span></span><br></pre></td></tr></table></figure>

<p>对一个已经声明的变量进行重声明时，<strong>新的类型必须与原来的类型相同</strong>，否则会产生编译错误。</p>
</li>
<li><p><strong>变量的重声明只可能发生在某一个代码块中。</strong>如果与当前的变量重名的是外层代码块中的变量，那么就是另外一种含义了，我在下一篇文章中会讲到。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    x := <span class="number">1</span> <span class="comment">// 外层 x</span></span><br><span class="line">    &#123;</span><br><span class="line">        x := <span class="number">2</span> <span class="comment">// 内层 x，重声明</span></span><br><span class="line">        fmt.Println(x) <span class="comment">// 输出: 2</span></span><br><span class="line">    &#125;</span><br><span class="line">    fmt.Println(x) <span class="comment">// 输出: 1，外层 x</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果外层代码块中有同名变量，<strong>那么在内层代码块中重声明时会被视为不同的变量</strong>。</p>
</li>
<li><p><strong>变量的重声明只有在使用短变量声明时才会发生，否则也无法通过编译。</strong>如果要在此处声明全新的变量，那么就应该使用包含关键字<code>var</code>的声明语句，但是这时就不能与同一个代码块中的任何变量有重名了。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    y := <span class="number">5</span></span><br><span class="line">    <span class="comment">// var y int // 错误：y 已经声明过，不能再次声明</span></span><br><span class="line">    y, z := <span class="number">10</span>, <span class="number">15</span> <span class="comment">// 这里 y 被重声明，z 是新变量</span></span><br><span class="line">    fmt.Println(y, z) <span class="comment">// 输出: 10 15</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>变量的重声明<strong>只能通过短变量声明实现，使用 <code>var</code> 声明时不会造成重声明</strong>。</p>
</li>
<li><p><strong>被“声明并赋值”的变量必须是多个，并且其中至少有一个是新的变量。</strong>这时我们才可以说对其中的旧变量进行了重声明。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    a := <span class="number">1</span></span><br><span class="line">    a, b := <span class="number">2</span>, <span class="number">3</span> <span class="comment">// a 被重声明，b 是新变量</span></span><br><span class="line">    fmt.Println(a, b) <span class="comment">// 输出: 2 3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重声明要求<strong>在同一语句中至少有一个新变量</strong>。</p>
</li>
</ol>
<p>这样来看，<strong>变量重声明其实算是一个语法糖（或者叫便利措施）。它允许我们在使用短变量声明时不用理会被赋值的多个变量中是否包含旧变量。</strong>可以想象，如果不这样会多写不少代码。</p>
<p>我把一个简单的例子写在了“Golang_Puzzlers”项目的<code>puzzlers/article4/q3</code>包中的 demo9.go 文件中，你可以去看一下。</p>
<p>这其中最重要的两行代码如下：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> err <span class="type">error</span></span><br><span class="line">n, err := io.WriteString(os.Stdout, <span class="string">&quot;Hello, everyone!\n&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>我使用短变量声明对新变量<code>n</code>和旧变量<code>err</code>进行了“声明并赋值”，这时也是对后者的重声明。</strong></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>在本篇中，我们聚焦于最基本的 Go 语言程序实体：变量。并详细解说了变量声明和赋值的基本方法，及其背后的重要概念和知识。<strong>我们使用关键字<code>var</code>和短变量声明，都可以实现对变量的“声明并赋值”</strong>。</p>
<p>这两种方式各有千秋，有着各自的特点和适用场景。<strong>前者可以被用在任何地方，而后者只能被用在函数或者其他更小的代码块中</strong>。</p>
<p>不过，<strong>通过前者我们无法对已有的变量进行重声明，也就是说它无法处理新旧变量混在一起的情况</strong>。不过它们也有一个很重要的共同点，即：基于类型推断，<strong>Go 语言的类型推断只应用在了对变量或常量的初始化方面</strong>。</p>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a><strong>思考题</strong></h3><p>本次的思考题只有一个：如果与当前的变量重名的是外层代码块中的变量，那么这意味着什么？</p>
<p>这道题对于你来说可能有些难，不过我鼓励你多做几次试验试试，你可以在代码中多写一些打印语句，然后运行它，并记录下每次试验的结果。如果有疑问也一定要写下来，答案将在下篇文章中揭晓。</p>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Java基础知识大全（一）</title>
    <url>/2024/09/19/Java01/</url>
    <content><![CDATA[<h2 id="JDK"><a href="#JDK" class="headerlink" title="JDK"></a>JDK</h2><p>JDK（Java Development Kit）是一个功能齐全的 Java 开发<strong>工具包</strong>，供开发者使用，用于创建和编译 Java 程序。它包含了 JRE（Java Runtime Environment），以及编译器 javac （用于<strong>将 Java 源代码编译为字节码（.class 文件）</strong>）和其他工具，如 ：</p>
<ul>
<li>javadoc（<strong>文档生成器</strong>，基于注释生成 HTML 格式的 API 文档）。</li>
<li>jdb（调试器，<strong>用于调试 Java 程序</strong>，允许开发者设置断点、查看变量值等）。</li>
<li>jconsole（监控工具，用于<strong>监控 Java 应用程序的性能和资源使用情况</strong>，支持 JMX（Java Management Extensions））。</li>
<li>javap（反编译工具，<strong>用于查看字节码内容</strong>，帮助开发者理解编译后的类文件结构）。</li>
</ul>
<p>上面提到的工具有一些我还使用过，一般开发不会使用到， IDE 已经帮助我们集成了大部分的功能。</p>
<p>JRE 是运行已编译 Java 程序所需的环境，主要包含以下两个部分：</p>
<ol>
<li><strong>JVM</strong> : 也就是我们上面提到的 Java 虚拟机。</li>
<li><strong>Java 基础类库（Class Library）</strong>：一组标准的类库，提供常用的功能和 API（如 I&#x2F;O 操作、网络通信、数据结构等）。</li>
</ol>
<p>简单来说，<strong>JRE 只包含运行 Java 程序所需的环境和类库，而 JDK 不仅包含 JRE，还包括用于开发和调试 Java 程序的工具</strong>。</p>
<p>下图清晰展示了 JDK、JRE 和 JVM 的关系。</p>
<p><img src="/2024/09/19/Java01/jdk-include-jre.png" alt="jdk-include-jre"></p>
<p>不过，从 JDK 9 开始，就不需要区分 JDK 和 JRE 的关系了，取而代之的是<strong>模块系统</strong>（JDK 被重新组织成 94 个模块）+ <a href="http://openjdk.java.net/jeps/282">jlinkopen in new window</a> 工具 (随 Java 9 一起发布的新命令行工具，用于生成自定义 Java 运行时映像，该映像仅包含给定应用程序所需的模块) 。并且，从 JDK 11 开始，Oracle 不再提供单独的 JRE 下载。</p>
<p>很多语言最后都走向模块化了，这里的 JDK 只是例子之一，比如 Go Modules ，使用的也是模块化思想。</p>
<h2 id="字节码"><a href="#字节码" class="headerlink" title="字节码"></a>字节码</h2><p>在 Java 中，<strong>JVM 可以理解的代码就叫做字节码（即扩展名为 <code>.class</code> 的文件）</strong>，它不面向任何特定的处理器，只面向虚拟机。<strong>Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点</strong>。所以， Java 程序运行时相对来说还是高效的（不过，和 C、 C++，Rust，Go 等语言还是有一定差距的），而且，由于字节码并不针对一种特定的机器，因此，Java 程序无须重新编译便可在多种不同操作系统的计算机上运行。</p>
<p><strong>Java 程序从源代码到运行的过程如下图所示</strong>：</p>
<p><img src="/2024/09/19/Java01/java-code-to-machine-code.png" alt="Java程序转变为机器代码的过程"></p>
<p>我们需要格外注意的是 <code>.class-&gt;机器码</code> 这一步。<strong>在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢</strong>。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 <strong>JIT（Just in Time Compilation）</strong> 编译器，而 JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 <strong>Java 是编译与解释共存的语言</strong> 。</p>
<p><img src="/2024/09/19/Java01/java-code-to-machine-code-with-jit.png" alt="Java程序转变为机器代码的过程"></p>
<blockquote>
<p>HotSpot 采用了惰性评估(Lazy Evaluation)的做法，根据二八定律，<strong>消耗大部分系统资源的只有那一小部分的代码（热点代码），而这也就是 JIT 所需要编译的部分</strong>。JVM 会根据代码每次被执行的情况收集信息并相应地做出一些优化，因此执行的次数越多，它的速度就越快。</p>
</blockquote>
<p>JDK、JRE、JVM、JIT 这四者的关系如下图所示。</p>
<p><img src="/2024/09/19/Java01/jdk-jre-jvm-jit.png" alt="JDK、JRE、JVM、JIT 这四者的关系"></p>
<p>JVM 的大致结构模型如下图所示。</p>
<p><img src="/2024/09/19/Java01/jvm-rough-structure-model.png" alt="JVM 的大致结构模型"></p>
<h2 id="Java-和-C-的区别"><a href="#Java-和-C-的区别" class="headerlink" title="Java 和 C++ 的区别?"></a>Java 和 C++ 的区别?</h2><p>虽然，Java 和 C++ 都是面向对象的语言，都支持封装、继承和多态，但是，它们还是有挺多不相同的地方：</p>
<ul>
<li>Java 不提供指针来直接访问内存，程序内存更加安全</li>
<li>Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。</li>
<li>Java 有自动内存管理垃圾回收机制(GC)，不需要程序员手动释放无用内存。</li>
<li>C ++同时支持方法重载和操作符重载，但是 Java 只支持方法重载（操作符重载增加了复杂性，这与 Java 最初的设计思想不符）。</li>
</ul>
<h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><p>《Clean Code》这本书明确指出：</p>
<blockquote>
<p><strong>代码的注释不是越详细越好。实际上好的代码本身就是注释，我们要尽量规范和美化自己的代码来减少不必要的注释。</strong></p>
<p><strong>若编程语言足够有表达力，就不需要注释，尽量通过代码来阐述。</strong></p>
<p>举个例子：</p>
<p>去掉下面复杂的注释，只需要创建一个与注释所言同一事物的函数即可</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// check to see if the employee is eligible for full benefits</span></span><br><span class="line"><span class="keyword">if</span> ((employee.flags &amp; HOURLY_FLAG) &amp;&amp; (employee.age &gt; <span class="number">65</span>))</span><br></pre></td></tr></table></figure>

<p>应替换为</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (employee.isEligibleForFullBenefits())</span><br></pre></td></tr></table></figure>
</blockquote>
<p>怎么说呢？注释不是越多越好的另外一个原因是软件的迭代比较快，如果有大量的注释会大大增加项目维护的难度，所以我们要尽量避免写出无用的注释，最好是直接让代码见名知意。</p>
<h2 id="移位运算符"><a href="#移位运算符" class="headerlink" title="移位运算符"></a>移位运算符</h2><p>Java 中有三种移位运算符：</p>
<ul>
<li><strong><code>&lt;&lt;</code> :左移运算符</strong>，向左移若干位，高位丢弃，低位补零。<code>x &lt;&lt; n</code>,相当于 x 乘以 2 的 n 次方(不溢出的情况下)。</li>
<li><strong><code>&gt;&gt;</code> :带符号右移</strong>，向右移若干位，<strong>高位补符号位</strong>，低位丢弃。正数高位补 0,负数高位补 1。<code>x &gt;&gt; n</code>,相当于 x 除以 2 的 n 次方。</li>
<li><strong><code>&gt;&gt;&gt;</code> :无符号右移</strong>，忽略符号位，空位都以 0 补齐。</li>
</ul>
<p>虽然移位运算本质上可以分为左移和右移，但在实际应用中，右移操作需要考虑符号位的处理方式。</p>
<p>由于 <code>double</code>，<code>float</code> 在二进制中的表现比较特殊，因此<strong>不能</strong>来进行移位操作。</p>
<p>移位操作符实际上支持的类型只有<code>int</code>和<code>long</code>，编译器在对<code>short</code>、<code>byte</code>、<code>char</code>类型进行移位前，都会将其转换为<code>int</code>类型再操作。</p>
<p><strong>如果移位的位数超过数值所占有的位数会怎样？</strong></p>
<p>当 int 类型左移&#x2F;右移位数大于等于 32 位操作时，<strong>会先求余（%）后再进行左移&#x2F;右移操作</strong>。也就是说左移&#x2F;右移 32 位相当于不进行移位操作（32%32&#x3D;0），左移&#x2F;右移 42 位相当于左移&#x2F;右移 10 位（42%32&#x3D;10）。<strong>当 long 类型进行左移&#x2F;右移操作时，由于 long 对应的二进制是 64 位，因此求余操作的基数也变成了 64。</strong></p>
<p>也就是说：<code>x&lt;&lt;42</code>等同于<code>x&lt;&lt;10</code>，<code>x&gt;&gt;42</code>等同于<code>x&gt;&gt;10</code>，<code>x &gt;&gt;&gt;42</code>等同于<code>x &gt;&gt;&gt; 10</code>。</p>
<p>这个我之前看 Java 核心技术卷I 的时候就看到过，还行，现在又看到了。</p>
<h2 id="Java的基本数据类型"><a href="#Java的基本数据类型" class="headerlink" title="Java的基本数据类型"></a>Java的基本数据类型</h2><p>这下面的东西书中的内容也提到过。</p>
<p>Java 中有 8 种基本数据类型，分别为：</p>
<ul>
<li>6 种数字类型： <ul>
<li>4 种整数型：<code>byte</code>、<code>short</code>、<code>int</code>、<code>long</code>。</li>
<li>2 种浮点型：<code>float</code>、<code>double</code>。</li>
</ul>
</li>
<li>1 种字符类型：<code>char</code>。</li>
<li>1 种布尔型：<code>boolean</code>。</li>
</ul>
<p><strong>注意：</strong></p>
<ol>
<li>Java 里使用 <code>long</code> 类型的数据一定要在数值后面加上 <strong>L</strong>，否则将作为整型解析。</li>
<li>Java 里使用 <code>float</code> 类型的数据一定要在数值后面加上 <strong>f 或 F</strong>，否则将无法通过编译。</li>
<li><code>char a = &#39;h&#39;</code> :单引号，<code>String a = &quot;hello&quot;</code> :双引号。</li>
</ol>
<p>这八种基本类型都有对应的包装类分别为：<code>Byte</code>、<code>Short</code>、<code>Integer</code>、<code>Long</code>、<code>Float</code>、<code>Double</code>、<code>Character</code>、<code>Boolean</code> 。</p>
<h2 id="基本类型和包装类型的区别？"><a href="#基本类型和包装类型的区别？" class="headerlink" title="基本类型和包装类型的区别？"></a>基本类型和包装类型的区别？</h2><ul>
<li><strong>用途</strong>：除了<strong>定义一些常量和局部变量</strong>之外，我们在其他地方比如<strong>方法参数、对象属性中很少会使用基本类型来定义变量</strong>。并且，包装类型可用于泛型，而基本类型不可以。</li>
<li><strong>存储方式</strong>：基本数据类型的局部变量存放在 <strong>Java 虚拟机栈中的局部变量表</strong>中，基本数据类型的成员变量（未被 <code>static</code> 修饰 ）<strong>存放在 Java 虚拟机的堆中</strong>。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中。</li>
<li><strong>占用空间</strong>：相比于包装类型（对象类型）， <strong>基本数据类型占用的空间往往非常小</strong>。</li>
<li><strong>默认值</strong>：成员变量包装类型不赋值就是 <code>null</code> ，而基本类型有默认值且不是 <code>null</code>。</li>
<li><strong>比较方式</strong>：对于基本数据类型来说，<code>==</code> 比较的是值。<strong>对于包装数据类型来说，<code>==</code> 比较的是对象的内存地址。所有整型包装类对象之间值的比较，全部使用 <code>equals()</code> 方法</strong>。</li>
</ul>
<p><strong>为什么说是几乎所有对象实例都存在于堆中呢？</strong> 这是因为 HotSpot 虚拟机引入了 JIT 优化之后，<strong>会对对象进行逃逸分析，如果发现某一个对象并没有逃逸到方法外部，那么就可能通过标量替换来实现栈上分配</strong>，而避免堆上分配内存。</p>
<p>⚠️ 注意：<strong>基本数据类型存放在栈中是一个常见的误区！</strong> 基本数据类型的存储位置<strong>取决于它们的作用域和声明方式</strong>。<strong>如果它们是局部变量，那么它们会存放在栈中；如果它们是成员变量，那么它们会存放在堆中</strong>。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span> &#123;</span><br><span class="line">    <span class="comment">// 成员变量，存放在堆中</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">a</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">    <span class="comment">// 被 static 修饰，也存放在堆中，但属于类，不属于对象</span></span><br><span class="line">    <span class="comment">// JDK1.7 静态变量从永久代移动了 Java 堆中</span></span><br><span class="line">    <span class="keyword">static</span> <span class="type">int</span> <span class="variable">b</span> <span class="operator">=</span> <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">method</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 局部变量，存放在栈中</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">c</span> <span class="operator">=</span> <span class="number">30</span>;</span><br><span class="line">        <span class="keyword">static</span> <span class="type">int</span> <span class="variable">d</span> <span class="operator">=</span> <span class="number">40</span>; <span class="comment">// 编译错误，不能在方法中使用 static 修饰局部变量</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="包装类型的缓存机制了解么？"><a href="#包装类型的缓存机制了解么？" class="headerlink" title="包装类型的缓存机制了解么？"></a>包装类型的缓存机制了解么？</h2><p>Java 基本数据类型的包装类型的<strong>大部分</strong>都用到了缓存机制来提升性能。</p>
<p><code>Byte</code>,<code>Short</code>,<code>Integer</code>,<code>Long</code> 这 4 种包装类默认创建了数值 <strong>[-128，127]</strong> 的相应类型的缓存数据，<code>Character</code> 创建了数值在 <strong>[0,127]</strong> 范围的缓存数据，<code>Boolean</code> 直接返回 <code>True</code> or <code>False</code>。</p>
<p><strong><code>Integer</code>缓存源码：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Integer <span class="title function_">valueOf</span><span class="params">(<span class="type">int</span> i)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)</span><br><span class="line">        <span class="keyword">return</span> IntegerCache.cache[i + (-IntegerCache.low)];</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Integer</span>(i);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntegerCache</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">low</span> <span class="operator">=</span> -<span class="number">128</span>;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> high;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="comment">// high value may be configured by property</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">h</span> <span class="operator">=</span> <span class="number">127</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong><code>Character</code> 缓存源码:</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Character <span class="title function_">valueOf</span><span class="params">(<span class="type">char</span> c)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (c &lt;= <span class="number">127</span>) &#123; <span class="comment">// must cache</span></span><br><span class="line">      <span class="keyword">return</span> CharacterCache.cache[(<span class="type">int</span>)c];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Character</span>(c);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">CharacterCache</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">CharacterCache</span><span class="params">()</span>&#123;&#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> Character cache[] = <span class="keyword">new</span> <span class="title class_">Character</span>[<span class="number">127</span> + <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; cache.length; i++)</span><br><span class="line">            cache[i] = <span class="keyword">new</span> <span class="title class_">Character</span>((<span class="type">char</span>)i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong><code>Boolean</code> 缓存源码：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Boolean <span class="title function_">valueOf</span><span class="params">(<span class="type">boolean</span> b)</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> (b ? TRUE : FALSE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>如果超出对应范围仍然会去创建新的对象，缓存的范围区间的大小只是在性能和资源之间的权衡。</strong></p>
<p>两种浮点数类型的包装类 <code>Float</code>,<code>Double</code> 并<strong>没有</strong>实现缓存机制。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Integer</span> <span class="variable">i1</span> <span class="operator">=</span> <span class="number">33</span>;</span><br><span class="line"><span class="type">Integer</span> <span class="variable">i2</span> <span class="operator">=</span> <span class="number">33</span>;</span><br><span class="line">System.out.println(i1 == i2);<span class="comment">// 输出 true</span></span><br><span class="line"></span><br><span class="line"><span class="type">Float</span> <span class="variable">i11</span> <span class="operator">=</span> <span class="number">333f</span>;</span><br><span class="line"><span class="type">Float</span> <span class="variable">i22</span> <span class="operator">=</span> <span class="number">333f</span>;</span><br><span class="line">System.out.println(i11 == i22);<span class="comment">// 输出 false</span></span><br><span class="line"></span><br><span class="line"><span class="type">Double</span> <span class="variable">i3</span> <span class="operator">=</span> <span class="number">1.2</span>;</span><br><span class="line"><span class="type">Double</span> <span class="variable">i4</span> <span class="operator">=</span> <span class="number">1.2</span>;</span><br><span class="line">System.out.println(i3 == i4);<span class="comment">// 输出 false</span></span><br></pre></td></tr></table></figure>

<p><strong>所有整型包装类对象之间值的比较，全部使用 equals 方法比较</strong>。具体可以看看阿里巴巴开发规范的说法。</p>
<p><img src="/2024/09/19/Java01/up-1ae0425ce8646adfb768b5374951eeb820d.png" alt="img"></p>
<h2 id="自动装箱与拆箱了解吗？原理是什么？"><a href="#自动装箱与拆箱了解吗？原理是什么？" class="headerlink" title="自动装箱与拆箱了解吗？原理是什么？"></a>自动装箱与拆箱了解吗？原理是什么？</h2><p><strong>什么是自动拆装箱？</strong></p>
<ul>
<li><strong>装箱</strong>：将<strong>基本类型用它们对应的引用类型包装起来</strong>；</li>
<li><strong>拆箱</strong>：将<strong>包装类型转换为基本数据类型</strong>；</li>
</ul>
<p>举例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Integer</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">10</span>;  <span class="comment">//装箱</span></span><br><span class="line"><span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> i;   <span class="comment">//拆箱</span></span><br></pre></td></tr></table></figure>

<p>上面这两行代码对应的字节码为：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">L1</span><br><span class="line"></span><br><span class="line"> LINENUMBER <span class="number">8</span> L1</span><br><span class="line"></span><br><span class="line"> ALOAD <span class="number">0</span></span><br><span class="line"></span><br><span class="line"> BIPUSH <span class="number">10</span></span><br><span class="line"></span><br><span class="line"> INVOKESTATIC java/lang/Integer.valueOf (I)Ljava/lang/Integer;</span><br><span class="line"></span><br><span class="line"> PUTFIELD AutoBoxTest.i : Ljava/lang/Integer;</span><br><span class="line"></span><br><span class="line">L2</span><br><span class="line"></span><br><span class="line"> LINENUMBER <span class="number">9</span> L2</span><br><span class="line"></span><br><span class="line"> ALOAD <span class="number">0</span></span><br><span class="line"></span><br><span class="line"> ALOAD <span class="number">0</span></span><br><span class="line"></span><br><span class="line"> GETFIELD AutoBoxTest.i : Ljava/lang/Integer;</span><br><span class="line"></span><br><span class="line"> INVOKEVIRTUAL java/lang/Integer.intValue ()I</span><br><span class="line"></span><br><span class="line"> PUTFIELD AutoBoxTest.n : I</span><br><span class="line"></span><br><span class="line"> RETURN</span><br></pre></td></tr></table></figure>

<p>从字节码中，我们发现装箱其实就是调用了 包装类的<code>valueOf()</code>方法，拆箱其实就是调用了 <code>xxxValue()</code>方法。</p>
<p>因此，</p>
<ul>
<li><code>Integer i = 10</code> 等价于 <code>Integer i = Integer.valueOf(10)</code></li>
<li><code>int n = i</code> 等价于 <code>int n = i.intValue()</code>;</li>
</ul>
<p>注意：<strong>如果频繁拆装箱的话，也会严重影响系统的性能。我们应该尽量避免不必要的拆装箱操作。</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">long</span> <span class="title function_">sum</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 应该使用 long 而不是 Long</span></span><br><span class="line">    <span class="type">Long</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0L</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">long</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt;= Integer.MAX_VALUE; i++)</span><br><span class="line">        sum += i;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>还真是，我之前看到一个项目遇到的瓶颈就是有几十万个装箱拆箱的操作导致系统的性能极低，<strong>怎么说呢，不在循环里面使用装箱拆箱</strong>一般都还好。</p>
<h2 id="如何解决浮点数运算的精度丢失问题？"><a href="#如何解决浮点数运算的精度丢失问题？" class="headerlink" title="如何解决浮点数运算的精度丢失问题？"></a>如何解决浮点数运算的精度丢失问题？</h2><p><code>BigDecimal</code> 可以实现对浮点数的运算，不会造成精度丢失。通常情况下，大部分需要浮点数精确运算结果的业务场景（比如涉及到钱的场景）都是通过 <code>BigDecimal</code> 来做的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">BigDecimal</span> <span class="variable">a</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BigDecimal</span>(<span class="string">&quot;1.0&quot;</span>);</span><br><span class="line"><span class="type">BigDecimal</span> <span class="variable">b</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BigDecimal</span>(<span class="string">&quot;1.00&quot;</span>);</span><br><span class="line"><span class="type">BigDecimal</span> <span class="variable">c</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BigDecimal</span>(<span class="string">&quot;0.8&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="type">BigDecimal</span> <span class="variable">x</span> <span class="operator">=</span> a.subtract(c);</span><br><span class="line"><span class="type">BigDecimal</span> <span class="variable">y</span> <span class="operator">=</span> b.subtract(c);</span><br><span class="line"></span><br><span class="line">System.out.println(x); <span class="comment">/* 0.2 */</span></span><br><span class="line">System.out.println(y); <span class="comment">/* 0.20 */</span></span><br><span class="line"><span class="comment">// 比较内容，不是比较值</span></span><br><span class="line">System.out.println(Objects.equals(x, y)); <span class="comment">/* false */</span></span><br><span class="line"><span class="comment">// 比较值相等用相等compareTo，相等返回0</span></span><br><span class="line">System.out.println(<span class="number">0</span> == x.compareTo(y)); <span class="comment">/* true */</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个了解一下就行，不是金融类型的业务不需要这么高的精度。</p>
<h2 id="成员变量与局部变量的区别？"><a href="#成员变量与局部变量的区别？" class="headerlink" title="成员变量与局部变量的区别？"></a>成员变量与局部变量的区别？</h2><ul>
<li><strong>语法形式</strong>：从语法形式上看，<strong>成员变量是属于类的，而局部变量是在代码块或方法中定义的变量或是方法的参数</strong>；成员变量可以被 <code>public</code>,<code>private</code>,<code>static</code> 等修饰符所修饰，而局部变量不能被访问控制修饰符及 <code>static</code> 所修饰；<strong>但是，成员变量和局部变量都能被 <code>final</code> 所修饰</strong>。</li>
<li><strong>存储方式</strong>：从变量在内存中的存储方式来看，如果成员变量是使用 <code>static</code> 修饰的，那么这个成员变量是属于类的，如果没有使用 <code>static</code> 修饰，这个成员变量是属于实例的。<strong>而对象存在于堆内存，局部变量则存在于栈内存</strong>。</li>
<li><strong>生存时间</strong>：从变量在内存中的生存时间上看，<strong>成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动生成，随着方法的调用结束而消亡</strong>。</li>
<li><strong>默认值</strong>：从变量是否有默认值来看，<strong>成员变量如果没有被赋初始值，则会自动以类型的默认值而赋值（一种情况例外:被 <code>final</code> 修饰的成员变量也必须显式地赋值）</strong>，而局部变量则不会自动赋值。</li>
</ul>
<p><strong>为什么成员变量有默认值？</strong></p>
<ol>
<li>先不考虑变量类型，如果没有默认值会怎样？<strong>变量存储的是内存地址对应的任意随机值，程序读取该值运行会出现意外</strong>。</li>
<li>默认值有两种设置方式：<strong>手动和自动，根据第一点，没有手动赋值一定要自动赋值</strong>。<strong>成员变量在运行时可借助反射等方法手动赋值，而局部变量不行</strong>。</li>
<li>对于编译器（javac）来说，局部变量没赋值很好判断，可以直接报错。<strong>而成员变量可能是运行时赋值，无法判断，误报“没默认值”又会影响用户体验，所以采用自动赋默认值</strong>。</li>
</ol>
<h2 id="静态变量有什么作用？"><a href="#静态变量有什么作用？" class="headerlink" title="静态变量有什么作用？"></a>静态变量有什么作用？</h2><p>静态变量也就是被 <code>static</code> 关键字修饰的变量。<strong>它可以被类的所有实例共享，无论一个类创建了多少个对象，它们都共享同一份静态变量</strong>。也就是说，静态变量只会被分配一次内存，即使创建多个对象，这样可以节省内存。</p>
<p>静态变量是通过类名来访问的，例如<code>StaticVariableExample.staticVar</code>（如果被 <code>private</code>关键字修饰就无法这样访问了）。</p>
<p>通常情况下，静态变量会被 <code>final</code> 关键字修饰成为常量。</p>
<p>这个说的还可以，主要是避免static的值被误操作了。</p>
<h2 id="字符型常量和字符串常量的区别"><a href="#字符型常量和字符串常量的区别" class="headerlink" title="字符型常量和字符串常量的区别?"></a>字符型常量和字符串常量的区别?</h2><ul>
<li><strong>形式</strong> : 字符常量是单引号引起的一个字符，字符串常量是双引号引起的 0 个或若干个字符。</li>
<li><strong>含义</strong> : 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; **字符串常量代表一个地址值(该字符串在内存中存放位置)**。</li>
<li><strong>占内存大小</strong>：字符常量只占 <strong>2 个</strong>字节; 字符串常量占若干个字节。</li>
</ul>
<p>⚠️ 注意 <code>char</code> 在 Java 中<strong>占两个字节</strong>。我记得在C语言里面是只占一个字节的。</p>
<p>字符型常量和字符串常量代码示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">StringExample</span> &#123;</span><br><span class="line">    <span class="comment">// 字符型常量</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">char</span> <span class="variable">LETTER_A</span> <span class="operator">=</span> <span class="string">&#x27;A&#x27;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 字符串常量</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">GREETING_MESSAGE</span> <span class="operator">=</span> <span class="string">&quot;Hello, world!&quot;</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;字符型常量占用的字节数为：&quot;</span>+Character.BYTES);</span><br><span class="line">        System.out.println(<span class="string">&quot;字符串常量占用的字节数为：&quot;</span>+GREETING_MESSAGE.getBytes().length);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">字符型常量占用的字节数为：2</span><br><span class="line">字符串常量占用的字节数为：13</span><br></pre></td></tr></table></figure>

<h2 id="静态方法为什么不能调用非静态成员"><a href="#静态方法为什么不能调用非静态成员" class="headerlink" title="静态方法为什么不能调用非静态成员?"></a>静态方法为什么不能调用非静态成员?</h2><p>这个需要结合 JVM 的相关知识，主要原因如下：</p>
<ol>
<li>静态方法是属于类的，<strong>在类加载的时候就会分配内存，可以通过类名直接访问</strong>。而非静态成员属于实例对象，<strong>只有在对象实例化之后才存在，需要通过类的实例对象去访问</strong>。</li>
<li><strong>在类的非静态成员不存在的时候静态方法就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作</strong>。相当于是调用了不存在的东西，所以肯定会报错啊。所以很多时候不是它不想这么设计优化，<strong>而是这样操作会产生未定义的行为</strong>。</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Example</span> &#123;</span><br><span class="line">    <span class="comment">// 定义一个字符型常量</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">char</span> <span class="variable">LETTER_A</span> <span class="operator">=</span> <span class="string">&#x27;A&#x27;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义一个字符串常量</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">GREETING_MESSAGE</span> <span class="operator">=</span> <span class="string">&quot;Hello, world!&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 输出字符型常量的值</span></span><br><span class="line">        System.out.println(<span class="string">&quot;字符型常量的值为：&quot;</span> + LETTER_A);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出字符串常量的值</span></span><br><span class="line">        System.out.println(<span class="string">&quot;字符串常量的值为：&quot;</span> + GREETING_MESSAGE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="静态方法和实例方法有何不同？"><a href="#静态方法和实例方法有何不同？" class="headerlink" title="静态方法和实例方法有何不同？"></a>静态方法和实例方法有何不同？</h2><p><strong>1、调用方式</strong></p>
<p>在外部调用静态方法时，可以使用 <code>类名.方法名</code> 的方式，也可以使用 <code>对象.方法名</code> 的方式，<strong>而实例方法只有后面这种方式</strong>。也就是说，<strong>调用静态方法可以无需创建对象</strong> 。</p>
<p>不过，需要注意的是一般不建议使用 <code>对象.方法名</code> 的方式来调用静态方法。这种方式非常容易造成混淆，静态方法不属于类的某个对象而是属于这个类。</p>
<p>因此，一般建议使用 <code>类名.方法名</code> 的方式来调用静态方法。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Person</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">method</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="comment">//......</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">staicMethod</span><span class="params">()</span>&#123;</span><br><span class="line">      <span class="comment">//......</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Person</span> <span class="variable">person</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Person</span>();</span><br><span class="line">        <span class="comment">// 调用实例方法</span></span><br><span class="line">        person.method();</span><br><span class="line">        <span class="comment">// 调用静态方法</span></span><br><span class="line">        Person.staicMethod()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>2、访问类成员是否存在限制</strong></p>
<p><strong>静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），不允许访问实例成员（即实例成员变量和实例方法）</strong>，而实例方法不存在这个限制。</p>
<p>这个上面已经解释过了，其实就是生命周期顺序导致的，前者先出生，不能预知未来的事情，不能访问到后面实例化才有的数据。</p>
<h2 id="重载和重写有什么区别？"><a href="#重载和重写有什么区别？" class="headerlink" title="重载和重写有什么区别？"></a>重载和重写有什么区别？</h2><blockquote>
<p>重载就是<strong>同样的一个方法能够根据输入数据的不同，做出不同的处理</strong></p>
<p>重写就是当子类继承自父类的相同方法，<strong>输入数据一样，但要做出有别于父类的响应时，你就要覆盖父类方法</strong></p>
</blockquote>
<h2 id="重载"><a href="#重载" class="headerlink" title="重载"></a>重载</h2><p>发生在同一个类中（或者父类和子类之间），<strong>方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同</strong>。</p>
<p>《Java 核心技术》这本书是这样介绍重载的：</p>
<blockquote>
<p>如果多个方法(比如 <code>StringBuilder</code> 的构造方法)<strong>有相同的名字、不同的参数， 便产生了重载</strong>。</p>
<p>编译器必须挑选出具体执行哪个方法，<strong>它通过用各个方法给出的参数类型与特定方法调用所使用的值类型进行匹配来挑选出相应的方法</strong>。 如果编译器找不到匹配的参数， **就会产生编译时错误， 因为根本不存在匹配， 或者没有一个比其他的更好(这个过程被称为重载解析(overloading resolution))**。</p>
<p>Java 允许重载任何方法， 而不只是构造器方法。</p>
</blockquote>
<p>综上：<strong>重载就是同一个类中多个同名方法根据不同的传参来执行不同的逻辑处理</strong>。</p>
<h2 id="重写"><a href="#重写" class="headerlink" title="重写"></a>重写</h2><p>重写发生在运行期，<strong>是子类对父类的允许访问的方法的实现过程进行重新编写</strong>。</p>
<ol>
<li>方法名、参数列表必须相同，子类方法返回值类型应比父类方法返回值类型更小或相等，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类。</li>
<li>如果父类方法访问修饰符为 <code>private/final/static</code> 则子类就不能重写该方法，但是被 <code>static</code> 修饰的方法能够被再次声明。</li>
<li><strong>构造方法无法被重写</strong></li>
</ol>
<p>一般在项目中配合 @Override 注解使用。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综上：<strong>重写就是子类对父类方法的重新改造，外部样子不能改变，内部逻辑可以改变。</strong></p>
<table>
<thead>
<tr>
<th align="left">区别点</th>
<th align="left">重载方法</th>
<th align="left">重写方法</th>
</tr>
</thead>
<tbody><tr>
<td align="left">发生范围</td>
<td align="left">同一个类</td>
<td align="left">子类</td>
</tr>
<tr>
<td align="left">参数列表</td>
<td align="left">必须修改</td>
<td align="left">一定不能修改</td>
</tr>
<tr>
<td align="left">返回类型</td>
<td align="left">可修改</td>
<td align="left">子类方法返回值类型应比父类方法返回值类型更小或相等</td>
</tr>
<tr>
<td align="left">异常</td>
<td align="left">可修改</td>
<td align="left">子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等；</td>
</tr>
<tr>
<td align="left">访问修饰符</td>
<td align="left">可修改</td>
<td align="left">一定不能做更严格的限制（可以降低限制）</td>
</tr>
<tr>
<td align="left">发生阶段</td>
<td align="left">编译期</td>
<td align="left">运行期</td>
</tr>
</tbody></table>
<p><strong>方法的重写要遵循“两同两小一大”</strong>（以下内容摘录自《疯狂 Java 讲义》，<a href="https://github.com/Snailclimb/JavaGuide/issues/892">issue#892open in new window</a> ）：</p>
<ul>
<li>“两同”即<strong>方法名相同、形参列表相同</strong>；</li>
<li>“两小”指的是<strong>子类方法返回值类型应比父类方法返回值类型更小或相等，子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等</strong>；</li>
<li>“一大”指的是<strong>子类方法的访问权限应比父类方法的访问权限更大或相等</strong>。</li>
</ul>
<p>⭐️ 关于 <strong>重写的返回值类型</strong> 这里需要额外多说明一下，上面的表述不太清晰准确：如果方法的返回类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写时是可以返回该引用类型的子类的。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hero</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">name</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;超级英雄&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SuperMan</span> <span class="keyword">extends</span> <span class="title class_">Hero</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">name</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;超人&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> Hero <span class="title function_">hero</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Hero</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SuperSuperMan</span> <span class="keyword">extends</span> <span class="title class_">SuperMan</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">name</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;超级超级英雄&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> SuperMan <span class="title function_">hero</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">SuperMan</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="什么是可变长参数？"><a href="#什么是可变长参数？" class="headerlink" title="什么是可变长参数？"></a>什么是可变长参数？</h2><p>从 Java5 开始，Java 支持定义可变长参数，所谓可变长参数就是允许在调用方法时传入不定长度的参数。就比如下面这个方法就可以接受 0 个或者多个参数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">method1</span><span class="params">(String... args)</span> &#123;</span><br><span class="line">   <span class="comment">//......</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>另外，可变参数只能作为函数的最后一个参数，但其前面可以有也可以没有任何其他参数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">method2</span><span class="params">(String arg1, String... args)</span> &#123;</span><br><span class="line">   <span class="comment">//......</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>遇到方法重载的情况怎么办呢？会优先匹配固定参数还是可变参数的方法呢？</strong></p>
<p>答案是会<strong>优先匹配固定参数的方法</strong>，因为<strong>固定参数的方法匹配度更高</strong>。</p>
<p>我们通过下面这个例子来证明一下。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 微信搜 JavaGuide 回复&quot;面试突击&quot;即可免费领取个人原创的 Java 面试手册</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Guide哥</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2021/12/13 16:52</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">VariableLengthArgument</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">printVariable</span><span class="params">(String... args)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (String s : args) &#123;</span><br><span class="line">            System.out.println(s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">printVariable</span><span class="params">(String arg1, String arg2)</span> &#123;</span><br><span class="line">        System.out.println(arg1 + arg2);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        printVariable(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>);</span><br><span class="line">        printVariable(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ab</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line">c</span><br><span class="line">d</span><br></pre></td></tr></table></figure>

<p>另外，Java 的可变参数编译后实际会被转换成一个数组，我们看编译后生成的 <code>class</code>文件就可以看出来了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">VariableLengthArgument</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">printVariable</span><span class="params">(String... args)</span> &#123;</span><br><span class="line">        String[] var1 = args;</span><br><span class="line">        <span class="type">int</span> <span class="variable">var2</span> <span class="operator">=</span> args.length;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">var3</span> <span class="operator">=</span> <span class="number">0</span>; var3 &lt; var2; ++var3) &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> var1[var3];</span><br><span class="line">            System.out.println(s);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ......</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>参考文章：<a href="https://javaguide.cn/java/basis/java-basic-questions-01.html#jvm-vs-jdk-vs-jre">Java基础常见面试题总结(上) | JavaGuide</a></p>
]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>程序实体的那些事儿（中）</title>
    <url>/2024/09/19/Golang05/</url>
    <content><![CDATA[<p>在前文中，我解释过代码块的含义。<strong>Go 语言的代码块是一层套一层的，就像大圆套小圆。一个代码块可以有若干个子代码块；但对于每个代码块，最多只会有一个直接包含它的代码块（后者可以简称为前者的外层代码块）。</strong></p>
<p>这种代码块的划分，<strong>也间接地决定了程序实体的作用域</strong>。我们今天就来看看它们之间的关系。</p>
<p>先说说作用域是什么呢？<strong>我们都知道，一个程序实体被创造出来大都是为了让别的代码引用的。那么，哪里的代码可以引用它？这就是在讨论它的作用域。</strong></p>
<p>我在前面说过，<strong>程序实体的访问权限有三种：包级私有的、模块级私有的和公开的</strong>。这其实就是 Go 语言在语言层面，依据代码块对程序实体的作用域进行的定义。</p>
<p>包级私有和模块级私有访问权限对应的都是<strong>代码包代码块</strong>，公开的访问权限对应的是<strong>全域代码块</strong>。然而，这个颗粒度是比较粗的，我们往往需要利用代码块再细化程序实体的作用域。</p>
<p><strong>比如，我在一个函数中声明了一个变量，那么在通常情况下，这个变量是无法被这个函数以外的代码引用的。这个函数就是一个代码块，而这个变量的作用域被限制在了该代码块中。</strong>当然了，还有例外的情况，这部分内容，我留到讲函数的时候再说。</p>
<p>总之，请记住，<strong>一个程序实体的作用域总是会被限制在某个代码块中，而这个作用域最大的用处，就是对程序实体的访问权限的控制。</strong>对“高内聚，低耦合”这种程序设计思想的实践恰恰可以从这里开始。</p>
<p>你应该可以通过下面的问题进一步感受到代码块和作用域的魅力。</p>
<p><strong>今天的问题是：如果一个变量与其外层代码块中的变量重名会出现什么状况？</strong></p>
<p>我把此题的代码存到了 demo10.go 文件中了。你可以在“Golang_Puzzlers”项目的<code>puzzlers/article5/q1</code>包中找到它。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> block = <span class="string">&quot;package&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">block := <span class="string">&quot;function&quot;</span></span><br><span class="line">&#123;</span><br><span class="line">block := <span class="string">&quot;inner&quot;</span></span><br><span class="line">fmt.Printf(<span class="string">&quot;The block is %s.\n&quot;</span>, block)</span><br><span class="line">&#125;</span><br><span class="line">fmt.Printf(<span class="string">&quot;The block is %s.\n&quot;</span>, block)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个命令源码文件中有四个代码块，它们是：<strong>全域代码块、<code>main</code>包代表的代码块、<code>main</code>函数代表的代码块，以及在<code>main</code>函数中的一个用花括号包起来的代码块</strong>。</p>
<p>我在后三个代码块中分别声明了一个名为<code>block</code>的变量，并分别把字符串值<code>&quot;package&quot;</code>、<code>&quot;function&quot;</code>和<code>&quot;inner&quot;</code>赋给了它们。此外，我在后两个代码块的最后分别尝试用<code>fmt.Printf</code>函数打印出“The block is %s.”。这里的“%s”只是为了占位，程序会用<code>block</code>变量的实际值替换掉。</p>
<p>具体的问题是：该源码文件中的代码能通过编译吗？如果不能，原因是什么？如果能，运行它后会打印出什么内容？</p>
<h3 id="典型回答"><a href="#典型回答" class="headerlink" title="典型回答"></a>典型回答</h3><p>能通过编译。运行后打印出的内容是：</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line">The block is inner.</span><br><span class="line">The block is function.</span><br></pre></td></tr></table></figure>

<h3 id="问题解析"><a href="#问题解析" class="headerlink" title="问题解析"></a>问题解析</h3><p>初看这道题，你可能会认为它无法通过编译，因为三处代码都声明了相同名称的变量。<strong>的确，声明重名的变量是无法通过编译的，用短变量声明对已有变量进行重声明除外，但这只是对于同一个代码块而言的</strong>。</p>
<p><strong>对于不同的代码块来说，其中的变量重名没什么大不了，照样可以通过编译</strong>。即使这些代码块有直接的嵌套关系也是如此，就像 demo10.go 中的<code>main</code>包代码块、<code>main</code>函数代码块和那个最内层的代码块那样。</p>
<p>这样规定显然很方便也很合理，否则我们会每天为了选择变量名而烦恼。<strong>但是这会导致另外一个问题，我引用变量时到底用的是哪一个？这也是这道题的第二个考点</strong>。</p>
<p>这其实有一个很有画面感的查找过程。<strong>这个查找过程不只针对于变量，还适用于任何程序实体</strong>。如下面所示。</p>
<ul>
<li><strong>首先，代码引用变量的时候总会最优先查找当前代码块中的那个变量。</strong>注意，这里的“当前代码块”仅仅是引用变量的代码所在的那个代码块，<strong>并不包含任何子代码块。</strong></li>
<li>其次，如果当前代码块中没有声明以此为名的变量，<strong>那么程序会沿着代码块的嵌套关系，从直接包含当前代码块的那个代码块开始，一层一层地查找。</strong></li>
<li>一般情况下，<strong>程序会一直查到当前代码包代表的代码块。如果仍然找不到，那么 Go 语言的编译器就会报错了。</strong></li>
</ul>
<p>还记得吗？<strong>如果我们在当前源码文件中导入了其他代码包，那么引用其中的程序实体时，是需要以限定符为前缀的。所以程序在找代表变量未加限定符的名字（即标识符）的时候，是不会去被导入的代码包中查找的</strong>。</p>
<blockquote>
<p>但有个特殊情况，如果我们把代码包导入语句写成<code>import . XXX</code>的形式（<strong>注意中间的那个“.”</strong>），那么就会让这个“XXX”包中公开的程序实体，被当前源码文件中的代码，视为当前代码包中的程序实体。</p>
<p>比如，如果有代码包导入语句<code>import . fmt</code>，那么我们在当前源码文件中引用<code>fmt.Printf</code>函数的时候直接用<code>Printf</code>就可以了。<strong>在这个特殊情况下，程序在查找当前源码文件后会先去查用这种方式导入的那些代码包。</strong></p>
</blockquote>
<p>好了，当你明白了上述过程之后，再去看 demo10.go 中的代码。是不是感觉清晰了很多？</p>
<p>从作用域的角度也可以说，<strong>虽然通过<code>var block = &quot;package&quot;</code>声明的变量作用域是整个<code>main</code>代码包，但是在<code>main</code>函数中，它却被那两个同名的变量“屏蔽”了</strong>。</p>
<p>相似的，<strong>虽然<code>main</code>函数首先声明的<code>block</code>的作用域，是整个<code>main</code>函数，但是在最内层的那个代码块中，它却是不可能被引用到的</strong>。反过来讲，<strong>最内层代码块中的<code>block</code>也不可能被该块之外的代码引用到，这也是打印内容的第二行是“The block is function.”的另一半原因</strong>。</p>
<p>你现在应该知道了，这道题看似简单，但是它考察以及可延展的范围并不窄。</p>
<h3 id="知识扩展"><a href="#知识扩展" class="headerlink" title="知识扩展"></a>知识扩展</h3><p><strong>不同代码块中的重名变量与变量重声明中的变量区别到底在哪儿？</strong></p>
<p>为了方便描述，<strong>我就把前者叫做“可重名变量”吧。注意，在同一个代码块中不允许出现重名的变量，这违背了 Go 语言的语法</strong>。关于这两者的表象和机理，我们已经讨论得足够充分了。你现在可以说出几条区别？请想一想，然后再看下面的列表。</p>
<ol>
<li><strong>变量重声明中的变量一定是在某一个代码块内的</strong>。注意，这里的“某一个代码块内”并不包含它的任何子代码块，否则就变成了“多个代码块之间”。而可重名变量指的正是在多个代码块之间由相同的标识符代表的变量。</li>
<li><strong>变量重声明是对同一个变量的多次声明，这里的变量只有一个</strong>。而可重名变量中涉及的变量肯定是有多个的。</li>
<li><strong>不论对变量重声明多少次，其类型必须始终一致，具体遵从它第一次被声明时给定的类型</strong>。而可重名变量之间不存在类似的限制，它们的类型可以是任意的。</li>
<li><strong>如果可重名变量所在的代码块之间存在直接或间接的嵌套关系，那么它们之间一定会存在“屏蔽”的现象</strong>。但是这种现象绝对不会在变量重声明的场景下出现。</li>
</ol>
<p>当然了，我们之前谈论过，对变量进行重声明还有一些前提条件，不过在这里并不是重点。我就不再赘述了。</p>
<p>以上 4 大区别中的第 3 条需要你再注意一下。<strong>既然可重名变量的类型可以是任意的，那么当它们之间存在“屏蔽”时你就更需要注意了。</strong></p>
<p>不同类型的值大都有着不同的特性和用法。<strong>当你在某一种类型的值上施加只有在其他类型值上才能做的操作时，Go 语言编译器一定会告诉你：“这不可以”</strong>。</p>
<p>这种情况很好，甚至值得庆幸，因为你的程序存在的问题被提前发现了。如若不然，程序没准儿会在运行过程中由此引发很隐晦的问题，让你摸不着头脑。相比之下，那时候排查问题的成本可就太高了。所以，我们应该尽量利用 Go 语言的语法、规范和命令来约束我们的程序。</p>
<p>具体到不同类型的可重名变量的问题上，让我们先来看一下<code>puzzlers/article5/q2</code>包中的源码文件 demo11.go。它是一个很典型的例子。</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line">gogo</span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> container = []<span class="type">string</span>&#123;<span class="string">&quot;zero&quot;</span>, <span class="string">&quot;one&quot;</span>, <span class="string">&quot;two&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">container := <span class="keyword">map</span>[<span class="type">int</span>]<span class="type">string</span>&#123;<span class="number">0</span>: <span class="string">&quot;zero&quot;</span>, <span class="number">1</span>: <span class="string">&quot;one&quot;</span>, <span class="number">2</span>: <span class="string">&quot;two&quot;</span>&#125;</span><br><span class="line">fmt.Printf(<span class="string">&quot;The element is %q.\n&quot;</span>, container[<span class="number">1</span>])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在 demo11.go 中，<strong>有两个都叫做<code>container</code>的变量，分别位于<code>main</code>包代码块和<code>main</code>函数代码块</strong>。<code>main</code>包代码块中的<strong>变量是切片（slice）类型的，另一个是字典（map）类型的</strong>。在<code>main</code>函数的最后，我试图打印出<code>container</code>变量的值中索引为<code>1</code>的那个元素。</p>
<p>如果你熟悉这两个类型肯定会知道，在它们的值上我们都可以施加索引表达式，比如<code>container[0]</code>。只要中括号里的整数在有效范围之内（这里是 [0, 2]），它就可以把值中的某一个元素取出来。</p>
<p><strong>如果<code>container</code>的类型不是数组、切片或字典类型，那么索引表达式就会引发编译错误。这正是利用 Go 语言语法，帮我们约束程序的一个例子；但是当我们想知道 container 确切类型的时候，利用索引表达式的方式就不够了</strong>。</p>
<p><strong>当可重名变量的值被转换成某个接口类型值，或者它们的类型本身就是接口类型的时候，严格的类型检查就很有必要了。</strong>至于怎么检查，我们在下篇文章中再讨论。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们先讨论了代码块，并且也谈到了它与程序实体的作用域，以及访问权限控制之间的巧妙关系。<strong>Go 语言本身对程序实体提供了相对粗粒度的访问控制。但我们自己可以利用代码块和作用域精细化控制它们。</strong></p>
<p><strong>如果在具有嵌套关系的不同代码块中存在重名的变量，那么我们应该特别小心，它们之间可能会发生“屏蔽”的现象。</strong>这样你在不同代码块中引用到变量很可能是不同的。具体的鉴别方式需要参考 Go 语言查找（代表了程序实体的）标识符的过程。</p>
<p>另外，<strong>请记住变量重声明与可重名变量之间的区别以及它们的重要特征</strong>。其中最容易产生隐晦问题的一点是，<strong>可重名变量可以各有各的类型</strong>。<strong>这时候我们往往应该在真正使用它们之前先对其类型进行检查。利用 Go 语言的语法、规范和命令做辅助的检查是很好的办法，但有些时候并不充分</strong>。</p>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><p>我们在讨论 Go 语言查找标识符时的范围的时候，提到过<code>import . XXX</code>这种导入代码包的方式。这里有个思考题：</p>
<p>如果通过这种方式导入的代码包中的变量与当前代码包中的变量重名了，那么 Go 语言是会把它们当做“可重名变量”看待还是会报错呢？</p>
<p>其实我们写个例子一试便知，但重点是为什么？请你尝试从代码块和作用域的角度解释试验得到的答案。</p>
<p>参考文章：<a href="https://jums.gitbook.io/36-lectures-on-golang/05-cheng-xu-shi-ti-de-na-xie-shi-er-zhong">05 | 程序实体的那些事儿（中） | Go语言核心36讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Java基础知识大全（二）</title>
    <url>/2024/09/19/Java02/</url>
    <content><![CDATA[<h2 id="面向对象和面向过程的区别"><a href="#面向对象和面向过程的区别" class="headerlink" title="面向对象和面向过程的区别"></a>面向对象和面向过程的区别</h2><p>面向过程编程（Procedural-Oriented Programming，POP）和面向对象编程（Object-Oriented Programming，OOP）是两种常见的编程范式，两者的主要区别在于解决问题的方式不同：</p>
<ul>
<li><strong>面向过程编程（POP）</strong>：面向过程把解决问题的过程拆成一个个方法，通过一个个方法的执行解决问题。</li>
<li><strong>面向对象编程（OOP）</strong>：面向对象会先抽象出对象，然后用对象执行方法的方式解决问题。</li>
</ul>
<p>相比较于 POP，OOP 开发的程序一般具有下面这些优点：</p>
<ul>
<li><strong>易维护</strong>：由于良好的结构和封装性，OOP 程序通常更容易维护。</li>
<li><strong>易复用</strong>：通过继承和多态，OOP 设计使得代码更具复用性，方便扩展功能。</li>
<li><strong>易扩展</strong>：模块化设计使得系统扩展变得更加容易和灵活。</li>
</ul>
<p>POP 的编程方式通常更为简单和直接，适合处理一些较简单的任务。</p>
<p>POP 和 OOP 的性能差异主要取决于它们的运行机制，而不仅仅是编程范式本身。因此，简单地比较两者的性能是一个常见的误区（相关 issue : <a href="https://github.com/Snailclimb/JavaGuide/issues/431">面向过程：面向过程性能比面向对象高？？open in new window</a> ）。</p>
<p><img src="/2024/09/19/Java02/pop-vs-oop-performance.png" alt=" POP 和 OOP  性能比较不合适"> </p>
<p><strong>在选择编程范式时，性能并不是唯一的考虑因素。代码的可维护性、可扩展性和开发效率同样重要。</strong></p>
<p><strong>现代编程语言基本都支持多种编程范式，既可以用来进行面向过程编程，也可以进行面向对象编程。</strong></p>
<h2 id="创建一个对象用什么运算符-对象实体与对象引用有何不同"><a href="#创建一个对象用什么运算符-对象实体与对象引用有何不同" class="headerlink" title="创建一个对象用什么运算符?对象实体与对象引用有何不同?"></a>创建一个对象用什么运算符?对象实体与对象引用有何不同?</h2><p>new 运算符，new 创建对象实例（<strong>对象实例在堆内存中</strong>），对象引用指向对象实例（<strong>对象引用存放在栈内存中</strong>）。</p>
<ul>
<li>一个对象引用可以指向 0 个或 1 个对象（一根绳子可以不系气球，也可以系一个气球）；</li>
<li>一个对象可以有 n 个引用指向它（可以用 n 条绳子系住一个气球）。</li>
</ul>
<h2 id="对象的相等和引用相等的区别"><a href="#对象的相等和引用相等的区别" class="headerlink" title="对象的相等和引用相等的区别"></a>对象的相等和引用相等的区别</h2><ul>
<li>对象的相等一般比较的是<strong>内存中存放的内容是否相等</strong>。</li>
<li>引用相等一般比较的是<strong>他们指向的内存地址是否相等</strong>。</li>
</ul>
<p>这里举一个例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">str1</span> <span class="operator">=</span> <span class="string">&quot;hello&quot;</span>;</span><br><span class="line"><span class="type">String</span> <span class="variable">str2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line"><span class="type">String</span> <span class="variable">str3</span> <span class="operator">=</span> <span class="string">&quot;hello&quot;</span>;</span><br><span class="line"><span class="comment">// 使用 == 比较字符串的引用相等</span></span><br><span class="line">System.out.println(str1 == str2);</span><br><span class="line">System.out.println(str1 == str3);</span><br><span class="line"><span class="comment">// 使用 equals 方法比较字符串的相等</span></span><br><span class="line">System.out.println(str1.equals(str2));</span><br><span class="line">System.out.println(str1.equals(str3));</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">false</span><br><span class="line">true</span><br><span class="line">true</span><br><span class="line">true</span><br></pre></td></tr></table></figure>

<p>从上面的代码输出结果可以看出：</p>
<ul>
<li><code>str1</code> 和 <code>str2</code> 不相等，而 <code>str1</code> 和 <code>str3</code> 相等。这是因为 <code>==</code> 运算符比较的是<strong>字符串的引用</strong>是否相等。</li>
<li><code>str1</code>、 <code>str2</code>、<code>str3</code> 三者的内容都相等。<strong>这是因为<code>equals</code> 方法比较的是字符串的内容，即使这些字符串的对象引用不同，只要它们的内容相等，就认为它们是相等的。</strong></li>
</ul>
<p>参考文章：<a href="https://javaguide.cn/java/basis/java-basic-questions-02.html#%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E5%92%8C%E9%9D%A2%E5%90%91%E8%BF%87%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB">Java基础常见面试题总结(中) | JavaGuide</a></p>
]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>一条SQL查询语句是如何执行的？</title>
    <url>/2024/09/09/MySQL01/</url>
    <content><![CDATA[<h1 id="一条SQL查询语句是如何执行的？"><a href="#一条SQL查询语句是如何执行的？" class="headerlink" title="一条SQL查询语句是如何执行的？"></a>一条SQL查询语句是如何执行的？</h1><p>平时我们使用数据库，看到的通常都是一个整体。比如，你有个最简单的表，表里只有一个ID字段，在执行下面这个查询语句时：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from table where ID = 10;</span><br></pre></td></tr></table></figure>

<p>我们看到的只是输入一条语句，返回一个结果，却不知道这条语句在MySQL内部的执行过程。</p>
<p>其实这也和SQL这门语言有关系，SQL这门语言的定位就是一种声明式编程的语言，具体可以看看这个<a href="https://composingprograms.netlify.app/4/3">声明式编程</a>，本来就是不直接描述计算过程，而是描述一些计算的预期结果，比如上面SQL语句的意思是我想从这张表中获得所有ID为10的数据，并将获得到的数据按字段全部映射到结果中。</p>
<p>下面给出的是MySQL的基本架构示意图，从中你可以清楚地看到SQL语句在MySQL的各个功能模块中的执行过程。</p>
<p><img src="/2024/09/09/MySQL01/image-20240909122001902.png" alt="image-20240909122001902"></p>
<p>大体来说，MySQL可以分为Server层和存储引擎层两部分。</p>
<p><strong>Server层</strong>包括<strong>连接器、查询缓存、分析器、优化器、执行器</strong>等，涵盖MySQL的大多数核心服务功能，<strong>以及所有的内置函数（如日期、时间、数学和加密函数等）</strong>，<strong>所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等</strong>。</p>
<p>而<strong>存储引擎层</strong>负责数据的<strong>存储和提取</strong>。其架构模式是<strong>插件式的</strong>，<strong>支持InnoDB、MyISAM、Memory等多个存储引擎</strong>。现在<strong>最常用的存储引擎是InnoDB</strong>，它从<strong>MySQL 5.5.5</strong>版本开始成为了默认存储引擎。</p>
<p>也就是说，你执行create table建表的时候，如果不指定引擎类型，<strong>默认使用的就是InnoDB</strong>。不过，你也可以通过指定存储引擎的类型来选择别的引擎，<strong>比如在create table语句中使用engine&#x3D;memory, 来指定使用内存引擎创建表</strong>。不同存储引擎的表数据存取方式不同，支持的功能也不同，在后面的文章中，我们会讨论到引擎的选择。</p>
<p>从图中不难看出，不同的存储引擎共用一个<strong>Server层</strong>，也就是从连接器到执行器的部分。你可以先对每个组件的名字有个印象，接下来会结合开头提到的那条SQL语句，带你走一遍整个执行流程，依次看下每个组件的作用。</p>
<h2 id="连接器"><a href="#连接器" class="headerlink" title="连接器"></a>连接器</h2><p>第一步，你要操作数据库，首先你就要连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql -h$ip -P$port -u$user -p</span><br></pre></td></tr></table></figure>

<p><strong>输完命令之后，你就需要在交互对话里面输入密码。虽然密码也可以直接跟在-p后面写在命令行中，但这样可能会导致你的密码泄露。如果你连的是生产服务器，强烈建议你不要这么做。</strong></p>
<p>大概就是这个样子</p>
<p><img src="/2024/09/09/MySQL01/image-20240909122445750.png" alt="image-20240909122445750"></p>
<p>连接命令中的mysql是客户端工具，<strong>主要就是用来跟服务端建立连接的</strong>。在完成经典的TCP握手后，连接器就要开始<strong>认证你的身份</strong>，这个时候用的就是你输入的用户名和密码。</p>
<ul>
<li>如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序结束执行。</li>
<li>如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。<strong>之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限</strong>。</li>
</ul>
<p>这就意味着，<strong>一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。</strong></p>
<p>对于上面过程还有一个比较详细的解释，建议看看</p>
<ul>
<li><a href="https://blog.csdn.net/LYue123/article/details/89256577">彻底弄懂mysql（一）–mysql的通信协议_mysql tcp protocol url-CSDN博客</a></li>
<li><a href="https://blog.csdn.net/LYue123/article/details/89285157?spm=1001.2014.3001.5502">彻底弄懂mysql（二）–连接方式_mysql连接池怎么算开启一次连接-CSDN博客</a></li>
</ul>
<p>这里我再提示一下，这个MySQL报文其实我们很久之前就见过了，特别是在IDEA中，那个数据连接的左下角就是报文。</p>
<p>总结一下，上面两篇文章：</p>
<p>第一篇主要是介绍了MySQL的通信具体是什么样的，先TCP三次握手，然后再到握手认证阶段（登陆认证报文就是在这个时候用的），最后就是命令执行阶段（我们可以输入命令啦）</p>
<p>有两个问题问的特别好：</p>
<ul>
<li>为什么还要进行三次握手认证？</li>
<li>因为TCP三次握手，<strong>只是将客户端与服务端建立起了连接，然后通过端口知道我要访问的是MySQL这个服务</strong>，但是MySQL它不同于HTTP，只要你知道URL就能得到一个响应，<strong>MySQL必须登陆后你才能进行操作</strong>。所以这个过程最重要的就是<strong>验证客户端的登陆权限</strong>。</li>
<li>为什么是服务端主动给客户端发送认证呢？</li>
<li>HTTP不是说<strong>只有客户端主动与服务端进行请求</strong>，<strong>服务端不是不能在没有请求的情况下主动进行响应吗</strong>？<br>首先，大家也不要陷入误区，<strong>在进行认证的这个交互中，实际上客户端与服务端还没有进行任何业务上的往来，只是进行一个认证</strong>，所以与上面说的HTTP的不同，如果细心的话你会发现，<strong>认证成功后，在命令执行阶段，MySQL这种通信方式是与HTTP非常类似的，在没有请求的情况下，服务端的MySQL也不会主动给你发送任何数据，所以这里不要混淆</strong>。<br>再说为什么服务端先发送，那肯定是因为他有不得不发送的道理，所以我们就需要理解一下，他发送的是什么东西。</li>
</ul>
<p>还包括了一个例子：</p>
<ul>
<li>小明找工作，投了一个简历给某个公司（TCP三次握手成功）<br>这时候，某公司就主动打电话了，告诉他，我们需要笔试，笔试的时间，网址，以及一些别的相关信息，规则等</li>
<li>小明接收到这个消息之后，到了那个时间他就会请求那个网址，并将自己的信息告诉他</li>
<li>这时候公司验证你的信息，验证成功后，你就可以开始笔试了</li>
</ul>
<p>特别是这个挑战随机数，有点意思：</p>
<p>MySQL报文<strong>挑战随机数</strong>（通常称为“随机数”或“挑战数字”）是在MySQL客户端与服务器之间进行身份验证时使用的一种机制。具体来说，它在以下场景中发挥作用：</p>
<ol>
<li><strong>身份验证</strong>: 当客户端连接到MySQL服务器时，服务器会生成一个随机数并发送给客户端。这个随机数用于生成加密的身份验证信息。</li>
<li><strong>提高安全性</strong>: 通过使用随机数，服务器可以确保每次连接的身份验证过程都是唯一的，防止重放攻击。</li>
<li><strong>握手协议</strong>: 在MySQL的握手协议中，随机数是服务器发送给客户端的一部分，客户端使用这个随机数与用户的密码结合，生成最终的身份验证信息。</li>
</ol>
<p>说实话有点像盐值加密</p>
<p>MySQL 客户端的<strong>权能标志</strong>（Client Capability Flags）是一个用于表示客户端所支持功能的位标志。这些标志在客户端与 MySQL 服务器进行连接时起着重要的作用，确保双方能够有效地通信。</p>
<ol>
<li><strong>功能协商</strong>: 客户端在建立连接时会发送其支持的功能标志，服务器根据这些标志来决定如何处理请求。</li>
<li><strong>兼容性</strong>: 通过交换权能标志，客户端和服务器可以确保双方功能的兼容性，避免因不兼容的功能导致的错误。</li>
<li><strong>优化性能</strong>: 权能标志可以帮助服务器优化数据传输方式，选择最适合双方的协议和功能。</li>
</ol>
<p>第二篇主要是比较了一下短链接和长连接的区别，然后建议一般还是使用连接池来提升性能。</p>
<p>总结一下这篇文章的重点部分：</p>
<ul>
<li>短连接：<br><strong>客户端连接–创建socket认证连接–维护连接–数据传输–关闭连接</strong></li>
<li>长连接：<br><strong>客户端连接–创建socket认证连接–维护连接–数据传输–维护连接–数据传输…-关闭连接</strong></li>
<li>连接池：<br>连接池的一个核心思想就是<strong>连接复用</strong>，<strong>通过建立一个数据库连接池以及一套连接使用、分配和管理策略</strong>，使得该连接池中的连接可以得到高效、安全的复用，<strong>避免了数据库连接频繁建立、关闭的开销</strong>。</li>
</ul>
<p>因为常见数据库的连接是一个比较影响性能的一个事情，所以使用连接池的目的也是尽量少的创建连接，然后呢又要避免去维护很多空闲的连接</p>
<p>所以建议了解一下连接池的一些参数设置：（感觉我们只用关心下面四个被加粗的就行了）</p>
<ul>
<li><strong>初始化连接就是，创建的时候想池里放入的连接对象数</strong></li>
<li>最大连接数量，就是允许同一时间能都进行工作的最大数量，如果是0表示没有限制</li>
<li><strong>最大空闲连接，就是连接池中最大空闲连接数，如果超了，就要释放一部分空闲超时连接，如果是0表示不需要释放</strong></li>
<li><strong>最小空闲连接数，太少了就要进行创建</strong></li>
<li><strong>超时等待时间，如果一个访问到来的时候，已经到达了最大连接数，那这个请求会被放在请求等待队列中，开启一个计时器，如果经过这个时间后，还没有可用连接，就直接抛出异常给此用户。如果是-1表示无限等待。</strong></li>
</ul>
<p>后面感觉还是有一点不够用，又找了一些文章来看</p>
<ul>
<li><p><a href="https://blog.csdn.net/weixin_44296929/article/details/102738568">连接池总结（作用、对比、参数含义、以及讲解）-CSDN博客</a></p>
<p>看完之后感觉连接池就是类似公交车的长连接，用了很久才会被回收，但是每一个人都可以用，不像TCP长连接只能供一个人使用</p>
</li>
<li><p><a href="https://www.jianshu.com/p/6c61cc49a0ed">数据库连接池-常用参数配置及含义 - 简书 (jianshu.com)</a></p>
<p>这个讲的配置相对详细一点</p>
</li>
</ul>
<p>连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在show processlist命令中看到它。文本中这个图是show processlist的结果，其中的Command列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。</p>
<p>客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数wait_timeout控制的，默认值是8小时。</p>
<p><img src="/2024/09/09/MySQL01/image-20240909145917788.png" alt="image-20240909145917788"></p>
<p>如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： <strong>Lost connection</strong> to MySQL server during query。<strong>这时候如果你要继续，就需要重连，然后再执行请求了。（这个情况我之前好像遇到过）</strong></p>
<p>数据库里面，<strong>长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个</strong>。</p>
<p><strong>建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。</strong></p>
<p>建议再看一篇文章，再看下面的东西</p>
<p>[<a href="https://zhuanlan.zhihu.com/p/43941022">玩转MySQL之二]MySQL连接机制浅析及运维 - 知乎 (zhihu.com)</a></p>
<p>这篇文章很好的解释了为什么有的时候是不能使用短连接的：</p>
<ul>
<li><p>在<a href="https://zhida.zhihu.com/search?q=%E6%85%A2%E9%80%9F%E7%BD%91%E7%BB%9C&zhida_source=entity&is_preview=1">慢速网络</a>下使用短连接，连接的开销会很大；在生产繁忙的系统中，连接也可能会受到系统端口数的限制，如果要每秒建立几千个连接，<strong>那么连接断开后，端口不会被马上回收利用，必须经历一个“FIN”阶段的等待，直到可被回收利用为止，这样就可能会导致端口资源不够用</strong>。</p>
</li>
<li><p>从客户端的角度来说，<strong>使用长连接有一个好处，可以不用每次创建新连接，若客户端对MySQL服务器的连接请求很频繁，永久连接将更加高效</strong>。对于**<a href="https://zhida.zhihu.com/search?q=%E9%AB%98%E5%B9%B6%E5%8F%91&zhida_source=entity&is_preview=1">高并发</a>业务<strong>，如果可能会碰到连接的冲击，</strong>推荐使用长连接或连接池。**</p>
</li>
<li><p>从服务器的角度来看，情况则略有不同，<strong>它可以节省创建连接的开销，但维持连接也是需要内存的</strong>。如果滥用长连接的话，可能会使用过多的MySQL<a href="https://zhida.zhihu.com/search?q=%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9E%E6%8E%A5&zhida_source=entity&is_preview=1">服务器连接</a>。<strong>现代的操作系统可以拥有几千个MySQL连接，但很有可能绝大部分都是睡眠（sleep）状态的，这样的工作方式不够高效，而且连接占据内存，也会导致内存的浪费。</strong></p>
</li>
<li><p>对于扩展性好的站点来说，其实大部分的访问并不需要连接数据库。<strong>如果用户需要频繁访问数据库，那么可能会在流量增大的时候产生性能问题</strong>，此时长短连接都是无法解决问题的，<strong>所以应该进行合理的设计和优化来避免性能问题。</strong></p>
</li>
<li><p>为了确保扩展性好的站点在高流量情况下仍能保持良好的性能，以下是一些<strong>合理的设计和优化策略</strong>：</p>
<p>\1. <strong>优化应用层</strong>—从应用程序代码来看</p>
<p>减少数据库请求: 在应用逻辑中优化数据访问，尽量减少数据库请求的次数。例如，<strong>通过批量查询或合并请求。</strong></p>
<p>使用分页: 对于大数据集，<strong>使用分页而不是一次性加载所有数据。</strong></p>
<p>\2.  <strong>缓存机制</strong>—从缓存中间件来看</p>
<p>使用缓存: <strong>在数据库前面引入缓存层（如 Redis、Memcached）来存储频繁访问的数据</strong>。这样可以减少数据库的直接查询次数。</p>
<p>页面缓存: <strong>对于静态内容或不常变化的页面，使用页面缓存可以减少数据库的压力</strong>。<strong>该放Nginx的放Nginx，该放CDN的放CDN。</strong></p>
<p>\3. **负载均衡 **—大的方面，从数据库实例来看</p>
<p><strong>多实例部署</strong>: 将数据库部署为<strong>主从复制或集群模式，使用负载均衡器分散请求，提高并发处理能力。</strong></p>
<p><strong>读写分离</strong>: 将读请求和写请求分开，<strong>写操作发送到主数据库，读操作发送到从数据库。</strong></p>
<p><strong>数据分片</strong>: 对于大规模数据，可以考虑数据分片，将数据分散到不同的数据库实例中，<strong>减少单个数据库的压力。</strong></p>
<p>\4.<strong>数据库优化</strong>—小的方面，从数据库表来看</p>
<p><strong>索引优化</strong>: 确保数据库表中的索引设置合理，以加速查询。</p>
<p><strong>查询优化</strong>: 使用 EXPLAIN 分析 SQL 查询，优化慢查询，减少不必要的数据检索。</p>
</li>
</ul>
<p>下面这个感觉又可以出一个场景题了</p>
<p>但是全部使用长连接后，你可能会发现，<strong>有些时候MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。</strong></p>
<p>怎么解决这个问题呢？你可以考虑以下两种方案。（至于为什么是这样，你看后面的查询缓存就知道了）</p>
<ol>
<li>定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。</li>
<li>如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。</li>
</ol>
<h2 id="查询缓存"><a href="#查询缓存" class="headerlink" title="查询缓存"></a>查询缓存</h2><p>连接建立完成后，你就可以执行select语句了。执行逻辑就会来到第二步：查询缓存。</p>
<p>MySQL拿到一个查询请求后，<strong>会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。</strong></p>
<p>如果语句不在查询缓存中，就会继续后面的执行阶段。<strong>执行完成后，执行结果会被存入查询缓存中。</strong>你可以看到，<strong>如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。</strong></p>
<p><strong>但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。</strong></p>
<p>查询缓存的失效非常频繁，<strong>只要有对一个表的更新，这个表上所有的查询缓存都会被清空</strong>。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，<strong>查询缓存的命中率会非常低</strong>。除非你的业务就是有一张<strong>静态表</strong>，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询<strong>才适合使用查询缓存</strong>。</p>
<p>好在MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。<strong>而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select SQL_CACHE * from table where ID = 10;</span><br></pre></td></tr></table></figure>

<p>需要注意的是，MySQL 8.0版本直接将查询缓存的整块功能删掉了，<strong>也就是说8.0开始彻底没有这个功能了</strong>。</p>
<h2 id="分析器"><a href="#分析器" class="headerlink" title="分析器"></a>分析器</h2><p><strong>如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。</strong></p>
<p>分析器先会做“<strong>词法分析</strong>”。<strong>你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。</strong></p>
<p>这个有点像SICP讲的Scheme分析器啊<a href="https://composingprograms.netlify.app/3/4">组合语言的解释器 (composingprograms.netlify.app)</a></p>
<p>还有一点像ES的IK分词器，那个分词器里面我记得还有具体的算法来进行分词</p>
<p>MySQL从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。</p>
<p>做完了这些识别以后，就要做“<strong>语法分析</strong>”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。</p>
<p>如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句select少打了开头的字母“s”。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; elect * from t where ID=1;</span><br><span class="line"></span><br><span class="line">ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#x27;elect * from t where ID=1&#x27; at line 1</span><br></pre></td></tr></table></figure>

<p>一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。</p>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>经过了<strong>分析器，MySQL就知道你要做什么了</strong>。在开始执行之前，还要先经过<strong>优化器的处理</strong>。</p>
<p>优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的join：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;</span><br></pre></td></tr></table></figure>

<ul>
<li>既可以先从表t1里面取出c&#x3D;10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。</li>
<li>也可以先从表t2里面取出d&#x3D;20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。</li>
</ul>
<p>这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。</p>
<p><strong>优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。</strong>如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等等，没关系，我会在后面的文章中单独展开说明优化器的内容。</p>
<h2 id="执行器"><a href="#执行器" class="headerlink" title="执行器"></a>执行器</h2><p>MySQL<strong>通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句</strong>。</p>
<p>开始执行的时候，<strong>要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误</strong>，如下所示。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from T where ID=10;</span><br><span class="line"></span><br><span class="line">ERROR 1142 (42000): SELECT command denied to user &#x27;b&#x27;@&#x27;localhost&#x27; for table &#x27;T&#x27;</span><br></pre></td></tr></table></figure>

<p>如果有权限，就<strong>打开表继续执行</strong>。打开表的时候，<strong>执行器就会根据表的引擎定义，去使用这个引擎提供的接口</strong>。</p>
<p>比如我们这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的：</p>
<ol>
<li><strong>调用InnoDB引擎接口</strong>取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；</li>
<li>调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。</li>
<li>执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。</li>
</ol>
<p>至此，这个语句就执行完成了。</p>
<p>对于有索引的表，执行的逻辑也差不多。<strong>第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口</strong>，这些接口都是引擎中已经定义好的。</p>
<p>你会在数据库的<strong>慢查询日志</strong>中看到一个<strong>rows_examined的字段</strong>，表示这个语句执行过程中扫描了多少行。<strong>这个值就是在执行器每次调用引擎获取数据行的时候累加的</strong>。</p>
<p>在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此<strong>引擎扫描行数跟rows_examined并不是完全相同的。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>如果表T中没有字段k，而你执行了这个语句 select * from T where k&#x3D;1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？</p>
<blockquote>
<p>在执行 SQL 查询的过程中，当出现类似于 “Unknown column ‘k’ in ‘where clause’” 的错误信息时，这是在语义分析阶段报出的错误。</p>
<p>具体的解析和执行过程如下：</p>
<ol>
<li><strong>词法分析和语法分析阶段</strong>：在这个阶段，MySQL 解析器将输入的 SQL 查询语句进行词法分析和语法分析，<strong>生成语法树（Parse Tree）或语法分析树（Parse Tree）</strong>。<strong>这个阶段主要是验证查询语句的语法正确性，检查语句中的关键字、标识符、运算符等是否符合语法规则。</strong></li>
<li><strong>语义分析阶段</strong>：<strong>在语义分析阶段，MySQL 对语法树进行进一步的分析和验证。这个阶段主要是检查语句的语义正确性，包括验证列和表的存在性、数据类型的匹配、约束条件的合法性等。</strong>在执行 SQL 查询时，MySQL 需要确保查询中使用的列和表是存在的。</li>
</ol>
<p>当执行语句 “SELECT * FROM T WHERE k&#x3D;1” 时，MySQL 会在<strong>语义分析阶段尝试解析和验证其中的列名</strong>。如果表 T 中不存在名为 ‘k’ 的列，MySQL 将报告 “Unknown column ‘k’ in ‘where clause’” 错误，表示查询中引用了不存在的列。</p>
<p>因此，<strong>该错误是在语义分析阶段报出的，即在验证查询语句的语义正确性时发现的</strong>。在这个阶段，MySQL 检查查询语句是否符合数据库模式的定义和约束条件。</p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/01-ji-chu-jia-gou-yi-tiao-sql-cha-xun-yu-ju-shi-ru-he-zhi-hang-de">01 基础架构：一条SQL查询语句是如何执行的？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>日志系统：一条SQL更新语句是如何执行的？</title>
    <url>/2024/09/09/MySQL02/</url>
    <content><![CDATA[<h1 id="日志系统：一条SQL更新语句是如何执行的？"><a href="#日志系统：一条SQL更新语句是如何执行的？" class="headerlink" title="日志系统：一条SQL更新语句是如何执行的？"></a>日志系统：一条SQL更新语句是如何执行的？</h1><p>前面我们系统了解了一个查询语句的执行流程，并介绍了执行过程中涉及的处理模块。相信你还记得，<strong>一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。</strong></p>
<p>那么，<strong>一条更新语句</strong>的执行流程又是怎样的呢？</p>
<p>之前你可能经常听DBA同事说，MySQL可以恢复到半个月内任意一秒的状态，惊叹的同时，你是不是心中也会不免会好奇，这是怎样做到的呢？</p>
<p>我们还是从一个表的一条更新语句说起，下面是这个表的创建语句，这个表有一个主键ID和一个整型字段c：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; create table T(ID int primary key, c int);</span><br></pre></td></tr></table></figure>

<p>如果要将ID&#x3D;2这一行的值加1，SQL语句就会这么写：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; update T set c=c+1 where ID=2;</span><br></pre></td></tr></table></figure>

<p>前面介绍过SQL语句基本的执行链路，这里我再把那张图拿过来，你也可以先简单看看这个图回顾下。首先，可以确定的说，查询语句的那一套流程，更新语句也是同样会走一遍。</p>
<p><img src="/2024/09/09/MySQL02/image-20240909163544965.png" alt="image-20240909163544965"></p>
<p>你执行语句前要<strong>先连接数据库，这是连接器的工作</strong>。</p>
<p>前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，<strong>所以这条语句就会把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因</strong>。</p>
<p>接下来，<strong>分析器</strong>会通过词法和语法解析知道这是一条更新语句。<strong>优化器</strong>决定要使用ID这个索引。然后，<strong>执行器</strong>负责具体执行，找到这一行，然后更新。</p>
<p>与查询流程不一样的是，<strong>更新流程还涉及两个重要的日志模块</strong>，它们正是我们今天要讨论的主角：<strong>redo log（重做日志）和 binlog（归档日志）</strong>。如果接触MySQL，那这两个词肯定是绕不过的，我后面的内容里也会不断地和你强调。不过话说回来，redo log和binlog在设计上有很多有意思的地方，这些设计思路也可以用到你自己的程序里。</p>
<h3 id="重要的日志模块：redo-log"><a href="#重要的日志模块：redo-log" class="headerlink" title="重要的日志模块：redo log"></a>重要的日志模块：redo log</h3><p>不知道你还记不记得《孔乙己》这篇文章，酒店掌柜有一个粉板，专门用来记录客人的赊账记录。如果赊账的人不多，那么他可以把顾客名和账目写在板上。但如果赊账的人多了，粉板总会有记不下的时候，这个时候掌柜一定还有一个专门记录赊账的账本。</p>
<p>如果有人要赊账或者还账的话，掌柜一般有两种做法：</p>
<ul>
<li>一种做法是直接把账本翻出来，把这次赊的账加上去或者扣除掉；</li>
<li>另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算。</li>
</ul>
<p>在生意红火柜台很忙时，掌柜一定会选择后者，因为前者操作实在是太麻烦了。首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找，找到之后再拿出算盘计算，最后再将结果写回到账本上。</p>
<p>这整个过程想想都麻烦。相比之下，还是先在粉板上记一下方便。你想想，如果掌柜没有粉板的帮助，每次记账都得翻账本，效率是不是低得让人难以忍受？</p>
<p>同样，在MySQL里也有这个问题，<strong>如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高</strong>。为了解决这个问题，MySQL的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。</p>
<p><strong>而粉板和账本配合的整个过程，其实就是MySQL里经常说到的WAL技术，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。（这个其实也是为了避免写了磁盘之后没有时间来得急写日志，导致Log不一致的问题）</strong></p>
<p>具体来说，当有一条记录需要更新的时候，<strong>InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存，这个时候更新就算完成了</strong>。同时，InnoDB引擎会在<strong>适当</strong>的时候，将这个操作记录<strong>更新到磁盘</strong>里面，<strong>而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事</strong>。</p>
<p>如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。</p>
<p>与此类似，<strong>InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。</strong></p>
<p><img src="/2024/09/09/MySQL02/image-20240909164238628.png" alt="image-20240909164238628"></p>
<p><strong>write pos是当前记录的位置</strong>，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。<strong>checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。（我记得之前好像看过一个类似的环形更新方法，反正Redis分布式采用的方法有一个是环形的）</strong></p>
<p><strong>write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。（有一点像JVM的STW）</strong></p>
<p>有了redo log，<strong>InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失</strong>，这个能力称为<strong>crash-safe</strong>。（这玩意经常在面试中问，大概就是什么MySQL数据库突然断电了，怎么找回数据的）</p>
<p>要理解crash-safe这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然<strong>停业</strong>几天，<strong>恢复</strong>生意后<strong>依然</strong>可以通过账本和粉板上的数据明确赊账账目。</p>
<h3 id="重要的日志模块：binlog"><a href="#重要的日志模块：binlog" class="headerlink" title="重要的日志模块：binlog"></a>重要的日志模块：binlog</h3><p>前面我们讲过，MySQL整体来看，其实就有两块：一块是Server层，它主要做的是<strong>MySQL功能层面</strong>的事情；还有一块是引擎层，<strong>负责存储</strong>相关的具体事宜。上面我们聊到的粉板redo log是<strong>InnoDB引擎特有的日志</strong>，而<strong>Server层也有自己的日志，称为binlog（归档日志）</strong>。</p>
<p>我想你肯定会问，为什么会有两份日志呢？</p>
<p>因为<strong>最开始</strong>MySQL里<strong>并没有InnoDB引擎</strong>。<strong>MySQL自带的引擎是MyISAM</strong>，但是MyISAM<strong>没有</strong>crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。</p>
<p>这两种日志有以下三点不同。</p>
<ol>
<li>从引擎来看：<strong>redo log是InnoDB引擎特有的</strong>；<strong>binlog是MySQL的Server层实现的，所有引擎都可以使用。</strong></li>
<li>从日志类型来看：<strong>redo log是物理日志</strong>，记录的是“在某个数据页上做了什么修改”；<strong>binlog是逻辑日志</strong>，记录的是这个语句的原始逻辑，比如“给ID&#x3D;2这一行的c字段加1 ”。<strong>（这个binlog记录的逻辑日志和Redis的AOF（Append Only File）挺类似的）</strong></li>
<li>从空间使用来看：<strong>redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</strong></li>
</ol>
<p>有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语句时的内部流程。</p>
<ol>
<li>执行器先找引擎取ID&#x3D;2这一行。ID是主键，<strong>引擎直接用树（B+树）搜索找到这一行</strong>。如果ID&#x3D;2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。</li>
<li>执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。<strong>（这个有点类似用户态和内核态，写入之类的不安全操作交给操作系统内核执行）</strong></li>
<li>引擎将这行新数据<strong>更新到内存</strong>中，同时将这个<strong>更新操作记录到redo log</strong>里面，<strong>此时redo log处于prepare状态</strong>。<strong>然后告知执行器执行完成了，随时可以提交事务</strong>。</li>
<li><strong>执行器生成这个操作的binlog，并把binlog写入磁盘。</strong></li>
<li>执行器调用引擎的提交事务接口，<strong>引擎把刚刚写入的redo log改成提交（commit）状态</strong>，更新完成。</li>
</ol>
<p>这里我给出这个update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的<strong>（反正只要是执行写操作就一定是在InnoDB中执行的，它这里深色表示只是代表调用了引擎的接口）</strong>，<strong>深色框表示是在执行器</strong>中执行的。</p>
<p><img src="/2024/09/09/MySQL02/image-20240909165928810.png" alt="image-20240909165928810"></p>
<p>你可能注意到了，最后三步看上去有点“绕”，<strong>将redo log的写入拆成了两个步骤</strong>：<strong>prepare和commit</strong>，这就是”两阶段提交”。（搞半天原来后面讲了）</p>
<h3 id="两阶段提交"><a href="#两阶段提交" class="headerlink" title="两阶段提交"></a>两阶段提交</h3><p>为什么必须有“两阶段提交”呢？<strong>这是为了让两份日志之间的逻辑一致。</strong>要说明这个问题，我们得从文章开头的那个问题说起：<strong>怎样让数据库恢复到半个月内任意一秒的状态？</strong></p>
<p>前面我们说过了，binlog会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的DBA承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，<strong>同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。</strong></p>
<p>当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，<strong>需要找回数据</strong>，那你可以这么做：</p>
<ul>
<li>首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，<strong>从这个备份恢复到临时库</strong>；</li>
<li><strong>然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。</strong></li>
</ul>
<p><strong>这里的思路挺像Docker的镜像构建方式的，都是将一个基础的东西作为我们的基石，然后再在上面加东西</strong></p>
<p><strong>这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。</strong></p>
<p>好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。</p>
<p>由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，<strong>要么就是先写完redo log再写binlog</strong>，或者采用反过来的顺序。我们看看这两种方式会有什么问题。</p>
<p>仍然用前面的update语句来做例子。假设当前ID&#x3D;2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？</p>
<ol>
<li><strong>先写redo log后写binlog</strong>。假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。 <strong>但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。 然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同</strong>。</li>
<li><strong>先写binlog后写redo log</strong>。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，<strong>与原库的值不同</strong>。</li>
</ol>
<p><strong>看样子意思是binlog才能恢复一个完整的数据库，redo log一般用于恢复数据库突然故障丢失的数据</strong></p>
<p><strong>可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。</strong></p>
<p>你可能会说，这个概率是不是很低，平时也没有什么动不动就需要<strong>恢复临时库</strong>的场景呀？</p>
<p>其实不是的，<strong>不只是误操作后需要用这个过程来恢复数据</strong>。当你需要<strong>扩容</strong>的时候，也就是需要<strong>再多搭建一些备库来增加系统的读能力的时候</strong>，现在常见的做法也是用<strong>全量备份</strong>加上<strong>应用binlog</strong>来实现的，<strong>这个“不一致”就会导致你的线上出现主从数据库不一致的情况</strong>。</p>
<p><strong>简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>介绍了MySQL里面最重要的两个日志，<strong>即物理日志redo log和逻辑日志binlog</strong>。</p>
<p><strong>redo log用于保证crash-safe能力</strong>。<strong>innodb_flush_log_at_trx_commit</strong>这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。<strong>这个参数我建议你设置成1，这样可以保证MySQL异常重启之后数据不丢失。</strong></p>
<p><strong>sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数也建议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失。</strong></p>
<p>此外还介绍了与MySQL日志系统密切相关的“两阶段提交”。两阶段提交是跨系统<strong>维持数据逻辑一致性</strong>时<strong>常用</strong>的一个方案，即使你不做数据库内核开发，<strong>日常开发</strong>中也有可能会用到。</p>
<p>前面说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？</p>
<p>补充资料：（小林Coding写的还是比较详细的）</p>
<p><a href="https://blog.csdn.net/weixin_63566550/article/details/129819638">【MySQL】一文彻底搞懂 Redo-log 为什么要两阶段提交？_mysql redolog两阶段提交-CSDN博客</a></p>
<p>把这篇文章比较重要的东西提取到下面了：</p>
<p>两阶段提交的过程是怎样的？</p>
<p>相信大家应该听说过<code>MySQL</code>事务两阶段提交方案，啥叫做事务两阶段提交呢？实则是指<code>Redo-log</code>分两次写入，如下：</p>
<p><img src="/2024/09/09/MySQL02/image-20240909191851549.png" alt="image-20240909191851549"></p>
<p>从图中可看出，事务的提交过程有两个阶段，<strong>就是将 redo log 的写入拆成了两个步骤：prepare 和 commit</strong>，<strong>中间再穿插写入binlog</strong>，具体如下：</p>
<ul>
<li><strong>prepare 阶段：</strong>将 XID（内部 XA 事务的 ID） 写入到 redo log，<strong>同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit &#x3D; 1 的作用）；</strong></li>
<li><strong>commit 阶段：</strong>把 XID 写入到 binlog，然后<strong>将 binlog 持久化到磁盘（sync_binlog &#x3D; 1 的作用），</strong>接着调用引擎的提交事务接口，<strong>将 redo log 状态设置为 commit</strong>，此时该状态并<strong>不需要持久化</strong>到磁盘，<strong>只需要 write 到文件系统的 page cache 中就够了</strong>，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，<strong>一样会被认为事务已经执行成功；</strong></li>
</ul>
<p>为什么需要两阶段提交？</p>
<blockquote>
<p>其实想要弄明白这个问题，要结合bin-log日志一起来聊。</p>
</blockquote>
<p>如果只写一次的话，那到底先写bin-log还是redo-log呢？</p>
<ul>
<li><strong>先写bin-log，再写redo-log：</strong>当事务提交后，先写bin-log成功，结果在写redo-log时断电宕机了，再重启后由于redo-log中没有该事务的日志记录，因此不会恢复该事务提交的数据。但要注意，主从架构中同步数据是使用bin-log来实现的，而宕机前bin-log写入成功了，就代表这个事务提交的数据会被同步到从机，<strong>也就意味着从机会比主机多出一条数据。</strong></li>
<li><strong>先写redo-log，再写bin-log：</strong>当事务提交后，先写redo-log成功，但在写bin-log时宕机了，主节点重启后，会根据redo-log恢复数据，但从机依旧是依赖bin-log来同步数据的，因此从机无法将这个事务提交的数据同步过去，毕竟bin-log中没有撒，<strong>最终从机会比主机少一条数据。</strong></li>
</ul>
<p>经过上述分析后可得知：如果redo-log<strong>只写一次</strong>，那不管谁先写，<strong>都有可能造成主从同步数据时的不一致问题</strong>出现，<strong>为了解决该问题，redo-log就被设计成了两阶段提交模式</strong>，设置成两阶段提交后，整个执行过程有三处崩溃点：</p>
<ul>
<li>redo-log(prepare)：在写入准备状态的redo记录时宕机，事务还未提交，<strong>不会影响一致性</strong>。</li>
<li>bin-log：在写bin记录时崩溃，重启后会根据redo记录中的事务ID，<strong>回滚</strong>前面已写入的数据。（然后会继续执行这个过程）</li>
<li>redo-log(commit)：在bin-log写入成功后，<strong>写redo(commit)记录时崩溃</strong>，因为bin-log中已经写入成功了，<strong>所以从机也可以同步数据，因此重启时直接再次提交事务</strong>，写入一条redo(commit)记录即可。（这种commit一半没有成功的，之后再commit一次就行了）</li>
</ul>
<p>通过这种两阶段提交的方案，就能够确保redo-log、bin-log两者的日志数据是相同的，<strong>bin-log中有的主机再恢复，如果bin-log没有则直接回滚主机上写入的数据，确保整个数据库系统的数据一致性。</strong></p>
<blockquote>
<p>OK~,最后再简单补充一点：为什么bin-log又被叫做二进制日志呢？因为记录日志时，MySQL写入的是二进制数据，而并非字符数据，也就意味着直接用cat&#x2F;vim这类工具是无法打开的，必须要通过MySQL提供的mysqlbinlog工具解析查看。</p>
</blockquote>
<p>两阶段提交有什么问题？</p>
<p>两阶段提交虽然保证了两个日志文件的数据一致性，但是性能很差，主要有两个方面的影响：</p>
<ul>
<li>磁盘 I&#x2F;O 次数高：对于“双1”配置，<strong>每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。</strong></li>
<li>锁竞争激烈：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。</li>
</ul>
<p>为什么两阶段提交的磁盘 <strong>I&#x2F;O 次数会很高</strong>？</p>
<p>binlog 和 redo log 在内存中都对应的缓存空间，binlog 会缓存在 binlog cache，redo log 会缓存在 redo log buffer，它们持久化到磁盘的时机分别由下面这两个参数控制。一般我们为了避免日志丢失的风险，会将这两个参数设置为 1：</p>
<ul>
<li>当 sync_binlog &#x3D; 1 的时候，表示每次提交事务都会将 binlog cache 里的 binlog 直接持久到磁盘；</li>
<li>当 innodb_flush_log_at_trx_commit &#x3D; 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘；</li>
</ul>
<p><strong>可以看到，如果 sync_binlog 和 当 innodb_flush_log_at_trx_commit 都设置为 1，那么在每个事务提交过程中， 都会至少调用 2 次刷盘操作，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。</strong></p>
<p>为什么锁竞争激烈？</p>
<p>在早期的 MySQL 版本中，通过使用 prepare_commit_mutex 互斥锁来保证事务提交的顺序，<strong>在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。</strong></p>
<p><strong>通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳。</strong></p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/02-ri-zhi-xi-tong-yi-tiao-sql-geng-xin-yu-ju-shi-ru-he-zhi-hang-de">02 日志系统：一条SQL更新语句是如何执行的？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>My-Backend-Learning-Journey</title>
    <url>/2024/09/07/My-Backend-Learning-Journey/</url>
    <content><![CDATA[<h1 id="后端技术知识一览"><a href="#后端技术知识一览" class="headerlink" title="后端技术知识一览"></a>后端技术知识一览</h1><blockquote>
<p>整理了一下自己在学习后端时的心得体会，争取可以让各位立志学习后端的朋友快速了解一下后端的技术 :horse_racing:</p>
<p>如果觉得项目不错，或者给你带来了一些帮助，不妨点个 Star ❤️</p>
<p>本篇文章属于本人的原创作品，如果需要转载，请保留出处，谢谢咯 😆</p>
<p>由于本人现在的技术可能不太成熟，对于一些概念可能理解的会有问题，所以如果你在文章中发现了问题不妨指出问题，提一个issue哦 :rose:</p>
</blockquote>
<h2 id="为什么会创建这样一个仓库？"><a href="#为什么会创建这样一个仓库？" class="headerlink" title="为什么会创建这样一个仓库？"></a>为什么会创建这样一个仓库？</h2><p>很多教程对于后端的技术栈说明的并不全面，或者是太全面了让人不知道从哪里开始学起，这导致初入编程的我们学习的过程中没有较强的连续性，很容易让人产生不知道学了有什么用，有那么多技术框架，我该先学哪个的疑问。</p>
<p>所以作为一名普通的在校大学生，我想在此分享一下自己学习后端的整个心路历程和经验，将整个后端要学习的技术尽量以一种启发式的方式介绍给大家，希望能为所有想学习后端的同学们提供一些帮助。</p>
<h2 id="这个仓库包含了哪些知识的介绍？"><a href="#这个仓库包含了哪些知识的介绍？" class="headerlink" title="这个仓库包含了哪些知识的介绍？"></a>这个仓库包含了哪些知识的介绍？</h2><ul>
<li>后端语言 — Java</li>
<li>关系型数据库 — MySQL</li>
<li>ORM框架 — Mybatis &#x2F; Mybatis-plus</li>
<li>Web开发框架 — Spring，SpringMVC，SpringBoot</li>
<li>接口管理工具 — Postman &#x2F; Apifox &#x2F; Swagger &#x2F; YAp</li>
<li>安全管理框架 — Spring Security &#x2F; Shiro</li>
<li>依赖管理工具 — Maven &#x2F; Gradle</li>
<li>版本控制工具 — Git</li>
<li>服务器 — Linux</li>
<li>前端基础 — 前端三剑客</li>
<li>设计模式 — 优化代码</li>
<li>缓存 — Redis</li>
<li>性能压测 — JMeter</li>
<li>消息队列 — RabbitMQ</li>
<li>反向代理服务器 — Nginx</li>
<li>网络编程 — Netty</li>
<li>微服务框架 — SpringCloud</li>
<li>容器 — Docker</li>
<li>容器编排 — Kubernetes</li>
<li>CI &#x2F; CD — Jenkins</li>
<li>并发编程 — JUC</li>
<li>虚拟机 — JVM</li>
<li>任务调度 — XXL-JOB</li>
<li>搜索引擎 — Elasticsearch</li>
<li>链路追踪 — SkyWalking</li>
<li>读写分离 — ShardingSphere &#x2F; MyCat</li>
<li>同步数据 — Canal</li>
<li>存储图片 — OSS</li>
<li>好用的工具包 — Hutool</li>
</ul>
<p>个人认为除了四大件和其他CS专业知识的介绍，关于纯后端知识的介绍已经非常全面了。</p>
<p>你可能会觉得还不是很全，比如这里有一些概念笔者并没有提及：CDN内容分发网络，DDD架构，分布式的Raft，拥抱云的GraalVM，新时代GC的ZGC，服务网格Istio，提高代码质量的SonarQube等等。</p>
<p>其实当然，后端知识浩如烟海，怎么可能仅仅只用一篇文章介绍完。倒不如说，这些后端知识应该是你在有了一定基础之后自己去看前沿文章去了解到的。</p>
<p>写这篇文章主要是觉得现在市面上的学习路线主要有以下两个极端:</p>
<ul>
<li>太全。让真正的初学者难以下手，不知道学什么，只会盲目的去学习一些新框架</li>
<li>太简单。让初学者不易于建立完整的后端知识体系，对于后端架构没有一个完整的认识</li>
</ul>
<p>所以这里就我在大学以来和在团队中学习的经验写一些我对于后端的看法，希望可以让朋友们拨开云雾见青天，知道自己为什么学这些技术，以及要学什么技术。</p>
<h2 id="我该怎么学习后端的技术"><a href="#我该怎么学习后端的技术" class="headerlink" title="我该怎么学习后端的技术"></a>我该怎么学习后端的技术</h2><p>我目前学习后端的主要语言是Java，所以在这里主要介绍一下Java后端的学习路线，当然如果你是其他语言的选手，这并不影响你阅读本章内容，因为业界要解决的问题是不会变的，变的只是框架。</p>
<p>比如你在Java中使用Mybatis-plus操作数据，在Go中使用GORM操作数据，在Java中使用SpringBoot作为基本的Web开发框架，在Go中使用Gin或Echo作为基本的Web开发框架。</p>
<p>下面，各位朋友可以根据自己的需求来学习。</p>
<p>这里直接总结一下最基础的后端开发需求，适合真正的初学者:</p>
<ul>
<li>会一门后端语言的基础语法 — 比如Java，Python，Go，Rust等</li>
<li>会基础的关系型数据库操作 — 一般会MySQL的基础操作就行了</li>
<li>会使用ORM框架来操作数据库 — 这个ORM框架你可以根据你选的语言去搜</li>
<li>会使用Web开发框架做基本的项目开发 — 这个Web开发框架你可以根据你选的语言去搜</li>
<li>会基本的接口管理工具给我们的程序发起请求 — 会使用Postman &#x2F; Apifox其中一个就行了</li>
<li>会一个依赖管理工具来管理我们的依赖 — 这个依赖管理工具你可以根据你选的语言去搜</li>
<li>会版本控制工具 — Git</li>
</ul>
<p>比如，我这里以Java为例:</p>
<ul>
<li>会Java的基础语法：掌握数据类型、控制结构、面向对象编程等基本概念</li>
<li>会关系型数据库MySQL的基础操作：能够执行基本的SQL查询、插入、更新和删除操作</li>
<li>会Mybatis来操作数据库：了解如何使用MyBatis进行数据持久化</li>
<li>会SpringBoot来做基本的项目开发：能够创建和配置SpringBoot应用，理解基本的注解和配置方式</li>
<li>会使用Apifox给我们的程序发起请求：能够使用Apifox测试和管理API接口</li>
<li>会使用Maven来管理我们的依赖：熟悉Maven的基本命令和POM文件配置</li>
<li>会使用Git提交推送代码到GitHub上：能够使用 Git 进行版本控制，掌握基本的提交、推送和分支操作</li>
</ul>
<p>又或者，以Go为例:</p>
<ul>
<li>会Go的基础语法：理解数据类型、控制结构、函数和并发编程等基本概念</li>
<li>会关系型数据库MySQL的基础操作：能够执行基本的 SQL 查询、插入、更新和删除操作</li>
<li>会GORM来操作数据库：了解如何使用GORM进行数据持久化</li>
<li>会Echo &#x2F; Gin来做基本的项目开发：能够使用 Echo 或 Gin 框架搭建基本的 Web 应用</li>
<li>会使用Apifox给我们的程序发起请求：能够使用 Apifox 测试和管理 API 接口</li>
<li>会使用Go Modules来管理我们的依赖：熟悉 Go Modules 的基本用法</li>
<li>会使用Git提交推送代码到GitHub上：能够使用 Git 进行版本控制，掌握基本的提交、推送和分支操作</li>
</ul>
<p>除了这些，建议你了解学习以下内容:</p>
<ul>
<li>如何使用Markdown规范自己的文档</li>
<li>如何规范自己的commit信息</li>
<li>如何遵守接口设计规范 RESTful API</li>
</ul>
<p>当你有了一定的基础后，你可以根据后面写的这些来查缺补漏，丰富自己的技术栈。</p>
<h2 id="语言—Java-必学"><a href="#语言—Java-必学" class="headerlink" title="语言—Java(必学)"></a><a href="https://www.oracle.com/cn/java/technologies/downloads/">语言—Java(必学)</a></h2><p>首先我们要编程，写项目，一定要会一门语言，正所谓万丈高楼平地起，语言就是我们搭建高楼的砖块。</p>
<p>不过初学一门编程语言的时候可能你会遇到很多问题，比如思考为什么要写OOP，为什么要有多线程，很多东西你可能暂时不太了解，不过这没有关系，当你学到后面的时候自然会体会到这样做的原因。</p>
<p>这里介绍一下我初学Java语言时的技巧:</p>
<ul>
<li><strong>多敲代码</strong>(代码是手上功夫，光看不练假把式)</li>
<li><strong>学习 Debug</strong> (当你遇到问题无法得到你预期的结果时，可以试试断点调试)</li>
<li><strong>学习Stream流和Lambda 表达式</strong>(这两个技术可以使我们的程序更加优雅—优化数据处理和匿名函数的编写)</li>
<li><strong>看看阿里巴巴的开发规范</strong>(这样更有利于你的程序写出更少的Bug，可读性更好，可扩展性更高)</li>
</ul>
<p>推荐文档和工具:</p>
<ul>
<li><a href="https://liaoxuefeng.com/books/java/introduction/index.html">简介 - Java教程 - 廖雪峰的官方网站 (liaoxuefeng.com)</a></li>
<li><a href="https://docs.oracle.com/javase/8/docs/api/">Overview (Java Platform SE 8 ) (oracle.com)</a></li>
<li><a href="https://www.matools.com/api/java8">Java 8 中文版 - 在线API手册 - 码工具 (matools.com)</a></li>
<li><a href="https://github.com/akullpp/awesome-java">akullpp&#x2F;awesome-java: A curated list of awesome frameworks, libraries and software for the Java programming language. (github.com)</a></li>
<li><a href="https://developer.aliyun.com/ebook/386">《阿里巴巴Java开发手册（终极版）》免费下载_在线阅读_藏经阁-阿里云开发者社区 (aliyun.com)</a></li>
</ul>
<h2 id="关系型数据库—MySQL-必学"><a href="#关系型数据库—MySQL-必学" class="headerlink" title="关系型数据库—MySQL(必学)"></a><a href="https://www.mysql.com/cn/">关系型数据库—MySQL(必学)</a></h2><p>当我们学完一门语言后，一般学校里面都会要求我们做一个课设(这里用贪吃蛇举例)，我们初步了解Java并上手之后，知道了可以使用文件来存储读取数据，那么一旦当数据多起来的时候，我们就不好处理了，这时我们不妨在网上搜搜看有什么东西可以帮助我们存储数据呢？</p>
<p>于是你找到了<a href="https://zh.wikipedia.org/wiki/MySQL">MySQL</a>(这里还推荐你了解一下<a href="https://zh.wikipedia.org/zh-cn/MariaDB">MariaDB</a>和<a href="https://zh.wikipedia.org/zh-cn/PostgreSQL">PostgreSQL</a>)。</p>
<p>那么我们学习MySQL到底要到一个什么程度才能进行开发呢？</p>
<ul>
<li>SQL语句编写(<strong>主要会CURD就行了</strong>，create、update、read和delete)</li>
<li>设计数据库表，字段(这里建议<strong>看看阿里巴巴对于数据库设计的规范</strong>，帮助你更好的设计数据库)</li>
</ul>
<p>推荐文档:</p>
<ul>
<li><a href="https://dev.mysql.com/doc/">MySQL :: MySQL Documentation</a></li>
<li><a href="https://mysql.net.cn/">MySQL 中文网</a></li>
<li><a href="https://www.mysqlzh.com/">MySQL 中文文档 | MySQL 中文网 (mysqlzh.com)</a></li>
<li><a href="https://developer.aliyun.com/ebook/386">《阿里巴巴Java开发手册（终极版）》免费下载_在线阅读_藏经阁-阿里云开发者社区 (aliyun.com)</a></li>
</ul>
<h2 id="ORM框架—MyBatis-Mybatis-plus-必学"><a href="#ORM框架—MyBatis-Mybatis-plus-必学" class="headerlink" title="ORM框架—MyBatis &#x2F; Mybatis-plus(必学)"></a><a href="https://zh.wikipedia.org/wiki/MyBatis">ORM框架—MyBatis &#x2F;</a> <a href="https://github.com/baomidou/mybatis-plus">Mybatis-plus(必学)</a></h2><p>当你写多了原生的<a href="https://zh.wikipedia.org/wiki/Java%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5"><strong>JDBC</strong></a>来操作数据库之后，会发现编码效率很低，写的过程中也比较容易出错，为了更好的和数据库进行交互，于是你又在网上看别人的博客，发现了<a href="https://zh.wikipedia.org/wiki/%E5%AF%B9%E8%B1%A1%E5%85%B3%E7%B3%BB%E6%98%A0%E5%B0%84">ORM</a>这个东西，这种框架可以帮助我们更快的写<a href="https://zh.wikipedia.org/wiki/SQL">SQL</a>，甚至有时候都不用写，而是自动生成。</p>
<p>这里简单介绍一下我写项目时的搭配(MyBatis &#x2F; Mybatis-plus)，当然<a href="https://zh.wikipedia.org/wiki/Java%E6%8C%81%E4%B9%85%E5%8C%96API"><strong>JPA</strong></a>也是可以的:</p>
<ul>
<li>Mybatis在这种搭配下常常用于写复杂的SQL</li>
<li>Mybatis-plus天生支持单表查询，不用自己手写单表查询，这里推荐写一些简单的SQL，对于复杂的SQL建议还是使用Mybatis来写</li>
</ul>
<p>这里简单说明一下，为什么这样搭配，因为在写复杂SQL的时候Mybatis-plus的wrapper会很复杂，可读性很低，我们写的时候犯错的几率也会更大，所以推荐这时使用Mybatis来写。</p>
<p>推荐文档:</p>
<ul>
<li><a href="https://mybatis.org/mybatis-3/zh_CN/index.html">mybatis – MyBatis 3 | 简介</a></li>
<li><a href="https://baomidou.com/">MyBatis-Plus 🚀 为简化开发而生 (baomidou.com)</a></li>
</ul>
<h2 id="Web开发框架—Spring，SpringMVC，Spring-Boot-必学"><a href="#Web开发框架—Spring，SpringMVC，Spring-Boot-必学" class="headerlink" title="Web开发框架—Spring，SpringMVC，Spring Boot(必学)"></a><a href="https://zh.wikipedia.org/wiki/Spring_Framework">Web开发框架—Spring，SpringMVC</a><a href="https://zh.wikipedia.org/wiki/Spring_Boot">，Spring Boot(必学)</a></h2><p>这是我们做<a href="https://zh.wikipedia.org/wiki/Web%E6%9C%8D%E5%8A%A1">Web服务</a>时常用的框架，如果不想从麻烦的原生配置开始做起，建议直接学习SpringBoot2。</p>
<p>SpringBoot3相对于SpringBoot2就是功能更多，但是API发生了较大的变化，可能你学习的时候大部分时间都用在处理冲突上了，所以建议新手入门直接学习SpringBoot2，等有了一定基础再学Spring家族的其他框架，做一般的项目SpringBoot也够用了。</p>
<p>还有就是不要看Spring，SpringMVC，Spring Boot有三个，其实Spring + SpringMVC ≈ SpringBoot，所以你其实可以直接学习SpringBoot。但是可能对于一些在前者(Spring，SpringMVC)中约定俗称的配置有些迷惑，所以这边还是建议有时间的话先把Spring，SpringMVC学了再学SpringBoot。</p>
<p>推荐文档:</p>
<ul>
<li><a href="https://spring.io/">Spring | Home</a></li>
<li><a href="https://springdoc.cn/docs/">spring 中文文档 - spring 中文网 (springdoc.cn)</a></li>
<li><a href="https://docs.springframework.org.cn/spring-framework/reference/spring-projects.html">Spring 项目 :: Spring 框架 - Spring 中文 (springframework.org.cn)</a></li>
</ul>
<h2 id="接口管理工具—Postman-Apifox-Swagger-YApi-必学"><a href="#接口管理工具—Postman-Apifox-Swagger-YApi-必学" class="headerlink" title="接口管理工具—Postman &#x2F; Apifox &#x2F; Swagger &#x2F; YApi(必学)"></a><a href="https://www.postman.com/">接口管理工具—Postman &#x2F;</a> <a href="https://apifox.com/?utm_source=bing&utm_medium=sem&utm_campaign=%E9%AB%98%E8%BD%AC%E5%8C%96%E8%AF%8D-%E4%BA%A7%E5%93%81&utm_content=Apifox&utm_term=apifox&search_term=Apifox&msclkid=d08923251e02177742af45ff8fc17a7f">Apifox &#x2F;</a> <a href="https://swagger.io/">Swagger &#x2F;</a> <a href="https://yapi.pro/">YApi(必学)</a></h2><p>什么，你还在使用<a href="https://hc.apache.org/httpcomponents-client-4.5.x/index.html">HttpClient</a>发送请求？</p>
<p>那么不妨试一试我上面提到的工具吧，这些工具的界面更加友好，功能更加强大，我们没有理由不去使用它。</p>
<p>最开始我使用的是Postman，后面使用过Apifox，Swagger，YApi等工具，就我个人这么久的使用体验而言，感觉Apifox的功能更多更强大，界面是中文，而且还可以导出接口文档，很方便，不用我们自己去写。</p>
<p>这个就不用什么文档了，建议直接<a href="https://zh.wikipedia.org/wiki/CSDN">CSDN</a>启动。</p>
<h2 id="安全管理框架—Spring-Security-Shiro框架-选学，项目有安全需要的学"><a href="#安全管理框架—Spring-Security-Shiro框架-选学，项目有安全需要的学" class="headerlink" title="安全管理框架—Spring Security &#x2F; Shiro框架(选学，项目有安全需要的学)"></a><a href="https://springdoc.cn/spring-security/index.html">安全管理框架—Spring Security &#x2F;</a> <a href="https://zh.wikipedia.org/wiki/Apache_Shiro">Shiro框架(选学，项目有安全需要的学)</a></h2><p>当你的系统缺少安全管理部分可以使用到它，你也许还听说过Shiro框架，它也是做安全的，只不过相比于Spring Security更加轻量。</p>
<p>当然，我们说安全管理框架可能你还不太清楚，为什么要这个东西，如果你写过登录系统，就应该明白，没有框架来管理接口访问，我们将无法对用户进行相应的验证和授权，也就无法区分管理员与普通用户的权限。</p>
<p>除了上面那些框架，我还可以尝试什么？</p>
<ul>
<li>当然，如果你觉得上面提到的框架过于笨重，那么这里还建议你尝试一下<a href="https://jwt.io/">JWT</a>，它可比上面两个轻量多了</li>
<li>同时也建议你了解一下<a href="https://zh.wikipedia.org/wiki/%E4%BB%A5%E8%A7%92%E8%89%B2%E7%82%BA%E5%9F%BA%E7%A4%8E%E7%9A%84%E5%AD%98%E5%8F%96%E6%8E%A7%E5%88%B6">RBAC理论</a>，这个在Spring Security中也有体现</li>
<li>还有就是建议你了解一下<a href="https://zh.wikipedia.org/wiki/%E5%BC%80%E6%94%BE%E6%8E%88%E6%9D%83">OAuth2.0协议</a></li>
</ul>
<p>推荐文档:</p>
<ul>
<li><a href="https://spring.io/">Spring | Home</a></li>
<li><a href="https://springdoc.cn/docs/">spring 中文文档 - spring 中文网 (springdoc.cn)</a></li>
<li><a href="https://docs.springframework.org.cn/spring-framework/reference/spring-projects.html">Spring 项目 :: Spring 框架 - Spring 中文 (springframework.org.cn)</a></li>
</ul>
<h2 id="依赖管理工具—Maven-Gradle-必学"><a href="#依赖管理工具—Maven-Gradle-必学" class="headerlink" title="依赖管理工具—Maven &#x2F; Gradle(必学)"></a><a href="https://zh.wikipedia.org/wiki/Apache_Maven">依赖管理工具—Maven &#x2F;</a> <a href="https://zh.wikipedia.org/wiki/Gradle">Gradle(必学)</a></h2><p>相信你之前使用依赖的方式是直接将对应的<a href="https://zh.wikipedia.org/wiki/JAR_(%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F)">JAR包</a>放入项目中，但是随着你项目中使用的依赖越来越多，自己管理依赖逐渐变成了一件麻烦的事情。</p>
<p>于是你发现了Maven，这个工具可以帮助你管理你的依赖，并且你可以通过<a href="https://blog.csdn.net/qq_20236937/article/details/135893883">pom.xml</a>的形式配置jar包的版本。从我们自己在Maven仓库中找到对应依赖的jar包，自己导入项目，自己管理，到Maven的一键配置，大大减少了我们为管理依赖头疼的时间。</p>
<p>相关建议:</p>
<ul>
<li><strong>当成工具用就行</strong>，重要性没有那些框架重要</li>
<li>如果缺什么的工具包(比如处理<a href="https://zh.wikipedia.org/wiki/JSON">JSON</a>的)，<strong>建议在awesome-java中找找看有没有推荐的工具包</strong>，然后在Maven仓库中找到对应的依赖，得到对应的pom文件导入就行了</li>
</ul>
<p>推荐文档和仓库:</p>
<ul>
<li><a href="https://maven.apache.org/guides/index.html">Maven – Maven Documentation (apache.org)</a></li>
<li><a href="https://mvnrepository.com/">Maven Repository: Search&#x2F;Browse&#x2F;Explore (mvnrepository.com)</a></li>
<li><a href="https://central.sonatype.com/?smo=true">Maven Central (sonatype.com)</a></li>
<li><a href="https://maven.org.cn/">Maven 中文网</a></li>
</ul>
<h2 id="版本控制工具—Git-必学"><a href="#版本控制工具—Git-必学" class="headerlink" title="版本控制工具—Git(必学)"></a><a href="https://zh.wikipedia.org/wiki/Git">版本控制工具—Git(必学)</a></h2><p>你可能想过：有没有一个工具记录自己做过的事情并且可以标准的区分自己各个版本的功能呢？</p>
<p>当然有咯，那就是Git，于此相应的概念还有<a href="https://zh.wikipedia.org/wiki/GitHub">GitHub</a>，不过前者是一个版本控制工具，类似的还有<a href="https://zh.wikipedia.org/wiki/Subversion">SVN</a>，后者是一个代码托管平台，类似的还有<a href="https://zh.wikipedia.org/wiki/GitLab">GitLab</a>。</p>
<p>我们该怎么学习Git？</p>
<ul>
<li>看过<a href="https://cloud.tencent.com/developer/article/1771872">Git底层设计</a>的朋友都知道，Git对于底层的抽象做的很好，比如底层基于快照的存储，以及无环图的设计，但是对于上层<a href="https://zh.wikipedia.org/wiki/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%8E%A5%E5%8F%A3">API</a>暴露的并不友好，所以推荐<strong>先看一下Git的底层设计</strong>之后，再学习一下Git的API，并结合Git底层的数据结构来思考，每次API操作的是数据的哪一部分，时间久了，你对于Git就会非常熟悉了</li>
</ul>
<p>以及，我们主要要学习Git的哪些API呢？</p>
<ul>
<li>主要是学习提交、推送、拉取、回退、重置、克隆，代码合并、解决冲突的命令。后续根据对应项目场景的需求再对应去学即可</li>
</ul>
<p>推荐文档和网站:</p>
<ul>
<li><a href="https://git-scm.com/book/zh/v2">Git - Book (git-scm.com)</a></li>
<li><a href="https://docs.github.com/zh">GitHub Docs</a></li>
<li><a href="https://nulab.com/zh-cn/learn/software-development/git-tutorial/">Simple Git tutorial for beginners | Nulab</a></li>
<li><a href="https://learngitbranching.js.org/?locale=zh_CN">Learn Git Branching</a></li>
<li><a href="https://training.github.com/downloads/zh_CN/github-git-cheat-sheet/">GitHub Git 备忘单 - GitHub Cheatsheets</a></li>
</ul>
<h2 id="服务器—Linux-必学"><a href="#服务器—Linux-必学" class="headerlink" title="服务器—Linux(必学)"></a><a href="https://zh.wikipedia.org/wiki/Linux">服务器—Linux(必学)</a></h2><p>你是不是有疑惑，为什么自己写的项目只能被自己的电脑访问，而不能被其他的电脑访问？</p>
<p>其实是因为你没有自己的服务器，如果你要让别人访问你的服务，你可以去买一个属于你的域名和一个云服务器，将自己写的服务部署到服务器上，供别人访问。</p>
<p>那么，使用Linux服务器最基本的技能是什么呢？</p>
<p>由于现在的很多前后台项目大多都是在Linux环境下部署的，所以我们至少要了解一些常用的Linux指令。</p>
<p>当然如果你不买现成的云服务器，你还可以使用虚拟机，这样同样可以模拟在云服务器上Linux的操作。学会了基础的Linux操作之后，你将会在之后学习微服务架构时将会如鱼得水。</p>
<p>那么多命令我该怎么记下来呢？</p>
<ul>
<li>你可以<strong>使用man或者tldr</strong>，这些命令会解释每一个命令具体的含义或给出详细的示例，你可以照葫芦画瓢的去使用</li>
<li><strong>每一个命令一般都是对应英文的缩写</strong>，你可以去网上找找看，这样便于你记忆和理解</li>
</ul>
<p>我该看什么了解Linux命令呢？</p>
<ul>
<li>你可以看看<strong>鸟哥的 Linux 私房菜</strong>，这本书对于Linux有一个较为全面的介绍</li>
</ul>
<p>我该怎么用Linux系统呢？</p>
<ul>
<li>我目前主要通过<a href="https://zh.wikipedia.org/wiki/VMware">VMware</a>，<a href="https://zh.wikipedia.org/wiki/VirtualBox">VirtualBox</a>和<a href="https://zh.wikipedia.org/wiki/%E9%80%82%E7%94%A8%E4%BA%8ELinux%E7%9A%84Windows%E5%AD%90%E7%B3%BB%E7%BB%9F#WSL_2">WSL2</a>来使用，推荐使用<a href="https://zh.wikipedia.org/wiki/Ubuntu">Ubuntu</a>，当然使用<a href="https://zh.wikipedia.org/wiki/CentOS">CentOS</a>学习也行</li>
<li>VMware是我一开始学习Linux的时候用的工具，其实也是挺好用的</li>
<li>VirtualBox可以结合<a href="https://zh.wikipedia.org/wiki/Vagrant">Vagrant</a>来使用，实现一键部署虚拟机，这个倒是挺方便的</li>
<li>对了对了，如果你要使用虚拟机来进行学习的话，推荐你使用<a href="https://zh.wikipedia.org/wiki/Xshell">Xshell</a>或者<a href="https://cloud.tencent.com/developer/article/1943148">FinalShell</a>来作为<a href="https://zh.wikipedia.org/wiki/Secure_Shell">SSH</a>连接的工具，要不然你就只能使用黑黑的终端来连接了（这边建议你实在是要用最好自己配置一下<a href="https://github.com/ohmyzsh/ohmyzsh">ohmyzsh</a>，它提供了非常友好的终端界面）</li>
<li>WSL2可以直接在Windows上运行，不用每次学习都要开虚拟机，更加方便</li>
</ul>
<p>这个Linux上有很多好玩的东西，下面我来介绍一下:</p>
<ul>
<li>脚本控制项目部署和起起停停 — <a href="https://www.runoob.com/linux/linux-shell.html">Shell</a></li>
<li>最流行的基于命令行的编辑器 — <a href="https://zh.wikipedia.org/wiki/Vim">Vim</a></li>
<li>最常用的构建系统之一 — <a href="https://zh.wikipedia.org/wiki/Make">make</a></li>
</ul>
<p>推荐网站:</p>
<ul>
<li><a href="https://wangchujiang.com/linux-command/">Linux命令搜索引擎 命令，Linux Linux命令搜索引擎 命令详解：最专业的Linux命令大全，内容包含Linux命令手册、详解、学习，值得收藏的Linux命令速查手册。 - Linux 命令搜索引擎 (wangchujiang.com)</a></li>
<li><a href="https://www.linuxcool.com/">Linux命令大全(手册) – 真正好用的Linux命令在线查询网站 (linuxcool.com)</a></li>
<li><a href="https://mirrors.tuna.tsinghua.edu.cn/">清华大学开源软件镜像站 | Tsinghua Open Source Mirror</a></li>
</ul>
<h2 id="前端基础—前端三剑客HTML，CSS，JavaScript-选学，这个不做要求，感兴趣可以了解一下"><a href="#前端基础—前端三剑客HTML，CSS，JavaScript-选学，这个不做要求，感兴趣可以了解一下" class="headerlink" title="前端基础—前端三剑客HTML，CSS，JavaScript(选学，这个不做要求，感兴趣可以了解一下)"></a><a href="https://zh.wikipedia.org/wiki/HTML">前端基础—前端三剑客HTML</a>，<a href="https://zh.wikipedia.org/wiki/CSS">CSS</a>，<a href="https://zh.wikipedia.org/wiki/JavaScript">JavaScript(选学，这个不做要求，感兴趣可以了解一下)</a></h2><p>作为一位想要从事后端工作的程序员来说，了解一点前端知识是很有用处的。</p>
<p>正如你所了解的一样，后端负责给前端提供数据，前端负责在页面上展示数据。为了使我们和前端的小伙伴们配合的更好，我们可以适当了解一些关于前端的知识，知道数据怎么提交给前端，以便于前端处理，知道前端是怎么获取到后端数据的等等。</p>
<p>那么，我们具体需要掌握到什么层次呢？</p>
<ul>
<li>当然，对于后端程序员来说，你<strong>不需要知道太多的前端知识</strong>，但是基础的你还是要会的，比如说：HTML，CSS， JavaScript。当然你还可以了解一点<a href="https://zh.wikipedia.org/wiki/Vue.js">Vue</a>等等前端框架，太多的我们不需要掌握，了解一下目前主流前端开发框架就行了</li>
</ul>
<p>好的，完成上面内容的基本学习之后，你将掌握基本的项目开发能力，但是想要做到一个合格的后端程序员是不容易的，接下来，我们还需要学会这些知识。</p>
<h2 id="设计模式—优化代码-选学，想要项目有更好设计的学"><a href="#设计模式—优化代码-选学，想要项目有更好设计的学" class="headerlink" title="设计模式—优化代码(选学，想要项目有更好设计的学)"></a><a href="https://zh.wikipedia.org/wiki/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F_(%E8%AE%A1%E7%AE%97%E6%9C%BA)">设计模式—优化代码(选学，想要项目有更好设计的学)</a></h2><p>也许你已经写出过几千行的代码了，但是你或许会发现自己每改动一个位置的代码，很多位置都会发生变化，你常常因为这件事情焦头烂额，觉得工作量太大。</p>
<p>其实大可不必，很多时候都是我们没有设计好类与类之间的关系，使得我们后期加需求，维护时的难度大大提升。</p>
<p>简而言之，设计模式可以让你写出<strong>高质量的代码(Safe from bugs,Easy to understand,Ready for change)。</strong></p>
<p>设计模式简单的说有三个模式：创建型模式，结构型模式，行为型模式。当然，你肯定不想让我在这里介绍一些枯燥的知识，所以我这里直接说说我对设计模式的看法：</p>
<ul>
<li>设计模式不应该一开始就强行运用上。也许你会疑惑，刚才不是说了设计模式很好很好云云之类的，为什么这里又不推荐用呢？其实是因为我们项目最开始的时候，需求并不清楚，代码并不复杂，如果直接使用设计模式会使得代码可读性降低，而且不能有的放矢(不是所有的类都需要通过设计模式来设计)，只有当后期需求逐渐完整，类与类之间的关系变得逐渐复杂，你<strong>发现可以优化的时候再推荐使用</strong>（毕竟软件开发又不是一次就结束了，而是会经历很多轮的迭代优化）</li>
<li>最开始的时候<strong>不要为了使用设计模式而使用设计模式</strong>。学习设计模式可以让我们在读对应优秀框架源码的时候游刃有余(比如Spring就是用了很多的设计模式，如果你不学，很可能看都看不懂这是在干什么)</li>
</ul>
<p>推荐资料(除了设计模式本身，也包含了其他让你写出高质量代码的资料):</p>
<ul>
<li><a href="https://java-design-patterns.com/">Explore, Learn, and Master Industry-Standard Patterns | Java Design Patterns (java-design-patterns.com)</a></li>
<li><a href="https://ocw.mit.edu/courses/6-005-software-construction-spring-2016/pages/readings/">Readings | Software Construction | Electrical Engineering and Computer Science | MIT OpenCourseWare</a></li>
</ul>
<h2 id="缓存—Redis-必学"><a href="#缓存—Redis-必学" class="headerlink" title="缓存—Redis(必学)"></a><a href="https://zh.wikipedia.org/wiki/Redis">缓存—Redis(必学)</a></h2><p>在你使用MySQL的时候不知道你有没有想到过，如果频繁的对于一些经常要访问的数据进行SQL查询是十分浪费性能的(因为数据库是从硬盘里面读取数据到内存中)。</p>
<p>在业务处理中IO往往是导致处理请求慢的罪魁祸首，所以我们不妨使用一下缓存技术吧，将数据缓存到内存中，然后直接省去了从硬盘中读取这一步！</p>
<p>要知道缓存经常访问的不易变化的数据对于提升系统性能是有很大帮助的！</p>
<p>缓存其实和数据库类似，毕竟都是对数据进行存储的，所以一般而言，我们对其的操作也不会超过增删改查。</p>
<p>但是与关系型数据库不同的是，缓存常常和业务是强耦合的，你需要根据实际的情况判断哪些需要缓存，哪些不需要缓存(对于经常变化的数据，不建议缓存，对于不经常变化的数据，建议缓存)。</p>
<p>如果你学完之后意犹未尽，不妨看看<a href="https://github.com/ben-manes/caffeine">Caffeine</a>(这是一个很优秀的本地缓存框架)和多级缓存(可以使用<a href="https://zh.wikipedia.org/wiki/Lua">Lua脚本</a>在<a href="https://zh.wikipedia.org/wiki/Nginx">Nginx</a>里构建缓存)。</p>
<p>推荐文档:</p>
<ul>
<li><a href="https://redis.io/docs/latest/commands/">Commands | Docs (redis.io)</a></li>
<li><a href="http://redis.ac.cn/docs/latest/">文档 - Redis 中文</a></li>
<li><a href="https://redis.com.cn/documentation.html">redis中文文档</a></li>
</ul>
<h2 id="性能压测—JMeter-选学，想要测试一下自己程序性能的可以试试"><a href="#性能压测—JMeter-选学，想要测试一下自己程序性能的可以试试" class="headerlink" title="性能压测—JMeter(选学，想要测试一下自己程序性能的可以试试)"></a><a href="https://en.wikipedia.org/wiki/Apache_JMeter">性能压测—JMeter(选学，想要测试一下自己程序性能的可以试试)</a></h2><p>对了，看到这里你可能会问，为什么用了缓存之后，你就知道性能提升了啊？</p>
<p>没错，我们接下来介绍一下JMeter。</p>
<p>JMeter就是用来做压测的，你可以多关注一下你加了缓存和没有加缓存的<a href="https://zhuanlan.zhihu.com/p/337708438">QPS</a>分别是多少，这时你就会发现缓存的厉害之处了。</p>
<p>这个就不用专门去看文档学习了，它只是一个工具，具体的可以去CSDN上看看是怎么实现压测的。</p>
<p>当然，这里提一下，我们这个压测仅仅只是初略的估计，因为你电脑不可能只跑你的服务，可能会打开一些Edge浏览器窗口，可能会挂着QQ，它们都会占用你电脑的系统资源。</p>
<p>同时，JMeter发送请求的时候也会占用你电脑的资源(毕竟是你电脑的一个进程)，所以我们这里只是大概看一下自己写的程序的性能罢了。</p>
<p>要想真正的测试你程序的性能，建议你把你的服务放在一台远程服务器上跑，然后用本地电脑发送JMeter请求，这样测试的相较于上面的就更加精确了。</p>
<h2 id="消息队列—RabbitMQ-必学"><a href="#消息队列—RabbitMQ-必学" class="headerlink" title="消息队列—RabbitMQ(必学)"></a><a href="https://zh.wikipedia.org/wiki/RabbitMQ">消息队列—RabbitMQ(必学)</a></h2><p>消息是消息，队列是队列，两个合在一起就是消息队列(在队列里存放的是一个一个的消息对象)。</p>
<p>这项技术主要是用于异步消息，比如说调用一个服务的链路很长，就以订单服务来说，首先创建订单，扣减库存，扣减账户余额，创建完成订单，一下子就调用到了订单服务，库存服务，用户账户服务这三个服务，如果是依次调用的话那么用户可能买完东西后会等好一会才有反应，那么如果链路变的更长呢？</p>
<p>所以我们急需一种技术可以实现传输和保存消息，将各个服务的请求交由消息队列去处理(这里再举一个小例子，快递员是直接上楼将快递给你快，还是统一将快递交由快递柜快，答案显而易见)。</p>
<p>消息队列也是同样的设计思想，与其将消息传递给对接的服务，不如解耦，去找一个代理的去异步处理消息。</p>
<p>消息队列那么多，我该学习哪个呢？</p>
<ul>
<li>推荐学习RabbitMQ，主要是因为RabbitMQ比<a href="https://zh.wikipedia.org/wiki/Kafka">Kafka</a>更简单，当然建议感兴趣的可以学学Kafka的设计理念。还有其他的消息队列你可以适当了解，比如<a href="https://zh.wikipedia.org/wiki/Apache_RocketMQ">RocketMQ</a>和<a href="https://zh.wikipedia.org/wiki/Apache_ActiveMQ">ActiveMQ</a>，但它们的社区没有前两者活跃，资料也比较少，所以不是很推荐初学者进行学习</li>
</ul>
<p>什么时候使用到它呢？</p>
<ul>
<li>当你的<strong>调用链路过长的时候</strong>，你就可以考虑使用消息队列来处理你项目中遇到的问题了</li>
</ul>
<p>推荐文档:</p>
<ul>
<li><a href="https://www.rabbitmq.com/docs">RabbitMQ Documentation | RabbitMQ</a></li>
<li><a href="https://rabbitmq.org.cn/">RabbitMQ：一个经纪人来管理所有队列 | RabbitMQ 中文</a></li>
</ul>
<h2 id="反向代理服务器—Nginx-选学，一般只要了解一下怎么用即可"><a href="#反向代理服务器—Nginx-选学，一般只要了解一下怎么用即可" class="headerlink" title="反向代理服务器—Nginx(选学，一般只要了解一下怎么用即可)"></a><a href="https://zh.wikipedia.org/wiki/Nginx">反向代理服务器—Nginx(选学，一般只要了解一下怎么用即可)</a></h2><p>你是否还在为Tomcat的性能而担忧，那么，来试试使用Nginx吧，Nginx一般使用在<a href="https://zh.wikipedia.org/wiki/Apache_Tomcat">Tomcat</a>的前面，用于抗住较高的并发量。</p>
<p>一般用于存储前端的静态资源，实现动静分离，屏蔽后面的服务器，实现反向代理。</p>
<p>如果你要学习Nginx的话，建议你了解一下<a href="https://zh.wikipedia.org/wiki/OpenResty">OpenResty</a>，相较于Nginx，OpenResty的功能更多，而且我们也更容易使用Lua脚本语言来改造，加上一些自己特制的功能，当然<a href="https://zh.wikipedia.org/wiki/Tengine">Tengine</a>也不错。</p>
<p>推荐文档:</p>
<ul>
<li><a href="https://nginx.org/en/docs/">nginx documentation</a></li>
<li><a href="https://nginxserver.cn/en/docs/">nginx 文档 - Nginx 中文 (nginxserver.cn)</a></li>
<li><a href="https://openresty.org/cn/">OpenResty® - 开源官方站</a></li>
<li><a href="https://tengine.taobao.org/documentation_cn.html">文档 - The Tengine Web Server (taobao.org)</a></li>
</ul>
<h2 id="网路编程—Netty-选学，除非你对于网络编程很感兴趣"><a href="#网路编程—Netty-选学，除非你对于网络编程很感兴趣" class="headerlink" title="网路编程—Netty(选学，除非你对于网络编程很感兴趣)"></a><a href="https://zh.wikipedia.org/wiki/Netty">网路编程—Netty(选学，除非你对于网络编程很感兴趣)</a></h2><p>如果你对于框架底层的网络编程很感兴趣，那么一定要学学Netty，可以说，很多框架的高性能是多亏了它。</p>
<p>可以说Netty才是集Java并发的大成者，它才是Java真正的高并发，有兴趣的可以了解一下，个人觉得难度还是不小的。</p>
<p>也许你觉得它和你太过遥远，其实不然，Redis客户端用过Netty，<a href="https://zh.wikipedia.org/wiki/Elasticsearch">Elasticsearch</a>底层使用的是Netty，很多微服务框架底层使用的也是Netty，比如<a href="https://zh.wikipedia.org/wiki/%E5%BE%AE%E6%9C%8D%E5%8B%99">Spring Cloud</a>、<a href="https://cn.dubbo.apache.org/zh-cn/">Dubbo</a>等，Netty的高性能使得很多框架都喜欢使用它。</p>
<h2 id="微服务框架—SpringCloud-必学"><a href="#微服务框架—SpringCloud-必学" class="headerlink" title="微服务框架—SpringCloud(必学)"></a><a href="https://zh.wikipedia.org/wiki/%E5%BE%AE%E6%9C%8D%E5%8B%99">微服务框架—SpringCloud(必学)</a></h2><p>微服务，什么是微服务，微服务就是将原来巨大的单体服务拆分成一个个职责明确、功能独立的细小模块，然后再将这些模块分别部署到服务器上。</p>
<p>那么，为什么会出现微服务呢？</p>
<ul>
<li>答案很简单，当你的项目不断的增加新的需求变的越来越大，越复杂时，总有一天会迎来被拆分的结局。就类似于大学物理，为了你更好理解，会将对应的章节拆分开，分成第几章第几节，不会说直接一大坨没有目录没有拆分的让你学。对于程序员也是一样，如果一个项目越来越庞大，那么为了使开发效率变高，必然会将一个庞大的项目拆分成一个个功能独立的小模块</li>
</ul>
<p>推荐学习的路线：</p>
<ul>
<li>先学习Dubbo，因为Dubbo对于底层的封装更少，可以使你快速的了解到<a href="https://blog.csdn.net/weixin_42046751/article/details/109510811">分布式</a>，<a href="https://zh.wikipedia.org/wiki/%E9%81%A0%E7%A8%8B%E9%81%8E%E7%A8%8B%E8%AA%BF%E7%94%A8">RPC</a>，然后再是Spring Cloud和<a href="https://github.com/alibaba/spring-cloud-alibaba">Spring Cloud Alibaba</a>，因为后两者对于底层的封装太完善了，如果直接学很有可能使你不知道一下子是怎么来的</li>
</ul>
<p>组件那么多，我该先学哪些？</p>
<ul>
<li>其实我觉得比较重要的组件是Consul &#x2F; Nacos服务注册中心，OpenFeign远程服务调用，Gateway服务网关，Sentinel熔断限流，Seata分布式事务，Micrometer Tracing服务链路追踪 + Zipkin 链路可视化</li>
<li>可能你会了解到一些其他的组件和我这里说的不一样，比如这里的Micrometer Tracing，以前是Sleuth，不过现在有点过时了，因为微服务作为一个新兴领域发展的很快嘛，所以一些组件可能过了一段事件之后就会变化，所以我们没有必要同一种类型的技术学那么多，学好一种，到时候直接带着原有的知识迁移就行了</li>
</ul>
<h2 id="容器—Docker-必学"><a href="#容器—Docker-必学" class="headerlink" title="容器—Docker(必学)"></a><a href="https://zh.wikipedia.org/wiki/Docker">容器—Docker(必学)</a></h2><p>当你想要在你的服务器上运行你自己的项目时，你会发现，你要配置对应的环境，而且自己的项目没有一个统一的工具进行管理。</p>
<p>那么为什么我们不能将这个流程自动化呢？毕竟计算机就是解放人类的，如果你想要减轻自己配置环境的负担而且不想因为自己小测试需要的环境污染整个计算机环境，那么Docker将会是你的一个不错的选择。</p>
<p>Docker主要做的是将你的项目和你项目所需的环境进行封装，可以使我们的程序直接在Linux服务器上跑，省去了我们亲自配置环境的麻烦事(直接写好一个<a href="https://zh.wikipedia.org/wiki/Docker#Dockerfile">Dockerfile</a>就行了)，而且由于其很好的隔离性，不会使得程序之间相互影响。</p>
<p>主要是多练练相关的命令，可以试着将自己的项目采用Docker的方式部署在虚拟机上试试，或者使用WSL2，这也是一个不错的选择。</p>
<p>推荐文档和仓库:</p>
<ul>
<li><a href="https://docs.docker.com/">Docker Docs</a></li>
<li><a href="https://docker.github.net.cn/">Docker中文网 (github.net.cn)</a></li>
<li><a href="https://hub.docker.com/">Docker Hub Container Image Library | App Containerization</a></li>
</ul>
<h2 id="容器编排—Kubernetes-选学，有自动化部署集成需求的学"><a href="#容器编排—Kubernetes-选学，有自动化部署集成需求的学" class="headerlink" title="容器编排—Kubernetes(选学，有自动化部署集成需求的学)"></a><a href="https://zh.wikipedia.org/wiki/Kubernetes">容器编排—Kubernetes(选学，有自动化部署集成需求的学)</a></h2><p>当你的容器启动的越来越多时，再采用原始的手动控制容器已经不适合了，毕竟你也不想将时间浪费在容器的起起停停上吧，于是我们有了更加自动化的工具Kubernetes，帮助我们对这些容器进行管理。</p>
<p>这里说说我一开始使用Kubernetes遇到的问题：</p>
<ul>
<li>不要直接在笔记本上部署完整的Kubkernetes集群，要不然你的电脑可能会承受不住直接死机，对于性能较差的笔记本电脑来说，部署<a href="https://kubernetes.io/zh-cn/docs/tutorials/hello-minikube/">Minikube</a>或<a href="https://docs.k3s.io/zh/">K3s</a>是一个更加明知的选择(比起Kubkernetes来说更加轻量)，毕竟我们只是想学习它，不是非要部署一个完整的才行</li>
</ul>
<p>推荐文档:</p>
<ul>
<li><a href="https://kubernetes.io/zh-cn/docs/home/">Kubernetes 文档 | Kubernetes</a></li>
<li><a href="https://kubernetes.ac.cn/">Kubernetes (K8s) 中文</a></li>
</ul>
<h2 id="CI-CD—Jenkins-选学，有自动化部署集成需求的学"><a href="#CI-CD—Jenkins-选学，有自动化部署集成需求的学" class="headerlink" title="CI &#x2F; CD—Jenkins(选学，有自动化部署集成需求的学)"></a><a href="https://zh.wikipedia.org/zh-cn/Jenkins_(%E8%BD%AF%E4%BB%B6)">CI &#x2F; CD—Jenkins(选学，有自动化部署集成需求的学)</a></h2><p>不知道各位有没有使用过<a href="https://zh.wikipedia.org/wiki/GitHub_Pages">Github Pages</a>这个功能，每次你将最新的静态网站代码交上Github之后，再次访问自己的网站就会发现内容更新了，那么到底是谁偷偷的为我们做了哪些本该由我们做的部署呢？</p>
<p>答案是<a href="https://docs.github.com/zh/actions">Github Actions</a>，GitHub有属于自己的一套<a href="https://zh.wikipedia.org/wiki/CI/CD">CI &#x2F;CD</a>(持续集成，持续交付，简单的理解就是持续交代码，持续帮你部署发布)。</p>
<p>这里讲讲我在学习Jenkins时遇到的一些问题：</p>
<ul>
<li>其实也没有多少，但是一定要注意Jenkins插件对于你下载Jenkins的版本是有要求的，为了较为快乐的学习这个工具，建议使用最新版的Jenkins来学习，至少你不会遇见一些因为插件版本不适配导致的奇怪Bug</li>
</ul>
<p>推荐文档:</p>
<ul>
<li><a href="https://www.jenkins.io/zh/doc/">Jenkins 用户手册</a></li>
</ul>
<h2 id="并发编程—JUC-必学"><a href="#并发编程—JUC-必学" class="headerlink" title="并发编程—JUC(必学)"></a><a href="https://www.cnblogs.com/javastack/p/15467349.html">并发编程—JUC(必学)</a></h2><p>都说Java百万并发，千万并发的，其实说的就是Java的JUC，JUC旨在教导你使用计算机有限的资源去应对更多的请求压力，看到这里相信你也明白了，这是属于程序优化的内容。</p>
<p>建议就是菜就多练，这部分是Java最难的部分之一，没有长时间的实践积累，学习原理，你是拿不下这块内容的。</p>
<p>推荐网站(建议自己找书看):</p>
<ul>
<li><a href="https://zh.singlelogin.re/">Z-Library – 世界上最大的电子图书馆。自由访问知识和文化 (singlelogin.re)</a></li>
</ul>
<h2 id="虚拟机—JVM-必学"><a href="#虚拟机—JVM-必学" class="headerlink" title="虚拟机—JVM(必学)"></a><a href="https://zh.wikipedia.org/wiki/Java%E8%99%9A%E6%8B%9F%E6%9C%BA">虚拟机—JVM(必学)</a></h2><p>和上面的JUC一样，都是Java中的大头，对于JVM而言，相信你可能听过JVM调优，没错，又是对Java程序优化，在这里你将学习到如何从JVM的层面了解Java代码，会使你看待Java程序的观点产生质的变化。</p>
<p>这边建议结合视频和书(《深入理解Java虚拟机（第3版）》)一起看，毕竟这个东西比较枯燥，可能你会看不下去书，还有就是推荐使用<a href="https://arthas.aliyun.com/doc/">Arthas</a>这个工具来帮助我们学习。</p>
<p>推荐工具和文档(建议自己找书看):</p>
<ul>
<li><a href="https://arthas.aliyun.com/doc/commands.html">命令列表 | arthas (aliyun.com)</a></li>
<li><a href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html">java (oracle.com)</a></li>
<li><a href="https://zh.singlelogin.re/">Z-Library – 世界上最大的电子图书馆。自由访问知识和文化 (singlelogin.re)</a></li>
</ul>
<h2 id="任务调度—XXL-JOB-选学，一般了解即可"><a href="#任务调度—XXL-JOB-选学，一般了解即可" class="headerlink" title="任务调度—XXL-JOB(选学，一般了解即可)"></a><a href="https://github.com/xuxueli/xxl-job">任务调度—XXL-JOB(选学，一般了解即可)</a></h2><p>任务交由谁来执行呢，定时任务该怎么做，如果你有这样的需求，那么XXL-JOB一定对你的胃口。</p>
<p>个人感觉和RabbitMQ的功能比较类似，但是比起RabbitMQ，XXL-JOB在实际的项目使用中更加轻量化。</p>
<p>推荐文档:</p>
<ul>
<li><a href="https://www.xuxueli.com/xxl-job/">分布式任务调度平台XXL-JOB (xuxueli.com)</a></li>
</ul>
<h2 id="搜索引擎—Elasticsearch-选学，等你项目需要搜索功能的时候再学"><a href="#搜索引擎—Elasticsearch-选学，等你项目需要搜索功能的时候再学" class="headerlink" title="搜索引擎—Elasticsearch(选学，等你项目需要搜索功能的时候再学)"></a><a href="https://zh.wikipedia.org/wiki/Elasticsearch">搜索引擎—Elasticsearch(选学，等你项目需要搜索功能的时候再学)</a></h2><p>这个其实也和数据库比较像，想想你平时搜索，如果匹配到了是会高亮的，或者说，不是完全匹配也可以找到数据，其实这都是它的功劳(比如你搜索笔记电脑，但是很明显，没有这个东西，可能是你打掉了一个字，但是搜索的时候依然可以搜索出来笔记本电脑)。</p>
<p>为要搜索的数据建立索引，主要用于快速搜索。</p>
<p>这个Elasticsearch是<a href="https://www.elastic.co/cn/elastic-stack/">ELK</a>的一部分，其中<a href="https://en.wikipedia.org/wiki/Kibana">Kibana</a>可以可视化你存在Elasticsearch中的数据，而且还提供了一个写<a href="https://zh.wikipedia.org/wiki/%E9%A2%86%E5%9F%9F%E7%89%B9%E5%AE%9A%E8%AF%AD%E8%A8%80">DSL</a>语句的页面，有点类似HttpClient。</p>
<p>推荐文档:</p>
<ul>
<li><a href="https://www.elastic.co/docs">Documentation (elastic.co)</a></li>
</ul>
<h2 id="链路追踪—SkyWalking-选学，除非你的项目很大，链路层级很深"><a href="#链路追踪—SkyWalking-选学，除非你的项目很大，链路层级很深" class="headerlink" title="链路追踪—SkyWalking(选学，除非你的项目很大，链路层级很深)"></a><a href="https://skywalking.apache.org/">链路追踪—SkyWalking(选学，除非你的项目很大，链路层级很深)</a></h2><p>当你链路长了的时候推荐使用，主要是帮助你分析调用链路的，比如是在哪里调用出现了问题，哪里的请求处理耗时太长等等。</p>
<p>这个你也可以不用学，因为我们之前已经学习了Micrometer Tracing服务链路追踪，已经学习了一个链路追踪的组件了，如果你学了一个的话，其实也没有必要学习太多，学好一个就行了。</p>
<p>推荐网站:</p>
<ul>
<li><a href="https://skywalking.apache.org/">Apache SkyWalking</a></li>
<li><a href="https://skywalking.apache.org/zh/2020-04-19-skywalking-quick-start/">SkyWalking 极简入门 | Apache SkyWalking</a></li>
</ul>
<h2 id="读写分离—ShardingSphere-MyCat-选学，除非你的项目一个数据库不够用"><a href="#读写分离—ShardingSphere-MyCat-选学，除非你的项目一个数据库不够用" class="headerlink" title="读写分离—ShardingSphere &#x2F; MyCat(选学，除非你的项目一个数据库不够用)"></a><a href="https://shardingsphere.apache.org/index_zh.html">读写分离—ShardingSphere &#x2F;</a> <a href="http://www.mycat.org.cn/">MyCat(选学，除非你的项目一个数据库不够用)</a></h2><p>可以帮助你实现读写分离，分库分表，一般是在你一个数据库不够应对现有并发量的情况下使用，否则不建议学习，建议有需要了再学习。</p>
<p>这个建议直接看官方文档，主要是官方文档写的比较精炼，而且基本上照着文档上面说的配置一般都没有什么问题，不过要注意使用的数据库连接客户端的版本不能太低，要不然会产生不兼容的问题。</p>
<p>至于这两个我们到底要选择哪一个，我推荐学习使用ShardingSphere，因为它的社区更活跃，功能更多，比MyCat更加完善。</p>
<p>推荐文档:</p>
<ul>
<li><a href="https://shardingsphere.apache.org/document/current/cn/overview/">概览 :: ShardingSphere (apache.org)</a></li>
</ul>
<h2 id="同步数据—Canal-选学，建议了解即可，毕竟我们也不是要做运维人员"><a href="#同步数据—Canal-选学，建议了解即可，毕竟我们也不是要做运维人员" class="headerlink" title="同步数据—Canal(选学，建议了解即可，毕竟我们也不是要做运维人员)"></a><a href="https://github.com/alibaba/canal">同步数据—Canal(选学，建议了解即可，毕竟我们也不是要做运维人员)</a></h2><p>主要是为了解决主从数据库数据的同步问题，一般要用的话会配合MySQL使用。</p>
<h2 id="存图片—OSS-选学，建议当你遇到存图片的需求的时候再去有针对性的学"><a href="#存图片—OSS-选学，建议当你遇到存图片的需求的时候再去有针对性的学" class="headerlink" title="存图片—OSS(选学，建议当你遇到存图片的需求的时候再去有针对性的学)"></a><a href="https://www.aliyun.com/product/oss">存图片—OSS(选学，建议当你遇到存图片的需求的时候再去有针对性的学)</a></h2><p>嗯，想想看，当你有了一个用户系统，然后每一个用户都会上传自己的头像，随着用户越来越多，你会发现这些图片好像浪费了你服务器的很多空间，那么你有没有考虑过将图片存储在其他人的服务器上以避免这种情况呢？</p>
<p>如果你想优化一下，那么不妨试一试阿里云的OSS云存储吧，免费又实用。</p>
<p>推荐网站:</p>
<ul>
<li><a href="https://www.aliyun.com/">阿里云-计算，为了无法计算的价值 (aliyun.com)</a></li>
</ul>
<h2 id="超好用的工具包—Hutool-必学"><a href="#超好用的工具包—Hutool-必学" class="headerlink" title="超好用的工具包—Hutool(必学)"></a><a href="https://www.hutool.cn/docs/#/">超好用的工具包—Hutool(必学)</a></h2><p>你是不是在为有时候没有好用的工具而烦恼，又或者是自己写的小工具总是有Bug，那么不妨尝试一下这款工具包吧，里面包含了对文件、流、加密解密、转码、正则、线程、XML等JDK方法的封装，用起来既方便又安全。</p>
<p>推荐文档:</p>
<ul>
<li><a href="https://hutool.cn/">Hutool🍬一个功能丰富且易用的Java工具库，涵盖了字符串、数字、集合、编码、日期、文件、IO、加密、数据库JDBC、JSON、HTTP客户端等功能</a></li>
</ul>
<h2 id="笔者最后想说的话"><a href="#笔者最后想说的话" class="headerlink" title="笔者最后想说的话"></a>笔者最后想说的话</h2><p>什么？你竟然看到这里来了，那么不管怎么说，你一定是热爱技术的，不过我想在此提提我学了这么久的一些心得体会：</p>
<ul>
<li>框架是为了解决问题而出现的，我们不应该痴迷于学框架，而是应该把自己的基础搞扎实，比如多看看类似<a href="https://book.douban.com/subject/1148282/">SICP</a>这样的书籍。所以一般推荐你遇到了相应的问题再去找框架解决，而不是盲目的去找框架学习。还有就是记住一点，框架会不停的变化，但是它们底层设计的思想一般是不会变化的(所以我们没有必要每一个都学，学一个经典的其他的都差不多了，感兴趣再多了解一下就行)</li>
<li>我们要锻炼自己看文档的能力。就以我为例吧，我之前学习的时候总是去在B站上找视频看，但是后来我发现这样学习的效率实在是太低了，于是开始慢慢尝试去看一些中文文档去学习（上面的ShardingSphere我就是照着文档学习的），你会发现，当你会通过文档学习之后，学习的速度会变的非常快</li>
<li>善于使用搜索平台。Stack Overflow和Github是你的好帮手，前者可以搜问题，后者可以找代码</li>
<li>不要过于依赖别人的回答。有时候，别人的回答效率还不如你自己去解决这个问题的效率高，建议是在自己的程序出现Bug之后，先自己在网上搜，用GPT问，各种Debug的方法尝试了，自己实在是解决不了了再去问，关于问问题的规范可以看看这个<a href="https://github.com/ryanhanwu/How-To-Ask-Questions-The-Smart-Way/blob/main/README-zh_CN.md">How-To-Ask-Questions-The-Smart-Way</a></li>
</ul>
<p>最后的最后，能帮作者点一个<a href="https://github.com/z0l0y/My-Backend-Learning-Journey">star</a>吗？举手之劳万分感谢。</p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Backend</tag>
      </tags>
  </entry>
  <entry>
    <title>Java9到22的一些新特性</title>
    <url>/2024/09/10/Java22-new-features/</url>
    <content><![CDATA[<h2 id="Java9新特性"><a href="#Java9新特性" class="headerlink" title="Java9新特性"></a>Java9新特性</h2><h3 id="接口里面可以声明私有方法了"><a href="#接口里面可以声明私有方法了" class="headerlink" title="接口里面可以声明私有方法了"></a>接口里面可以声明私有方法了</h3><p>在JDK9中新增了接口私有方法，我们可以在接口中声明private修饰的方法了。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">Test</span> &#123;</span><br><span class="line">    <span class="comment">// 定义私有方法</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">solution</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;Hello Java!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="改进try-with-resource"><a href="#改进try-with-resource" class="headerlink" title="改进try with resource"></a>改进try with resource</h3><p>Java7中新增了try with resource语法用来自动关闭资源文件，在IO流和JDBC部分使用的比较多。</p>
<p><strong>使用方式是将需要自动关闭的资源对象的创建放到try后面的小括号中，在JDK9中我们可以将这些资源对象的创建代码放到小括号外面，然后将需要关闭的对象名放到try后面的小括号中即可</strong>，示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    改进了try-with-resources语句，可以在try外进行初始化，在括号内填写引用名，即可实现资源自动关闭</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TryWithResource</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> FileNotFoundException &#123;</span><br><span class="line">        <span class="comment">// JDK8以前</span></span><br><span class="line">        <span class="keyword">try</span> (<span class="type">FileInputStream</span> <span class="variable">fileInputStream</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="string">&quot;&quot;</span>);</span><br><span class="line">             <span class="type">FileOutputStream</span> <span class="variable">fileOutputStream</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="string">&quot;&quot;</span>)) &#123;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// JDK9</span></span><br><span class="line">        <span class="type">FileInputStream</span> <span class="variable">fis</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="string">&quot;&quot;</span>);</span><br><span class="line">        <span class="type">FileOutputStream</span> <span class="variable">fos</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(<span class="string">&quot;&quot;</span>);</span><br><span class="line">        <span class="comment">// 多资源用分号隔开</span></span><br><span class="line">        <span class="keyword">try</span> (fis; fos) &#123;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="不能使用下划线命名变量"><a href="#不能使用下划线命名变量" class="headerlink" title="不能使用下划线命名变量 __"></a>不能使用下划线命名变量 __</h3><p>下面语句在JDK9之前可以正常编译通过，但是在JDK9（含）之后编译报错，在后面的版本中会将下划线作为关键字来使用。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">_</span> <span class="operator">=</span> <span class="string">&quot;Java&quot;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="String字符串的变化"><a href="#String字符串的变化" class="headerlink" title="String字符串的变化"></a>String字符串的变化</h3><p>写程序的时候会经常用到String字符串，在以前的版本中String内部使用了char数组存储，对于使用英语的人来说，一个字符用一个字节就能存储，使用char存储字符会浪费一半的内存空间，因此在JDK9中将String内部的char数组改成了byte数组，这样就节省了一半的内存占用。</p>
<p>String中增加了下面2个成员变量</p>
<ul>
<li>COMPACT_STRINGS：判断是否压缩，默认是true，若为false，则不压缩，使用UTF16编码</li>
<li>coder用来区分使用的字符编码，分别为LATIN1（值为0）和UTF16（值为1）</li>
</ul>
<p>byte数组如何存储中文呢？通过源码（StringUTF16类中的toBytes方法）得知，<strong>在使用中文字符串时，1个中文会被存储到byte数组中的两个元素上</strong>，即存储1个中文，<strong>byte数组长度为2，存储2个中文，byte数组长度为4</strong>。</p>
<p>以如下代码为例进行分析：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> <span class="string">&quot;好&quot;</span></span><br></pre></td></tr></table></figure>

<p>好对应的Unicode码二进制为0101100101111101，**分别取出高8位和低8位，放入到byte数组中{01011001,01111101}**，这样就利用byte数组的2个元素保存了1个中文。</p>
<p>当字符串中存储了<strong>中英混合的内容时，1个英文字符会占用2个byte数组位置</strong>。</p>
<p>在获取字符串长度时，若存储的内容存在中文，是不能直接获取byte数组的长度作为字符串长度的，String源码中有向右移动1位的操作（即除以2），这样才能获取正确的字符串长度。</p>
<h3 id="Deprecated注解的变化"><a href="#Deprecated注解的变化" class="headerlink" title="@Deprecated注解的变化"></a>@Deprecated注解的变化</h3><p>该注解用于标识废弃的内容，在JDK9中新增了2个内容：</p>
<ul>
<li>String since() default “”：标识是从哪个版本开始废弃</li>
<li>boolean forRemoval() default false：标识该废弃的内容会在未来的某个版本中移除</li>
</ul>
<h3 id="jshell"><a href="#jshell" class="headerlink" title="jshell"></a>jshell</h3><p>在一些编程语言中，例如：python，Ruby等，都提供了REPL（Read Eval Print Loop 简单的交互式编程环境）。jshell就是Java语言平台中的REPL。</p>
<p>有的时候我们只是想写一段简单的代码，例如HelloWorld，按照以前的方式，还需要自己创建Java文件，创建class，编写main方法，但实际上里面的代码其实就是一个打印语句，此时还是比较麻烦的。在JDK9中新增了jshell工具，可以帮助我们快速的运行一些简单的代码。</p>
<p>从命令提示符里面输入jshell，进入到jshell之后输入：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.out.println(<span class="string">&quot;HelloWorld&quot;</span>);</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/10/Java22-new-features/image-20240911000358964.png" alt="image-20240911000358964"></p>
<p>如果要退出jshell的话，输入&#x2F;exit即可。</p>
<h2 id="Java10新特性"><a href="#Java10新特性" class="headerlink" title="Java10新特性"></a>Java10新特性</h2><h3 id="局部变量类型推断-var"><a href="#局部变量类型推断-var" class="headerlink" title="局部变量类型推断 var"></a>局部变量类型推断 var</h3><p>在JDK10以前声明变量的时候，我们会像下面这样：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">oldName</span> <span class="operator">=</span> <span class="string">&quot;jack&quot;</span>;</span><br><span class="line"><span class="type">int</span> <span class="variable">oldAge</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line"><span class="type">long</span> <span class="variable">oldMoney</span> <span class="operator">=</span> <span class="number">88888888L</span>;</span><br><span class="line"><span class="type">Object</span> <span class="variable">oldObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Object</span>();</span><br></pre></td></tr></table></figure>

<p>上面我们声明的时候使用了4种不同类型的变量，在JDK10中前面的类型都可以使用var来代替，JVM会自动推断该变量是什么类型的，例如可以这样写：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">var</span> <span class="variable">newName</span> <span class="operator">=</span> <span class="string">&quot;jack&quot;</span>;</span><br><span class="line"><span class="type">var</span> <span class="variable">newAge</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line"><span class="type">var</span> <span class="variable">newMoney</span> <span class="operator">=</span> <span class="number">88888888L</span>;</span><br><span class="line"><span class="type">var</span> <span class="variable">newObj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Object</span>();</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<p><strong>当然这个var的使用是有限制的，仅适用于局部变量，增强for循环的索引，以及普通for循环的本地变量；它不能使用于方法形参，构造方法形参，方法返回类型等。</strong></p>
<p>除了上面的新特性之外，还对JVM进行了一些优化，这里就不罗列了。</p>
<h2 id="Java11新特性"><a href="#Java11新特性" class="headerlink" title="Java11新特性"></a>Java11新特性</h2><h3 id="直接运行-java"><a href="#直接运行-java" class="headerlink" title="直接运行 java"></a>直接运行 java</h3><p>在以前的版本中，我们在命令提示下，需要先编译，生成class文件之后再运行，例如：</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">javac HelloWorld.java</span><br><span class="line">java HelloWorld</span><br></pre></td></tr></table></figure>

<p>在java 11中，我们可以这样直接运行</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">java HelloWorld.java</span><br></pre></td></tr></table></figure>

<p>去掉了 javac 的环节，可以直接使用java来运行代码，而且如果你的代码有其他对象的引用，也是没有任何问题的。</p>
<p><strong>可以帮助我们做一个级联的无感知编译。</strong></p>
<h3 id="lambda表达式中的变量类型推断"><a href="#lambda表达式中的变量类型推断" class="headerlink" title="lambda表达式中的变量类型推断"></a>lambda表达式中的变量类型推断</h3><p>JDK11中允许在lambda表达式的参数中使用var修饰</p>
<p>函数式接口：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@FunctionalInterface</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">MyInterface</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">m1</span><span class="params">(String a, <span class="type">int</span> b)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 支持lambda表达式参数中使用var</span></span><br><span class="line"><span class="type">MyInterface</span> <span class="variable">mi</span> <span class="operator">=</span> (<span class="keyword">var</span> a,<span class="keyword">var</span> b)-&gt;&#123;</span><br><span class="line">    System.out.println(a);</span><br><span class="line">    System.out.println(b);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">mi.m1(<span class="string">&quot;monkey&quot;</span>,<span class="number">1024</span>);</span><br></pre></td></tr></table></figure>

<h2 id="Java12新特性"><a href="#Java12新特性" class="headerlink" title="Java12新特性"></a>Java12新特性</h2><h3 id="升级的switch语句"><a href="#升级的switch语句" class="headerlink" title="升级的switch语句"></a>升级的switch语句</h3><p>在JDK12之前的switch语句中，如果没有写break，则会出现case穿透现象，下面是对case穿透的一个应用，根据输入的月份打印相应的季节。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">month</span> <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line"><span class="keyword">switch</span> (month) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">4</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">5</span>:</span><br><span class="line">        System.out.println(<span class="string">&quot;spring&quot;</span>);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">6</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">7</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">8</span>:</span><br><span class="line">        System.out.println(<span class="string">&quot;summer&quot;</span>);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">9</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">10</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">11</span>:</span><br><span class="line">        System.out.println(<span class="string">&quot;autumn&quot;</span>);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">12</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">        System.out.println(<span class="string">&quot;winter&quot;</span>);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">        System.out.println(<span class="string">&quot;wrong&quot;</span>);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在JDK12之后我们可以省略全部的break和部分case，这样使用</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">month</span> <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">switch</span> (month) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span> -&gt; System.out.println(<span class="string">&quot;spring&quot;</span>);</span><br><span class="line">        <span class="keyword">case</span> <span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span> -&gt; System.out.println(<span class="string">&quot;summer&quot;</span>);</span><br><span class="line">        <span class="keyword">case</span> <span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span> -&gt; System.out.println(<span class="string">&quot;autumn&quot;</span>);</span><br><span class="line">        <span class="keyword">case</span> <span class="number">12</span>, <span class="number">1</span>,<span class="number">2</span> -&gt; System.out.println(<span class="string">&quot;winter&quot;</span>);</span><br><span class="line">        <span class="keyword">default</span> -&gt; System.out.println(<span class="string">&quot;wrong&quot;</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>这个是预览功能，如果需要编译和运行的话需要使用下面命令，预览功能在2个版本之后会成为正式版，即如果你使用的是JDK14以上的版本，正常的编译和运行即可。否则需要使用预览功能来编译和运行</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">编译:</span><br><span class="line">javac --enable-preview -source 12 Test.java</span><br><span class="line"></span><br><span class="line">运行：</span><br><span class="line">java --enable-preview Test</span><br></pre></td></tr></table></figure>

<h2 id="Java13新特性"><a href="#Java13新特性" class="headerlink" title="Java13新特性"></a>Java13新特性</h2><h3 id="升级的switch语句-1"><a href="#升级的switch语句-1" class="headerlink" title="升级的switch语句"></a>升级的switch语句</h3><p>JDK13中对switch语句又进行了升级，<strong>可以switch的获取返回值</strong></p>
<p>示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="variable">month</span> <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line">   <span class="type">String</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">switch</span> (month) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span> -&gt; <span class="string">&quot;spring&quot;</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span> -&gt; <span class="string">&quot;summer&quot;</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span> -&gt; <span class="string">&quot;autumn&quot;</span>;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">12</span>, <span class="number">1</span>,<span class="number">2</span> -&gt; <span class="string">&quot;winter&quot;</span>;</span><br><span class="line">        <span class="keyword">default</span> -&gt; <span class="string">&quot;wrong&quot;</span>;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    System.out.println(result);</span><br></pre></td></tr></table></figure>

<p>对于JDK15之后的版本可以直接编译和运行，否则需要使用下面命令执行该预览功能</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">编译:</span><br><span class="line">    javac --enable-preview -source 13 Test.java</span><br><span class="line"></span><br><span class="line">运行：</span><br><span class="line">    java --enable-preview Test</span><br></pre></td></tr></table></figure>

<h3 id="文本块的变化-Python的插值表达式-fstring"><a href="#文本块的变化-Python的插值表达式-fstring" class="headerlink" title="文本块的变化 Python的插值表达式 fstring"></a>文本块的变化 Python的插值表达式 fstring</h3><p>在JDK13之前的版本中如果输入的字符串中有换行的话，需要添加换行符</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> <span class="string">&quot;Hello\nWorld\nLearn\nJava&quot;</span>;</span><br><span class="line">    System.out.println(s);</span><br></pre></td></tr></table></figure>

<p>JDK13之后可以直接这样写：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            Hello</span></span><br><span class="line"><span class="string">            World</span></span><br><span class="line"><span class="string">            Learn</span></span><br><span class="line"><span class="string">            Java</span></span><br><span class="line"><span class="string">           &quot;&quot;&quot;</span>;</span><br><span class="line">  System.out.println(s);</span><br></pre></td></tr></table></figure>

<p>这样的字符串更加一目了然。</p>
<h2 id="Java14新特性"><a href="#Java14新特性" class="headerlink" title="Java14新特性"></a><strong>Java14新特性</strong></h2><p>java 14 新增了很多特性，我们针对较为突出的特性进行说明。JDK12和JDK13中预览版的switch特性，在JDK14中已经是正式的语法了。</p>
<h3 id="instanceof模式匹配"><a href="#instanceof模式匹配" class="headerlink" title="instanceof模式匹配"></a>instanceof模式匹配</h3><p><strong>该特性可以减少强制类型转换的操作</strong>，简化了代码，代码示例：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestInstanceof</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">    </span><br><span class="line">        <span class="comment">// JDK14之前的写法</span></span><br><span class="line">        <span class="type">Object</span> <span class="variable">obj</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Integer</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span>(obj <span class="keyword">instanceof</span> Integer)&#123;</span><br><span class="line">            <span class="type">Integer</span> <span class="variable">i</span> <span class="operator">=</span> (Integer)obj;</span><br><span class="line">            <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> i + <span class="number">10</span>;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// JDK14新特性  不用再强制转换了</span></span><br><span class="line">        <span class="comment">// 这里相当于是将obj强制为Integer之后赋值给i了</span></span><br><span class="line">        <span class="keyword">if</span>(obj <span class="keyword">instanceof</span> Integer i)&#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> i + <span class="number">10</span>;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 作用域问题，这里是无法访问i的</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个是预览版的功能所以需要使用下面命令编译和运行</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">编译:</span><br><span class="line">    javac --enable-preview -source 14 TestInstanceof.java</span><br><span class="line"></span><br><span class="line">运行：</span><br><span class="line">    java --enable-preview TestInstanceof</span><br></pre></td></tr></table></figure>

<h3 id="友好的空指针（NullPointerException）提示"><a href="#友好的空指针（NullPointerException）提示" class="headerlink" title="友好的空指针（NullPointerException）提示"></a>友好的空指针（NullPointerException）提示</h3><p>JDK14中添加了对于空指针异常友好的提示，便于开发者快速定位空指针的对象。示例代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Machine</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">start</span><span class="params">()</span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;启动&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Engine</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Machine machine;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Car</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> Engine engine;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestNull</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">        <span class="comment">// 这里会报出空指针，但是哪个对象是null呢？</span></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Car</span>().engine.machine.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们在运行上面代码的时候，错误信息就可以明确的指出那个对象为null了。此外，还可以使用下面参数来查看:</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">java -XX:+ShowCodeDetailsInExceptionMessages TestNull</span><br></pre></td></tr></table></figure>

<p>这样编译器会明确的告诉开发者哪个对象是null。</p>
<h3 id="record类型"><a href="#record类型" class="headerlink" title="record类型"></a>record类型</h3><p>之前在编写javabean类的时候，<strong>需要编写成员变量，get方法，构造方法，toString方法，hashcode方法，equals方法。这些方法通常会通过开发工具来生成，在JDK14中新增了record类型，通过该类型可以省去这些代码的编写。</strong></p>
<p>JDK14编写User</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">record</span> <span class="title class_">User</span><span class="params">(String name,Integer age)</span>&#123;&#125;</span><br></pre></td></tr></table></figure>

<p>通过反编译命令可以看到该字节码文件中的内容，<strong>User类</strong>是继承了Record类型：</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">javap -p -private user</span><br></pre></td></tr></table></figure>

<p>编写测试类：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TestUser</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">        <span class="type">User</span> <span class="variable">u</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">User</span>(<span class="string">&quot;jack&quot;</span>,<span class="number">15</span>);</span><br><span class="line">        System.out.println(u);</span><br><span class="line">        System.out.println(u.name());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个是预览版的功能所以需要使用下面命令编译和运行</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">编译:</span><br><span class="line">javac --enable-preview -source 14 TestUser.java</span><br><span class="line"></span><br><span class="line">运行：</span><br><span class="line">java --enable-preview TestUser</span><br></pre></td></tr></table></figure>

<p>记录类型有自动生成的成员，包括：</p>
<ul>
<li><strong>状态描述中的每个组件都有对应的private final字段。</strong></li>
<li><strong>状态描述中的每个组件都有对应的public访问方法。方法的名称与组件名称相同。</strong></li>
<li><strong>一个包含全部组件的公开构造器，用来初始化对应组件。</strong></li>
<li><strong>实现了equals()和hashCode()方法。equals()要求全部组件都必须相等。</strong></li>
<li><strong>实现了toString()，输出全部组件的信息。</strong></li>
</ul>
<h2 id="Java15新特性"><a href="#Java15新特性" class="headerlink" title="Java15新特性"></a><strong>Java15新特性</strong></h2><p>java 15中更新了一些新的内容，这里仅列出对于写代码方面的新特性。</p>
<h3 id="Sealed-Classes"><a href="#Sealed-Classes" class="headerlink" title="Sealed Classes"></a>Sealed Classes</h3><p>密封类和接口，作用是限制一个类可以由哪些子类继承或者实现。</p>
<ol>
<li>如果指定模块的话，sealed class和其子类必须在同一个模块下。如果没有指定模块，则需要在同一个包下。</li>
<li><strong>sealed class指定的子类必须直接继承该sealed class。</strong></li>
<li>sealed class的<strong>子类要用final修饰。</strong></li>
<li>sealed class的<strong>子类如果不想用final修饰的话，可以将子类声明为sealed class。</strong></li>
</ol>
<p>Animal类，在指定允许继承的子类时可以使用全限定名</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">sealed</span> <span class="keyword">class</span> <span class="title class_">Animal</span> <span class="keyword">permits</span> Cat, Dog&#123;<span class="comment">// 多个子类之间用,隔开</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">eat</span><span class="params">()</span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Cat类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">Cat</span> <span class="keyword">extends</span> <span class="title class_">Animal</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">eat</span><span class="params">()</span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;123&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Dog类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">sealed</span> <span class="keyword">class</span> <span class="title class_">Dog</span> <span class="keyword">extends</span> <span class="title class_">Animal</span></span><br><span class="line">    <span class="keyword">permits</span> Husky &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>Husky类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">Husky</span> <span class="keyword">extends</span> <span class="title class_">Dog</span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Test类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">        <span class="type">Cat</span> <span class="variable">c</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Cat</span>();</span><br><span class="line">        c.eat();</span><br><span class="line">        <span class="type">Dog</span> <span class="variable">d</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Dog</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Java16新特性"><a href="#Java16新特性" class="headerlink" title="Java16新特性"></a>Java16新特性</h2><p>这里只介绍一些跟开发关联度较大的特性，除此之外JDK16还更新了许多其他新特性，感兴趣的同学可以去Oracle官网查看</p>
<h3 id="包装类构造方法的警告"><a href="#包装类构造方法的警告" class="headerlink" title="包装类构造方法的警告"></a>包装类构造方法的警告</h3><p><strong>使用包装类的构造方法在编译的时候会出现警告，不建议再使用包装类的构造方法。</strong>下面代码在javac编译之后会出现警告。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Integer</span> <span class="variable">i</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Integer</span>(<span class="number">8</span>);</span><br></pre></td></tr></table></figure>

<p><strong>不建议使用包装类作为锁对象，倘若使用包装类作为锁对象，在编译时会出现警告。</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">Integer</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">8</span>;</span><br><span class="line">    <span class="keyword">synchronized</span>(i)&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="新增日时段"><a href="#新增日时段" class="headerlink" title="新增日时段"></a>新增日时段</h3><p>在DateTimeFormatter.ofPattern传入B可以获取现在时间对应的日时段，上午，下午等</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">System.out.println(DateTimeFormatter.ofPattern(<span class="string">&quot;B&quot;</span>).format(LocalDateTime.now()));</span><br></pre></td></tr></table></figure>

<p>在之前JDK版本中作为预览功能的Record类，模式匹配的instanceof，打包工具jpackage，已成为正式版。JDK16对GC，JVM运行时内存等内容有一些变化，例如：<strong>ZGC并发栈处理</strong>，<strong>弹性meta space</strong></p>
<h2 id="Java17新特性"><a href="#Java17新特性" class="headerlink" title="Java17新特性"></a>Java17新特性</h2><p>java17是一个LTS（long term support）长期支持的版本，根据计划来看java17会支持到2029年（java8会支持到2030年，OMG），同时Oracle提议下一个LTS版本是java21，在2023年9月发布，这样讲LST版本的发布周期由之前的3年变为了2年。这里只介绍一些跟开发关联度较大的特性，除此之外JDK17还更新了一些其他新特性，感兴趣的同学可以从这里查看：<a href="https://www.oracle.com/news/announcement/oracle-releases-java-17-2021-09-14/">https://www.oracle.com/news/announcement/oracle-releases-java-17-2021-09-14/</a></p>
<h3 id="switch语法的变化-预览"><a href="#switch语法的变化-预览" class="headerlink" title="switch语法的变化(预览)"></a>switch语法的变化(预览)</h3><p>在之前版本中新增的instanceof模式匹配的特性在switch中也支持了，即我们可以在switch中减少强转的操作。比如下面的代码：</p>
<p>Rabbit和Bird均实现了Animal接口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">interface</span> <span class="title class_">Animal</span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Rabbit</span> <span class="keyword">implements</span> <span class="title class_">Animal</span>&#123;</span><br><span class="line">    <span class="comment">//特有的方法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;run&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bird</span> <span class="keyword">implements</span> <span class="title class_">Animal</span>&#123;</span><br><span class="line">    <span class="comment">//特有的方法</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">fly</span><span class="params">()</span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;fly&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>新特性可以减少Animal强转操作代码的编写：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Switch01</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Animal</span> <span class="variable">a</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Rabbit</span>();</span><br><span class="line">        animalEat(a);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">animalEat</span><span class="params">(Animal a)</span>&#123;</span><br><span class="line">        <span class="keyword">switch</span>(a)&#123;</span><br><span class="line">            <span class="comment">//如果a是Rabbit类型，则在强转之后赋值给r，然后再调用其特有的run方法</span></span><br><span class="line">            <span class="keyword">case</span> Rabbit r -&gt; r.run();</span><br><span class="line">            <span class="comment">//如果a是Bird类型，则在强转之后赋值给b，然后调用其特有的fly方法</span></span><br><span class="line">            <span class="keyword">case</span> Bird b -&gt; b.fly();</span><br><span class="line">            <span class="comment">//支持null的判断</span></span><br><span class="line">            <span class="keyword">case</span> <span class="literal">null</span> -&gt; System.out.println(<span class="string">&quot;null&quot;</span>);</span><br><span class="line">            <span class="keyword">default</span> -&gt; System.out.println(<span class="string">&quot;no animal&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该功能在java17中是预览的，编译和运行需要加上额外的参数:</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">javac --enable-preview -source 17 Switch01.java</span><br><span class="line">java  --enable-preview Switch01</span><br></pre></td></tr></table></figure>

<h3 id="去除了AOT和JIT"><a href="#去除了AOT和JIT" class="headerlink" title="去除了AOT和JIT"></a>去除了AOT和JIT</h3><p>AOT（Ahead-of-Time）是java9中新增的功能，可以先将应用中中的字节码编译成机器码。</p>
<p>Graal编译器作为使用java开发的JIT（just-in-time ）即时编译器在java10中加入（注意这里的JIT不是之前java中的JIT，在JEP 317中有说明<a href="https://openjdk.java.net/jeps/317"><strong>https://openJDK.java.net/jeps/317</strong></a>）。</p>
<p>以上两项功能由于使用量较少，且需要花费很多精力来维护，因此在java17中被移除了。当然你可以通过Graal VM来继续使用这些功能。</p>
<h2 id="Java18新特性"><a href="#Java18新特性" class="headerlink" title="Java18新特性"></a>Java18新特性</h2><p>这里只介绍一些跟开发关联度较大的特性，除此之外JDK18还更新了许多其他新特性，感兴趣的同学可以去Oracle官网查看：</p>
<p><a href="https://www.oracle.com/java/technologies/javase/18-relnote-issues.html#NewFeature">https://www.oracle.com/java/technologies/javase/18-relnote-issues.html#NewFeature</a></p>
<h3 id="默认使用UTF-8字符编码"><a href="#默认使用UTF-8字符编码" class="headerlink" title="默认使用UTF-8字符编码"></a>默认使用UTF-8字符编码</h3><p>从JDK18开始，默认使用UTF-8字符编码。我们可以通过如下参数修改其他字符编码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-Dfile.encoding=UTF-8</span><br></pre></td></tr></table></figure>

<h3 id="将被移除的方法"><a href="#将被移除的方法" class="headerlink" title="将被移除的方法"></a>将被移除的方法</h3><p><strong>在JDK18中标记了Object中的finalize方法，Thread中的stop方法将在未来被移除。</strong></p>
<h3 id="简单的web服务器"><a href="#简单的web服务器" class="headerlink" title="简单的web服务器"></a>简单的web服务器</h3><p>可以通过jwebserver命令启动JDK18中提供的静态web服务器，可以利用该工具查看一些原型，做简单的测试。在命令提示符中输入jwebserver命令后会启动，然后在浏览器中输入:<a href="http://127.0.0.1:8000/">http://127.0.0.1:8000/</a> 即可看到当前命令提示符路径下的文件了。</p>
<h3 id="snippet注解"><a href="#snippet注解" class="headerlink" title="@snippet注解"></a>@snippet注解</h3><p>以前在文档注释中编写代码时需要添加code标签，使用较为不便，通过**@snippet注解可以更方便的将文档注释中的代码展示在api文档中。**</p>
<h2 id="Java19新特性"><a href="#Java19新特性" class="headerlink" title="Java19新特性"></a><strong>Java19新特性</strong></h2><h3 id="Virtual-Threads-Preview-（虚拟线程）"><a href="#Virtual-Threads-Preview-（虚拟线程）" class="headerlink" title="Virtual Threads (Preview)（虚拟线程）"></a>Virtual Threads (Preview)（虚拟线程）</h3><p>该特性在java19中是预览版，<strong>虚拟线程是一种用户态下的线程，类似go语言中的goroutines 和Erlang中的processes</strong>，虚拟线程并非比线程快，<strong>而是提高了应用的吞吐量，相比于传统的线程是由操作系统调度来看，虚拟线程是我们自己程序调度的线程</strong>。如果你对之前java提供的线程API比较熟悉了，那么在学习虚拟线程的时候会比较轻松，<strong>传统线程能运行的代码，虚拟线程也可以运行。虚拟线程的出现，并没有修改java原有的并发模型，也不会替代原有的线程</strong>。<strong>虚拟线程主要作用是提升服务器端的吞吐量。</strong></p>
<h4 id="吞吐量的瓶颈"><a href="#吞吐量的瓶颈" class="headerlink" title="吞吐量的瓶颈"></a>吞吐量的瓶颈</h4><p>服务器应用程序的伸缩性受<strong>利特尔法则（Little’s Law）</strong>的制约，与下面3点有关</p>
<ol>
<li>延迟：请求处理的耗时</li>
<li>并发量：同一时刻处理的请求数量</li>
<li>吞吐量：单位时间内处理的数据数量</li>
</ol>
<p>比如一个服务器应用程序的延迟是50ms，处理10个并发请求，则吞吐量是200请求&#x2F;秒（10 &#x2F; 0.05），如果吞吐量要达到2000请求&#x2F;秒，则处理的并发请求数量是100。<strong>按照1个请求对应一个线程的比例来看，要想提高吞吐量，线程数量也要增加。</strong></p>
<p><strong>java中的线程是在操作系统线程（OS thread）进行了一层包装，而操作系统中线程是重量级资源，在硬件配置确定的前提下，我们就不能创建更多的线程了，此时线程数量就限制了系统性能，为了解决该问题，虚拟线程就出现了。</strong></p>
<p><img src="/2024/09/10/Java22-new-features/image-20240911004144255.png" alt="image-20240911004144255"></p>
<p><strong>与虚拟地址可以映射到物理内存类似，java是将大量的虚拟线程映射到少量的操作系统线程，多个虚拟线程可以使用同一个操作系统线程，其创建所耗费的资源也是极其低廉的，无需系统调用和系统级别的上下文切换，且虚拟线程的生命周期短暂，不会有很深的栈的调用，一个虚拟线程的生命周期中只运行一个任务，因此我们可以创建大量的虚拟线程，且虚拟线程无需池化。</strong></p>
<h4 id="虚拟线程的应用场景"><a href="#虚拟线程的应用场景" class="headerlink" title="虚拟线程的应用场景"></a>虚拟线程的应用场景</h4><p>在服务器端的应用程序中，<strong>可能会有大量的并发任务需要执行，而虚拟线程能够明显的提高应用的吞吐量。下面的场景能够显著的提高程序的吞吐量</strong>：</p>
<ul>
<li>至少几千的并发任务量</li>
<li>任务为io密集型</li>
</ul>
<p>下面代码中为每个任务创建一个线程，当任务量较多的时候，你的电脑可以感受到明显的卡顿（如果没有，可以增加任务数量试下）：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ExecutorService实现了AutoCloseable接口，可以自动关闭了</span></span><br><span class="line"><span class="keyword">try</span> (<span class="type">ExecutorService</span> <span class="variable">executor</span> <span class="operator">=</span> Executors.newCachedThreadPool()) &#123;</span><br><span class="line">    <span class="comment">// 向executor中提交1000000个任务</span></span><br><span class="line">    IntStream.range(<span class="number">0</span>, <span class="number">1000000</span>).forEach(</span><br><span class="line">        i -&gt; &#123;</span><br><span class="line">            executor.submit(() -&gt; &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    <span class="comment">// 睡眠1秒，模拟耗时操作</span></span><br><span class="line">                    Thread.sleep(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">                    System.out.println(<span class="string">&quot;执行任务:&quot;</span> + i);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>将上面的代码改成虚拟线程之后，电脑不会感受到卡顿了：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// newVirtualThreadPerTaskExecutor为每个任务创建一个虚拟线程</span></span><br><span class="line"><span class="keyword">try</span> (<span class="type">ExecutorService</span> <span class="variable">executor</span> <span class="operator">=</span> Executors.newVirtualThreadPerTaskExecutor()) &#123;</span><br><span class="line">    IntStream.range(<span class="number">0</span>, <span class="number">1000_000</span>).forEach(i -&gt; &#123;</span><br><span class="line">        executor.submit(() -&gt; &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 睡眠1秒，模拟耗时操作</span></span><br><span class="line">                Thread.sleep(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">                System.out.println(<span class="string">&quot;执行任务:&quot;</span> + i);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="平台线程和虚拟线程"><a href="#平台线程和虚拟线程" class="headerlink" title="平台线程和虚拟线程"></a>平台线程和虚拟线程</h4><p>平台线程（platform thread）：<strong>指java中的线程，比如通过Executors.newFixedThreadPool()创建出来的线程，我们称之为平台线程。</strong></p>
<p>虚拟线程并不会直接分配给cpu去执行，而是通过调度器分配给平台线程，平台线程再被调度器管理。java中虚拟线程的调度器采用了工作窃取的模式进行FIFO的操作，调度器的并行数默认是JVM获取的处理器数量（通过该方法获取的数量Runtime.getRuntime().availableProcessors()），调度器并非分时（time sharing）的。在使用虚拟线程编写程序时，不能控制虚拟线程何时分配给平台线程，也不能控制平台线程何时分配给cpu。</p>
<p>以前任务和平台线程的关系：</p>
<p><img src="/2024/09/10/Java22-new-features/image-20240911004612979.png" alt="image-20240911004612979"></p>
<p>使用虚拟线程之后，任务-虚拟线程-调度器-平台线程的关系，1个平台线程可以被调度器分配不同的虚拟线程：</p>
<p><img src="/2024/09/10/Java22-new-features/image-20240911004633649.png" alt="image-20240911004633649"></p>
<h4 id="创建虚拟线程的方式"><a href="#创建虚拟线程的方式" class="headerlink" title="创建虚拟线程的方式"></a>创建虚拟线程的方式</h4><p>java中创建的虚拟线程本质都是通过Thread.Builder.OfVirtual对象进行创建的，我们后面再来讨论这个对象，下面先看下创建虚拟线程的三种方式：</p>
<p><strong>1.通过Thread.startVirtualThread直接创建一个虚拟线程</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建任务</span></span><br><span class="line"><span class="type">Runnable</span> <span class="variable">task</span> <span class="operator">=</span> () -&gt; &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;执行任务&quot;</span>);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建虚拟线程将任务task传入并启动</span></span><br><span class="line">Thread.startVirtualThread(task);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 主线程睡眠，否则可能看不到控制台的打印</span></span><br><span class="line">TimeUnit.SECONDS.sleep(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p><strong>2.使用Thread.ofVirtual()方法创建</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建任务</span></span><br><span class="line"><span class="type">Runnable</span> <span class="variable">task</span> <span class="operator">=</span> () -&gt; &#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName());</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建虚拟线程命名为诺手，将任务task传入</span></span><br><span class="line"><span class="type">Thread</span> <span class="variable">vt1</span> <span class="operator">=</span> Thread.ofVirtual().name(<span class="string">&quot;诺手&quot;</span>).unstarted(task);</span><br><span class="line">vt1.start();<span class="comment">// 启动虚拟线程</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 主线程睡眠，否则可能看不到控制台的打印</span></span><br><span class="line">TimeUnit.SECONDS.sleep(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>也可以在创建虚拟线程的时候直接启动</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建任务</span></span><br><span class="line"><span class="type">Runnable</span> <span class="variable">task</span> <span class="operator">=</span> () -&gt; &#123;</span><br><span class="line">    System.out.println(Thread.currentThread().getName());</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建虚拟线程命名为诺手，将任务task传入并启动</span></span><br><span class="line"><span class="type">Thread</span> <span class="variable">vt1</span> <span class="operator">=</span> Thread.ofVirtual().name(<span class="string">&quot;诺手&quot;</span>).start(task);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 主线程睡眠，否则可能看不到控制台的打印</span></span><br><span class="line">TimeUnit.SECONDS.sleep(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p><strong>3.通过ExecutorService创建</strong>，为每个任务分配一个虚拟线程，下面代码中提交了100个任务，对应会有100个虚拟线程进行处理。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    通过ExecutorService创建虚拟线程</span></span><br><span class="line"><span class="comment">    ExecutorService实现了AutoCloseable接口，可以自动关闭了</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">try</span> (<span class="type">ExecutorService</span> <span class="variable">executor</span> <span class="operator">=</span> Executors.newVirtualThreadPerTaskExecutor()) &#123;</span><br><span class="line">    <span class="comment">// 向executor中提交100个任务</span></span><br><span class="line">    IntStream.range(<span class="number">0</span>, <span class="number">100</span>).forEach(i -&gt; &#123;</span><br><span class="line">        executor.submit(() -&gt; &#123;</span><br><span class="line">            <span class="comment">// 睡眠1秒</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">                System.out.println(i);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;                    </span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>现在平台线程和虚拟线程都是Thread的对象，那该如何区分该对象是平台线程还是虚拟线程？可以利用Thread中的isVirtual()方法进行判断，返回true表示虚拟线程：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建任务</span></span><br><span class="line"><span class="type">Runnable</span> <span class="variable">task</span> <span class="operator">=</span> () -&gt; &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;执行任务&quot;</span>);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建虚拟线程将任务task传入并启动</span></span><br><span class="line"><span class="type">Thread</span> <span class="variable">vt</span> <span class="operator">=</span> Thread.startVirtualThread(task);</span><br><span class="line">System.out.println(vt.isVirtual());</span><br></pre></td></tr></table></figure>

<h2 id="Java21新特性"><a href="#Java21新特性" class="headerlink" title="Java21新特性"></a><strong>Java21新特性</strong></h2><p>java20中没有太大的变化，这里主要聊下java21的新特性，21是继java17之后，最新的LTS版本，该版本中虚拟线程称为了正式版，对虚拟线程不了解的同学可以看下之前的java19中的介绍。接下来我们看下java21中一些新特性。</p>
<h3 id="字符串模板-STR"><a href="#字符串模板-STR" class="headerlink" title="字符串模板 STR"></a>字符串模板 STR</h3><p>字符串模板可以让开发者更简洁的进行字符串拼接（例如拼接sql，xml，json等）。该特性并不是为字符串拼接运算符+提供的语法糖，也并非为了替换SpringBuffer和StringBuilder。</p>
<p>这个和<strong>Python的插值表达式很类似，可以做变量的替换</strong>，多行文本的变量替换，还可以做一些计算 。 </p>
<p>利用STR模板进行字符串与变量的拼接：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">sport</span> <span class="operator">=</span> <span class="string">&quot;basketball&quot;</span>;</span><br><span class="line"><span class="type">String</span> <span class="variable">msg</span> <span class="operator">=</span> STR.<span class="string">&quot;i like \&#123;sport&#125;&quot;</span>;</span><br><span class="line"></span><br><span class="line">System.out.println(msg);<span class="comment">// i like basketball</span></span><br></pre></td></tr></table></figure>

<p>这个特性目前是预览版，编译和运行需要添加额外的参数：</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">javac --enable-preview -source 21 Test.java</span><br><span class="line">java --enable-preview Test</span><br></pre></td></tr></table></figure>

<p>在js中字符串进行拼接时会采用下面的字符串插值写法</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> sport = <span class="string">&quot;basketball&quot;</span></span><br><span class="line"><span class="keyword">let</span> msg = <span class="string">`i like <span class="subst">$&#123;sport&#125;</span>`</span></span><br></pre></td></tr></table></figure>

<p>看起来字符串插值写法更简洁移动，<strong>不过若在java中使用这种字符串插值的写法拼接sql，可能会出现sql注入的问题，为了防止该问题，java提供了字符串模板表达式的方式</strong>。</p>
<p>上面使用的STR是java中定义的模板处理器，<strong>它可以将变量的值取出，完成字符串的拼接。在每个java源文件中都引入了一个public static final修饰的STR属性，因此我们可以直接使用STR</strong>，STR通过打印STR可以知道它是java.lang.StringTemplate，是一个接口。</p>
<p>在StringTemplate中是通过调用interpolate方法来执行的，该方法分别传入了两个参数：</p>
<ul>
<li>fragements：包含字符串模板中所有的字面量，是一个List</li>
<li>values：包含字符串模板中所有的变量，是一个List</li>
</ul>
<p>而该方法又调用了JavaTemplateAccess中的interpolate方法，经过分析可以得知，它最终<strong>是通过String中的join方法将字面量和变量进行的拼接</strong>。</p>
<h3 id="scoped-values-传递参数时无需声明形参"><a href="#scoped-values-传递参数时无需声明形参" class="headerlink" title="scoped values 传递参数时无需声明形参"></a>scoped values 传递参数时无需声明形参</h3><h4 id="ThreadLocal的问题"><a href="#ThreadLocal的问题" class="headerlink" title="ThreadLocal的问题"></a>ThreadLocal的问题</h4><p>scoped values 是一个隐藏的方法参数，<strong>只有方法可以访问scoped values，它可以让两个方法之间传递参数时无需声明形参</strong>。例如在UserDao类中编写了saveUser方法，LogDao类中编写了saveLog方法，那么在保存用户的时候需要保证事务，此时就需要在service层获取Connection对象，然后将该对象分别传入到两个Dao的方法中，但对于saveUser方法来说并不是直接使用Connection对象，却又不得不在方法的形参中写上该对象，其实仅从业务上来看，该方法中只要传入User对象就可以了。</p>
<p>int saveUser(Connection connection,User user);</p>
<p><strong>对于上面的问题，开发者通常会使用ThreadLocal解决，但由于ThreadLocal在设计上的瑕疵，导致下面问题：</strong></p>
<ol>
<li><strong>内存泄漏，在用完ThreadLocal之后若没有调用remove，这样就会出现内存泄漏。</strong></li>
<li><strong>增加开销，在具有继承关系的线程中，子线程需要为父线程中ThreadLocal里面的数据分配内存。</strong></li>
<li><strong>混乱的可变，任何可以调用ThreadLocal中get方法的代码都可以随时调用set方法，这样就不易辨别哪些方法是按照什么顺序来更新的共享数据。</strong></li>
</ol>
<p><strong>随着虚拟线程的到来，内存泄漏问题就不用担心了，由于虚拟线程会很快的终止，此时会自动删除ThreadLocal中的数据，这样就不用调用remove方法了</strong>。但虚拟线程的数量通常是多的，试想下上百万个虚拟线程都要拷贝一份ThreadLocal中的变量，这会使内存承受更大的压力。为了解决这些问题，scoped values就出现了。</p>
<h4 id="ScopeValue初体验"><a href="#ScopeValue初体验" class="headerlink" title="ScopeValue初体验"></a>ScopeValue初体验</h4><p>在java21中新增了ScopeValue类，为了便于多个方法使用，通常会将该类的对象声明为static final ，每个线程都能访问自己的scope value，与ThreadLocal不同的是，它只会被write 1次且仅在线程绑定的期间内有效。</p>
<p>下面代码模拟了送礼和收礼的场景</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> ScopedValue&lt;String&gt; GIFT = ScopedValue.newInstance();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Test</span> <span class="variable">t</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Test</span>();</span><br><span class="line">        t.giveGift();</span><br><span class="line">    &#125;     </span><br><span class="line"></span><br><span class="line">    <span class="comment">//送礼</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">giveGift</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">            在对象GIFT中增加字符串手机，当run方法执行时，</span></span><br><span class="line"><span class="comment">            会拷贝一份副本与当前线程绑定，当run方法结束时解绑。</span></span><br><span class="line"><span class="comment">            由此可见，这里GIFT中的字符串仅在收礼方法中可以取得。</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ScopedValue.where(GIFT, <span class="string">&quot;手机&quot;</span>).run(() -&gt; receiveGift());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//收礼</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">receiveGift</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(GIFT.get()); <span class="comment">// 手机</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="多线程操作相同的ScopeValue"><a href="#多线程操作相同的ScopeValue" class="headerlink" title="多线程操作相同的ScopeValue"></a>多线程操作相同的ScopeValue</h4><p>不同的线程在操作同一个ScopeValue时，相互间不会影响，其本质是利用了Thread类中scopedValueBindings属性进行的线程绑定。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Executors;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> ScopedValue&lt;String&gt; GIFT = ScopedValue.newInstance();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Test</span> <span class="variable">t</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Test</span>();</span><br><span class="line"></span><br><span class="line">        <span class="type">ExecutorService</span> <span class="variable">pool</span> <span class="operator">=</span> Executors.newCachedThreadPool();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            pool.submit(()-&gt;&#123;</span><br><span class="line">                t.giveGift();</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        pool.shutdown();</span><br><span class="line">    &#125;     </span><br><span class="line"></span><br><span class="line">    <span class="comment">//向ScopedValue中添加当前线程的名字</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">giveGift</span><span class="params">()</span> &#123;</span><br><span class="line">        ScopedValue.where(GIFT, Thread.currentThread().getName()).run(() -&gt; receiveGift());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">receiveGift</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(GIFT.get()); </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="record-pattern-解构"><a href="#record-pattern-解构" class="headerlink" title="record pattern 解构"></a>record pattern 解构</h3><p>通过该特性可以解构record类型中的值，例如</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Student</span> <span class="variable">s</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Student</span>(<span class="number">10</span>, <span class="string">&quot;jordan&quot;</span>);</span><br><span class="line">        printSum(s);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">printSum</span><span class="params">(Object obj)</span> &#123;</span><br><span class="line">        <span class="comment">//这里的Student(int a, String b)就是 record pattern</span></span><br><span class="line">        <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> <span class="title function_">Student</span><span class="params">(<span class="type">int</span> a, String b)</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;id:&quot;</span> + a);</span><br><span class="line">            System.out.println(<span class="string">&quot;name:&quot;</span> + b);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">record</span> <span class="title class_">Student</span><span class="params">(<span class="type">int</span> id, String name)</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="switch格式匹配"><a href="#switch格式匹配" class="headerlink" title="switch格式匹配"></a>switch格式匹配</h3><p>之前的写法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Integer</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> getObjInstance(i);</span><br><span class="line">        System.out.println(str);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String <span class="title function_">getObjInstance</span><span class="params">(Object obj)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">objInstance</span> <span class="operator">=</span> <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">if</span>(obj == <span class="literal">null</span>)&#123;</span><br><span class="line">            objInstance = <span class="string">&quot;空对象&quot;</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> Integer i) &#123;</span><br><span class="line">            objInstance = <span class="string">&quot;Integer 对象：&quot;</span> + i;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> Double d) &#123;</span><br><span class="line">            objInstance = <span class="string">&quot;Double 对象：&quot;</span> + d;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj <span class="keyword">instanceof</span> String s) &#123;</span><br><span class="line">            objInstance = <span class="string">&quot;String 对象：&quot;</span> + s;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> objInstance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>新的写法，代码更加简洁</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Integer</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">str</span> <span class="operator">=</span> getObjInstance(i);</span><br><span class="line">        System.out.println(str);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> String <span class="title function_">getObjInstance</span><span class="params">(Object obj)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">switch</span>(obj)&#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="literal">null</span> -&gt; <span class="string">&quot;空对象&quot;</span>;</span><br><span class="line">            <span class="keyword">case</span> Integer i -&gt; <span class="string">&quot;Integer 对象：&quot;</span> + i;</span><br><span class="line">            <span class="keyword">case</span> Double d -&gt; <span class="string">&quot;Double对象：&quot;</span> + d;</span><br><span class="line">            <span class="keyword">case</span> String s -&gt; <span class="string">&quot;String对象：&quot;</span> + s;</span><br><span class="line">            <span class="keyword">default</span> -&gt; obj.toString();</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以在switch中使用when</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        yesOrNo(<span class="string">&quot;yes&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">yesOrNo</span><span class="params">(String obj)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span>(obj)&#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="literal">null</span> -&gt; &#123;System.out.println(<span class="string">&quot;空对象&quot;</span>);&#125;</span><br><span class="line">            <span class="keyword">case</span> String s</span><br><span class="line">                when s.equalsIgnoreCase(<span class="string">&quot;yes&quot;</span>) -&gt; &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;确定&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">case</span> String s</span><br><span class="line">                when s.equalsIgnoreCase(<span class="string">&quot;no&quot;</span>) -&gt; &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;取消&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">                <span class="comment">//最后的case要写，否则编译回报错</span></span><br><span class="line">            <span class="keyword">case</span> String s -&gt; &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;请输入yes或no&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Unnamed-Classes-and-Instance-Main-Methods"><a href="#Unnamed-Classes-and-Instance-Main-Methods" class="headerlink" title="Unnamed Classes and Instance Main Methods"></a>Unnamed Classes and Instance Main Methods</h3><p>对于初学者来说，写的第一个HelloWorld代码有太多的概念，为了方便初学者快速编写第一段java代码，这里提出了无名类和实例main方法，下面代码可以直接运行编译，相当于是少了类的定义，main方法的修饰符和形参也省略掉了</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;Hello, World!&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Structured-Concurrency"><a href="#Structured-Concurrency" class="headerlink" title="Structured Concurrency"></a>Structured Concurrency</h3><p>该特性主要作用是在使用虚拟线程时，可以使任务和子任务的代码编写起来可读性更强，维护性更高，更加可靠。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.StructuredTaskScope;</span><br><span class="line"><span class="keyword">import</span> java.util.function.Supplier;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Test</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException &#123;</span><br><span class="line">        <span class="type">Food</span> <span class="variable">f</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Test</span>().handle();</span><br><span class="line">        System.out.println(f);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Food <span class="title function_">handle</span><span class="params">()</span> <span class="keyword">throws</span> ExecutionException, InterruptedException &#123;</span><br><span class="line">        <span class="keyword">try</span> (<span class="type">var</span> <span class="variable">scope</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StructuredTaskScope</span>.ShutdownOnFailure()) &#123;</span><br><span class="line">            Supplier&lt;String&gt; yaoZi = scope.fork(() -&gt; <span class="string">&quot;新鲜大腰子烤好了&quot;</span>);<span class="comment">// 烤腰子的任务</span></span><br><span class="line">            Supplier&lt;String&gt; drink = scope.fork(() -&gt; <span class="string">&quot;奶茶做好了&quot;</span>);<span class="comment">// 买饮料的任务</span></span><br><span class="line"></span><br><span class="line">            scope.join() <span class="comment">// 将2个子任务都加入</span></span><br><span class="line">            .throwIfFailed(); <span class="comment">// 失败传播</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// 当两个子任务都成功后，最终才能吃上饭</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Food</span>(yaoZi.get(), drink.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">record</span> <span class="title class_">Food</span><span class="params">(String yaoZi, String drink)</span> &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Java22新特性"><a href="#Java22新特性" class="headerlink" title="Java22新特性"></a>Java22新特性</h2><h3 id="无名变量"><a href="#无名变量" class="headerlink" title="无名变量 __"></a>无名变量 __</h3><p>这个和Go的下划线挺类似的，<strong>一般使用在try-catch中，直接将e，变为下划线</strong>。主要是<strong>区分出需要使用的变量和不需要使用的变量</strong>，因为不是所有的返回值都是你需要的，这样可以<strong>使代码逻辑更加清晰</strong>。</p>
<p>以下场景可以使用Unnamed Variables</p>
<ul>
<li><p>局部变量</p>
</li>
<li><p>try-with-resource</p>
</li>
<li><p>循环头中声明的变量</p>
</li>
<li><p>catch中声明的变量</p>
</li>
<li><p>lambda表达式中的参数</p>
</li>
</ul>
<p>参考文章：<a href="https://www.yuque.com/monkey1024/javase/uhzrnnl2ragygqfm">1.java9新特性 (yuque.com)</a></p>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java新特性</tag>
      </tags>
  </entry>
  <entry>
    <title>深入浅出索引（下）</title>
    <url>/2024/09/10/MySQL05/</url>
    <content><![CDATA[<p>在开始这篇文章之前，我们先来看一下这个问题：</p>
<p>在下面这个表T中，如果我执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？</p>
<p>下面是这个表的初始化语句。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; create table T (</span><br><span class="line">ID int primary key,</span><br><span class="line">k int NOT NULL DEFAULT 0, </span><br><span class="line">s varchar(16) NOT NULL DEFAULT &#x27;&#x27;,</span><br><span class="line">index k(k))</span><br><span class="line">engine=InnoDB;</span><br><span class="line"></span><br><span class="line">insert into T values(100,1, &#x27;aa&#x27;),(200,2,&#x27;bb&#x27;),(300,3,&#x27;cc&#x27;),(500,5,&#x27;ee&#x27;),(600,6,&#x27;ff&#x27;),(700,7,&#x27;gg&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/10/MySQL05/image-20240910092741486.png" alt="image-20240910092741486"></p>
<p>现在，我们一起来看看这条SQL查询语句的执行流程：</p>
<ol>
<li>在k索引树上找到k&#x3D;3的记录，取得 ID &#x3D; 300；</li>
<li>再到ID索引树查到ID&#x3D;300对应的R3；</li>
<li>在k索引树取下一个值k&#x3D;5，取得ID&#x3D;500；</li>
<li>再回到ID索引树查到ID&#x3D;500对应的R4；</li>
<li>在k索引树取下一个值k&#x3D;6，不满足条件，循环结束。</li>
</ol>
<p>在这个过程中，<strong>回到主键索引树搜索的过程，我们称为回表</strong>。可以看到，这个查询过程读了k索引树的3条记录（步骤1、3和5），<strong>回表了两次</strong>（步骤2和4）。</p>
<p>在这个例子中<strong>，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，有没有可能经过索引优化，避免回表过程呢？</strong></p>
<h3 id="覆盖索引"><a href="#覆盖索引" class="headerlink" title="覆盖索引"></a>覆盖索引</h3><p>如果执行的语句是select <strong>ID</strong> from T where k between 3 and 5，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，<strong>索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。（说白了就是后面的这个k索引包含了前面要映射的列）</strong></p>
<p><strong>由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。</strong></p>
<p>需要注意的是，在引擎内部使用覆盖索引在索引k上其实读了三个记录，R3~R5（对应的索引k上的记录项），但是对于MySQL的Server层来说，它就是找引擎拿到了两条记录，因此MySQL认为扫描行数是2。<strong>（这样看来扫描行数是MySQL的Server层获取到的实际的行数，和存储引擎读的行数一般是不同的）</strong></p>
<blockquote>
<p>备注：关于如何查看扫描行数的问题，我将会在第16文章《如何正确地显示随机消息？》中，和你详细讨论。</p>
</blockquote>
<p>基于上面覆盖索引的说明，我们来讨论一个问题：<strong>在一个市民信息表上，是否有必要将身份证号和名字建立联合索引？</strong></p>
<p>假设这个市民表的定义是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `tuser` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `id_card` varchar(32) DEFAULT NULL,</span><br><span class="line">  `name` varchar(32) DEFAULT NULL,</span><br><span class="line">  `age` int(11) DEFAULT NULL,</span><br><span class="line">  `ismale` tinyint(1) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `id_card` (`id_card`),</span><br><span class="line">  KEY `name_age` (`name`,`age`)</span><br><span class="line">) ENGINE=InnoDB</span><br></pre></td></tr></table></figure>

<p>我们知道，身份证号是市民的唯一标识。也就是说，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？</p>
<p><strong>如果现在有一个高频请求，要根据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个高频请求上用到覆盖索引，不再需要回表查整行记录，减少语句的执行时间。</strong></p>
<p><strong>当然，索引字段的维护总是有代价的。</strong>因此，在建立冗余索引来支持覆盖索引时就需要权衡考虑了。这正是业务DBA，或者称为业务数据架构师的工作。</p>
<h3 id="最左前缀原则"><a href="#最左前缀原则" class="headerlink" title="最左前缀原则"></a>最左前缀原则</h3><p>看到这里你一定有一个疑问，<strong>如果为每一种查询都设计一个索引，索引是不是太多了</strong>。如果我现在要按照市民的身份证号去查他的家庭地址呢？虽然这个查询需求在业务中出现的概率不高，但总不能让它走全表扫描吧？<strong>反过来说，单独为一个不频繁的请求创建一个（身份证号，地址）的索引又感觉有点浪费。应该怎么做呢？</strong></p>
<p>这里，我先和你说结论吧。<strong>B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。</strong></p>
<p>为了直观地说明这个概念，我们用（name，age）这个联合索引来分析。</p>
<p>关于联合索引可以看这几篇文章</p>
<ul>
<li><p><a href="https://blog.csdn.net/qq_35590091/article/details/107536144">【MySQL】联合索引的使用_select a,b from table where a order by b ab联合索引能不能-CSDN博客</a></p>
<p>什么是<a href="https://so.csdn.net/so/search?q=%E8%81%94%E5%90%88%E7%B4%A2%E5%BC%95&spm=1001.2101.3001.7020">联合索引</a>？</p>
<p>MySQL可以使用<strong>多个字段同时建立一个索引,叫做联合索引：如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE INDEX INDEX_NAME ON TABLE_NAME(a，b,c)</span><br></pre></td></tr></table></figure>

<p><strong>这个联合索引实际上效果等同于创建了索引a，索引（a，b）,索引（a,b,c）这三个索引。因此联合索引更节约空间。</strong></p>
<p>如果是创建完表之后添加索引，那么可以：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ALTER TABLE `table_name` ADD INDEX index_name ( `a`, `b`, `c` )</span><br></pre></td></tr></table></figure>

<p>也可以在创建表的时候这样：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> T&#123;</span><br><span class="line">a <span class="type">INT</span>,</span><br><span class="line">b <span class="type">INT</span>,</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (a),</span><br><span class="line">KEY idx_a_b (a,b)</span><br><span class="line">&#125;ENGINE<span class="operator">=</span>INNODB</span><br></pre></td></tr></table></figure>

<p><strong>联合索引的本质</strong></p>
<p>联合索引本质上来说也是一个B+树，但是不同的是，以前的B+树的键值数量是1（即一个键值+一个页指针），但是联合索引的键值数量是大于等于2。如果根据上面创建表的语句，可以得到下图的B+树，也就是说，<strong>数据按照a，b的顺序来进行排序的，即先按照a排，再按照b排：</strong></p>
<p><img src="/2024/09/10/MySQL05/image-20240910095424112.png" alt="image-20240910095424112"></p>
<p>联合索引的作用</p>
<p>作用1、在如下这种查询中，我们可以提前创建一个联合索引，提高查询效率：</p>
<p><strong>select * FROM TABLE WHERE a&#x3D;</strong> and b&#x3D;<strong>,我们就可以使用（a，b）联合索引来进行查询。</strong></p>
<p>对于单个列a也可以使用，但是只对于b 的查询不可以，因为之前说的，我们是先对a排序，再对b排序，<strong>单独看b 的话，都是乱的，比如上面的1,2,1,4,1,2。这个其实就是我们经常说的“最左原则”。本质就是因为B+树里面的排序是先排a，再排b。</strong></p>
<p>作用2、第二个好处是，<strong>联合索引它内部已经对第二个键值进行了排序处理（就像上面说的，首先对a排序，然后对b排序）</strong>，很多情况下我们要查询某个用户的购物情况，然后按照时间排序，然后取出前3次的记录，<strong>这个时候使用联合索引就可以节约一次“排序操作”（filesort）</strong>，<strong>因为在叶子节点上都已经排好序了。</strong>创建了联合索引（a，b），就可以这样直接使用联合索引查询：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT ,,FROM TABLE WHERE a=** ORDER BY b; </span><br></pre></td></tr></table></figure>

<p>最左原则</p>
<p>MySQL使用索引时需要索引有序,假设现在建立了”name,age,school”的联合索引,那么索引的排序为: 先按照name排序,如果name相同,则按照age排序,如果age的值也相等,则按照school进行排序。</p>
<p>当进行查询时,此时索引仅仅按照name严格有序,因此必须首先使用name字段进行等值查询,之后对于匹配到的列而言,其按照age字段严格有序,此时可以使用age字段用做索引查找,,,以此类推.因此在建立联合索引的时候应该注意索引列的顺序,**一般情况下,将查询需求频繁或者字段选择性高的列放在前面.**此外可以根据特例的查询或者表结构进行单独的调整。</p>
<p>此外，MySQL会<strong>一直向右匹配直到遇到范围查询</strong>。如有索引 (a,b,c,d)，查询条件 a&#x3D;1 and b&#x3D;2 and c&gt;3 and d&#x3D;4，<strong>则会在每个节点依次命中a、b、c，无法命中d。</strong></p>
</li>
<li><p><a href="https://www.cnblogs.com/wongdw/p/12887174.html">mysql的联合索引（复合索引） - wongdw - 博客园 (cnblogs.com)</a></p>
<p>当创建(<code>col1</code>,<code>col2</code>,<code>col3</code>)联合索引时，相当于创建了**(<code>col</code>)单列索引<strong>，</strong>(<code>clo1</code>,<code>clo2</code>)联合索引<strong>以及</strong>(<code>col1</code>,<code>col2</code>,<code>col3</code>)联合索引<strong>想要索引生效，只能使用</strong><code>col1</code><strong>和</strong><code>col1</code>,<code>col2</code><strong>和</strong><code>col1</code>,<code>col2</code>,<code>col3</code><strong>三种组合；当然，</strong><code>col1</code>,<code>col3</code>组合也可以，但实际上只用到了<code>col1</code>的索引，<code>col3</code>并没有用到！**</p>
<p><img src="/2024/09/10/MySQL05/image-20240910095021156.png" alt="image-20240910095021156"></p>
<p><strong>联合索引相当于一个按照<code>姓氏——名字</code>的一个电话簿，只能先确定姓氏才可以命中索引</strong>，下列可以正确命中联合索引的语句（ <code>= </code> 和<code>IN</code>直接的字段都可以乱序，MySQL的查询优化器可以优化成索引识别的形式）</p>
<p><img src="/2024/09/10/MySQL05/image-20240910094824818.png" alt="image-20240910094824818"></p>
</li>
</ul>
<p><img src="/2024/09/10/MySQL05/image-20240910093958284.png" alt="image-20240910093958284"></p>
<p>可以看到，索引项是按照索引定义里面出现的字段顺序排序的。</p>
<p>当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到ID4，然后向后遍历得到所有需要的结果。</p>
<p><strong>如果你要查的是所有名字第一个字是“张”的人，你的SQL语句的条件是”where name like ‘张%’”。这时，你也能够用上这个索引，查找到第一个符合条件的记录是ID3，然后向后遍历，直到不满足条件为止。</strong></p>
<p>可以看到，不只是索引的全部定义，<strong>只要满足最左前缀，就可以利用索引来加速检索</strong>。这个最左前缀可以是联合索引的<strong>最左N个字段，也可以是字符串索引的最左M个字符</strong>。</p>
<p>基于上面对最左前缀索引的说明，我们来讨论一个问题：<strong>在建立联合索引的时候，如何安排索引内的字段顺序。</strong></p>
<p>这里我们的评估标准是，<strong>索引的复用能力</strong>。因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了。因此，<strong>第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。</strong></p>
<p>所以现在你知道了，这段开头的问题里，<strong>我们要为高频请求创建(身份证号，姓名）这个联合索引，并用这个索引支持“根据身份证号查询地址”的需求。</strong></p>
<p>那么，如果既有联合查询，又有基于a、b各自的查询呢？查询条件里面只有b的语句，是无法使用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，<strong>也就是说你需要同时维护(a,b)、(b) 这两个索引。</strong></p>
<p>这时候，我们要<strong>考虑的原则就是空间</strong>了。比如上面这个市民表的情况，<strong>name字段是比age字段大的 ，那我就建议你创建一个（name,age)的联合索引和一个(age)的单字段索引。</strong></p>
<h3 id="索引下推"><a href="#索引下推" class="headerlink" title="索引下推"></a>索引下推</h3><p>上一段我们说到满足最左前缀原则的时候，最左前缀可以用于在索引中定位记录。这时，你可能要问，那些<strong>不符合</strong>最左前缀的部分，会怎么样呢？</p>
<p>我们还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是10岁的所有男孩”。那么，SQL语句是这么写的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from tuser where name like &#x27;张%&#x27; and age=10 and ismale=1;</span><br></pre></td></tr></table></figure>

<p>你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录ID3。当然，这还不错，总比全表扫描要好。</p>
<p>然后呢？</p>
<p>当然是判断其他条件是否满足。</p>
<p><strong>在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。</strong></p>
<p><strong>而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。</strong></p>
<p>图3和图4，是这两个过程的执行流程图。</p>
<p><img src="/2024/09/10/MySQL05/image-20240910100843207.png" alt="image-20240910100843207"></p>
<p>图3 无索引下推执行流程</p>
<p><img src="/2024/09/10/MySQL05/image-20240910100850704.png" alt="image-20240910100850704"></p>
<p>图4 索引下推执行流程</p>
<p>在图3和4这两个图里面，<strong>每一个虚线箭头表示回表一次。</strong></p>
<p>图3中，在(name,age)索引里面我特意去掉了age的值，这个过程InnoDB并不会去看age的值，<strong>只是按顺序把“name第一个字是’张’”的记录一条条取出来回表</strong>。因此，需要回表4次。</p>
<p>图4跟图3的区别是，InnoDB<strong>在(name,age)索引内部就判断了age是否等于10，对于不等于10的记录，直接判断并跳过</strong>。在我们的这个例子中，只需要对ID4、ID5这两条记录回表取数据判断，就只需要回表2次。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天这篇文章继续讨论了数据库索引的概念，<strong>包括了覆盖索引、前缀索引、索引下推</strong>。你可以看到，在满足语句需求的情况下， <strong>尽量少地访问资源是数据库设计的重要原则之一。</strong>我们在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。</p>
<p>实际上主键索引也是可以使用多个字段的。DBA小吕在入职新公司的时候，就发现自己接手维护的库里面，有这么一个表，表结构定义类似这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `geek` (</span><br><span class="line">  `a` int(11) NOT NULL,</span><br><span class="line">  `b` int(11) NOT NULL,</span><br><span class="line">  `c` int(11) NOT NULL,</span><br><span class="line">  `d` int(11) NOT NULL,</span><br><span class="line">  PRIMARY KEY (`a`,`b`),</span><br><span class="line">  KEY `c` (`c`),</span><br><span class="line">  KEY `ca` (`c`,`a`),</span><br><span class="line">  KEY `cb` (`c`,`b`)</span><br><span class="line">) ENGINE=InnoDB;</span><br></pre></td></tr></table></figure>

<p>公司的同事告诉他说，由于历史原因，这个表需要a、b做联合主键，这个小吕理解了。</p>
<p>但是，学过本章内容的小吕又纳闷了，既然主键包含了a、b这两个字段，那意味着单独在字段c上创建一个索引，就已经包含了三个字段了呀，为什么要创建“ca”“cb”这两个索引？</p>
<p>同事告诉他，是因为他们的业务里面有这样的两种语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from geek where c=N order by a limit 1;</span><br><span class="line">select * from geek where c=N order by b limit 1;</span><br></pre></td></tr></table></figure>

<p>我给你的问题是，这位同事的解释对吗，为了这两个查询模式，这两个索引是否都是必须的？为什么呢？</p>
<p><strong>想了一下，应该是对的，这个问题的答案在上面提到过。就是减少了一次sort，提高了效率。</strong></p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期的问题是，通过两个alter 语句重建索引k，以及通过两个alter语句重建主键索引是否合理。</p>
<p>有同学问到为什么要重建索引。<strong>我们文章里面有提到，索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。（我记得之前看到别人的面经的时候就提到过空洞这个玩意，原来是在这里出现的）</strong></p>
<p>这道题目，我给你的“参考答案”是：</p>
<p><strong>重建索引k的做法是合理的，可以达到省空间的目的。</strong>但是，<strong>重建主键的过程不合理</strong>。不论是删除主键还是创建主键，<strong>都会将整个表重建</strong>。所以连着执行这两个语句的话，第一个语句就<strong>白做了</strong>。<strong>这两个语句，你可以用这个语句代替 ： alter table T engine&#x3D;InnoDB。</strong>在专栏的第12篇文章《为什么表数据删掉一半，表文件大小不变？》中，我会和你分析这条语句的执行流程。</p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/05-shen-ru-qian-chu-suo-yin-xia">05 深入浅出索引（下） | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>深入浅出索引（上）</title>
    <url>/2024/09/09/MySQL04/</url>
    <content><![CDATA[<p>提到数据库索引，我想你并不陌生，在日常工作中会经常接触到。<strong>比如某一个SQL查询比较慢，分析完原因之后，你可能就会说“给某个字段加个索引吧”之类的解决方案</strong>。但到底什么是索引，索引又是如何工作的呢？今天就让我们一起来聊聊这个话题吧。</p>
<p>数据库索引的内容比较多，我分成了上下两篇文章。<strong>索引是数据库系统里面最重要的概念之一</strong>，所以希望你能够耐心看完。在后面的实战文章中，我也会经常引用这两篇文章中提到的知识点，加深你对数据库索引的理解。</p>
<p><strong>一句话简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。</strong>一本500页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一会儿。<strong>同样，对于数据库的表而言，索引其实就是它的“目录”。（这个索引还有一个兄弟，叫做倒排索引，是ES中的一个概念）</strong></p>
<h3 id="索引的常见模型"><a href="#索引的常见模型" class="headerlink" title="索引的常见模型"></a>索引的常见模型</h3><p>索引的出现是为了<strong>提高查询效率</strong>，但是实现索引的方式却有很多种，所以这里也就引入了索引模型的概念。<strong>可以用于提高读写效率的数据结构很多，这里我先给你介绍三种常见、也比较简单的数据结构，它们分别是哈希表、有序数组和搜索树。</strong></p>
<p>下面我主要从使用的角度，为你简单分析一下这三种模型的区别。</p>
<p><strong>哈希表是一种以键-值（key-value）存储数据的结构，我们只要输入待查找的值即key，就可以找到其对应的值即Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。（这里是直接将HashMap的下标当作索引了）</strong></p>
<p>不可避免地，多个key值经过哈希函数的换算，会出现同一个值的情况。<strong>处理这种情况的一种方法是，拉出一个链表。（当然在一定的情况下会升级为红黑树）</strong></p>
<p>假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字，这时对应的哈希索引的示意图如下所示：</p>
<p><img src="/2024/09/09/MySQL04/image-20240909233015979.png" alt="image-20240909233015979"></p>
<p>图中，User2和User4根据身份证号算出来的值都是N，但没关系，后面还跟了一个链表。假设，这时候你要查ID_card_n2对应的名字是什么，处理步骤就是：首先，将ID_card_n2通过哈希函数算出N；然后，按顺序遍历，找到User2。</p>
<p>需要注意的是，图中四个ID_card_n的值<strong>并不是递增的</strong>，这样做的好处是<strong>增加新的User时速度会很快</strong>，只需要往后追加。但缺点是，因为不是有序的，所以<strong>哈希索引做区间查询的速度是很慢的</strong>。</p>
<p>你可以设想下，如果你现在要找身份证号在[ID_card_X, ID_card_Y]这个区间的所有用户，就必须全部扫描一遍了。</p>
<p>所以，<strong>哈希表这种结构适用于只有等值查询的场景</strong>，比如<strong>Memcached及其他一些NoSQL引擎</strong>。</p>
<p>而<strong>有序数组在等值查询和范围查询场景中的性能就都非常优秀</strong>。还是上面这个根据身份证号查名字的例子，如果我们使用有序数组来实现的话，示意图如下所示：</p>
<p><img src="/2024/09/09/MySQL04/image-20240909233223301.png" alt="image-20240909233223301"></p>
<p>这里我们假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你要查ID_card_n2对应的名字，<strong>用二分法就可以快速得到，这个时间复杂度是O(log(N))。</strong></p>
<p>同时很显然，这个索引结构支持范围查询。你要查身份证号在[ID_card_X, ID_card_Y]区间的User，可以先用二分法找到ID_card_X（如果不存在ID_card_X，就找到大于ID_card_X的第一个User），然后向右遍历，直到查到第一个大于ID_card_Y的身份证号，退出循环。</p>
<p><strong>如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。</strong></p>
<p>所以，<strong>有序数组索引只适用于静态存储引擎</strong>，比如你要保存的是2017年某个城市的所有人口信息，这类<strong>不会再修改的数据</strong>。</p>
<p>二叉搜索树也是课本里的经典数据结构了。还是上面根据身份证号查名字的例子，如果我们用二叉搜索树来实现的话，示意图如下所示：</p>
<p><img src="/2024/09/09/MySQL04/image-20240909233347671.png" alt="image-20240909233347671"></p>
<p><strong>二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。</strong>这样如果你要查ID_card_n2的话，按照图中的搜索顺序就是按照UserA -&gt; UserC -&gt; UserF -&gt; User2这个路径得到。这个时间复杂度是O(log(N))。</p>
<p><strong>当然为了维持O(log(N))的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是O(log(N))。</strong></p>
<p>树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。<strong>二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。</strong></p>
<p>你可以想象一下一棵100万节点的平衡二叉树，树高20。一次查询可能需要访问20个数据块。<strong>在机械硬盘时代，从磁盘随机读一个数据块需要10 ms左右的寻址时间。也就是说，对于一个100万行的表，如果使用二叉树来存储，单独访问一个行可能需要20个10 ms的时间，这个查询可真够慢的。</strong></p>
<p>为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，<strong>而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小</strong>。</p>
<p><strong>以InnoDB的一个整数字段索引为例，这个N差不多是1200。这棵树高是4的时候，就可以存1200的3次方个值，这已经17亿了。考虑到树根的数据块总是在内存中的，一个10亿行的表上一个整数字段的索引，查找一个值最多只需要访问3次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。</strong></p>
<p><strong>N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。</strong></p>
<p><strong>不管是哈希还是有序数组，或者N叉树，它们都是不断迭代、不断优化的产物或者解决方案</strong>。数据库技术发展到今天，跳表、LSM树等数据结构也被用于引擎设计中，这里我就不再一一展开了。</p>
<p>这里补充一下关于红黑树，跳表，LSM树的资料：</p>
<ul>
<li><a href="https://www.cnblogs.com/henuliulei/p/15114440.html">b树，b+树，b-树,红黑树详解一锅端 - 你的雷哥 - 博客园 (cnblogs.com)</a></li>
<li><a href="https://blog.csdn.net/cy973071263/article/details/122543826">【数据结构】史上最好理解的红黑树讲解，让你彻底搞懂红黑树-CSDN博客</a></li>
<li><a href="https://cloud.tencent.com/developer/article/2392723">【全网最易懂的红黑树讲解】一眼看懂二叉树、平衡树、红黑树，一文打尽-腾讯云开发者社区-腾讯云 (tencent.com)</a></li>
<li><a href="https://www.jianshu.com/p/9d8296562806">Skip List–跳表（全网最详细的跳表文章没有之一） - 简书 (jianshu.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/415799237">深入浅出分析LSM树（日志结构合并树） - 知乎 (zhihu.com)</a></li>
</ul>
<p>你心里要有个概念，<strong>数据库底层存储的核心就是基于这些数据模型的。每碰到一个新数据库，我们需要先关注它的数据模型，这样才能从理论上分析出这个数据库的适用场景（这个说的确实很在理啊，不要一直闷着用，还是要学会分析的）</strong>。</p>
<p>截止到这里，我用了半篇文章的篇幅和你介绍了不同的数据结构，以及它们的适用场景，你可能会觉得有些枯燥。但是，我建议你还是要多花一些时间来理解这部分内容，毕竟这是数据库处理数据的核心概念之一，在分析问题的时候会经常用到。<strong>当你理解了索引的模型后，就会发现在分析问题的时候会有一个更清晰的视角，体会到引擎设计的精妙之处。</strong></p>
<p>现在，我们一起进入相对偏实战的内容吧。</p>
<p><strong>在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。（还记得前面存储引擎层实现了undo log的存储吗？）</strong>而即使多个存储引擎支持<strong>同一种类型</strong>的索引，其底层的实现也可能不同。由于InnoDB存储引擎在MySQL数据库中使用最为广泛，所以下面我就以InnoDB为例，和你分析一下其中的索引模型。</p>
<h3 id="InnoDB-的索引模型"><a href="#InnoDB-的索引模型" class="headerlink" title="InnoDB 的索引模型"></a>InnoDB 的索引模型</h3><p>在InnoDB中，表都是根据<strong>主键顺序以索引的形式存放的</strong>，这种存储方式的表称为<strong>索引组织表</strong>。又因为前面我们提到的，InnoDB使用了<strong>B+树索引模型</strong>，所以数据都是存储在B+树中的。</p>
<p><strong>每一个索引在InnoDB里面对应一棵B+树。（想想everything先建立索引，然后再便于我们查找文件就知道了，又或者是ES的倒排索引，索引的目的就是加快查找速度）</strong></p>
<p>假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引。</p>
<p>这个表的建表语句是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; create table T(</span><br><span class="line">id int primary key, </span><br><span class="line">k int not null, </span><br><span class="line">name varchar(16),</span><br><span class="line">index (k))engine=InnoDB;</span><br></pre></td></tr></table></figure>

<p>表中R1~R5的(ID,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)，两棵树的示例示意图如下。</p>
<p><img src="/2024/09/09/MySQL04/image-20240910081032872.png" alt="image-20240910081032872"></p>
<p>从图中不难看出，根据<strong>叶子节点</strong>的内容，索引类型分为主键索引和非主键索引。</p>
<ul>
<li><strong>主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。</strong></li>
<li><strong>非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。</strong></li>
</ul>
<p>根据上面的索引结构说明，我们来讨论一个问题：<strong>基于主键索引和普通索引的查询有什么区别？</strong></p>
<ul>
<li>如果语句是select * from T where ID&#x3D;500，即主键查询方式，则<strong>只需要</strong>搜索ID这棵B+树；</li>
<li>如果语句是select * from T where k&#x3D;5，即普通索引查询方式，则需要先搜索k索引树，<strong>得到ID的值为500</strong>，<strong>再到</strong>ID索引树搜索一次。这个过程称为<strong>回表</strong>。</li>
</ul>
<p>也就是说，<strong>基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。</strong></p>
<h3 id="索引维护"><a href="#索引维护" class="headerlink" title="索引维护"></a>索引维护</h3><p>B+树为了<strong>维护索引有序性，在插入新值的时候需要做必要的维护</strong>。以上面这个图为例，如果插入新的行ID值为700，则只需要在R5的记录后面插入一个新记录。如果新插入的ID值为400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。</p>
<p>而更糟的情况是，<strong>如果R5所在的数据页已经满了，根据B+树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂</strong>。在这种情况下，性能自然会受影响。</p>
<p>建议先看看这几篇博客</p>
<ul>
<li><p><a href="https://blog.csdn.net/mingyuli/article/details/120349671">Mysql基础(八)：表空间、段、区、页的关系_mysql 段 区 页-CSDN博客</a></p>
<p>总结</p>
<p><strong>表空间</strong>表示一本<strong>书</strong>，<strong>段</strong>表示书中的<strong>章节</strong>，<strong>区</strong>表示每章节的<strong>小节</strong>，<strong>页</strong>表示书的<strong>每一页</strong>，<strong>行</strong>就是每页的<strong>每行数据</strong>。<strong>表空间里有多个段，一个段包含256个区，一个区包含64个页，一个页为16K。</strong></p>
<p>这个总结还可以</p>
<p><img src="/2024/09/09/MySQL04/image-20240910083029325.png" alt="image-20240910083029325"></p>
</li>
<li><p><a href="https://www.cnblogs.com/ZhuChangwu/p/14041410.html">一看就懂的：MySQL数据页以及页分裂机制 - 赐我白日梦 - 博客园 (cnblogs.com)</a></p>
<p>数据页长这样，可以对比上面的图看一下</p>
<p><img src="/2024/09/09/MySQL04/image-20240910083201460.png" alt="image-20240910083201460"></p>
</li>
<li><p><a href="https://blog.csdn.net/LO_YUN/article/details/112061699">图解MySQL页分裂_mysql页分列-CSDN博客</a></p>
<p><img src="/2024/09/09/MySQL04/image-20240910083342055.png" alt="image-20240910083342055"></p>
<p>上面就是数据页的结构了，<strong>首先两个数据页之间会有指针指向上一个和下一个数据页</strong>，形成一个<strong>双向链表</strong>，在数据页中存储的就是一行行的数据了，<strong>每个数据行之间会有单向指针连接，组成一个单向链表</strong></p>
<p>我们还可以看到图中有一些数字，这些代表的是这行数据行的类型，<strong>第一行的行类型是2，就说明这一行是起始行，代表最小的一行</strong>，指针指向了下一行的数据；<strong>接下来的数据行的行类型是0，也就是普通的数据行</strong>，里面存储了各种字段；<strong>最后一行的行类型是3，代表了最大的一行</strong></p>
<p>当一个数据页中的数据行太多放不下的下，<strong>就会生成一个新的数据页来存储，同时使用双向链表来相连</strong></p>
<p>结论就是<strong>主键值最好是有序的，这样就可以不用页分裂，还能充分使用到索引，否则就必须进行页分裂来保证索引的使用</strong></p>
</li>
<li><p><a href="https://cloud.tencent.com/developer/article/1656122">Innodb页合并和页分裂-腾讯云开发者社区-腾讯云 (tencent.com)</a></p>
<p>记住InnoDB<strong>不能以单行基础上工作</strong>是非常重要的。InnoDB<strong>总是在页上操作</strong>。一旦页被加载，它就会扫描页以寻找所请求的行&#x2F;记录。</p>
<p><strong>删除之后页合并（50%），插入新数据页分裂（不能乱序插入）。</strong></p>
<p>还记得我们说过的链表吗？此时第10页之前的页为第9页，之后的页为第11页。</p>
<p>第11页保持原样。<strong>改变的是页之间的关系</strong>：</p>
<ul>
<li>第10页之前的页为第9页，之后的页为第12页</li>
<li>第12页之前的页为第10页，之后的页是第11页</li>
<li>第11页之前的页为第12页，之后的页为第13页</li>
</ul>
<p>9 –&gt; 10 –&gt;12 –&gt; 11 –&gt;13</p>
<p>大概就是执行了这个操作</p>
<p>InnoDB做的是（简化版）：</p>
<ol>
<li>创建一个新页。</li>
<li><strong>确定原始页（第10页）可以在哪里拆分（在记录级别）</strong></li>
<li><strong>移动记录</strong></li>
<li><strong>重新定义页之间关系</strong></li>
</ol>
<p>这里讲的还可以，提取出来了：</p>
<p><strong>当心批量插入失败或者回滚时带来的MySQL表碎片</strong></p>
<p>通常，DBA都了解使用DELETE语句会产生表碎片。在大多数情况下，当执行大量的删除时，DBA总会重新构建表以回收磁盘空间。<strong>但是，您是否认为只有删除才会导致表碎片？（答案：并不是）</strong>。</p>
<p>在这篇博文中，我将解释插入如何会带来碎片。</p>
<p>在讨论这个主题之前，我们需要了解MySQL，有两种碎片：</p>
<ul>
<li><strong>在表中的InnoDB页完全空闲引起的碎片。</strong></li>
<li><strong>InnoDB页未填充满（页中还有一些空闲空间）引起的碎片。</strong></li>
</ul>
<p>主要有三种由插入引起的碎片场景：</p>
<ul>
<li><strong>插入之后回滚会产生碎片</strong></li>
<li><strong>插入语句失败会产生碎片</strong></li>
<li><strong>页分裂引起的碎片</strong></li>
</ul>
<p>这篇讲的比前几篇详细一点，建议仔细看看</p>
</li>
</ul>
<p><strong>除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约50%。</strong></p>
<p><strong>当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。</strong></p>
<p>基于上面的索引维护过程说明，我们来讨论一个案例：</p>
<blockquote>
<p>你可能在一些建表规范里面见到过类似的描述，<strong>要求建表语句里一定要有自增主键</strong>。当然事无绝对，<strong>我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该。</strong></p>
</blockquote>
<p>自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。</p>
<p>插入新记录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。</p>
<p>也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。<strong>每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。</strong></p>
<p><strong>而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。</strong></p>
<p>除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？</p>
<p><strong>由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。</strong></p>
<p><strong>显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。</strong></p>
<p>所以，从性能和存储空间方面考量，<strong>自增主键往往是更合理的选择</strong>。</p>
<p>有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：</p>
<ol>
<li><strong>只有一个索引；</strong></li>
<li><strong>该索引必须是唯一索引。</strong></li>
</ol>
<p><strong>你一定看出来了，这就是典型的KV场景。（这下看懂了）</strong></p>
<p><strong>由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。</strong></p>
<p><strong>这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天分析了数据库引擎可用的数据结构，介绍了<strong>InnoDB采用的B+树结构，以及为什么InnoDB要这么选择</strong>。B+树能够很好地<strong>配合磁盘的读写特性，减少单次查询的磁盘访问次数</strong>。</p>
<p>由于InnoDB是<strong>索引组织表，一般情况下我会建议你创建一个自增主键，这样非主键索引占用的空间最小</strong>。但事无绝对，我也跟你讨论了<strong>使用业务逻辑字段做主键的应用场景</strong>。</p>
<p>对于上面例子中的InnoDB表T，如果你要重建索引 k，你的两个SQL语句可以这么写：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table T drop index k;</span><br><span class="line">alter table T add index(k);</span><br></pre></td></tr></table></figure>

<p>如果你要重建主键索引，也可以这么写：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table T drop primary key;</span><br><span class="line">alter table T add primary key(id);</span><br></pre></td></tr></table></figure>

<p>我的问题是，对于上面这两个重建索引的作法，说出你的理解。如果有不合适的，为什么，更好的方法是什么？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>如何避免长事务对业务的影响？</p>
<p>这个问题，我们可以从应用开发端和数据库端来看。</p>
<p><strong>首先，从应用开发端来看：</strong></p>
<ol>
<li>确认是否<strong>使用了set autocommit&#x3D;0</strong>。这个确认工作可以在测试环境中开展，把MySQL的general_log开起来，然后随便跑一个业务逻辑，通过general_log的日志来确认。<strong>一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成1。</strong></li>
<li>确认<strong>是否有不必要的只读事务</strong>。<strong>有些框架会习惯不管什么语句先用begin&#x2F;commit框起来。我见过有些是业务并没有这个需要，但是也把好几个select语句放到了事务中。这种只读事务可以去掉。</strong></li>
<li>业务连接数据库的时候，根据业务本身的预估<strong>，通过SET MAX_EXECUTION_TIME命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间</strong>。（为什么会意外？在后续的文章中会提到这类案例）</li>
</ol>
<p><strong>其次，从数据库端来看：（这后面实战味道太浓了，没用过先跳过咯）</strong></p>
<ol>
<li>监控 information_schema.Innodb_trx表，设置长事务阈值，超过就报警&#x2F;或者kill；</li>
<li>Percona的pt-kill这个工具不错，推荐使用；</li>
<li>在业务功能测试阶段要求输出所有的general_log，分析日志行为提前发现问题；</li>
<li>如果使用的是MySQL 5.6或者更新版本，把innodb_undo_tablespaces设置成2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。</li>
</ol>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/04-shen-ru-qian-chu-suo-yin-shang">04 深入浅出索引（上） | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>事务隔离：为什么你改了我还看不见？</title>
    <url>/2024/09/09/MySQL03/</url>
    <content><![CDATA[<p>提到事务，你肯定不陌生，和数据库打交道的时候，我们总是会用到事务。最经典的例子就是转账，你要给朋友小王转100块钱，而此时你的银行卡只有100块钱。</p>
<p>转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这100块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。</p>
<p><strong>简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。</strong>在MySQL中，<strong>事务支持是在引擎层实现的。</strong>你现在知道，MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。<strong>比如MySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代的重要原因之一。</strong></p>
<p>今天的文章里，我将会以InnoDB为例，剖析MySQL在事务支持方面的特定实现，并基于原理给出相应的实践建议，希望这些案例能加深你对MySQL事务原理的理解。</p>
<h3 id="隔离性与隔离级别"><a href="#隔离性与隔离级别" class="headerlink" title="隔离性与隔离级别"></a>隔离性与隔离级别</h3><p>提到事务，你肯定会想到<strong>ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性）</strong>，今天我们就来说说其中I，也就是“隔离性”。</p>
<p>当数据库上有多个事务同时执行的时候，就可能出现<strong>脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题</strong>，为了解决这些问题，就有了“隔离级别”的概念。（简单的说，隔离级别是对于事务而言的，隔离的是事务不是其他的东西）</p>
<p>在谈隔离级别之前，你首先要知道，你<strong>隔离得越严实，效率就会越低</strong>。因此很多时候，我们都要在二者之间寻找一个平衡点。<strong>SQL标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。</strong>下面我逐一为你解释：</p>
<ul>
<li><strong>读未提交</strong>是指：一个事务还<strong>没提交</strong>时，它做的变更<strong>就能被别的事务看到</strong>。（可以读到别人没有提交的事务，等同于考试的时候可以直接看到别人试卷的答案）</li>
<li><strong>读提交</strong>是指：一个事务提交之后，它做的变更才会被其他事务看到。（你提交试卷之后我才能看到你的答案）</li>
<li><strong>可重复读</strong>是指：<strong>一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的</strong>。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。（始终相信自己最开始见到的数据是不会变化的，可以重复读取到最开始的值。<strong>这个其实底层是靠undo log的回滚指针不停往前找的</strong>）</li>
<li><strong>串行化</strong>：顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。<strong>当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。</strong>（<strong>串行就是排队等待，排成一行</strong>）</li>
</ul>
<p>其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表T中只有一列，其中一行的值为1，下面是按照时间顺序执行两个事务的行为。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; create table T(c int) engine=InnoDB;</span><br><span class="line">insert into T(c) values(1);</span><br></pre></td></tr></table></figure>

<p>我们来看看在不同的隔离级别下，事务A会有哪些不同的返回结果，也就是图里面V1、V2、V3的返回值分别是什么。</p>
<p><img src="/2024/09/09/MySQL03/image-20240909200825360.png" alt="image-20240909200825360"></p>
<ul>
<li><strong>若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。</strong>因此，V2、V3也都是2。</li>
<li>若隔离级别是“读提交”，则V1是1，V2的值是2。<strong>事务B的更新在提交后才能被A看到。</strong>所以， V3的值也是2。</li>
<li>若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：<strong>事务在执行期间看到的数据前后必须是一致的。</strong></li>
<li>若隔离级别是“串行化”，<strong>则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。</strong>所以从A的角度看， V1、V2值是1，V3的值是2。</li>
</ul>
<p>在实现上，数据库里面会<strong>创建一个视图</strong>，访问的时候<strong>以视图的逻辑结果为准</strong>。（视图的作用体现在这里了）</p>
<ul>
<li>在“可重复读”隔离级别下，<strong>这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。</strong></li>
<li>在“读提交”隔离级别下，<strong>这个视图是在每个SQL语句开始执行的时候创建的。</strong></li>
<li>这里需要注意的是，<strong>“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念。</strong></li>
<li>而“串行化”隔离级别下<strong>直接用加锁的方式来避免并行访问。</strong></li>
</ul>
<p>我们可以看到在不同的隔离级别下，数据库行为是有所不同的。<strong>Oracle数据库的默认隔离级别其实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致，你一定要记得将MySQL的隔离级别设置为“读提交”。（MySQL 的默认事务隔离级别是 <strong>REPEATABLE READ</strong>（可重复读））</strong></p>
<p>配置的方式是，<strong>将启动参数transaction-isolation的值设置成READ-COMMITTED</strong>。你可以用show variables来查看当前的值。（这个输出的东西很多，你可以指定输出流输入到一个文件中看看）</p>
<p><img src="/2024/09/09/MySQL03/image-20240909201622812.png" alt="image-20240909201622812"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; show variables like &#x27;transaction_isolation&#x27;;</span><br><span class="line"></span><br><span class="line">+-----------------------+----------------+</span><br><span class="line"></span><br><span class="line">| Variable_name | Value |</span><br><span class="line"></span><br><span class="line">+-----------------------+----------------+</span><br><span class="line"></span><br><span class="line">| transaction_isolation | READ-COMMITTED |</span><br><span class="line"></span><br><span class="line">+-----------------------+----------------+</span><br></pre></td></tr></table></figure>

<p>总结来说，存在即合理，哪个隔离级别都有它自己的使用场景，你要根据自己的业务情况来定。我想<strong>你可能会问那什么时候需要“可重复读”的场景呢</strong>？我们来看一个数据校对逻辑的案例。</p>
<p>假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。<strong>这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。</strong></p>
<p><strong>这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。</strong></p>
<h3 id="事务隔离的实现"><a href="#事务隔离的实现" class="headerlink" title="事务隔离的实现"></a>事务隔离的实现</h3><p>理解了事务的隔离级别，我们再来看看事务隔离具体是怎么实现的。这里我们展开说明“可重复读”。</p>
<p><strong>在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。（这就是Undo log用于回滚的。Undo Log（撤销日志）在数据库管理系统中用于实现回滚操作，确保数据的完整性和一致性）</strong></p>
<p>建议先看完这篇博客再看下面的内容</p>
<p><a href="https://blog.csdn.net/Weixiaohuai/article/details/117867353">MySQL回滚日志（undo log）总结_mysql undo log-CSDN博客</a></p>
<p>下面是这篇文章比较重要的部分：</p>
<p>MVCC（Multi-Version Concurrency Control，<strong>多版本并发控制</strong>）是一种用于数据库管理系统的并发控制机制，旨在允许<strong>多个事务同时读取和写入数据，而不互相阻塞</strong>。</p>
<p>在MySQL数据库InnoDB存储引擎中，用undo Log来实现多版本并发控制(MVCC)。</p>
<p><strong>当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据版本是怎样的，从而让用户能够读取到当前事务操作之前的数据【快照读】。</strong></p>
<p>下面解释一下什么是快照读，与之对应的还有一个是—当前读。</p>
<ul>
<li>快照读：<br>SQL读取的数据是快照版本【可见版本】，<strong>也就是历史版本</strong>，不用加锁，普通的SELECT就是快照读。</li>
<li>当前读：<br><strong>SQL读取的数据是最新版本。</strong>通过锁机制来保证读取的数据无法通过其他事务进行修改UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE都是当前读。</li>
</ul>
<p>说实话，这个玩意的底层实现很类似于Git的底层实现</p>
<p>undo log的存储机制</p>
<p>undo log的存储<strong>由InnoDB存储引擎</strong>实现，<strong>数据保存在InnoDB的数据文件中</strong>。</p>
<p>在InnoDB存储引擎中，undo log是<strong>采用分段(segment)的方式进行存储的</strong>。<strong>rollback segment称为回滚段，每个回滚段中有1024个undo log segment。</strong></p>
<p>在MySQL5.5之前，只支持1个rollback segment，也就是<strong>只能记录1024个undo操作</strong>。（这个记录的方式和Redis的AOF很类似）</p>
<p><strong>在MySQL5.5之后，可以支持128个rollback segment，分别从resg slot0 - resg slot127，每一个resg slot，也就是每一个回滚段，内部由1024个undo segment 组成，即总共可以记录128 * 1024个undo操作。（这个又有一点像Redis分片集群的slot槽）</strong></p>
<p><img src="/2024/09/09/MySQL03/image-20240909210933285.png" alt="image-20240909210933285"></p>
<p><strong>undo log日志里面不仅存放着数据更新前的记录，还记录着RowID、事务ID、回滚指针。（这个玩意和Git底层很类似）</strong>其中事务ID每次递增，回滚指针第一次如果是insert语句的话，回滚指针为NULL，第二次update之后的undo log的回滚指针就会指向刚刚那一条undo log日志，依次类推，就会形成一条undo log的回滚链，方便找到该条记录的历史版本。</p>
<p>undo log的工作原理</p>
<p>在更新数据之前，MySQL会<strong>提前</strong>生成undo log日志，当<strong>事务提交</strong>的时候，并<strong>不会立即删除undo log</strong>，要执行回滚（rollback）操作时，从缓存中读取数据。<strong>undo log日志的删除是通过通过后台purge线程进行回收处理的。</strong></p>
<p><img src="/2024/09/09/MySQL03/image-20240909211811977.png" alt="image-20240909211811977"></p>
<p>如上图：</p>
<ul>
<li>1、事务A执行update操作，此时事务还没提交，<strong>会将数据进行备份到对应的undo buffer，然后由undo buffer持久化到磁盘中的undo log文件中，此时undo log保存了未提交之前的操作日志</strong>，接着将操作的数据，也就是Teacher表的数据持久保存到<strong>InnoDB的数据文件IBD。</strong>（注意是先在缓存中记录到undo buffer再持久化到undo log日志中，最后才是将数据持久化到存储引擎的IBD文件中）</li>
<li>2、此时事务B进行查询操作，直接从undo buffer缓存中进行读取，这时事务A还没提交事务。如果要回滚（rollback）事务，是不读磁盘的，<strong>先直接从undo buffer缓存读取。</strong></li>
</ul>
<p>用undo log实现原子性和持久化的事务的简化过程：</p>
<p>假设有A、B两个数据，值分别为1,2。</p>
<ul>
<li>A. 事务开始</li>
<li>B. 记录A&#x3D;1到undo log中</li>
<li>C. 修改A&#x3D;3</li>
<li>D. 记录B&#x3D;2到undo log中</li>
<li>E. 修改B&#x3D;4</li>
<li>F. 将undo log写到磁盘 ——-<strong>undo log持久化</strong></li>
<li>G. 将数据写到磁盘 ——-<strong>数据持久化</strong></li>
<li>H. 事务提交 ——-<strong>提交事务</strong></li>
</ul>
<p>之所以能同时保证原子性和持久化，是因为以下特点：</p>
<ul>
<li><strong>更新数据前</strong>记录undo log。</li>
<li>为了保证持久性，<strong>必须将数据在事务提交前写到磁盘，只要事务成功提交，数据必然已经持久化到磁盘。</strong></li>
<li><strong>undo log必须先于数据持久化到磁盘。</strong>如果在G,H之间发生<strong>系统崩溃</strong>，undo log是完整的，<strong>可以用来回滚。</strong></li>
<li>如果在A - F之间发生系统崩溃，因为数据没有持久化到磁盘，所以磁盘上的数据还是保持在事务开始前的状态。</li>
</ul>
<p>缺陷：每个事务提交前将数据和undo log写入磁盘，这样会导致大量的磁盘IO，因此性能较差。 <strong>如果能够将数据缓存一段时间，就能减少IO提高性能，但是这样就会失去事务的持久性。</strong></p>
<blockquote>
<p><strong>undo日志属于逻辑日志，redo（recovery）是物理日志</strong>，所谓逻辑日志是undo log<strong>是记录一个操作过程</strong>，不会物理删除</p>
<p><strong>binlog也是逻辑日志，AOF也是逻辑日志</strong></p>
<p>undo log，sql执行delete或者update操作都会记录一条undo日志</p>
<p><strong>undo是应对数据库回滚的情况，redo是应对数据库执行到一半故障继续执行的情况，binlog是应对恢复数据库的情况</strong></p>
</blockquote>
<p>再看一篇试试</p>
<p><a href="https://cloud.tencent.com/developer/article/2220871">图文结合带你搞定MySQL日志之Undo log(回滚日志)-腾讯云开发者社区-腾讯云 (tencent.com)</a></p>
<p>下面提取了部分重点内容：</p>
<p><strong>如何理解Undo Log</strong></p>
<p>事务需要保证<strong>原子性</strong>，也就是事务中的操作要么全部完成，要么什么也不做。但有时候事务执行到一半会出现一些情况，比如：</p>
<ul>
<li>情况一：事务执行过程中可能遇到<strong>各种错误</strong>，比如<a href="https://cloud.tencent.com/act/pro/promotion-cvm?from_column=20065&from=20065">服务器</a>本身的错误，操作系统错误，甚至是突然断电导致的错误。</li>
<li>情况二：DBA可以在事务执行过程中手动输入ROLLBACK语句结束当前事务的执行。以上情况出现，我们需要把数据改回原先的样子，这个过程称之为回滚。</li>
</ul>
<p>每当我们要对一条记录做改动时**(这里的改动可以指<code>INSERT、DELETE、UPDATE</code>)，都需要把回滚时所需的东西记下来。**比如:</p>
<ul>
<li>你插入一条记录时，至少要把这条记录的主键值记下来，之后回滚的时候只需要把这个主键值对应的记录删掉就好了。**(对于每个<code>INSERT</code>, InnoDB存储引擎会完成一个<code>DELETE</code>)**</li>
<li>你删除了一条记录，至少要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了。**(对于每个<code>DELETE</code>,InnoDB存储引擎会执行一个<code>INSERT</code>)**</li>
<li>你修改了一条记录，至少要把修改这条记录前的旧值都记录下来，这样之后回滚时再把这条记录更新为旧值就好了。**(对于每个<code>UPDATE</code>，InnoDB存储引擎会执行一个<code>相反的UPDATE</code>，将修改前的行放回去)**</li>
</ul>
<p><a href="https://cloud.tencent.com/product/cdb?from_column=20065&from=20065">MySQL</a>把这些为了回滚而记录的这些内容称之为<code>撤销日志</code>或者<code>回滚日志</code>(即Undo Log)。<strong>注意，由于查询操作(SELECT）并不会修改任何用户记录，所以在杳询操作行时，并不需要记录相应的Undo日志</strong></p>
<p><strong>此外，Undo Log会产生<code>Redo Log</code>，也就是Undo Log的产生会伴随着Redo Log的产生，这是因为Undo Log也需要持久性的保护。</strong></p>
<p>这里解释一下，为什么它们是相辅相成的，这里简单的说一下，本来我们执行到一半挂了，undo是回到开始，redo是继续执行完</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在数据库管理系统中，Undo Log 和 Redo Log 是用于实现数据一致性和事务管理的两种重要日志，它们通常会一起生成，以确保在事务过程中能够安全地恢复数据。以下是详细解释：</span><br><span class="line"></span><br><span class="line">### 1. **Undo Log 的作用**</span><br><span class="line"></span><br><span class="line">- **回滚功能**: Undo Log 记录了事务对数据的修改前状态，允许在事务失败或被中断时将数据恢复到修改之前的状态。</span><br><span class="line">- **持久性保护**: 为了确保在系统崩溃或故障后仍能进行数据恢复，Undo Log 必须被持久化到磁盘。</span><br><span class="line"></span><br><span class="line">### 2. **Redo Log 的作用**</span><br><span class="line"></span><br><span class="line">- **恢复功能**: Redo Log 记录了已提交事务的修改操作，用于在系统崩溃后重放这些操作，从而恢复数据到最新状态。</span><br><span class="line">- **事务的完整性**: Redo Log 确保即使在故障发生时，已提交的事务也不会丢失。</span><br><span class="line"></span><br><span class="line">### 3. **二者的关系**</span><br><span class="line"></span><br><span class="line">- **同时生成**: 在事务执行过程中，每当数据库修改数据时，系统会同时生成 Undo Log 和 Redo Log。</span><br><span class="line">  - **Undo Log**: 记录修改前的状态，以便在需要回滚时使用。</span><br><span class="line">  - **Redo Log**: 记录修改后的状态，以便在系统重启时重放已提交的事务。</span><br><span class="line">- **持久性需求**:</span><br><span class="line">  - Undo Log 需要被持久化，以确保在崩溃后能够恢复未完成的事务或回滚操作。</span><br><span class="line">  - Redo Log 则确保已完成的事务能够在故障后恢复。</span><br><span class="line"></span><br><span class="line">### 4. **总结**</span><br><span class="line"></span><br><span class="line">因此，Undo Log 和 Redo Log 的产生是相辅相成的。Undo Log 提供了回滚的能力，而 Redo Log 确保了已提交事务的持久性。两者的结合使得数据库能够在发生故障时，既能恢复到一致的状态，又能保证数据的完整性和安全性。</span><br></pre></td></tr></table></figure>

<p><strong>Undo Log的存储结构</strong></p>
<p>InnoDB对Undo Log的管理采用段的方式，也就是<code>回滚段（rollback segment）</code>。每个回滚段记录了<code>1024 个Undo Log segment</code>，而在每个Undo Log segment段中进行<code>Undo页</code>的申请。</p>
<p>在<code>InnoDB1.1版本</code>之前（不包括1.1版本），只有一个<code>rollback segment</code>，因此支持<strong>同时在线</strong>的事务限制为 <strong>1024</strong>。虽然对绝大多数的应用来说都已经够用。</p>
<p>从1.1版本开始InnoDB支持最大<code>128个rollback segment</code>，故其支持<strong>同时在线的事务</strong>限制提高到了<code>128*1024</code>。</p>
<p>虽然InnoDB1.1版本支持了128个<code>rollback segment</code>，但是这些<code>rollback segment</code>都存储于共享表空间<strong>ibdata（上面提到的IBD文件）</strong>中。从lnnoDB1.2版本开始，可通过参数对<code>rollback segment</code>做进一步的设置。这些参数包括:</p>
<ul>
<li><code>innodb_undo_directory:</code>设置rollback segment文件所在的路径。这意味着rollback segment可以存放在共享表空间以外的位置，即可以设置为独立表空间。该参数的默认值为“.&#x2F;”，表示当前InnoDB存储引擎的目录。</li>
<li><code>innodb_undo_logs:</code>设置rollback segment的个数，默认值为128。在InnoDB1.2版本中，该参数用来替换之前版本的参数innodb_rollback_segments。</li>
<li><code>innodb_undo_tablespaces:</code>设置构成rollback segment文件的数量，这样rollback segment可以较为平均地分布在多个文件中。设置该参数后，会在路径innodb_undo_directory看到undo为前缀的文件，该文件就代表rollback segment文件。</li>
</ul>
<p>当事务提交时，InnoDB存储引擎会做以下两件事情：    </p>
<ul>
<li>1.将Undo Log放入列表中，<strong>以供之后的purge(清洗、清除)操作</strong>    </li>
<li>2.判断Undo Log所在的页是否可以重用(低于3&#x2F;4可以重用)，若可以分配给下个事务使用</li>
</ul>
<p><strong>Undo页的重用</strong></p>
<p>当我们开启一个事务需要写Undo log的时候，就得先去<code>Undo Log segment</code>中去找到一个空闲的位置，当有空位的时候，就去申请Undo页，在这个申请到的Undo页中进行Undo Log的写入。<strong>我们知道MySQL默认一页的大小是<code>16k</code>。</strong></p>
<p>为每一个事务分配一个页，<strong>是非常浪费的</strong>(除非你的事务非常长)，假设你的应用的TPS(每秒处理的事务数目)为1000，那么1s就需要1000个页，大概需要16M的存储，1分钟大概需要1G的存储。<strong>如果照这样下去除非MySQL清理的非常勤快，否则随着时间的推移，磁盘空间会增长的非常快</strong>，而且很多空间都是浪费的。</p>
<p><strong>于是Undo页就被设计的可以重用了</strong>，当事务提交时，并<strong>不会立刻删除</strong>Undo页。因为重用，所以这个Undo页可能混杂着其他事务的Undo Log。<strong>Undo Log在commit后，会被放到一个链表中，然后判断Undo页的使用空间是否小于3&#x2F;4，如果小于3&#x2F;4的话，则表示当前的Undo页可以被重用，那么它就不会被回收，其他事务的Undo Log可以记录在当前Undo页的后面。</strong>由于Undo Log是离散的，所以清理对应的磁盘空间时，效率不高。</p>
<p><strong>Undo Log的类型</strong></p>
<p>在InnoDB存储引擎中，Undo Log分为：</p>
<ul>
<li><p><strong>insert Undo Log</strong> </p>
<p>insert Undo Log是指在insert操作中产生的Undo Log。<strong>因为insert操作的记录，只对事务本身可见，对其他事务不可见(这是事务隔离性的要求)，故该Undo Log可以在事务提交后直接删除。不需要进行purge操作。</strong></p>
</li>
<li><p><strong>update Undo Log</strong></p>
<p>update Undo Log记录的是对delete和update操作产生的Undo Log。该Undo Log可能需要提供MVCC机制，因此<strong>不能在事务提交时就进行删除</strong>。提交时放入Undo Log链表，等待purge线程进行最后的删除。</p>
</li>
</ul>
<p><strong>Undo Log的生命周期</strong></p>
<p>以下是Undo+Redo事务的简化过程: 假设有2个数值，分别为 A&#x3D;1 和 B&#x3D;2 ，然后将A修改为3，B修改为4</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> start transaction;</span><br><span class="line"><span class="number">2</span>．记录A=<span class="number">1</span>到<span class="title class_">Undo</span> <span class="title class_">Log</span>;</span><br><span class="line"><span class="number">3.</span> update A = <span class="number">3</span>;</span><br><span class="line"><span class="number">4</span>．记录A=<span class="number">3</span> 到<span class="title class_">Redo</span> <span class="title class_">Log</span>;</span><br><span class="line"><span class="number">5</span>．记录B=<span class="number">2</span>到<span class="title class_">Undo</span> <span class="title class_">Log</span>;</span><br><span class="line"><span class="number">6.</span> update B = <span class="number">4</span>;</span><br><span class="line"><span class="number">7</span>．记录B=<span class="number">4</span>到<span class="title class_">Redo</span> <span class="title class_">Log</span>;</span><br><span class="line"><span class="number">8</span>．将<span class="title class_">Redo</span> <span class="title class_">Log</span>刷新到磁盘;</span><br><span class="line"><span class="number">9.</span> commit</span><br></pre></td></tr></table></figure>

<ul>
<li>在1-8步骤的任意一步系统宕机，<strong>事务未提交，该事务就不会对磁盘上的数据做任何影响</strong>。</li>
<li>如果在8-9之间宕机。<ul>
<li>Redo Log 进行恢复</li>
<li>Undo Log 发现有事务没完成进行回滚。</li>
</ul>
</li>
<li>若在9之后系统宕机，内存映射中变更的数据<strong>还来不及刷回磁盘</strong>，那么系统恢复之后，<strong>可以根据Redo Log把数据刷回磁盘。</strong></li>
</ul>
<p>流程图：</p>
<p><strong>这里还有一个binlog没有画，突然想起来之前使用Seata的时候好像就出现过这个redo，undo，binlog了</strong></p>
<p><strong>又想起来了，之前使用ShardingSphere的时候就是使用的它的binlog来实现主从复制的</strong></p>
<p><img src="/2024/09/09/MySQL03/image-20240909220208902.png" alt="image-20240909220208902"></p>
<p>接下来再继续看</p>
<p>假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。</p>
<p>当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，<strong>同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）</strong>。对于read-view A，要得到1，就必须将当前值依次执行图中所有的回滚操作得到。</p>
<p><img src="/2024/09/09/MySQL03/image-20240909204425101.png" alt="image-20240909204425101"></p>
<p>同时你会发现，即使现在有另外一个事务正在将4改成5，这个事务跟read-view A、B、C对应的事务是不会冲突的，因为只要是修改没有commit，看的都是undo log中的备份数据。</p>
<p><strong>你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。</strong></p>
<p>什么时候才不需要了呢？<strong>就是当系统里没有比这个回滚日志更早的read-view的时候。（这里的回收机制有点类似GCRoot）</strong></p>
<p>基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。</p>
<p>长事务意味着<strong>系统里面会存在很老的事务视图</strong>。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会<strong>导致大量占用存储空间</strong>。</p>
<p>这里简单提一下为什么会有那么长的事务</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 业务逻辑复杂</span><br><span class="line">复杂的处理流程: 某些业务逻辑涉及多个步骤，可能需要长时间才能完成。例如，涉及审批、审核或多方协作的事务。</span><br><span class="line">2. 用户交互</span><br><span class="line">等待用户输入: 在许多应用中，事务可能需要等待用户的输入或确认。例如，在线购物中的支付过程，用户可能需要时间来确认付款。</span><br><span class="line">3. 网络延迟</span><br><span class="line">网络问题: 在分布式系统中，网络延迟可能导致事务处理时间延长。如果事务需要访问多个服务或数据库，网络问题可能导致长时间未提交。</span><br><span class="line">4. 锁竞争</span><br><span class="line">资源争用: 长事务可能会持有锁，导致其他事务无法执行。为了避免死锁，开发者可能选择不立即提交事务，而是等待某个条件满足。</span><br><span class="line">5. 性能问题</span><br><span class="line">数据库性能瓶颈: 如果数据库性能较差，查询或写入操作可能会耗时较长，导致事务延迟提交。</span><br><span class="line">6. 故障恢复</span><br><span class="line">故障处理: 如果系统或应用在处理事务时遇到故障，可能会导致事务未能及时提交，处于挂起状态。</span><br><span class="line">7. 设计缺陷</span><br><span class="line">系统设计不当: 如果系统设计没有考虑到事务的合理管理，可能会导致长时间持有未提交的事务。</span><br><span class="line">8. 长时间运行的报告或查询</span><br><span class="line">复杂查询: 某些查询可能需要长时间执行，尤其是在大数据集上进行复杂的分析和报告时，可能会导致事务维持较长时间。</span><br><span class="line">总结</span><br><span class="line">长事务未提交的原因可能涉及业务逻辑、用户交互、网络延迟、锁竞争等多种因素。长时间存在的事务视图会导致存储空间占用增加，影响数据库性能，因此在设计时应该尽量避免长事务，及时提交或回滚事务，以提高系统的效率和稳定性。</span><br></pre></td></tr></table></figure>

<p>在MySQL 5.5及以前的版本，<strong>回滚日志是跟数据字典一起放在ibdata文件里的</strong>，即使<strong>长事务最终提交，回滚段被清理，文件也不会变小</strong>。我见过数据只有20GB，而回滚段有200GB的库。最终只好为了清理回滚段，重建整个库。</p>
<p>除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库，这个我们会在后面讲锁的时候展开。</p>
<h3 id="事务的启动方式"><a href="#事务的启动方式" class="headerlink" title="事务的启动方式"></a>事务的启动方式</h3><p>如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL的事务启动方式有以下几种：</p>
<ol>
<li><p>显式启动事务语句， begin 或 start transaction。配套的提交语句是commit，回滚语句是rollback。</p>
</li>
<li><p>set autocommit&#x3D;0，<strong>这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个select语句，这个事务就启动了，而且并不会自动提交</strong>。这个事务持续存在直到你<strong>主动执行commit 或 rollback 语句</strong>，或者断开连接。</p>
</li>
</ol>
<p>有些客户端连接框架会默认连接成功后先执行一个set autocommit&#x3D;0的命令。<strong>这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。</strong></p>
<p><strong>因此，我会建议你总是使用set autocommit&#x3D;1, 通过显式语句的方式来启动事务。</strong></p>
<p>但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用commit work and chain语法。</p>
<p>在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。<strong>如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。（详细的可以看看这篇文章<a href="https://juejin.cn/post/6987373561836994590">带你看看事务的分类和在MySQL中使用链式事务 | SQL全面教程七：事务(3)事务分类和MySQL中的commit work and chain - 掘金 (juejin.cn)</a>）</strong></p>
<p>你可以在information_schema库的innodb_trx这个表中查询长事务，比如下面这个语句，用于查找持续时间超过60s的事务。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> information_schema.innodb_trx <span class="keyword">where</span> TIME_TO_SEC(timediff(now(),trx_started))<span class="operator">&gt;</span><span class="number">60</span></span><br></pre></td></tr></table></figure>

<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>主要介绍了MySQL的事务隔离级别的现象和实现，根据实现原理分析了长事务存在的风险，以及如何用正确的方式避免长事务。希望我举的例子能够帮助你理解事务，并更好地使用MySQL的事务特性。</p>
<p>现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>在上期文章的最后，留下的问题是一天一备跟一周一备的对比。</p>
<p>好处是“最长恢复时间”更短。</p>
<p>在一天一备的模式里，最坏情况下需要应用一天的binlog。比如，你每天0点做一次全量备份，而要恢复出一个到昨天晚上23点的备份。</p>
<p>一周一备最坏情况就要应用一周的binlog了。</p>
<p><strong>系统的对应指标就是RTO（恢复目标时间）。</strong></p>
<p>当然这个是有成本的，因为更频繁全量备份需要消耗更多存储空间，所以这个RTO是成本换来的，就需要你根据业务重要性来评估了。</p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/03-shi-wu-ge-li-wei-shi-mo-ni-gai-le-wo-huan-kan-bu-jian">03 事务隔离：为什么你改了我还看不见？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>事务到底是隔离的还是不隔离的？</title>
    <url>/2024/09/10/MySQL08/</url>
    <content><![CDATA[<p>在第3篇文章和你讲事务隔离级别的时候提到过，<strong>如果是可重复读隔离级别，事务T启动的时候会创建一个视图read-view</strong>，之后事务T执行期间，即使有其他事务修改了数据，事务T看到的<strong>仍然跟在启动时看到的一样</strong>。也就是说，一个在可重复读隔离级别下执行的事务，好像与世无争，不受外界影响。</p>
<p>但是，我在上一篇文章中，和你分享行锁的时候又提到，<strong>一个事务要更新一行，如果刚好有另外一个事务拥有这一行的行锁</strong>，<strong>它又不能这么超然了，会被锁住，进入等待状态</strong>。问题是，既然进入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？</p>
<p>我给你举一个例子吧。下面是一个只有两行的表的初始化语句。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE `t` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `k` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB;</span><br><span class="line">insert into t(id, k) values(1,1),(2,2);</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/10/MySQL08/image-20240910163254527.png" alt="image-20240910163254527"></p>
<p>这里，我们需要注意的是事务的启动时机。</p>
<p><strong>begin&#x2F;start transaction 命令并不是一个事务的起点</strong>，在执行到它们之后的<strong>第一个操作InnoDB表的语句，事务才真正启动</strong>。<strong>如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令。（怪不得之前会加上一个后缀 with consistent snapshot）</strong></p>
<p>还需要注意的是，在整个专栏里面，我们的例子中如果没有特别说明，都是默认autocommit&#x3D;1。</p>
<p>在这个例子中，事务C没有显式地使用begin&#x2F;commit，<strong>表示这个update语句本身就是一个事务，语句完成的时候会自动提交</strong>。事务B在更新了行之后查询; 事务A在一个只读事务中查询，并且时间顺序上是在事务B的查询之后。</p>
<p>这时，如果我告诉你事务B查到的k的值是3，而事务A查到的k的值是1，你是不是感觉有点晕呢？</p>
<p>所以，今天这篇文章，我其实就是想和你说明白这个问题，希望借由把这个疑惑解开的过程，能够帮助你对InnoDB的事务和锁有更进一步的理解。</p>
<p>在MySQL里，<strong>有两个“视图”的概念</strong>：</p>
<ul>
<li><strong>一个是view。</strong>它是一个用查询语句定义的虚拟表，在调用的时候<strong>执行查询语句并生成结果</strong>。创建视图的语法是create view … ，<strong>而它的查询方法与表一样。</strong></li>
<li><strong>另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view。</strong>用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。</li>
</ul>
<p>它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。</p>
<p>在第3篇文章<a href="https://time.geekbang.org/column/article/68963">《事务隔离：为什么你改了我还看不见？》</a>中，我跟你解释过一遍MVCC的实现逻辑。今天为了说明查询和更新的区别，我换一个方式来说明，<strong>把read view拆开</strong>。你可以结合这两篇文章的说明来更深一步地理解MVCC。</p>
<h3 id="“快照”在MVCC里是怎么工作的？"><a href="#“快照”在MVCC里是怎么工作的？" class="headerlink" title="“快照”在MVCC里是怎么工作的？"></a>“快照”在MVCC里是怎么工作的？</h3><p>在可重复读隔离级别下，<strong>事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。</strong></p>
<p>这时，你会说这看上去不太现实啊。如果一个库有100G，那么我启动一个事务，MySQL就要拷贝100G的数据出来，这个过程得多慢啊。<strong>可是，我平时的事务执行起来很快啊。</strong></p>
<p>实际上，我们并不需要拷贝出这100G的数据。我们先来看看这个快照是怎么实现的。</p>
<p><strong>InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。（有主键那味了）</strong></p>
<p><strong>而每行数据也都是有多个版本的。</strong>每次事务更新数据的时候，<strong>都会生成一个新的数据版本</strong>，并且<strong>把transaction id赋值给这个数据版本的事务ID</strong>，记为<strong>row trx_id</strong>。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。（这个更新版本的操作和ES挺像的）</p>
<p>也就是说，数据表中的一行记录，<strong>其实可能有多个版本(row)<strong>，</strong>每个版本有自己的row trx_id。</strong></p>
<p>如图2所示，就是一个记录被多个事务连续更新后的状态。</p>
<p><img src="/2024/09/10/MySQL08/image-20240910163823376.png" alt="image-20240910163823376"></p>
<p>图中虚线框里是<strong>同一行数据</strong>的4个版本，当前最新版本是V4，k的值是22，<strong>它是被transaction id 为25的事务更新的，因此它的row trx_id也是25。</strong>（Transaction ID: <strong>trx_id</strong> 的全称是 <strong>Transaction Identifier</strong>，即<strong>事务标识符</strong>。通过追踪每行数据的 trx_id，数据库能够确保在读取数据时<strong>只返回对应于某个事务快照的数据</strong>，从而保持数据的一致性和隔离性。）</p>
<p>你可能会问，前面的文章不是说，语句更新会生成undo log（回滚日志）吗？那么，<strong>undo log在哪呢？</strong></p>
<p><strong>实际上，图2中的三个虚线箭头，就是undo log；而V1、V2、V3并不是物理上真实存在的，而是每次需要的时候根据当前版本和undo log计算出来的。比如，需要V2的时候，就是通过V4依次执行U3、U2算出来。</strong></p>
<p>明白了多版本和row trx_id的概念后，我们再来想一下，InnoDB是怎么定义那个“100G”的快照的。</p>
<p>按照可重复读的定义，一个事务启动的时候，能够看到所有<strong>已经提交的事务结果</strong>。但是之后，这个事务执行期间，其他事务的更新对它不可见。</p>
<p>因此，一个事务只需要在启动的时候声明说，<strong>“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。</strong></p>
<p>当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的。</p>
<p><strong>在实现上， InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。</strong></p>
<p><strong>数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。</strong></p>
<p>这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。</p>
<p>而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。</p>
<p>这个视图数组把所有的row trx_id 分成了几种不同的情况。</p>
<p><img src="/2024/09/10/MySQL08/image-20240910163838038.png" alt="image-20240910163838038"></p>
<p>这样，对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能：</p>
<ol>
<li>如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，<strong>这个数据是可见的</strong>；</li>
<li>如果落在红色部分，表示这个版本是由将来启动的事务生成的，<strong>是肯定不可见的</strong>；</li>
<li>如果落在黄色部分，那就包括两种情况 a. 若 row trx_id在数组中，<strong>表示这个版本是由还没提交的事务生成的，不可见</strong>； b. 若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。</li>
</ol>
<p><img src="/2024/09/10/MySQL08/image-20240910165332970.png" alt="image-20240910165332970"></p>
<p>比如，对于图中的数据来说，如果有一个事务，它的低水位是18，那么当它访问这一行数据时，就会从V4通过U3计算出V3，所以在它看来，这一行的值是11。</p>
<p>你看，有了这个声明后，系统里面随后发生的更新，是不是就跟这个事务看到的内容无关了呢？<strong>因为之后的更新，生成的版本一定属于上面的2或者3(a)的情况，而对它来说，这些新的数据版本是不存在的，所以这个事务的快照，就是“静态”的了。</strong></p>
<p>所以你现在知道了，<strong>InnoDB利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。</strong></p>
<p><img src="/2024/09/10/MySQL08/image-20240910165529160.png" alt="image-20240910165529160"></p>
<p>接下来，我们继续看一下图中的三个事务，分析下事务A的语句返回的结果，为什么是k&#x3D;1。</p>
<p>这里，我们不妨做如下假设：</p>
<ol>
<li><strong>事务A开始前，系统里面只有一个活跃事务ID是99；</strong></li>
<li><strong>事务A、B、C的版本号分别是100、101、102，且当前系统里只有这四个事务；</strong></li>
<li><strong>三个事务开始前，（1，1）这一行数据的row trx_id是90。</strong></li>
</ol>
<p>这样，事务A的视图数组就是[99,100], 事务B的视图数组是[99,100,101], 事务C的视图数组是[99,100,101,102]。</p>
<p>为了简化分析，我先把其他干扰语句去掉，只画出跟事务A查询逻辑有关的操作：</p>
<p><img src="/2024/09/10/MySQL08/image-20240910163854472.png" alt="image-20240910163854472"></p>
<p>从图中可以看到，第一个有效更新是事务C，把数据从(1,1)改成了(1,2)。这时候，<strong>这个数据的最新版本的row trx_id是102，而90这个版本已经成为了历史版本。</strong></p>
<p>第二个有效更新是事务B，<strong>把数据从(1,2)改成了(1,3)。这时候，这个数据的最新版本（即row trx_id）是101，而102又成为了历史版本。</strong></p>
<p>你可能注意到了，在事务A查询的时候，其实事务B还没有提交，但是它生成的(1,3)这个版本已经变成当前版本了。但这个版本对事务A<strong>必须是不可见的</strong>，否则就变成脏读了。</p>
<p>好，现在事务A要来读数据了，它的视图数组是[99,100]。当然了，读数据都是从当前版本读起的。所以，事务A查询语句的读数据流程是这样的：</p>
<ul>
<li><strong>找到(1,3)的时候，判断出row trx_id&#x3D;101，比高水位大，处于红色区域，不可见；</strong></li>
<li><strong>接着，找到上一个历史版本，一看row trx_id&#x3D;102，比高水位大，处于红色区域，不可见；</strong></li>
<li><strong>再往前找，终于找到了（1,1)，它的row trx_id&#x3D;90，比低水位小，处于绿色区域，可见。</strong></li>
</ul>
<p>这样执行下来，虽然期间这一行数据被修改过，但是事务A不论在什么时候查询，看到这行数据的结果都是一致的，<strong>所以我们称之为一致性读。</strong></p>
<p>这个判断规则是从代码逻辑直接转译过来的，但是正如你所见，这样分析可见性很麻烦。</p>
<p>所以，我来给你翻译一下。<strong>一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：</strong></p>
<ol>
<li><strong>版本未提交，不可见；</strong></li>
<li><strong>版本已提交，但是是在视图创建后提交的，不可见；</strong></li>
<li><strong>版本已提交，而且是在视图创建前提交的，可见。</strong></li>
</ol>
<p>现在，我们用这个规则来判断图4中的查询结果，<strong>事务A的查询语句的视图数组是在事务A启动的时候生成的，这时候：</strong></p>
<ul>
<li><strong>(1,3)还没提交，属于情况1，不可见；</strong></li>
<li><strong>(1,2)虽然提交了，但是是在视图数组创建之后提交的，属于情况2，不可见；</strong></li>
<li><strong>(1,1)是在视图数组创建之前提交的，可见。</strong></li>
</ul>
<p><strong>你看，去掉数字对比后，只用时间先后顺序来判断，分析起来是不是轻松多了。所以，后面我们就都用这个规则来分析。</strong></p>
<h3 id="更新逻辑"><a href="#更新逻辑" class="headerlink" title="更新逻辑"></a>更新逻辑</h3><p>细心的同学可能有疑问了：<strong>事务B的update语句，如果按照一致性读，好像结果不对哦？</strong></p>
<p>你看图5中，事务B的视图数组是先生成的，之后事务C才提交，不是应该看不见(1,2)吗，怎么能算出(1,3)来？</p>
<p><img src="/2024/09/10/MySQL08/image-20240910170206616.png" alt="image-20240910170206616"></p>
<p>是的，如果事务B在更新之前<strong>查询</strong>一次数据，这个查询返回的k的值确实是1。</p>
<p>但是，当它要去<strong>更新数据</strong>的时候，就不能再在历史版本上更新了，否则事务C的更新就丢失了。<strong>因此，事务B此时的set k&#x3D;k+1是在（1,2）的基础上进行的操作。</strong></p>
<p>所以，这里就用到了这样一条规则：<strong>更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。</strong></p>
<p>因此，在更新的时候，当前读拿到的数据是(1,2)，更新后生成了新版本的数据(1,3)，<strong>这个新版本的row trx_id是101</strong>。</p>
<p>所以，在执行事务B<strong>查询</strong>语句的时候，一看自己的版本号是101，最新数据的版本号也是101，是自己的更新，可以直接使用，所以查询得到的k的值是3。</p>
<p>这里我们提到了一个概念，叫作<strong>当前读</strong>。<strong>其实，除了update语句外，select语句如果加锁，也是当前读。</strong></p>
<p>建议先看看这篇博客再看下面的内容：</p>
<ul>
<li><a href="https://learnku.com/articles/12800/lock-in-share-mode-mysql-shared-lock-exclusive-lock-for-update">MySQL 共享锁 (lock in share mode)，排他锁 (for update) | Laravel China 社区 (learnku.com)</a></li>
</ul>
<p>总结一下重点内容：</p>
<p>共享锁 (lock in share mode)：</p>
<ul>
<li>允许其它事务也增加共享锁读取</li>
<li>不允许其它事物增加排他锁 (for update)</li>
<li>当事务同时增加共享锁时候，事务的更新必须等待先执行的事务 commit 后才行，如果同时并发太大可能很容易造成死锁</li>
</ul>
<p>共享锁，事务都加，都能读。修改是惟一的，必须等待前一个事务 commit，才可</p>
<p>排他锁 (for update)：</p>
<ul>
<li>事务之间不允许其它排他锁或共享锁读取，修改更不可能</li>
<li>一次只能有一个排他锁执行 commit 之后，其它事务才可执行</li>
</ul>
<p>不允许其它事务增加共享或排他锁读取。修改是惟一的，必须等待前一个事务 commit，才可</p>
<p>接下来再看本篇内容</p>
<p>所以，如果把事务A的查询语句select * from t where id&#x3D;1修改一下，加上lock in share mode 或 for update，也都可以读到版本号是101的数据，返回的k的值是3。下面这两个select语句，就是分别加了读锁（S锁，共享锁）和写锁（X锁，排他锁）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select k from t where id=1 lock in share mode;</span><br><span class="line">mysql&gt; select k from t where id=1 for update;</span><br></pre></td></tr></table></figure>

<p>再往前一步，假设事务C不是马上提交的，而是变成了下面的事务C’，会怎么样呢？</p>
<p><img src="/2024/09/10/MySQL08/image-20240910170244857.png" alt="image-20240910170244857"></p>
<p>事务C’的不同是，更新后并没有马上提交，在它提交前，事务B的更新语句先发起了。前面说过了，虽然事务C’还没提交，但是(1,2)这个版本也已经生成了，并且是当前的最新版本。那么，事务B的更新语句会怎么处理呢？</p>
<p>这时候，我们在上一篇文章中提到的“两阶段锁协议”就要上场了。事务C’没提交，也就是说(1,2)这个版本上的<strong>写锁</strong>还没释放。而事务B是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务C’释放这个锁，才能继续它的当前读。</p>
<p><img src="/2024/09/10/MySQL08/image-20240910170256116.png" alt="image-20240910170256116"></p>
<p>到这里，我们把一致性读、当前读和行锁就串起来了。</p>
<p>现在，我们再回到文章开头的问题：<strong>事务的可重复读的能力是怎么实现的？</strong></p>
<p><strong>可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。</strong>如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。</p>
<p>而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：</p>
<ul>
<li>在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，<strong>之后事务里的其他查询都共用这个一致性视图</strong>；</li>
<li>在读提交隔离级别下，每一个语句执行前都会重新算出一个<strong>新的视图</strong>。</li>
</ul>
<p>那么，我们再看一下，在读提交隔离级别下，事务A和事务B的查询语句查到的k，分别应该是多少呢？</p>
<p>这里需要说明一下，“start transaction with consistent snapshot; ”的意思是从这个语句开始，创建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于普通的start transaction。</p>
<p>下面是读提交时的状态图，可以看到这两个查询语句的创建视图数组的时机发生了变化，就是图中的read view框。（注意：<strong>这里，我们用的还是事务C的逻辑直接提交，而不是事务C’）</strong></p>
<p><img src="/2024/09/10/MySQL08/image-20240910170318298.png" alt="image-20240910170318298"></p>
<p>这时，事务A的查询语句的视图数组是在执行这个语句的时候创建的，时序上(1,2)、(1,3)的生成时间都在创建这个视图数组的时刻之前。但是，在这个时刻：</p>
<ul>
<li><strong>(1,3)还没提交，属于情况1，不可见；</strong></li>
<li><strong>(1,2)提交了，属于情况3，可见。</strong></li>
</ul>
<p>所以，这时候事务A查询语句返回的是k&#x3D;2。</p>
<p>显然地，事务B查询结果k&#x3D;3。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>InnoDB的<strong>行数据有多个版本，每个数据版本有自己的row trx_id，每个事务或者语句有自己的一致性视图</strong>。普通查询语句是一致性读，一致性读会根据row trx_id和一致性视图确定数据版本的可见性。</p>
<ul>
<li><strong>对于可重复读，查询只承认在事务启动前就已经提交完成的数据；</strong></li>
<li><strong>对于读提交，查询只承认在语句启动前就已经提交完成的数据；</strong></li>
<li><strong>而当前读，总是读取已经提交完成的最新版本。</strong></li>
</ul>
<p>你也可以想一下，为什么表结构不支持“可重复读”？<strong>这是因为表结构没有对应的行数据，也没有row trx_id，因此只能遵循当前读的逻辑。</strong></p>
<p>当然，MySQL 8.0已经可以把表结构放在InnoDB字典里了，<strong>也许以后会支持表结构的可重复读</strong>。</p>
<p>又到思考题时间了。我用下面的表结构和初始化语句作为试验环境，事务隔离级别是<strong>可重复读</strong>。</p>
<p><img src="/2024/09/10/MySQL08/image-20240910184154368.png" alt="image-20240910184154368"></p>
<p>现在，我要把所有“字段c和id值相等的行”的c值清零，但是却发现了一个“诡异”的、改不掉的情况。请你构造出这种情况，并说明其原理。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE `t` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `c` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB;</span><br><span class="line">insert into t(id, c) values(1,1),(2,2),(3,3),(4,4);</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/10/MySQL08/image-20240910173401745.png" alt="image-20240910173401745"></p>
<p>复现出来以后，请你再思考一下，在实际的业务开发中有没有可能碰到这种情况？你的应用代码会不会掉进这个“坑”里，你又是怎么解决的呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我在上一篇文章最后，留给你的问题是：怎么删除表的前10000行。比较多的留言都选择了第二种方式，<strong>即：在一个连接中循环执行20次 delete from T limit 500。</strong></p>
<p>确实是这样的，第二种方式是相对较好的。</p>
<p>第一种方式（即：直接执行delete from T limit 10000）里面，<strong>单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。</strong></p>
<p>第三种方式（即：在<strong>20个连接中同时执行</strong>delete from T limit 500），<strong>会人为造成锁冲突。</strong></p>
<blockquote>
<p>大家需要区分行锁、MDL锁和表锁的区别。对InnoDB表更新一行，可能过了MDL关，却被挡在行锁阶段。</p>
</blockquote>
<p>直接看这篇博客即可，甚至还总结了一下前几节的内容</p>
<ul>
<li><p><a href="https://blog.csdn.net/weixin_42970433/article/details/108592789">【mysql45讲】全局锁、表级锁(表锁、MDL)、行锁的作用与区别_有mdl锁了为什么还要有表锁-CSDN博客</a></p>
</li>
<li><p><strong>全局锁</strong>一般用于<strong>数据库备份</strong>的时候使用，<strong>会对整个数据库加锁</strong>。会<strong>阻塞数据更新语句、数据定义语句、更新事务的提交</strong>（一般是在数据库备份的时候使用）</p>
</li>
<li><p><strong>表级锁</strong>现在用的比较少，如果你用的存储引擎是MyISAM的话，那么用的倒是很多。<strong>不仅会限制其他线程的读写，同时也会限制本教程接下来的操作对象</strong>（举个例子：lock tables t1 read,t2 write，使本线程只能读t1,写t2,不能写t1也不能读写其他表）（一般是存储引擎不支持行锁才会使用表级锁）</p>
</li>
<li><p><strong>元数据锁（mete data lock MDL)<strong>，不需要显示的使用，在访问表的时候会</strong>自动加上</strong>。当<strong>对一个表增删改查</strong>时，加<strong>MDL读锁</strong>；当<strong>对一个表做结构变更操作</strong>时，加<strong>MDL写锁</strong>。读锁<strong>不互斥</strong>，可以有多个线程对同一张表增删改查；写锁<strong>互斥</strong>，比如两个线程要对一张表修改表结构，第二个线程要等第一个线程结束后再修改。语句的MDL锁，在语句执行完后不会立即释放，<strong>而是等事务提交后才会释放</strong>。（简单的说，MDL是用来保护表结构的一种锁）</p>
</li>
<li><p><strong>行锁</strong>，<strong>不是</strong>所有的存储引擎都支持行锁，InnoDB是支持行锁的。InnoDB行锁是通过给索引项加锁来实现的，即只有通过索引条件检索数据，InnoDB才使用行级锁，否则将使用表锁。也就是说，<strong>没有索引的时候，使用表锁。</strong>在InnoDB中，行锁<strong>是需要时才加上</strong>，但不是执行完那条语句就释放，<strong>要等事务提交后释放</strong>。<br>所以，在应用中，<strong>如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。</strong>（简单的说，<strong>行锁是针对索引的锁</strong>）</p>
</li>
<li><p>死锁，并发系统中不同线程间<strong>循环资源等待时会出现死锁</strong>（字面意思，这个锁解不开了，是一个Dead的锁）</p>
</li>
</ul>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/08-shi-wu-dao-di-shi-ge-li-de-huan-shi-bu-ge-li-de">08 事务到底是隔离的还是不隔离的？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>全局锁和表锁 ：给表加个字段怎么有这么多阻碍？</title>
    <url>/2024/09/10/MySQL06/</url>
    <content><![CDATA[<p> 今天我要跟你聊聊MySQL的锁。<strong>数据库锁设计的初衷是处理并发问题。</strong>作为多用户共享的资源，当出现并发访问的时候，<strong>数据库需要合理地控制资源的访问规则</strong>。而锁就是用来实现这些访问规则的重要数据结构。</p>
<p><strong>根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类</strong>。今天这篇文章，我会和你分享全局锁和表级锁。而关于行锁的内容，我会留着在下一篇文章中再和你详细介绍。</p>
<p>这里需要说明的是，锁的设计比较复杂，这两篇文章不会涉及锁的具体实现细节，主要介绍的是碰到锁时的现象和其背后的原理。</p>
<h3 id="全局锁"><a href="#全局锁" class="headerlink" title="全局锁"></a>全局锁</h3><p>顾名思义，全局锁就是对<strong>整个数据库实例加锁</strong>。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。<strong>当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。</strong></p>
<p><strong>全局锁的典型使用场景是，做全库逻辑备份。</strong>也就是把整库每个表都select出来存成文本。</p>
<p>以前有一种做法，是通过FTWRL确保不会有其他线程对数据库做更新，然后对整个库做备份。<strong>注意，在备份过程中整个库完全处于只读状态。</strong></p>
<p><strong>但是让整库都只读，听上去就很危险：</strong></p>
<ul>
<li>如果你在主库上备份，<strong>那么在备份期间都不能执行更新，业务基本上就得停摆；</strong></li>
<li>如果你在从库上备份，<strong>那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。</strong></li>
</ul>
<p>看来加全局锁不太好。但是细想一下，备份为什么要加锁呢？我们来看一下不加锁会有什么问题。</p>
<p>假设你现在要维护“极客时间”的购买系统，关注的是用户账户余额表和用户课程表。</p>
<p><strong>现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉他的余额，然后往已购课程里面加上一门课。</strong></p>
<p>如果时间顺序上是先备份账户余额表(u_account)，然后用户购买，然后备份用户课程表(u_course)，会怎么样呢？你可以看一下这个图：</p>
<p><img src="/2024/09/10/MySQL06/image-20240910114231615.png" alt="image-20240910114231615"></p>
<p><strong>可以看到，这个备份结果里，用户A的数据状态是“账户余额没扣，但是用户课程表里面已经多了一门课”。如果后面用这个备份来恢复数据的话，用户A就发现，自己赚了。</strong></p>
<p>作为用户可别觉得这样可真好啊，你可以试想一下：如果备份表的顺序反过来，先备份用户课程表再备份账户余额表，又可能会出现什么结果？</p>
<p><strong>也就是说，不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。（所以在全库逻辑备份的时候要STW，得到那一个时间点的快照数据）</strong></p>
<p>说到视图你肯定想起来了，我们在前面讲事务隔离的时候，其实是有一个方法能够拿到一致性视图的（还记得是通过undo log实现的），对吧？</p>
<p><strong>是的，就是在可重复读隔离级别下开启一个事务。</strong></p>
<blockquote>
<p>备注：如果你对事务隔离级别的概念不是很清晰的话，可以再回顾一下第3篇文章<a href="https://time.geekbang.org/column/article/68963">《事务隔离：为什么你改了我还看不见？》</a>中的相关内容。</p>
</blockquote>
<p><strong>官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。</strong>而由于MVCC的支持，这个过程中数据是可以正常更新的。</p>
<p>你一定在疑惑，有了这个功能，为什么还需要FTWRL呢？<strong>一致性读是好，但前提是引擎要支持这个隔离级别。</strong>比如，对于<strong>MyISAM这种不支持事务的引擎（事务这一层面的东西是在存储引擎中实现的）</strong>，如果备份过程中有更新，<strong>总是只能取到最新的数据，那么就破坏了备份的一致性。</strong>这时，我们就需要使用FTWRL命令了。</p>
<p>所以，<strong>single-transaction方法只适用于所有的表使用事务引擎的库。</strong>如果有的表使用了<strong>不支持事务</strong>的引擎，那么备份就<strong>只能通过FTWRL方法</strong>。这往往是<strong>DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一</strong>。</p>
<p>你也许会问，<strong>既然要全库只读，为什么不使用set global readonly&#x3D;true的方式呢</strong>？确实readonly方式也可以让全库进入只读状态，<strong>但我还是会建议你用FTWRL方式</strong>，主要有两个原因：</p>
<ul>
<li><strong>一是，在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。</strong>因此，修改global变量的方式<strong>影响面更大</strong>，我不建议你使用。</li>
<li><strong>二是，在异常处理机制上有差异。</strong>如果执行FTWRL命令之后由于<strong>客户端发生异常断开</strong>，那么MySQL会<strong>自动释放这个全局锁</strong>，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，<strong>如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。</strong></li>
</ul>
<p><strong>业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。</strong></p>
<p>但是，<strong>即使没有被全局锁住</strong>，加字段也不是就能一帆风顺的，因为你还会碰到接下来我们要介绍的表级锁。</p>
<h3 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h3><p>MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。</p>
<p><strong>表锁的语法是 lock tables … read&#x2F;write。</strong>与FTWRL类似，可以用<strong>unlock tables主动释放锁</strong>，也可以在客户端断开的时候<strong>自动释放</strong>。需要注意，lock tables语法除了会限制别的线程的读写外，<strong>也限定了本线程接下来的操作对象</strong>。</p>
<p>举个例子, 如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。<strong>同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。（简单的说，你对于你自己锁住的资源要负责，不能在锁还没有释放的时候就访问其他的资源）</strong></p>
<p>在还没有出现更细粒度的锁的时候，<strong>表锁是最常用的处理并发的方式。而对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。</strong></p>
<p><strong>另一类表级的锁是MDL（metadata lock)。</strong>MDL<strong>不需要显式使用</strong>，在访问一个表的时候会<strong>被自动加上</strong>。MDL的作用是，<strong>保证读写的正确性</strong>。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。</p>
<p>因此，在MySQL 5.5版本中引入了MDL，<strong>当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。</strong></p>
<p>关于读锁和写锁，这里有比较详细的介绍</p>
<p><a href="https://blog.csdn.net/qq_44766883/article/details/105879308">mysql的锁机制(读锁，写锁，表锁，行锁，悲观锁，乐观锁，间隙锁)_mysql 读锁 写锁 兼容性-CSDN博客</a></p>
<ul>
<li>读锁之间不互斥，<strong>这意味着多个线程可以同时对同一张表进行读取操作，而不会相互阻塞。（对同一个数据，多个读操作可以同时进行，互不干扰。加锁的会话只能对此表进行读操作,其他会话也只能进行读操作。）</strong></li>
<li>读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。<strong>因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。（如果当前写操作没有完毕，则无法进行其他的读操作、写操作。当前会话只能对此表进行读,写操作,其他会话无法进行任何操作。）</strong></li>
</ul>
<p>虽然<strong>MDL锁是系统默认会加的，但却是你不能忽略的一个机制</strong>。比如下面这个例子，我经常看到有人掉到这个坑里：<strong>给一个小表加个字段，导致整个库挂了。</strong></p>
<p>你肯定知道，<strong>给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据</strong>。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。我们来看一下下面的操作序列，假设表t是一个小表。</p>
<blockquote>
<p>备注：这里的实验环境是MySQL 5.6。</p>
</blockquote>
<p><img src="/2024/09/10/MySQL06/image-20240910115825808.png" alt="image-20240910115825808"></p>
<p>我们可以看到session A先启动，<strong>这时候会对表t加一个MDL读锁</strong>。<strong>由于session B需要的也是MDL读锁，因此可以正常执行。</strong></p>
<p>之后session C会被blocked，是因为session A的MDL读锁还没有释放，<strong>而session C需要MDL写锁，因此只能被阻塞</strong>。</p>
<p>如果只有session C自己被阻塞还没什么关系，但是之后所有<strong>要在表t上新申请MDL读锁的请求也会被session C阻塞。前面我们说了，所有对表的增删改操作都需要先申请MDL写锁，如果session C自己阻塞了，就会导致后面的session都被锁住，等于这个表现在完全不可读写了。</strong></p>
<p>如果某个表上的<strong>查询语句频繁</strong>，而且客户端有重试机制，<strong>也就是说超时后会再起一个新session再请求的话，这个库的线程很快就会爆满。</strong></p>
<p>你现在应该知道了，事务中的MDL锁，<strong>在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。</strong></p>
<p>基于上面的分析，我们来讨论一个问题，<strong>如何安全地给小表加字段？</strong></p>
<p><strong>首先我们要解决长事务，事务不提交，就会一直占着MDL锁。</strong>在MySQL的information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。<strong>如果你要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务。</strong></p>
<p>但考虑一下这个场景。<strong>如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？</strong></p>
<p>这时候kill可能未必管用，<strong>因为新的请求马上就来了</strong>。比较理想的机制是，在alter table语句里面设定等待时间，<strong>如果在这个指定的等待时间里面能够拿到MDL写锁最好</strong>，拿不到也不要阻塞后面的业务语句，<strong>先放弃。之后开发人员或者DBA再通过重试命令重复这个过程</strong>。</p>
<p>MariaDB已经合并了<a href="https://github.com/alibaba/AliSQL">AliSQL</a>的这个功能，所以这两个开源分支目前都支持DDL NOWAIT&#x2F;WAIT n这个语法。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ALTER TABLE tbl_name NOWAIT add column ...</span><br><span class="line">ALTER TABLE tbl_name WAIT N add column ... </span><br></pre></td></tr></table></figure>

<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天介绍了MySQL的全局锁和表级锁。</p>
<p><strong>全局锁主要用在逻辑备份过程中。对于全部是InnoDB引擎的库，我建议你选择使用–single-transaction参数，对应用会更友好。</strong></p>
<p><strong>表锁一般是在数据库引擎不支持行锁的时候才会被用到的。</strong>如果你发现你的应用程序里有lock tables这样的语句，你需要追查一下，比较可能的情况是：</p>
<ul>
<li><strong>要么是你的系统现在还在用MyISAM这类不支持事务的引擎，那要安排升级换引擎；</strong></li>
<li>要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，<strong>最后业务开发就是把lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。</strong></li>
</ul>
<p><strong>MDL会直到事务提交才释放，在做表结构变更的时候，你一定要小心，不要锁住整张表导致线上查询和更新出现阻塞。</strong></p>
<p>备份一般都会在备库上执行，你在用–single-transaction方法做逻辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？</p>
<p>说明：这篇文章没有介绍到物理备份，<strong>物理备份会有一篇单独的文章</strong>。</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期的问题是关于对联合主键索引和InnoDB索引组织表的理解。</p>
<p><img src="/2024/09/10/MySQL06/image-20240910123652692.png" alt="image-20240910123652692"></p>
<p>主键 a，b的聚簇索引组织顺序相当于 order by a,b ，也就是先按a排序，再按b排序，c无序。</p>
<p>索引 ca 的组织是先按c排序，再按a排序，同时记录主键 </p>
<p><img src="/2024/09/10/MySQL06/image-20240910123710753.png" alt="image-20240910123710753"></p>
<p>这个跟索引c的数据是一模一样的。</p>
<p>索引 cb 的组织是先按c排序，在按b排序，同时记录主键 </p>
<p><img src="/2024/09/10/MySQL06/image-20240910123725156.png" alt="image-20240910123725156"></p>
<p><strong>所以，结论是ca可以去掉，cb需要保留。确实有道理，因为现在已经有了a索引，（a，b）索引，c索引，（c，a）索引，（c，b）索引了，（c，a）索引 &#x3D;&#x3D; c索引加上a索引，所以多余的这个ca可以去掉。</strong></p>
<p>MRR优化—这篇文章讲的还可以，建议看一下</p>
<p><a href="https://zhuanlan.zhihu.com/p/110154066">MySQL 的 MRR 到底是什么？ - 知乎 (zhihu.com)</a></p>
<p>把这篇文章的尾声搬了一下：</p>
<ul>
<li>MRR 跟索引有很大的关系。</li>
<li><strong>索引是 MySQL 对查询做的一个优化，把原本杂乱无章的数据，用有序的结构组织起来，让全表扫描变成有章可循的查询。</strong></li>
<li><strong>而我们讲的 MRR，则是 MySQL 对基于索引的查询做的一个的优化，可以说是对优化的优化了。</strong></li>
<li>要优化 MySQL 的查询，就得先知道 MySQL 的查询过程；而要优化索引的查询，则要知道 MySQL 索引的原理。</li>
</ul>
<p>文中的两个图画的很好</p>
<p><img src="/2024/09/10/MySQL06/image-20240910130042818.png" alt="image-20240910130042818"></p>
<p><img src="/2024/09/10/MySQL06/image-20240910130053955.png" alt="image-20240910130053955"></p>
<p>总结：</p>
<ul>
<li>MRR，全称「Multi-Range Read Optimization」。</li>
<li>简单说：<strong>MRR 通过把「随机磁盘读」，转化为「<a href="https://zhida.zhihu.com/search?q=%E9%A1%BA%E5%BA%8F%E7%A3%81%E7%9B%98&zhida_source=entity&is_preview=1">顺序磁盘</a>读」，从而提高了索引查询的性能。</strong></li>
</ul>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/06-quan-ju-suo-he-biao-suo-gei-biao-jia-ge-zi-duan-zen-mo-you-zhe-mo-duo-zu-ai">06 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>普通索引和唯一索引，应该怎么选择？</title>
    <url>/2024/09/10/MySQL09/</url>
    <content><![CDATA[<p>在前面的基础篇文章中，我给你介绍过索引的基本概念，相信你已经了解了<strong>唯一索引和普通索引</strong>的区别。今天我们就继续来谈谈，在不同的业务场景下，应该选择普通索引，还是唯一索引？</p>
<p>不太清楚唯一索引和普通索引的右转看这篇文章：</p>
<ul>
<li><a href="https://cloud.tencent.com/developer/article/1948984">MySQL的普通索引和唯一索引到底什么区别？-腾讯云开发者社区-腾讯云 (tencent.com)</a></li>
</ul>
<p>提取了一些重点出来：</p>
<p><strong>主键 V.S 唯一索引：</strong></p>
<p>主键保证DB的每一行都是<strong>唯一、不重复</strong>，比如身份证，学号等，不重复。 唯一索引的作用跟主键一样。 <strong>但在一张表里面只能有一个主键，不能为空，唯一索引可有多个。唯一索引可有一条记录为null。</strong></p>
<p>比如学生表：</p>
<ul>
<li>在学校，一般用学号做主键，身份证号作为唯一索引</li>
<li>在教育局，就把身份证号弄成主键，学号作为唯一索引</li>
</ul>
<p>所以选谁做主键，取决于业务需求。</p>
<p><strong>更新性能：</strong></p>
<p>往表中插入一个新记录(4,400)，InnoDB会有什么反应？</p>
<p>这要看该记录要更新的目标页是否在内存：</p>
<p>在内存：</p>
<ul>
<li>普通索引 找到3和5之间的位置，插入值，结束。 </li>
<li>唯一索引 找到3和5之间的位置，<code>判断到没有冲突</code>，插入值，结束。</li>
</ul>
<p>只是一个判断的差别，耗费微小CPU时间。</p>
<p>不在内存：</p>
<ul>
<li>唯一索引将数据页读入内存，判断到没有冲突，插入值，结束。 </li>
<li>普通索引将更新记录在change buffer，结束。</li>
</ul>
<p><strong>将数据从磁盘读入内存涉及随机I&#x2F;O访问，是DB里成本最高的操作之一。而change buffer可以减少随机磁盘访问，所以更新性能提升明显。</strong></p>
<p><strong>索引选择最佳实践：</strong></p>
<p><strong>普通索引、唯一索引在查询性能上无差别，主要考虑更新性能。所以，推荐尽量选择普通索引。</strong></p>
<p>若所有更新后面，都紧跟对该记录的查询，就该关闭change buffer。其它情况下，change buffer都能提升更新性能。 <strong>普通索引和change buffer的配合使用，对数据量大的表的更新优化还是明显的。</strong></p>
<p><strong>在使用机械硬盘时，change buffer收益也很大。</strong>所以，当你有“历史数据”库，且出于成本考虑用机械硬盘，应该关注这些表里的索引，尽量用普通索引，把change buffer开大，确保“历史数据”表的数据写性能。</p>
<p>建议看完这几篇文章之后再看下面的内容</p>
<ul>
<li><p><a href="https://www.cnblogs.com/mengxinJ/p/14211427.html">MySQL 中的WAL机制 - 萌新J - 博客园 (cnblogs.com)</a></p>
<p><strong>WAL，全称是Write-Ahead Logging， 预写日志系统。</strong>指的是 MySQL 的写操作<strong>并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上</strong>。这样的好处是错开高峰期。日志主要分为 <strong>undo log（MVCC）、redo log（防止写操作因为宕机而丢失）、binlog（写操作的备份，保证主从一致）</strong>。这三种在之前的博客已经详细说过了，作用分别是 “ <strong>完成MVCC从而实现 MySQL 的隔离级别</strong> “、” <strong>降低随机写的性能消耗（转成顺序写），同时防止写操作因为宕机而丢失</strong> “、” <strong>写操作的备份，保证主从一致</strong> “。关于这三种日志的内容讲的比较分散且具体的执行过程没有提到，所以这里来总结一下这三种日志。</p>
</li>
<li><p><a href="https://www.cnblogs.com/mengxinJ/p/14045520.html">一条 sql 的执行过程详解 - 萌新J - 博客园 (cnblogs.com)</a></p>
<p>这个可以搭配着45讲看</p>
</li>
</ul>
<p><strong>插入流程：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t(id,k)</span><br><span class="line">values (id1,k1),(id2,k2);</span><br></pre></td></tr></table></figure>

<p>假设当前k索引树的状态，查找到位置后：</p>
<ul>
<li>k1所在数据页在内存(buffer pool)</li>
<li>k2数据页不在内存</li>
</ul>
<p>看如下流程：</p>
<p><strong>带change buffer的更新流程</strong></p>
<blockquote>
<p> 图中箭头都是后台操作，不影响更新请求的响应。</p>
</blockquote>
<p><img src="/2024/09/10/MySQL09/image-20240910204423231.png" alt="image-20240910204423231"></p>
<p>该更新做了如下操作：</p>
<ol>
<li>Page1在内存，直接更新内存</li>
<li>Page2不在内存，<strong>就往change buffer区，缓存一个“往Page2插一行记录”的信息</strong></li>
<li><strong>将前两个动作记入redo log</strong></li>
</ol>
<p>至此，事务完成。<strong>执行该更新语句成本很低，只是写两处内存，然后写一处磁盘（前两次操作合在一起写了一次磁盘），还是顺序写。</strong></p>
<p><strong>处理之后的读请求：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t </span><br><span class="line">where k </span><br><span class="line">	in (k1, k2);</span><br></pre></td></tr></table></figure>

<p>读语句紧随更新语句之后，<strong>这时内存中的数据都还在，所以此时这俩读操作就与系统表空间和 redo log 无关。</strong></p>
<p><img src="/2024/09/10/MySQL09/image-20240910204622083.png" alt="image-20240910204622083"></p>
<p>读Page1时，<strong>直接从内存返回</strong>。</p>
<p>WAL之后若读数据，是否一定要读盘？一定要从redo log将数据更新后才能返回？ 其实不用。看上图状态，虽然磁盘上还是之前的数据，但这里直接从内存返回结果，结果是正确的。</p>
<p>读Page2时，需将Page2从磁盘读入内存，然后应用change buffer里的操作日志，生成一个正确版本并返回结果。<strong>所以一直到需要读Page2时，该数据页才会被从磁盘读入内存。</strong></p>
<p>综上，这俩机制的更新性能：</p>
<ul>
<li><strong>redo log 主要节省随机写磁盘的I&#x2F;O消耗（转成顺序写）</strong></li>
<li><strong>change buffer主要节省随机读磁盘的I&#x2F;O消耗</strong></li>
</ul>
<h5 id="带change-buffer的读过程"><a href="#带change-buffer的读过程" class="headerlink" title="带change buffer的读过程"></a>带change buffer的读过程</h5><p><strong>总结：</strong></p>
<p>因为<strong>唯一索引用不了change buffer</strong>，若业务可以接受，<strong>从性能角度，优先考虑非唯一索引</strong>。</p>
<p><strong>到底何时使用唯一索引：</strong></p>
<p>问题就在于“业务可能无法确保”，而本文前提是“业务代码已保证不会写入重复数据”，才讨论的性能问题。</p>
<ul>
<li><strong>若业务无法保证或业务就是要求<a href="https://cloud.tencent.com/solution/database?from_column=20065&from=20065">数据库</a>来做约束 没有撤退可言，必须创建唯一索引。</strong>那本文意义就在于，若碰上大量插入数据慢、内存命中率低时，多提供了一个排查思路</li>
<li><strong>“归档库”场景，可考虑使用唯一索引。</strong>比如线上数据只需保留半年，然后历史数据存在归档库。此时，归档数据已是确保没有唯一键冲突。要提高归档效率，可考虑把表的唯一索引改为普通索引。</li>
</ul>
<p>若某次写入使用了change buffer，之后主机异常重启，是否会丢失change buffer数据</p>
<p>不会！虽然是只更新内存，<strong>但在事务提交时，change buffer的操作也被记录到了redo log。所以崩溃恢复时，change buffer也能找回。</strong></p>
<p>接下来再看</p>
<p>假设你在维护一个市民系统，每个人都有一个唯一的身份证号，而且业务代码已经保证了不会写入两个重复的身份证号。<strong>如果市民系统需要按照身份证号查姓名，就会执行类似这样的SQL语句：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select name from CUser where id_card = &#x27;xxxxxxxyyyyyyzzzzz&#x27;;</span><br></pre></td></tr></table></figure>

<p><strong>所以，你一定会考虑在id_card字段上建索引。</strong></p>
<p>由于身份证号<strong>字段比较大</strong>，我<strong>不建议</strong>你把身份证号当做主键，那么现在你有两个选择，<strong>要么给id_card字段创建唯一索引，要么创建一个普通索引。如果业务代码已经保证了不会写入重复的身份证号，那么这两个选择逻辑上都是正确的。</strong></p>
<p>现在我要问你的是，<strong>从性能的角度考虑</strong>，你选择唯一索引还是普通索引呢？选择的依据是什么呢？</p>
<p>简单起见，我们还是用第4篇文章<a href="https://time.geekbang.org/column/article/69236">《深入浅出索引（上）》</a>中的例子来说明，假设字段 k 上的值都不重复。</p>
<p><img src="/2024/09/10/MySQL09/image-20240910210027396.png" alt="image-20240910210027396"></p>
<p>接下来，我们就从这两种索引对查询语句和更新语句的性能影响来进行分析。</p>
<h3 id="查询过程"><a href="#查询过程" class="headerlink" title="查询过程"></a>查询过程</h3><p>假设，执行查询的语句是 select id from T where k&#x3D;5。这个查询语句在索引树上查找的过程，<strong>先是通过B+树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。</strong></p>
<ul>
<li><strong>对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k&#x3D;5条件的记录。</strong></li>
<li><strong>对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。</strong></li>
</ul>
<p>那么，这个不同带来的性能差距会有多少呢？<strong>答案是，微乎其微。</strong></p>
<p><strong>你知道的，InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小默认是16KB。（这里就是想要说明InnoDB读数据不是一条一条读的，而是一页一页读的）</strong></p>
<p><strong>因为引擎是按页读写的，所以说，当找到k&#x3D;5的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。（简单的说，不是IO就行）</strong></p>
<p>当然，<strong>如果k&#x3D;5这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。</strong></p>
<p>但是，我们之前计算过，<strong>对于整型字段，一个数据页可以放近千个key，因此出现这种情况的概率会很低</strong>。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的CPU来说可以忽略不计。</p>
<h3 id="更新过程"><a href="#更新过程" class="headerlink" title="更新过程"></a>更新过程</h3><p>为了说明普通索引和唯一索引对更新语句性能的影响这个问题，我需要先跟你介绍一下change buffer。</p>
<p>当需要更新一个数据页时，<strong>如果数据页在内存中就直接更新</strong>，而如果这个数据页<strong>还没有在内存</strong>中的话，在不影响数据一致性的前提下，<strong>InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了（WAL）</strong>。在下次查询<strong>需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作</strong>。通过这种方式就能保证这个数据逻辑的正确性。</p>
<p>需要说明的是，<strong>虽然名字叫作change buffer，实际上它是可以持久化的数据</strong>。也就是说，<strong>change buffer在内存中有拷贝，也会被写入到磁盘上</strong>。</p>
<p><strong>将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge</strong>。除了访问这个数据页会触发merge外，<strong>系统有后台线程会定期merge</strong>。在数据库<strong>正常关闭（shutdown）的过程中，也会执行merge操作</strong>。</p>
<p>显然，如果能够将更新操作<strong>先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升</strong>。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。</p>
<p>那么，<strong>什么条件下可以使用change buffer呢？</strong></p>
<p>对于唯一索引来说，<strong>所有的更新操作都要先判断这个操作是否违反唯一性约束</strong>。比如，要插入(4,400)这个记录，<strong>就要先判断现在表中是否已经存在k&#x3D;4的记录，而这必须要将数据页读入内存才能判断</strong>。<strong>如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer了</strong>。</p>
<p>因此，唯一索引的更新就<strong>不能使用change buffer</strong>，实际上<strong>也只有普通索引可以使用</strong>。</p>
<p><strong>change buffer用的是buffer pool里的内存，因此不能无限增大</strong>。change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。</p>
<p>现在，你已经理解了change buffer的机制，那么我们再一起来看看<strong>如果要在这张表中插入一个新记录(4,400)的话，InnoDB的处理流程是怎样的。</strong></p>
<p>第一种情况是，<strong>这个记录要更新的目标页在内存中</strong>。这时，InnoDB的处理流程如下：</p>
<ul>
<li><strong>对于唯一索引来说，找到3和5之间的位置，判断到没有冲突，插入这个值，语句执行结束；</strong></li>
<li><strong>对于普通索引来说，找到3和5之间的位置，插入这个值，语句执行结束。</strong></li>
</ul>
<p>这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的CPU时间。</p>
<p>但，这不是我们关注的重点。</p>
<p>第二种情况是，<strong>这个记录要更新的目标页不在内存中</strong>。这时，InnoDB的处理流程如下：</p>
<ul>
<li>对于唯一索引来说，<strong>需要将数据页读入内存，判断到没有冲突</strong>，插入这个值，语句执行结束；</li>
<li>对于普通索引来说，<strong>则是将更新记录在change buffer</strong>，语句执行就结束了。（充分体现了WAL）</li>
</ul>
<p><strong>将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一</strong>。change buffer因为<strong>减少了随机磁盘访问，所以对更新性能的提升是会很明显的</strong>。</p>
<p>之前我就碰到过一件事儿，有个DBA的同学跟我反馈说，他负责的某个业务的库内存命中率突然从99%降低到了75%，整个系统处于阻塞状态，更新语句全部堵住。而探究其原因后，<strong>我发现这个业务有大量插入数据的操作，而他在前一天把其中的某个普通索引改成了唯一索引</strong>。</p>
<h3 id="change-buffer的使用场景"><a href="#change-buffer的使用场景" class="headerlink" title="change buffer的使用场景"></a>change buffer的使用场景</h3><p>通过上面的分析，你已经清楚了<strong>使用change buffer对更新过程的加速作用，也清楚了change buffer只限于用在普通索引的场景下，而不适用于唯一索引</strong>。那么，现在有一个问题就是：普通索引的所有场景，使用change buffer都可以起到加速作用吗？</p>
<p><strong>因为merge的时候是真正进行数据更新的时刻，而change buffer的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做merge之前，change buffer记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。</strong></p>
<p>因此，<strong>对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好</strong>。这种业务模型常见的就是<strong>账单类、日志类的系统</strong>。</p>
<p>反过来，假设一个业务的更新模式是<strong>写入之后马上会做查询</strong>，那么即使满足了条件，将更新先记录在change buffer，但之后由于<strong>马上要访问这个数据页</strong>，会<strong>立即触发merge过程</strong>。这样随机访问IO的次数<strong>不会减少</strong>，<strong>反而增加了change buffer的维护代价。所以，对于这种业务模式来说，change buffer反而起到了副作用</strong>。</p>
<h3 id="索引选择和实践"><a href="#索引选择和实践" class="headerlink" title="索引选择和实践"></a>索引选择和实践</h3><p>回到我们文章开头的问题，普通索引和唯一索引应该怎么选择。<strong>其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。</strong></p>
<p><strong>如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭change buffer。而在其他情况下，change buffer都能提升更新性能。</strong></p>
<p>在实际使用中，你会发现，<strong>普通索引和change buffer的配合使用，对于数据量大的表的更新优化还是很明显的</strong>。</p>
<p>特别地，在使用机械硬盘时，change buffer这个机制的收效是非常显著的。所以，当你有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘时，那你应该特别关注这些表里的索引，尽量使用普通索引，然后把change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。</p>
<h3 id="change-buffer-和-redo-log"><a href="#change-buffer-和-redo-log" class="headerlink" title="change buffer 和 redo log"></a>change buffer 和 redo log</h3><p>理解了change buffer的原理，你可能会联想到我在前面文章中和你介绍过的redo log和WAL。</p>
<p>在前面文章的评论中，我发现有同学混淆了redo log和change buffer。WAL 提升性能的核心机制，也的确是尽量减少随机读写，这两个概念确实容易混淆。所以，这里我把它们放到了同一个流程里来说明，便于你区分这两个概念。</p>
<blockquote>
<p>备注：这里，你可以再回顾下第2篇文章<a href="https://time.geekbang.org/column/article/68633">《日志系统：一条SQL更新语句是如何执行的？》</a>中的相关内容。</p>
</blockquote>
<p>现在，我们要在表上执行这个插入语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; insert into t(id,k) values(id1,k1),(id2,k2);</span><br></pre></td></tr></table></figure>

<p>这里，我们假设当前k索引树的状态，查找到位置后，k1所在的数据页在内存(InnoDB buffer pool)中，k2所在的数据页不在内存中。如图2所示是带change buffer的更新状态图。</p>
<p><img src="/2024/09/10/MySQL09/image-20240910210049413.png" alt="image-20240910210049413"></p>
<p>分析这条更新语句，你会发现它涉及了四个部分：内存、redo log（ib_log_fileX）、 数据表空间（t.ibd）、系统表空间（ibdata1）。</p>
<p>这条更新语句做了如下的操作（按照图中的数字顺序）：</p>
<ol>
<li><strong>Page 1在内存中，直接更新内存；</strong></li>
<li><strong>Page 2没有在内存中，就在内存的change buffer区域，记录下“我要往Page 2插入一行”这个信息</strong></li>
<li><strong>将上述两个动作记入redo log中（图中3和4）。</strong></li>
</ol>
<p>做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。</p>
<p>同时，图中的两个虚线箭头，是后台操作，不影响更新的响应时间。</p>
<p>那在这之后的读请求，要怎么处理呢？</p>
<p>比如，我们现在要执行 select * from t where k in (k1, k2)。这里，我画了这两个读请求的流程图。</p>
<p><strong>如果读语句发生在更新语句后不久，内存中的数据都还在，</strong>那么此时的这两个读操作就与系统表空间（ibdata1）和 redo log（ib_log_fileX）无关了。所以，我在图中就没画出这两部分。</p>
<p><img src="/2024/09/10/MySQL09/image-20240910210104418.png" alt="image-20240910210104418"></p>
<p>从图中可以看到：</p>
<ol>
<li><strong>读Page 1的时候，直接从内存返回</strong>。WAL之后如果读数据，是不是一定要读盘，<strong>是不是一定要从redo log里面把数据更新以后才可以返回</strong>？其实是不用的。你可以看一下图3的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。（简单的说，WAL就是延迟更新的，拖到没有办法了再更新）</li>
<li><strong>要读Page 2的时候，需要把Page 2从磁盘读入内存中，然后应用change buffer里面的操作日志，生成一个正确的版本并返回结果。</strong></li>
</ol>
<p>可以看到，直到需要读Page 2的时候，这个数据页才会被读入内存。</p>
<p>所以，如果要简单地对比这两个机制在提升更新性能上的收益的话，<strong>redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗。（一个优化了写磁盘，一个优化了读磁盘）</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天，我从普通索引和唯一索引的选择开始，和你分享了数据的查询和更新过程，然后说明了change buffer的机制以及应用场景，最后讲到了索引选择的实践。</p>
<p><strong>由于唯一索引用不上change buffer的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑非唯一索引。</strong></p>
<p>最后，又到了思考题时间。</p>
<p><img src="/2024/09/10/MySQL09/image-20240910211808617.png" alt="image-20240910211808617"></p>
<p>通过图2你可以看到，change buffer一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致change buffer丢失呢？change buffer丢失可不是小事儿，再从磁盘读入数据可就没有了merge过程，就等于是数据丢失了。会不会出现这种情况呢？</p>
<p><strong>补充：</strong> 大家对“是否使用唯一索引”有比较多的讨论，主要是纠结在“业务可能无法确保”的情况。这里，我再说明一下：</p>
<ul>
<li><strong>首先，业务正确性优先。</strong>咱们这篇文章的前提是“业务代码<strong>已经保证不会写入重复数据</strong>”的情况下，讨论性能问题。<strong>如果业务不能保证，或者业务就是要求数据库来做约束，那么没得选，必须创建唯一索引</strong>。这种情况下，本篇文章的意义在于，<strong>如果碰上了大量插入数据慢、内存命中率低的时候，可以给你多提供一个排查思路。</strong></li>
<li><strong>然后，在一些“归档库”的场景，你是可以考虑使用唯一索引的。</strong>比如，线上数据只需要保留半年，然后历史数据保存在归档库。这时候，归档数据已经是确保没有唯一键冲突了。<strong>要提高归档效率，可以考虑把表里面的唯一索引改成普通索引。</strong></li>
</ul>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期的问题是：如何构造一个“数据无法修改”的场景。</p>
<p><img src="/2024/09/10/MySQL09/image-20240910212018837.png" alt="image-20240910212018837"></p>
<p>这样，session A看到的就是我截图的效果了。</p>
<p>其实，还有另外一种场景</p>
<p><img src="/2024/09/10/MySQL09/image-20240910212034843.png" alt="image-20240910212034843"></p>
<p>这个操作序列跑出来，session A看的内容也是能够复现我截图的效果的。这个session B’启动的事务比A要早，其实是上期我们描述事务版本的可见性规则时留的彩蛋，因为规则里还有一个“活跃事务的判断”，我是准备留到这里再补充的。</p>
<p>当我试图在这里讲述完整规则的时候，发现第8篇文章<a href="https://time.geekbang.org/column/article/70562">《事务到底是隔离的还是不隔离的？》</a>中的解释引入了太多的概念，以致于分析起来非常复杂。</p>
<p>因此，我重写了第8篇，这样我们人工去判断可见性的时候，才会更方便。【看到这里，我建议你能够再重新打开第8篇文章并认真学习一次。】</p>
<p>用新的方式来分析session B’的更新为什么对session A不可见就是：在session A视图数组创建的瞬间，session B’是活跃的，属于“版本未提交，不可见”这种情况。</p>
<p>业务中如果要绕过这类问题，可以尝试使用“乐观锁”解决。</p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/09-pu-tong-suo-yin-he-wei-yi-suo-yin-ying-gai-zen-mo-xuan-ze">09 普通索引和唯一索引，应该怎么选择？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL为什么有时候会选错索引？</title>
    <url>/2024/09/10/MySQL10/</url>
    <content><![CDATA[<p>前面我们介绍过索引，<strong>你已经知道了在MySQL中一张表其实是可以支持多个索引的</strong>。但是，<strong>你写SQL语句的时候，并没有主动指定使用哪个索引。也就是说，使用哪个索引是由MySQL来确定的。</strong></p>
<p>不知道你有没有碰到过这种情况，一条本来可以执行得很快的语句，<strong>却由于MySQL选错了索引</strong>，而导致执行速度变得很慢？</p>
<p>我们一起来看一个例子吧。</p>
<p>我们先建一个简单的表，表里有a、b两个字段，并分别建上索引：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `t` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `a` int(11) DEFAULT NULL,</span><br><span class="line">  `b` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `a` (`a`),</span><br><span class="line">  KEY `b` (`b`)</span><br><span class="line">) ENGINE=InnoDB；</span><br></pre></td></tr></table></figure>

<p>然后，我们往表t中插入10万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到(100000,100000,100000)。</p>
<p>我是用<strong>存储过程</strong>来插入数据的，这里我贴出来方便你复现：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">delimiter ;;</span><br><span class="line">create procedure idata()</span><br><span class="line">begin</span><br><span class="line">  declare i int;</span><br><span class="line">  set i=1;</span><br><span class="line">  while(i&lt;=100000)do</span><br><span class="line">    insert into t values(i, i, i);</span><br><span class="line">    set i=i+1;</span><br><span class="line">  end while;</span><br><span class="line">end;;</span><br><span class="line">delimiter ;</span><br><span class="line">call idata();</span><br></pre></td></tr></table></figure>

<p>接下来，我们分析一条SQL语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t where a between 10000 and 20000;</span><br></pre></td></tr></table></figure>

<p>你一定会说，这个语句还用分析吗，很简单呀，a上有索引，肯定是要使用索引a的。</p>
<p>你说得没错，图1显示的就是使用explain命令看到的这条语句的执行情况。</p>
<p><img src="/2024/09/10/MySQL10/image-20240911073643013.png" alt="image-20240911073643013"></p>
<p>从图1看上去，这条查询语句的执行也确实符合预期，key这个字段值是’a’，表示优化器选择了索引a。</p>
<p>不过别急，这个案例不会这么简单。在我们已经准备好的包含了10万行数据的表上，我们再做如下操作。</p>
<p><img src="/2024/09/10/MySQL10/image-20240911073701695.png" alt="image-20240911073701695"></p>
<p>这里，session A的操作你已经很熟悉了，它就是<strong>开启了一个事务</strong>。随后，session B把数据都删除后，又调用了 idata这个存储过程，插入了10万行数据。</p>
<p>这时候，session B的查询语句select * from t where a between 10000 and 20000就<strong>不会再选择索引a了</strong>。我们可以通过慢查询日志（slow log）来查看一下具体的执行情况。</p>
<p>为了说明优化器选择的结果是否正确，我增加了一个对照，即：<strong>使用force index(a)来让优化器强制使用索引a（这部分内容，我还会在这篇文章的后半部分中提到）</strong>。</p>
<p>下面的三条SQL语句，就是这个实验过程。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set long_query_time=0;</span><br><span class="line">select * from t where a between 10000 and 20000; /*Q1*/</span><br><span class="line">select * from t force index(a) where a between 10000 and 20000;/*Q2*/</span><br></pre></td></tr></table></figure>

<ul>
<li>第一句，<strong>是将慢查询日志的阈值设置为0，表示这个线程接下来的语句都会被记录入慢查询日志中；</strong></li>
<li>第二句，Q1是session B原来的查询；</li>
<li>第三句，Q2是加了**force index(a)**来和session B原来的查询语句执行情况对比。</li>
</ul>
<p><img src="/2024/09/10/MySQL10/image-20240911073735882.png" alt="image-20240911073735882"></p>
<p>可以看到，<strong>Q1扫描了10万行，显然是走了全表扫描，执行时间是40毫秒。Q2扫描了10001行，执行了21毫秒。也就是说，我们在没有使用force index的时候，MySQL用错了索引，导致了更长的执行时间。</strong></p>
<p>这个例子对应的是我们<strong>平常不断地删除历史数据和新增数据的场景</strong>。这时，MySQL竟然会选错索引，是不是有点奇怪呢？今天，我们就从这个奇怪的结果说起吧。</p>
<h2 id="优化器的逻辑"><a href="#优化器的逻辑" class="headerlink" title="优化器的逻辑"></a>优化器的逻辑</h2><p>在第一篇文章中，我们就提到过，<strong>选择索引是优化器的工作</strong>。</p>
<p><strong>而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句</strong>。在数据库里面，<strong>扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的CPU资源越少</strong>。</p>
<p>当然，扫描行数并不是唯一的判断标准，<strong>优化器还会结合是否使用临时表、是否排序等因素进行综合判断</strong>。</p>
<p>我们这个简单的查询语句并没有涉及到临时表和排序，<strong>所以MySQL选错索引肯定是在判断扫描行数的时候出问题了</strong>。</p>
<p>那么，问题就是：<strong>扫描行数是怎么判断的？</strong></p>
<p>MySQL在真正开始执行语句之前，<strong>并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数</strong>。</p>
<p>这个<strong>统计信息就是索引的“区分度”</strong>。显然，一个索引上不同的值越多，这个索引的区分度就越好。<strong>而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好</strong>。</p>
<p>我们可以<strong>使用show index方法，看到一个索引的基数</strong>。如图4所示，就是表t的show index 的结果 。虽然这个表的每一行的三个字段值都是一样的，但是在统计信息中，这三个索引的基数值并不同，而且<strong>其实都不准确</strong>。</p>
<p><img src="/2024/09/10/MySQL10/image-20240911074038621.png" alt="image-20240911074038621"></p>
<p>那么，<strong>MySQL是怎样得到索引的基数的呢？</strong>这里，我给你简单介绍一下<strong>MySQL采样统计</strong>的方法。</p>
<p>为什么要采样统计呢？<strong>因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。</strong></p>
<p><strong>采样统计的时候，InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数</strong>。</p>
<p>而数据表是会<strong>持续更新</strong>的，索引统计信息也不会固定不变。所以，当变更的数据行数超过1&#x2F;M的时候，会<strong>自动触发重新做一次索引统计</strong>。</p>
<p>在MySQL中，有两种存储索引统计的方式，可以通过设置参数innodb_stats_persistent的值来选择：</p>
<ul>
<li>设置为on的时候，<strong>表示统计信息会持久化存储。</strong>这时，默认的N是20，M是10。</li>
<li>设置为off的时候，<strong>表示统计信息只存储在内存中。</strong>这时，默认的N是8，M是16。</li>
</ul>
<p>由于是采样统计，<strong>所以不管N是20还是8，这个基数都是很容易不准的。</strong></p>
<p>但，这还不是全部。</p>
<p>你可以从上图中看到，这次的索引统计值（cardinality列）虽然不够精确，但大体上还是差不多的，选错索引一定还有别的原因。</p>
<p>其实索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要扫描多少行。</p>
<p>接下来，我们再一起看看优化器预估的，这两个语句的扫描行数是多少。</p>
<p><img src="/2024/09/10/MySQL10/image-20240911074053831.png" alt="image-20240911074053831"></p>
<p>rows这个字段表示的是预计扫描行数。</p>
<p>其中，Q1的结果还是符合预期的，rows的值是104620；但是Q2的rows值是37116，偏差就大了。<strong>而图1中我们用explain命令看到的rows是只有10001行，是这个偏差误导了优化器的判断。</strong></p>
<p>到这里，<strong>可能你的第一个疑问不是为什么不准，而是优化器为什么放着扫描37000行的执行计划不用，却选择了扫描行数是100000的执行计划呢</strong>？</p>
<p>这是因为，<strong>如果使用索引a，每次从索引a上拿到一个值，都要回到主键索引上查出整行数据，这个代价优化器也要算进去的</strong>。</p>
<p>而如果选择扫描10万行，<strong>是直接在主键索引上扫描的，没有额外的代价</strong>。</p>
<p>优化器会<strong>估算这两个选择的代价，从结果看来，优化器认为直接扫描主键索引更快</strong>。当然，从执行时间看来，这个选择并不是最优的。</p>
<p>使用普通索引<strong>需要把回表的代价算进去，在图1执行explain的时候，也考虑了这个策略的代价 ，但图1的选择是对的</strong>。也就是说，这个策略并没有问题。</p>
<p>所以冤有头债有主，MySQL选错索引，<strong>这件事儿还得归咎到没能准确地判断出扫描行数。至于为什么会得到错误的扫描行数</strong>，这个原因就作为课后问题，留给你去分析了。</p>
<p>既然是统计信息不对，那就修正。<strong>analyze table t 命令，可以用来重新统计索引信息</strong>。我们来看一下执行效果。</p>
<p><img src="/2024/09/10/MySQL10/image-20240911074130011.png" alt="image-20240911074130011"></p>
<p>这回对了。</p>
<p><strong>所以在实践中，如果你发现explain的结果预估的rows值跟实际情况差距比较大，可以采用这个方法来处理。</strong></p>
<p>其实，如果只是索引统计不准确，<strong>通过analyze命令可以解决很多问题，但是前面我们说了，优化器可不止是看扫描行数。</strong></p>
<p>依然是基于这个表t，我们看看另外一个语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t where (a between 1 and 1000)  and (b between 50000 and 100000) order by b limit 1;</span><br></pre></td></tr></table></figure>

<p>从条件上看，这个查询没有符合条件的记录，因此会返回空集合。</p>
<p>在开始执行这条语句之前，你可以先设想一下，如果你来选择索引，会选择哪一个呢？</p>
<p>为了便于分析，我们先来看一下a、b这两个索引的结构图。</p>
<p><img src="/2024/09/10/MySQL10/image-20240911074144966.png" alt="image-20240911074144966"></p>
<p>如果使用索引a进行查询，<strong>那么就是扫描索引a的前1000个值，然后取到对应的id，再到主键索引上去查出每一行，然后根据字段b来过滤。显然这样需要扫描1000行。</strong></p>
<p>如果使用索引b进行查询，<strong>那么就是扫描索引b的最后50001个值，与上面的执行过程相同，也是需要回到主键索引上取值再判断，所以需要扫描50001行。</strong></p>
<p>所以你一定会想，如果使用索引a的话，执行速度明显会快很多。那么，下面我们就来看看到底是不是这么一回事儿。</p>
<p>图8是执行explain的结果。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; explain select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1;</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/10/MySQL10/image-20240911074207327.png" alt="image-20240911074207327"></p>
<p>可以看到，返回结果中key字段显示，这次优化器选择了索引b，而rows字段显示需要扫描的行数是50198。</p>
<p><strong>从这个结果中，你可以得到两个结论：</strong></p>
<ol>
<li><strong>扫描行数的估计值依然不准确；</strong></li>
<li><strong>这个例子里MySQL又选错了索引。</strong></li>
</ol>
<h3 id="索引选择异常和处理"><a href="#索引选择异常和处理" class="headerlink" title="索引选择异常和处理"></a>索引选择异常和处理</h3><p>其实<strong>大多数时候优化器都能找到正确的索引，但偶尔你还是会碰到我们上面举例的这两种情况</strong>：原本可以执行得很快的SQL语句，执行速度却比你预期的慢很多，你应该怎么办呢？</p>
<p><strong>一种方法是，像我们第一个例子一样，采用force index强行选择一个索引。</strong>MySQL会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。<strong>如果force index指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。</strong></p>
<p>我们来看看第二个例子。刚开始分析时，我们认为选择索引a会更好。现在，我们就来看看执行效果：</p>
<p><img src="/2024/09/10/MySQL10/image-20240911075335025.png" alt="image-20240911075335025"></p>
<p>可以看到，原本语句需要执行2.23秒，而当你使用force index(a)的时候，只用了0.05秒，<strong>比优化器的选择快了40多倍。</strong></p>
<p><strong>也就是说，优化器没有选择正确的索引，force index起到了“矫正”的作用。</strong></p>
<p>不过很多程序员不喜欢使用force index，<strong>一来这么写不优美</strong>，二来<strong>如果索引改了名字，这个语句也得改</strong>，显得很麻烦。而且如果以后<strong>迁移到别的数据库的话，这个语法还可能会不兼容</strong>。</p>
<p>但其实使用force index最主要的问题还是变更的及时性。<strong>因为选错索引的情况还是比较少出现的，所以开发的时候通常不会先写上force index。而是等到线上出现问题的时候，你才会再去修改SQL语句、加上force index。</strong>但是修改之后还要测试和发布，<strong>对于生产系统来说，这个过程不够敏捷</strong>。</p>
<p>所以，数据库的问题最好还是在数据库内部来解决。那么，在数据库里面该怎样解决呢？</p>
<p>既然优化器放弃了使用索引a，说明a还不够合适，所以<strong>第二种方法就是，我们可以考虑修改语句，引导MySQL使用我们期望的索引。</strong>比如，在这个例子里，<strong>显然把“order by b limit 1” 改成 “order by b,a limit 1” ，语义的逻辑是相同的</strong>。</p>
<p><img src="/2024/09/10/MySQL10/image-20240911075503759.png" alt="image-20240911075503759"></p>
<p>之前优化器选择使用索引b，<strong>是因为它认为使用索引b可以避免排序（b本身是索引，已经是有序的了，如果选择索引b的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。</strong></p>
<p>现在order by b,a 这种写法，要求按照b,a排序，就意味着使用这两个索引都需要排序。<strong>因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描1000行的索引a。</strong></p>
<p>当然，这种修改并不是通用的优化手段，只是刚好在这个语句里面有limit 1，<strong>因此如果有满足条件的记录， order by b limit 1和order by b,a limit 1 都会返回b是最小的那一行，逻辑上一致，才可以这么做。</strong></p>
<p>如果你觉得修改语义这件事儿不太好，这里还有一种改法，图11是执行效果。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from  (select * from t where (a between 1 and 1000)  and (b between 50000 and 100000) order by b limit 100)alias limit 1;</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/10/MySQL10/image-20240911075527162.png" alt="image-20240911075527162"></p>
<p>在这个例子里，<strong>我们用limit 100让优化器意识到，使用b索引代价是很高的。其实是我们根据数据特征诱导了一下优化器，也不具备通用性。</strong></p>
<p><strong>第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。</strong></p>
<p>不过，在这个例子中，<strong>我没有找到通过新增索引来改变优化器行为的方法。这种情况其实比较少，尤其是经过DBA索引优化过的库，再碰到这个bug，找到一个更合适的索引一般比较难。</strong></p>
<p><strong>如果我说还有一个方法是删掉索引b，你可能会觉得好笑。</strong>但实际上我碰到过两次这样的例子，最终是DBA跟业务开发沟通后，<strong>发现这个优化器错误选择的索引其实根本没有必要存在，于是就删掉了这个索引，优化器也就重新选择到了正确的索引。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天我们一起聊了聊<strong>索引统计的更新机制，并提到了优化器存在选错索引的可能性</strong>。</p>
<p>对于<strong>由于索引统计信息不准确导致的问题，你可以用analyze table来解决</strong>。</p>
<p>而对于其他优化器误判的情况：</p>
<ul>
<li><strong>你可以在应用端用force index来强行指定索引</strong></li>
<li><strong>也可以通过修改语句来引导优化器</strong></li>
<li><strong>还可以通过增加或者删除索引来绕过这个问题</strong></li>
</ul>
<p>你可能会说，今天这篇文章后面的几个例子，怎么都没有展开说明其原理。我要告诉你的是，<strong>今天的话题，我们面对的是MySQL优化器的bug，每一个展开都必须深入到一行行代码去量化，实在不是我们在这里应该做的事情</strong>。</p>
<p>所以，我把我用过的解决方法跟你分享，希望你在碰到类似情况的时候，能够有一些思路。</p>
<p>前面我们在构造第一个例子的过程中，通过session A的配合，让session B删除数据后又重新插入了一遍数据，然后就发现explain结果中，rows字段从10001变成37000多。</p>
<p>而如果没有session A的配合，只是单独执行delete from t 、call idata()、explain这三句话，会看到rows字段其实还是10000左右。你可以自己验证一下这个结果。</p>
<p>这是什么原因呢？也请你分析一下吧。</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我在上一篇文章最后留给你的问题是，如果某次写入使用了change buffer机制，之后主机异常重启，是否会丢失change buffer和数据。</p>
<p>这个问题的答案是<strong>不会丢失</strong>，虽然是只更新内存，但是在事务提交的时候，<strong>我们把change buffer的操作也记录到redo log里了，所以崩溃恢复的时候，change buffer也能找回来</strong>。</p>
<p>merge的过程是否会把数据直接写回磁盘，这是个好问题。这里，我再为你分析一下。</p>
<p>merge的执行流程是这样的：</p>
<ol>
<li><strong>从磁盘读入数据页到内存（老版本的数据页）；</strong></li>
<li><strong>从change buffer里找出这个数据页的change buffer 记录(可能有多个），依次应用，得到新版数据页；</strong></li>
<li><strong>写redo log。这个redo log包含了数据的变更和change buffer的变更。</strong></li>
</ol>
<p>到这里merge过程就结束了。<strong>这时候，数据页和内存中change buffer对应的磁盘位置都还没有修改，属于脏页，之后各自刷回自己的物理数据，就是另外一个过程了。</strong></p>
<p><strong>merge 操作</strong>通常在<strong>数据操作、读取数据页、后台处理、Buffer Pool 管理、系统事件和显式刷新操作</strong>等情况下触发。主要是<strong>处理 Change Buffer 中的变更记录，并将这些变更合并到相应的数据页</strong>中。</p>
<p>合并到相应数据页的过程实际上是将<strong>变更应用到内存中的数据页，但在这个阶段，数据页本身并不会立即更新到磁盘，而是保持在内存中，处于脏页的状态，实际的磁盘更新会延迟进行</strong>。</p>
<p><strong>Checkpoint机制</strong>，具体可以看看这篇文章</p>
<ul>
<li><a href="https://www.cnblogs.com/chenpingzhao/p/5107480.html">【mysql】关于checkpoint机制 - 踏雪无痕SS - 博客园 (cnblogs.com)</a></li>
</ul>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/10-mysql-wei-shi-mo-you-shi-hou-hui-xuan-cuo-suo-yin">10 MySQL为什么有时候会选错索引？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>行锁功过：怎么减少行锁对性能的影响？</title>
    <url>/2024/09/10/MySQL07/</url>
    <content><![CDATA[<p>在上一篇文章中，<strong>介绍了MySQL的全局锁和表级锁，今天我们就来讲讲MySQL的行锁。</strong></p>
<p><strong>MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。</strong>不支持行锁<strong>意味着并发控制只能使用表锁</strong>，对于这种引擎的表，<strong>同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。</strong></p>
<p>我们今天就主要来聊聊InnoDB的行锁，以及如何通过减少锁冲突来提升业务并发度。</p>
<p>顾名思义，<strong>行锁就是针对数据表中行记录的锁。这很好理解，比如事务A更新了一行，而这时候事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。</strong></p>
<p>当然，数据库中<strong>还有一些没那么一目了然的概念和设计，这些概念如果理解和使用不当，容易导致程序出现非预期行为，比如两阶段锁。</strong></p>
<h3 id="从两阶段锁说起"><a href="#从两阶段锁说起" class="headerlink" title="从两阶段锁说起"></a>从两阶段锁说起</h3><p>我先给你举个例子。在下面的操作序列中，事务B的update语句执行时会是什么现象呢？假设字段id是表t的主键。</p>
<p><img src="/2024/09/10/MySQL07/image-20240910130922763.png" alt="image-20240910130922763"></p>
<p>这个问题的结论取决于事务A在执行完两条update语句后，持有哪些锁，以及在什么时候释放。<strong>你可以验证一下：实际上事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。（这个我在最初学习MySQL的时候就试验过了）</strong></p>
<p>知道了这个答案，你一定知道了事务A持有的两个记录的行锁，<strong>都是在commit的时候才释放的</strong>。</p>
<p>也就是说，<strong>在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。</strong></p>
<p>知道了这个设定，对我们使用事务有什么帮助呢？那就是，<strong>如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。</strong>我给你举个例子。</p>
<p>假设你负责实现一个电影票在线交易业务，顾客A要在影院B购买电影票。我们简化一点，这个业务需要涉及到以下操作：</p>
<ol>
<li>从顾客A账户余额中扣除电影票价；</li>
<li>给影院B的账户余额增加这张电影票价；</li>
<li>记录一条交易日志。</li>
</ol>
<p><strong>也就是说，要完成这个交易，我们需要update两条记录，并insert一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。（我们在Spring程序中直接一个@Transactional注解就行了）</strong>那么，你会怎样安排这三个语句在事务中的顺序呢？</p>
<p>试想如果同时有另外一个顾客C要在影院B买票，那么这两个事务冲突的部分就是语句2了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。</p>
<p>根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。<strong>所以，如果你把语句2安排在最后，比如按照3、1、2这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。</strong></p>
<p><strong>好了，现在由于你的正确设计，影院余额这一行的行锁在一个事务中不会停留很长时间。</strong>但是，这并没有完全解决你的困扰。</p>
<p>如果这个影院做活动，可以低价预售一年内所有的电影票，而且这个活动只做一天。于是在活动时间开始的时候，你的MySQL就挂了。<strong>你登上服务器一看，CPU消耗接近100%，但整个数据库每秒就执行不到100个事务。</strong>这是什么原因呢？</p>
<p>这里，我就要说到<strong>死锁和死锁检测了</strong>。</p>
<h3 id="死锁和死锁检测"><a href="#死锁和死锁检测" class="headerlink" title="死锁和死锁检测"></a>死锁和死锁检测</h3><p><strong>当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。</strong>这里我用数据库中的行锁举个例子。</p>
<p><img src="/2024/09/10/MySQL07/image-20240910131442146.png" alt="image-20240910131442146"></p>
<p><strong>这时候，事务A在等待事务B释放id&#x3D;2的行锁，而事务B在等待事务A释放id&#x3D;1的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了死锁状态。</strong>当出现死锁以后，有两种策略：</p>
<ul>
<li><strong>一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。</strong></li>
<li>另一种策略是，<strong>发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。</strong></li>
</ul>
<p>在InnoDB中，innodb_lock_wait_timeout的<strong>默认值是50s</strong>，意味着如果采用第一个策略，当出现死锁以后，<strong>第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。</strong></p>
<p>但是，我们又不可能直接把这个时间设置成一个很小的值，比如1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。</p>
<p>所以，<strong>正常情况下我们还是要采用第二种策略</strong>，即：<strong>主动死锁检测，而且innodb_deadlock_detect的默认值本身就是on</strong>。主动死锁检测在发生死锁的时候，<strong>是能够快速发现并进行处理的</strong>，但是它也是有额外负担的。</p>
<p>你可以想象一下这个过程：<strong>每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁</strong>。</p>
<p>那如果是我们上面说到的所有事务都要更新同一行的场景呢？</p>
<p>每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，<strong>这是一个时间复杂度是O(n^2)的操作（1 + 2 + 3 + …… + n）</strong>。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。<strong>虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到CPU利用率很高，但是每秒却执行不了几个事务。</strong></p>
<p>根据上面的分析，我们来讨论一下，怎么解决由这种<strong>热点行更新</strong>导致的性能问题呢？<strong>问题的症结在于，死锁检测要耗费大量的CPU资源。</strong></p>
<p><strong>一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。</strong>但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。<strong>而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。</strong></p>
<p><strong>另一个思路是控制并发度。</strong>根据上面的分析，你会发现如果并发能够控制住，<strong>比如同一行同时最多只有10个线程在更新，那么死锁检测的成本很低</strong>，就不会出现这个问题。一个直接的想法就是，<strong>在客户端做并发控制</strong>。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有600个客户端，<strong>这样即使每个客户端控制到只有5个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到3000。</strong></p>
<p>因此，<strong>这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改MySQL源码的人，也可以做在MySQL里面</strong>。基本思路就是，<strong>对于相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。</strong></p>
<p>可能你会问，<strong>如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？</strong></p>
<p><strong>你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。</strong>还是以影院账户为例，可以考虑放在多条记录上，<strong>比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1&#x2F;10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗。（原来有时候这样设计不是屎山啊，这是为了避免并发死锁）</strong></p>
<p>这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做<strong>详细设计</strong>。<strong>如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成0的时候，代码要有特殊处理。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天介绍了<strong>MySQL的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。</strong></p>
<p>这里<strong>两阶段加锁（2PL(Two-phase locking)）协议</strong>可以看看这几篇文章：</p>
<ul>
<li><p>[关于MySQL 两阶段加锁（2PL(Two-phase locking)）协议 - _Origin - 博客园 (cnblogs.com)](<a href="https://www.cnblogs.com/origin-zy/p/17395701.html#:~:text=%E5%9C%A8MySQL%E4%B8%AD%EF%BC%8C%E4%B8%A4%E9%98%B6%E6%AE%B5%E9%94%81%E5%8D%8F%E8%AE%AE%E7%9A%84%E5%90%AB%E4%B9%89%E6%98%AF%EF%BC%9A%E5%BD%93%E4%B8%80%E4%B8%AA%E4%BA%8B%E5%8A%A1%E8%8E%B7%E5%8F%96%E5%88%B0%E4%BA%86%E6%9F%90%E4%B8%80%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%94%81%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%B9%B6%E4%B8%8D%E6%98%AF%E5%BD%93%E5%89%8D%E4%BA%8B%E5%8A%A1%E4%B8%8D%E9%9C%80%E8%A6%81%E6%93%8D%E4%BD%9C%E5%AE%83%E4%BA%86%E4%B9%8B%E5%90%8E%EF%BC%8C%E8%BF%99%E4%B8%AA%E8%AF%B4%E5%B0%B1%E4%BC%9A%E9%A9%AC%E4%B8%8A%E9%87%8A%E6%94%BE%E6%8E%89%EF%BC%8C%E8%BF%99%E4%B8%AA%E9%94%81%E4%BC%9A%E4%B8%80%E7%9B%B4%E8%A2%AB%E8%BF%99%E4%B8%AA%E4%BA%8B%E5%8A%A1%E6%8C%81%E6%9C%89%EF%BC%8C%E7%9B%B4%E5%88%B0%E8%BF%99%E4%B8%AA%E4%BA%8B%E5%8A%A1%E8%A2%AB%E6%8F%90%E4%BA%A4%E6%88%96%E5%9B%9E%E6%BB%9A%E5%90%8E%EF%BC%8C%E8%BF%99%E4%B8%AA%E9%94%81%E6%89%8D%E4%BC%9A%E8%A2%AB%E9%87%8A%E6%94%BE%E6%8E%89%E3%80%82">https://www.cnblogs.com/origin-zy/p/17395701.html#:~:text=在MySQL中，两阶段锁协议的含义是：当一个事务获取到了某一个数据库对象的锁之后，并不是当前事务不需要操作它了之后，这个说就会马上释放掉，这个锁会一直被这个事务持有，直到这个事务被提交或回滚后，这个锁才会被释放掉。</a> 所以，在当前事务还没有结束的时候，任何其他事务尝试获取这个锁的时候，都会被阻塞。,知道当前事务提交或回滚后，前提事务才可以获取到这把锁。 这就是MySQL中2PL两阶段锁协议的含义。 它在事务并发的时候，为数据的一致性提供有力的保障。)</p>
<p>两阶段锁协议的含义是：<strong>当一个事务获取到了某一个数据库对象的锁之后，并不是当前事务不需要操作它了之后，这个说就会马上释放掉，这个锁会一直被这个事务持有，直到这个事务被提交或回滚后，这个锁才会被释放掉。</strong>所以，在当前事务还没有结束的时候，任何其他事务尝试获取这个锁的时候，都会被阻塞。知道当前事务提交或回滚后，前提事务才可以获取到这把锁。</p>
</li>
<li><p><a href="https://blog.csdn.net/xmtblog/article/details/104980789">8 张图，给你完整的剖析 MySQL 两阶段加锁（2PL）协议-CSDN博客</a></p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">两阶段加锁（2PL，Two-Phase Locking）协议是数据库管理系统中用于实现事务隔离的一种协议。它分为两个阶段：</span><br><span class="line"></span><br><span class="line">1. 扩展阶段（Growing Phase）</span><br><span class="line">在这个阶段，事务可以申请锁（包括读锁和写锁），并且可以不断地增加所持有的锁。</span><br><span class="line">事务在扩展阶段内，可以获得新的锁，但不能释放任何已持有的锁。</span><br><span class="line">这个阶段持续到事务第一次释放锁为止。</span><br><span class="line">2. 收缩阶段（Shrinking Phase）</span><br><span class="line">一旦事务释放了第一个锁，就进入收缩阶段。</span><br><span class="line">在这个阶段，事务不能再申请新的锁，只能释放已持有的锁。</span><br><span class="line">事务可以在此阶段释放任何锁，但不能再获取新的锁。</span><br><span class="line"></span><br><span class="line">扩展阶段允许事务收集必要的锁，而收缩阶段则确保锁的释放顺序，从而避免死锁和其他并发问题。两阶段加锁协议是确保数据库事务满足串行化的关键机制之一。</span><br></pre></td></tr></table></figure>

<p>其中，<strong>我以两阶段协议为起点，和你一起讨论了在开发的时候如何安排正确的事务语句</strong>。这里的原则&#x2F;我给你的建议是：<strong>如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放。</strong></p>
<p>但是，调整语句顺序<strong>并不能完全避免死锁</strong>。所以我们引入了死锁和死锁检测的概念，以及提供了<strong>三个方案</strong>，来减少死锁对数据库的影响。<strong>减少死锁的主要方向，就是控制访问相同资源的并发事务量。</strong></p>
<p>如果你要删除一个表里面的前10000行数据，有以下三种方法可以做到：</p>
<ul>
<li>第一种，直接执行delete from T limit 10000;</li>
<li>第二种，在一个连接中循环执行20次 delete from T limit 500;</li>
<li>第三种，在20个连接中同时执行delete from T limit 500。</li>
</ul>
<p>你会选择哪一种方法呢？为什么呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期我给你留的问题是：当备库用–single-transaction做逻辑备份的时候，如果从主库的binlog传来一个DDL语句会怎么样？</p>
<p>假设这个DDL是针对表t1的， 这里我把备份过程中几个关键的语句列出来：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;</span><br><span class="line">Q2:START TRANSACTION  WITH CONSISTENT SNAPSHOT；</span><br><span class="line">/* other tables */</span><br><span class="line">Q3:SAVEPOINT sp;</span><br><span class="line">/* 时刻 1 */</span><br><span class="line">Q4:show create table `t1`;</span><br><span class="line">/* 时刻 2 */</span><br><span class="line">Q5:SELECT * FROM `t1`;</span><br><span class="line">/* 时刻 3 */</span><br><span class="line">Q6:ROLLBACK TO SAVEPOINT sp;</span><br><span class="line">/* 时刻 4 */</span><br><span class="line">/* other tables */</span><br></pre></td></tr></table></figure>

<p>在<strong>备份开始</strong>的时候，为了<strong>确保</strong>RR（可重复读）隔离级别，再设置一次RR隔离级别(Q1);</p>
<p><strong>启动事务</strong>，这里用 WITH CONSISTENT SNAPSHOT<strong>确保这个语句执行完就可以得到一个一致性视图（Q2)；</strong></p>
<p><strong>设置一个保存点</strong>，这个很重要（Q3）；</p>
<p>这里介绍一下<strong>保存点（SAVEPOINT）</strong>的概念:</p>
<ul>
<li><strong>保存点的定义</strong>: 保存点是事务中的一个标记，你可以在事务的某个时刻设置一个保存点，<strong>之后可以选择回滚到这个点，而不是回滚整个事务。</strong></li>
</ul>
<p>详细的可以看看这篇文章:</p>
<ul>
<li><p><a href="https://blog.csdn.net/hammring/article/details/106958696">事务之保存点（savepoint）-CSDN博客</a></p>
<p>下面将重点提取出来:</p>
<p>如果开启一个事务，已经写了很多语句，突然发现执行完上一语句时发现语句写错了。</p>
<p>你只好使用rollback语句让数据库恢复到事务开启之前的状态。以前写完的语句也需要重新执行。</p>
<p>所以mysql数据库的作者<strong>提出了保存点（savepoint）的概念</strong>。<strong>开启事务后在执行语句后面打几个点，我们调用rollback语句就会回滚到指定的点。而不是回到事务执行之前的样子。</strong></p>
</li>
</ul>
<p>这里还介绍一下show create table是什么意思,具体可以看看这篇文章:</p>
<ul>
<li><a href="https://deepinout.com/mysql/mysql-questions/201_tk_1706620105.html">mysql show create table命令的详解|极客笔记 (deepinout.com)</a></li>
</ul>
<p>show create 是为了<strong>拿到表结构</strong>(Q4)，然后正式导数据 （Q5），<strong>回滚到SAVEPOINT sp，在这里的作用是释放 t1的MDL锁 （Q6。当然这部分属于“超纲”，上文正文里面都没提到。）</strong></p>
<p>DDL从主库传过来的时间按照<strong>效果不同</strong>，我打了四个时刻。题目<strong>设定为小表</strong>，我们假定到达后，如果开始执行，则很快能够执行完成。</p>
<p>参考答案如下：</p>
<ol>
<li>如果在Q4语句(Q4:show create table <code>t1</code>;)执行<strong>之前</strong>（时刻 1前）到达，现象：<strong>没有影响，备份拿到的是DDL后的表结构</strong>。（在开始备份之前就同步好了，没有任何问题,这个时候binlog已经同步好了）</li>
<li>如果在“时刻 2”(Q5:SELECT * FROM <code>t1</code>;)到达，<strong>则表结构被改过</strong>，Q5执行的时候，报 <strong>Table definition has changed, please retry transaction</strong>，现象：mysqldump终止；（正式导数据的时候发现表结构定义被修改，中止备份操作）</li>
<li>如果在“时刻2”和“时刻3”(Q6:ROLLBACK TO SAVEPOINT sp;)之间到达，mysqldump占着t1的MDL读锁，<strong>binlog被阻塞</strong>，<strong>现象：主从延迟，直到Q6执行完成。</strong>（导出数据完成,但是还没有提交,此时binlog会被阻塞）</li>
<li>从“时刻4”(&#x2F;* other tables *&#x2F;)开始，<strong>mysqldump释放了MDL读锁，现象：没有影响，备份拿到的是DDL前的表结构。</strong>（已经备份完了，这时候的修改不会影响到之前的备份）</li>
</ol>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/07-hang-suo-gong-guo-zen-mo-jian-shao-hang-suo-dui-xing-neng-de-ying-xiang">07 行锁功过：怎么减少行锁对性能的影响？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么给字符串字段加索引？</title>
    <url>/2024/09/11/MySQL11/</url>
    <content><![CDATA[<p>现在，几乎所有的系统都支持邮箱登录，<strong>如何在邮箱这样的字段上建立合理的索引</strong>，是我们今天要讨论的问题。</p>
<p>假设，你现在<strong>维护一个支持邮箱登录的系统</strong>，用户表是这么定义的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; create table SUser(</span><br><span class="line">ID bigint unsigned primary key,</span><br><span class="line">email varchar(64), </span><br><span class="line">... </span><br><span class="line">)engine=innodb; </span><br></pre></td></tr></table></figure>

<p><strong>由于要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select f1, f2 from SUser where email=&#x27;xxx&#x27;;</span><br></pre></td></tr></table></figure>

<p>从第4和第5篇讲解索引的文章中，<strong>我们可以知道，如果email这个字段上没有索引，那么这个语句就只能做全表扫描</strong>。</p>
<p>同时，<strong>MySQL是支持前缀索引的</strong>，也就是说，<strong>你可以定义字符串的一部分作为索引。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。</strong></p>
<p>比如，这两个在email字段上创建索引的语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; alter table SUser add index index1(email);</span><br><span class="line">或</span><br><span class="line">mysql&gt; alter table SUser add index index2(email(6));</span><br></pre></td></tr></table></figure>

<p>第一个语句创建的index1索引里面，包含了<strong>每个记录的整个字符串</strong>；而第二个语句创建的index2索引里面，<strong>对于每个记录都是只取前6个字节</strong>。</p>
<p>那么，这两种不同的定义在数据结构和存储上有什么区别呢？如图2和3所示，就是这两个索引的示意图。</p>
<p><img src="/2024/09/11/MySQL11/image-20240911103026039.png" alt="image-20240911103026039"></p>
<p><img src="/2024/09/11/MySQL11/image-20240911103031756.png" alt="image-20240911103031756"></p>
<p>从图中你可以看到，<strong>由于email(6)这个索引结构中每个邮箱字段都只取前6个字节（即：zhangs），所以占用的空间会更小，这就是使用前缀索引的优势。</strong></p>
<p>但，这同时带来的损失是，<strong>可能会增加额外的记录扫描次数。</strong></p>
<p>接下来，我们再看看下面这个语句，在这两个索引定义下分别是怎么执行的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select id,name,email from SUser where email=&#x27;zhangssxyz@xxx.com&#x27;;</span><br></pre></td></tr></table></figure>

<p><strong>如果使用的是index1</strong>（即email整个字符串的索引结构），执行顺序是这样的：</p>
<p>1.<br>   从index1<strong>索引树</strong>找到满足索引值是’<a href="mailto:&#122;&#104;&#97;&#110;&#103;&#x73;&#x73;&#x78;&#121;&#x7a;&#x40;&#x78;&#120;&#120;&#46;&#99;&#x6f;&#109;">&#122;&#104;&#97;&#110;&#103;&#x73;&#x73;&#x78;&#121;&#x7a;&#x40;&#x78;&#120;&#120;&#46;&#99;&#x6f;&#109;</a>’的这条记录，取得ID2的值；<br>2. <strong>到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集；</strong><br>3. <strong>取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足email&#x3D;‘<a href="mailto:&#x7a;&#x68;&#x61;&#110;&#x67;&#115;&#x73;&#x78;&#x79;&#x7a;&#x40;&#120;&#x78;&#120;&#x2e;&#99;&#x6f;&#109;">&#x7a;&#x68;&#x61;&#110;&#x67;&#115;&#x73;&#x78;&#x79;&#x7a;&#x40;&#120;&#x78;&#120;&#x2e;&#99;&#x6f;&#109;</a>’的条件了，循环结束。</strong></p>
<p>这个过程中，<strong>只需要回主键索引取一次数据，所以系统认为只扫描了一行。</strong></p>
<p><strong>如果使用的是index2</strong>（即email(6)索引结构），执行顺序是这样的：</p>
<p>1.<br>   从index2索引树找到满足索引值是’zhangs’的记录，找到的第一个是ID1；<br>2. 到主键上查到主键值是ID1的行，判断出email的值不是’<a href="mailto:&#x7a;&#104;&#97;&#x6e;&#x67;&#x73;&#x73;&#120;&#x79;&#x7a;&#64;&#x78;&#x78;&#x78;&#46;&#x63;&#111;&#109;">&#x7a;&#104;&#97;&#x6e;&#x67;&#x73;&#x73;&#120;&#x79;&#x7a;&#64;&#x78;&#x78;&#x78;&#46;&#x63;&#111;&#109;</a>’，这行记录丢弃；<br>3. 取index2上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出ID2，再到ID索引上取整行然后判断，这次值对了，将这行记录加入结果集；<br>4. 重复上一步，直到在idxe2上取到的值不是’zhangs’时，循环结束。</p>
<p><strong>在这个过程中，要回主键索引取4次数据，也就是扫描了4行。</strong></p>
<p>通过这个对比，你很容易就可以发现，<strong>使用前缀索引后，可能会导致查询语句读数据的次数变多</strong>。</p>
<p>但是，对于这个查询语句来说，<strong>如果你定义的index2不是email(6)而是email(7），也就是说取email字段的前7个字节来构建索引的话，即满足前缀’zhangss’的记录只有一个，也能够直接查到ID2，只扫描一行就结束了</strong>。</p>
<p>也就是说<strong>使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。</strong></p>
<p>于是，你就有个问题：当要给字符串创建前缀索引时，<strong>有什么方法能够确定我应该使用多长的前缀呢</strong>？</p>
<p>实际上，<strong>我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。</strong></p>
<p>首先，你可以使用下面这个语句，算出这个列上有多少个不同的值：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select count(distinct email) as L from SUser;</span><br></pre></td></tr></table></figure>

<p>然后，依次选取不同长度的前缀来看这个值，比如我们要看一下4~7个字节的前缀索引，可以用这个语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select </span><br><span class="line">  count(distinct left(email,4)）as L4,</span><br><span class="line">  count(distinct left(email,5)）as L5,</span><br><span class="line">  count(distinct left(email,6)）as L6,</span><br><span class="line">  count(distinct left(email,7)）as L7,</span><br><span class="line">from SUser;</span><br></pre></td></tr></table></figure>

<p>当然，<strong>使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如5%<strong>。然后，在返回的L4~L7中，找出</strong>不小于</strong> L * 95%的值，<strong>假设这里L6、L7都满足，你就可以选择前缀长度为6</strong>。</p>
<h3 id="前缀索引对覆盖索引的影响"><a href="#前缀索引对覆盖索引的影响" class="headerlink" title="前缀索引对覆盖索引的影响"></a>前缀索引对覆盖索引的影响</h3><p>前面我们说了使用<strong>前缀索引可能会增加扫描行数，这会影响到性能。其实，前缀索引的影响不止如此，我们再看一下另外一个场景。</strong></p>
<p>你先来看看这个SQL语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select id,email from SUser where email=&#x27;zhangssxyz@xxx.com&#x27;;</span><br></pre></td></tr></table></figure>

<p>与前面例子中的SQL语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select id,name,email from SUser where email=&#x27;zhangssxyz@xxx.com&#x27;;</span><br></pre></td></tr></table></figure>

<p>相比，这个语句只要求返回id和email字段。</p>
<p>所以，<strong>如果使用index1（即email整个字符串的索引结构）的话，可以利用覆盖索引，从index1查到结果后直接就返回了，不需要回到ID索引再去查一次。而如果使用index2（即email(6)索引结构）的话，就不得不回到ID索引再去判断email字段的值。</strong></p>
<p>即使你将index2的定义修改为<strong>email(18)的前缀索引</strong>，这时候虽然index2<strong>已经包含了所有的信息</strong>，但InnoDB<strong>还是要回到id索引再查一下</strong>，<strong>因为系统并不确定前缀索引的定义是否截断了完整信息。</strong></p>
<p>也就是说，<strong>使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。</strong></p>
<h3 id="其他方式"><a href="#其他方式" class="headerlink" title="其他方式"></a>其他方式</h3><p>对于类似于邮箱这样的字段来说，<strong>使用前缀索引的效果可能还不错。但是，遇到前缀的区分度不够好的情况时，我们要怎么办呢？</strong></p>
<p>比如，我们国家的身份证号，一共18位，<strong>其中前6位是地址码，所以同一个县的人的身份证号前6位一般会是相同的</strong>。</p>
<p>假设你维护的数据库是一个市的公民信息系统，这时候如果对身份证号做长度为6的前缀索引的话，<strong>这个索引的区分度就非常低了。</strong></p>
<p>按照我们前面说的方法，<strong>可能你需要创建长度为12以上的前缀索引，才能够满足区分度要求。</strong></p>
<p>但是，索引选取的越长，<strong>占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。</strong></p>
<p>那么，如果我们能够确定业务需求里面只有按照身份证进行等值查询的需求，还有没有别的处理方法呢？这种方法，既可以占用更小的空间，也能达到相同的查询效率。</p>
<p>答案是，有的。</p>
<p><strong>第一种方式是使用倒序存储。****如果你存储身份证号的时候把它倒过来存，每次查询的时候，你可以这么写：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select field_list from t where id_card = reverse(&#x27;input_id_card_string&#x27;);</span><br></pre></td></tr></table></figure>

<p><strong>由于身份证号的最后6位没有地址码这样的重复逻辑，所以最后这6位很可能就提供了足够的区分度。当然了，实践中你不要忘记使用count(distinct)方法去做个验证。</strong></p>
<p><strong>第二种方式是使用hash字段。</strong>你可以在表上再创建一个整数字段，来保存身份证的校验码，<strong>同时在这个字段上创建索引。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; alter table t add id_card_crc int unsigned, add index(id_card_crc);</span><br></pre></td></tr></table></figure>

<p>然后每次插入新记录的时候，<strong>都同时用crc32()这个函数得到校验码填到这个新字段。由于校验码可能存在冲突，也就是说两个不同的身份证号通过crc32()函数得到的结果可能是相同的，所以你的查询语句where部分要判断id_card的值是否精确相同。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select field_list from t where id_card_crc=crc32(&#x27;input_id_card_string&#x27;) and id_card=&#x27;input_id_card_string&#x27;</span><br></pre></td></tr></table></figure>

<p>这样，<strong>索引的长度变成了4个字节</strong>，比原来小了很多。</p>
<p>接下来，我们再一起看看<strong>使用倒序存储和使用hash字段这两种方法的异同点。</strong></p>
<p>首先，它们的相同点是，<strong>都不支持范围查询</strong>。倒序存储的字段上创建的索引<strong>是按照倒序字符串的方式排序的，已经没有办法利用索引方式查出身份证号码在[ID_X, ID_Y]的所有市民了</strong>。同样地，hash字段的方式<strong>也只能支持等值查询</strong>。</p>
<p>它们的区别，主要体现在以下三个方面：</p>
<ol>
<li>从占用的额外空间来看，<strong>倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字段方法需要增加一个字段</strong>。当然，倒序存储方式使用4个字节的前缀长度应该是不够的，<strong>如果再长一点，这个消耗跟额外这个hash字段也差不多抵消了</strong>。</li>
<li>在CPU消耗方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash字段的方式需要额外调用一次crc32()函数。如果只从这两个函数的计算复杂度来看的话，<strong>reverse函数额外消耗的CPU资源会更小些</strong>。</li>
<li>从查询效率上看，<strong>使用hash字段方式的查询性能相对更稳定一些。因为crc32算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近1</strong>。而倒序存储方式毕竟还是用的前缀索引的方式，<strong>也就是说还是会增加扫描行数</strong>。</li>
</ol>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在今天这篇文章中，我跟你聊了聊<strong>字符串字段创建索引</strong>的场景。我们来回顾一下，你可以使用的方式有：</p>
<ul>
<li><strong>直接创建完整索引</strong>，这样可能比较占用空间；</li>
<li><strong>创建前缀索引</strong>，节省空间，<strong>但会增加查询扫描次数，并且不能使用覆盖索引</strong>；</li>
<li><strong>倒序存储</strong>，<strong>再创建前缀索引</strong>，用于绕过字符串本身前缀的区分度不够的问题；</li>
<li><strong>创建hash字段索引</strong>，查询性能稳定，有额外的存储和计算消耗，<strong>跟第三种方式一样，都不支持范围扫描</strong>。</li>
</ul>
<p>在实际应用中，你要根据业务字段的特点选择使用哪种方式。</p>
<p>如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号@gmail.com”, 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。</p>
<p>系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上篇文章中的第一个例子，大家要检查一下隔离级别是不是RR（Repeatable Read，可重复读），创建的表t是不是InnoDB引擎。</p>
<p>在上一篇文章最后，我给你留的问题是，为什么经过这个操作序列，explain的结果就不对了？这里，我来为你分析一下原因。</p>
<p>delete 语句删掉了所有的数据，然后再通过call idata()插入了10万行数据，看上去是覆盖了原来的10万行。</p>
<p>但是，session A<strong>开启了事务并没有提交，所以之前插入的10万行数据是不能删除的</strong>。这样，<strong>之前的数据每一行数据都有两个版本，旧版本是delete之前的数据，新版本是标记为deleted的数据。</strong></p>
<p>这样，索引a上的数据其实就有两份。</p>
<p>然后你会说，不对啊，主键上的数据也不能删，那没有使用force index的语句，使用explain命令看到的扫描行数为什么还是100000左右？（潜台词，如果这个也翻倍，也许优化器还会认为选字段a作为索引更合适）</p>
<p><strong>是的，不过这个是主键，主键是直接按照表的行数来估计的。而表的行数，优化器直接用的是show table status的值。</strong></p>
<p>这个值的计算方法，我会在后面有文章为你详细讲解。</p>
<p><img src="/2024/09/11/MySQL11/image-20240911103410211.png" alt="image-20240911103410211"></p>
<blockquote>
<p>大家的机器如果<strong>IO能力比较差</strong>的话，做这个验证的时候，可以把<strong>innodb_flush_log_at_trx_commit 和 sync_binlog 都设置成0。</strong></p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/11-zen-mo-gei-zi-fu-chuan-zi-duan-jia-suo-yin">11 怎么给字符串字段加索引？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么我的MySQL会“抖”一下？</title>
    <url>/2024/09/11/MySQL12/</url>
    <content><![CDATA[<p>平时的工作中，不知道你有没有遇到过这样的场景，<strong>一条SQL语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。</strong></p>
<p>看上去，这就像是数据库“抖”了一下。今天，我们就一起来看一看这是什么原因。</p>
<h3 id="你的SQL语句为什么变“慢”了"><a href="#你的SQL语句为什么变“慢”了" class="headerlink" title="你的SQL语句为什么变“慢”了"></a>你的SQL语句为什么变“慢”了</h3><p>在前面第2篇文章<a href="https://time.geekbang.org/column/article/68633">《日志系统：一条SQL更新语句是如何执行的？》</a>中，我为你介绍了WAL机制。现在你知道了，<strong>InnoDB在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作redo log（重做日志），也就是《孔乙己》里咸亨酒店掌柜用来记账的粉板，在更新内存写完redo log后，就返回给客户端，本次更新成功。</strong></p>
<p>做下类比的话，<strong>掌柜记账的账本是数据文件，记账用的粉板是日志文件（redo log），掌柜的记忆就是内存</strong>。</p>
<p>掌柜总要找时间把账本更新一下，<strong>这对应的就是把内存里的数据写入磁盘的过程，术语就是flush</strong>。在这个flush操作执行之前，孔乙己的赊账总额，其实跟掌柜手中账本里面的记录是不一致的。因为孔乙己今天的赊账金额还只在粉板上，而账本里的记录是老的，还没把今天的赊账算进去。</p>
<p><strong>当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”</strong>。</p>
<p>不论是脏页还是干净页，都在内存中。在这个例子里，内存对应的就是掌柜的记忆。</p>
<p>接下来，我们用一个示意图来展示一下“孔乙己赊账”的整个操作过程。假设原来孔乙己欠账10文，这次又要赊9文。</p>
<p><img src="/2024/09/11/MySQL12/image-20240911105754102.png" alt="image-20240911105754102"></p>
<p>回到文章开头的问题，<strong>你不难想象，平时执行很快的更新操作，其实就是在写内存和日志，而MySQL偶尔“抖”一下的那个瞬间，可能就是在刷脏页（flush）</strong>。</p>
<p>那么，什么情况会引发数据库的flush过程呢？</p>
<p>我们还是继续用咸亨酒店掌柜的这个例子，想一想：掌柜在什么情况下会把粉板上的赊账记录改到账本上？</p>
<ul>
<li>第一种场景是，<strong>粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活儿，将粉板上的记录擦掉一些，留出空位以便继续记账。</strong>当然在擦掉之前，他必须先将正确的账目记录到账本中才行。 这个场景，<strong>对应的就是InnoDB的redo log写满了。这时候系统会停止所有更新操作（STW），把checkpoint往前推进，redo log留出空间可以继续写</strong>。我在第二讲画了一个redo log的示意图，这里我改成环形，便于大家理解。</li>
</ul>
<p><img src="/2024/09/11/MySQL12/image-20240911105804571.png" alt="image-20240911105804571"></p>
<p>checkpoint可不是随便往前修改一下位置就可以的。比如图2中，<strong>把checkpoint位置从CP推进到CP’</strong>，就需要将两个点之间的日志（浅绿色部分），<strong>对应的所有脏页都flush到磁盘上。之后，图中从write pos到CP’之间就是可以再写入的redo log的区域。</strong></p>
<ul>
<li>第二种场景是，这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，<strong>赶紧找出账本把孔乙己这笔账先加进去。 这种场景，对应的就是系统内存不足</strong>。当需要新的内存页，而内存不够用的时候，<strong>就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。</strong> 你一定会说，<strong>这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿redo log出来应用不就行了？这里其实是从性能考虑的。如果刷脏页一定会写盘</strong>，就保证了每个数据页有两种状态：<ul>
<li><strong>一种是内存里存在，内存里就肯定是正确的结果，直接返回；</strong></li>
<li><strong>另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。 这样的效率最高。</strong></li>
</ul>
</li>
<li><strong>第三种场景是，生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不如更新账本。</strong> 这种场景，<strong>对应的就是MySQL认为系统“空闲”的时候</strong>。当然，MySQL“这家酒店”的生意好起来可是会很快就能把粉板记满的，所以“掌柜”要合理地安排时间，<strong>即使是“生意好”的时候，也要见缝插针地找时间，只要有机会就刷一点“脏页”</strong>。</li>
<li><strong>第四种场景是，年底了咸亨酒店要关门几天，需要把账结清一下。</strong>这时候掌柜要把所有账都记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了。 <strong>这种场景，对应的就是MySQL正常关闭的情况。这时候，MySQL会把内存的脏页都flush到磁盘上，这样下次MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快</strong>。</li>
</ul>
<p>接下来，<strong>你可以分析一下上面四种场景对性能的影响。</strong></p>
<p>其中，<strong>第三种情况是属于MySQL空闲时的操作，这时系统没什么压力，而第四种场景是数据库本来就要关闭了。这两种情况下，你不会太关注“性能”问题</strong>。所以这里，我们主要来分析一下前两种场景下的性能问题。</p>
<p>第一种是“redo log写满了，要flush脏页”，这种情况是InnoDB要尽量避免的。<strong>因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为0。</strong></p>
<p>第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。<strong>InnoDB用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：</strong></p>
<ul>
<li><strong>第一种是，还没有使用的；</strong></li>
<li><strong>第二种是，使用了并且是干净页；</strong></li>
<li><strong>第三种是，使用了并且是脏页。</strong></li>
</ul>
<p>InnoDB的策略是<strong>尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少</strong>。</p>
<p>而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。<strong>这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。</strong></p>
<p>所以，<strong>刷脏页虽然是常态</strong>，但是出现以下这两种情况，都是会明显影响性能的：</p>
<ol>
<li><strong>一个查询要获取的新的数据页较多，会淘汰太多脏页，导致查询的响应时间明显变长；</strong></li>
<li><strong>日志写满，更新全部堵住，写性能跌为0，这种情况对敏感业务来说，是不能接受的。</strong></li>
</ol>
<p>所以，InnoDB需要有控制脏页比例的机制，来尽量避免上面的这两种情况。</p>
<h3 id="InnoDB刷脏页的控制策略"><a href="#InnoDB刷脏页的控制策略" class="headerlink" title="InnoDB刷脏页的控制策略"></a>InnoDB刷脏页的控制策略</h3><p>接下来，我就来和你说说InnoDB脏页的控制策略，以及和这些策略相关的参数。</p>
<p><strong>首先，你要正确地告诉InnoDB所在主机的IO能力，这样InnoDB才能知道需要全力刷脏页的时候，可以刷多快。</strong></p>
<p>这就要用到<strong>innodb_io_capacity这个参数了，它会告诉InnoDB你的磁盘能力</strong>。<strong>这个值我建议你设置成磁盘的IOPS。磁盘的IOPS可以通过fio这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令：</strong></p>
<p>关于IOPS可以看看这篇文章：</p>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/668976764">一文读懂：IOPS、延迟和吞吐量等存储性能指标 - 知乎 (zhihu.com)</a></p>
<p><strong>我们举个简单的栗子</strong>，就像我们上班坐地铁一样，<strong>地铁上一个和下一个人可以看作存储系统的I&#x2F;O，IOPS就相当于（地铁上的人+地铁下的人）&#x2F;停留时间（秒）。</strong></p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest </span><br></pre></td></tr></table></figure>

<p>其实，<strong>因为没能正确地设置innodb_io_capacity参数，而导致的性能问题也比比皆是</strong>。之前，就曾有其他公司的开发负责人找我看一个库的性能问题，说<strong>MySQL的写入速度很慢，TPS很低，但是数据库主机的IO压力并不大。经过一番排查，发现罪魁祸首就是这个参数的设置出了问题</strong>。</p>
<p>他的主机磁盘用的是SSD，<strong>但是innodb_io_capacity的值设置的是300。于是，InnoDB认为这个系统的能力就这么差，所以刷脏页刷得特别慢，甚至比脏页生成的速度还慢，这样就造成了脏页累积，影响了查询和更新性能</strong>。</p>
<p><strong>虽然我们现在已经定义了“全力刷脏页”的行为，但平时总不能一直是全力刷吧？毕竟磁盘能力不能只用来刷脏页，还需要服务用户请求。所以接下来，我们就一起看看InnoDB怎么控制引擎按照“全力”的百分比来刷脏页。</strong></p>
<p>根据我前面提到的知识点，试想一下，<strong>如果你来设计策略控制刷脏页的速度，会参考哪些因素呢？</strong></p>
<p>这个问题可以这么想，如果刷太慢，会出现什么情况？<strong>首先是内存脏页太多，其次是redo log写满。</strong></p>
<p>所以，InnoDB的刷盘速度就是要参考这两个因素：<strong>一个是脏页比例，一个是redo log写盘速度。</strong></p>
<p>InnoDB会根据这两个因素先单独算出两个数字。</p>
<p>参数innodb_max_dirty_pages_pct是脏页比例上限，默认值是75%。InnoDB会根据当前的脏页比例（假设为M），算出一个范围在0到100之间的数字，计算这个数字的伪代码类似这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">F1(M)</span><br><span class="line">&#123;</span><br><span class="line">  if M&gt;=innodb_max_dirty_pages_pct then</span><br><span class="line">      return 100;</span><br><span class="line">  return 100*M/innodb_max_dirty_pages_pct;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>InnoDB每次写入的日志都有一个序号，<strong>当前写入的序号跟checkpoint对应的序号之间的差值，我们假设为N</strong>。InnoDB会根据这个N算出一个<strong>范围在0到100之间的数字</strong>，这个计算公式可以记为F2(N)。F2(N)算法比较复杂，你只要知道N越大，算出来的值越大就好了。</p>
<p>然后，<strong>根据上述算得的F1(M)和F2(N)两个值，取其中较大的值记为R，之后引擎就可以按照innodb_io_capacity定义的能力乘以R%来控制刷脏页的速度。</strong></p>
<p>上述的计算流程比较抽象，不容易理解，所以我画了一个简单的流程图。图中的F1、F2就是上面我们通过脏页比例和redo log写入速度算出来的两个值。</p>
<p><img src="/2024/09/11/MySQL12/image-20240911105833495.png" alt="image-20240911105833495"></p>
<p>现在你知道了，<strong>InnoDB会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用IO资源并可能影响到了你的更新语句</strong>，都可能是造成你从业务端感知到MySQL“抖”了一下的原因。</p>
<p>要尽量避免这种情况，你就要<strong>合理地设置innodb_io_capacity</strong>的值，并且**平时要多关注脏页比例，不要让它经常接近75%**。</p>
<p>其中，脏页比例是通过Innodb_buffer_pool_pages_dirty&#x2F;Innodb_buffer_pool_pages_total得到的，具体的命令参考下面的代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = &#x27;Innodb_buffer_pool_pages_dirty&#x27;;</span><br><span class="line">select VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = &#x27;Innodb_buffer_pool_pages_total&#x27;;</span><br><span class="line">select @a/@b;</span><br></pre></td></tr></table></figure>

<p>接下来，我们再看一个有趣的策略。</p>
<p><strong>一旦一个查询请求需要在执行过程中先flush掉一个脏页时，这个查询就可能要比平时慢了</strong>。而MySQL中的一个机制，可能让你的查询会更慢：<strong>在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷</strong>。</p>
<p>在InnoDB中，innodb_flush_neighbors 参数就是用来控制这个行为的，<strong>值为1的时候会有上述的“连坐”机制，值为0时表示不找邻居，自己刷自己的</strong>。</p>
<p>找“邻居”这个优化在机械硬盘时代是很有意义的，<strong>可以减少很多随机IO。机械硬盘的随机IOPS一般只有几百，相同的逻辑操作减少随机IO就意味着系统性能的大幅度提升</strong>。</p>
<p>而如果使用的是SSD这类IOPS比较高的设备的话，<strong>我就建议你把innodb_flush_neighbors的值设置成0。因为这时候IOPS往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少SQL语句响应时间。</strong></p>
<p><strong>在MySQL 8.0中，innodb_flush_neighbors参数的默认值已经是0了。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天这篇文章，我延续第2篇中介绍的WAL的概念，<strong>和你解释了这个机制后续需要的刷脏页操作和执行时机</strong>。利用WAL技术，<strong>数据库将随机写转换成了顺序写，大大提升了数据库的性能</strong>。</p>
<p>但是，<strong>由此也带来了内存脏页的问题。脏页会被后台线程自动flush，也会由于数据页淘汰而触发flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些</strong>。在文章里，我也给你介绍了控制刷脏页的方法和对应的监控方式。</p>
<p>一个内存配置为128GB、innodb_io_capacity设置为20000的大规格实例，正常会建议你将redo log设置成4个1GB的文件。</p>
<p>但如果你在配置的时候不慎将redo log设置成了1个100M的文件，会发生什么情况呢？又为什么会出现这样的情况呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期我留给你的问题是，给一个学号字段创建索引，有哪些方法。</p>
<p>由于这个学号的规则，<strong>无论是正向还是反向的前缀索引，重复度都比较高。因为维护的只是一个学校的，因此前面6位（其中，前三位是所在城市编号、第四到第六位是学校编号）其实是固定</strong>的，邮箱后缀都是@gamil.com，<strong>因此可以只存入学年份加顺序编号，它们的长度是9位</strong>。</p>
<p>而其实在此基础上，<strong>可以用数字类型来存这9位数字。比如201100001，这样只需要占4个字节。其实这个就是一种hash</strong>，只是它用了最简单的转换规则：<strong>字符串转数字的规则，而刚好我们设定的这个背景，可以保证这个转换后结果的唯一性</strong>。</p>
<p>一个学校的总人数这种数据量，50年才100万学生，这个表肯定是小表。<strong>为了业务简单，直接存原来的字符串。这个答复里面包含了“优化成本和收益”的思想，我觉得值得at出来</strong>。</p>
<p>如果碰到表数据量特别大的场景，通过这种方式的收益是很不错的。</p>
<blockquote>
<p>用整型存<strong>“四位年份+五位编号”</strong>的方法； <strong>由于整个学号的值超过了int上限，用8个字节的bigint来存的方法。</strong></p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/12-wei-shi-mo-wo-de-mysql-hui-dou-yi-xia">12 为什么我的MySQL会“抖”一下？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么表数据删掉一半，表文件大小不变？</title>
    <url>/2024/09/11/MySQL13/</url>
    <content><![CDATA[<p>经常会有同学来问我，<strong>我的数据库占用空间太大，我把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？</strong></p>
<p>那么今天，我就和你聊聊<strong>数据库表的空间回收</strong>，看看如何解决这个问题。</p>
<p>这里，我们还是针对MySQL中应用最广泛的InnoDB引擎展开讨论。<strong>一个InnoDB表包含两部分，即：表结构定义和数据</strong>。在MySQL 8.0版本以前，<strong>表结构是存在以.frm为后缀的文件里</strong>。而MySQL 8.0版本，<strong>则已经允许把表结构定义放在系统数据表中了</strong>。<strong>因为表结构定义占用的空间很小，所以我们今天主要讨论的是表数据。</strong></p>
<p>接下来，我会先和你说明为什么简单地删除表数据达不到表空间回收的效果，然后再和你介绍正确回收空间的方法。</p>
<h3 id="参数innodb-file-per-table"><a href="#参数innodb-file-per-table" class="headerlink" title="参数innodb_file_per_table"></a>参数innodb_file_per_table</h3><p>表数据既可以<strong>存在共享表空间里，也可以是单独的文件。这个行为是由参数innodb_file_per_table控制的</strong>：</p>
<ol>
<li>这个参数设置为OFF表示的是，<strong>表的数据放在系统共享表空间，也就是跟数据字典放在一起</strong>；</li>
<li>这个参数设置为ON表示的是，<strong>每个InnoDB表数据存储在一个以 .ibd为后缀的文件中</strong>。</li>
</ol>
<p>从MySQL 5.6.6版本开始，<strong>它的默认值就是ON了</strong>。</p>
<p>我建议你不论使用MySQL的哪个版本，<strong>都将这个值设置为ON。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过drop table命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。</strong></p>
<p>所以，<strong>将innodb_file_per_table设置为ON，是推荐做法，我们接下来的讨论都是基于这个设置展开的。</strong></p>
<p>我们在删除整个表的时候，可以使用drop table命令回收表空间。<strong>但是，我们遇到的更多的删除数据的场景是删除某些行，这时就遇到了我们文章开头的问题：表中的数据被删除了，但是表空间却没有被回收。</strong></p>
<p>我们要彻底搞明白这个问题的话，就要从数据删除流程说起了。</p>
<h3 id="数据删除流程"><a href="#数据删除流程" class="headerlink" title="数据删除流程"></a>数据删除流程</h3><p>我们先再来看一下InnoDB中一个索引的示意图。在前面<a href="https://time.geekbang.org/column/article/69236">第4</a>和<a href="https://time.geekbang.org/column/article/69636">第5</a>篇文章中，我和你介绍索引时曾经提到过，<strong>InnoDB里的数据都是用B+树的结构组织的。</strong></p>
<p><img src="/2024/09/11/MySQL13/image-20240911145519977.png" alt="image-20240911145519977"></p>
<p>假设，我们要删掉R4这个记录，<strong>InnoDB引擎只会把R4这个记录标记为删除。如果之后要再插入一个ID在300和600之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。</strong></p>
<p>现在，你已经知道了InnoDB的数据是按页存储的，那么如果我们删掉了一个数据页上的所有记录，会怎么样？</p>
<p><strong>答案是，整个数据页就可以被复用了。</strong></p>
<p>但是，<strong>数据页的复用跟记录的复用是不同的。</strong></p>
<p>记录的复用，只限于符合范围条件的数据。比如上面的这个例子，R4这条记录被删除后，<strong>如果插入一个ID是400的行，可以直接复用这个空间。但如果插入的是一个ID是800的行，就不能复用这个位置了。</strong></p>
<p><strong>而当整个页从B+树里面摘掉以后，可以复用到任何位置。以图1为例，如果将数据页page A上的所有记录删除以后，page A会被标记为可复用。这时候如果要插入一条ID&#x3D;50的记录需要使用新页的时候，page A是可以被复用的。</strong></p>
<p><strong>如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。</strong></p>
<p>进一步地，如果我们用delete命令把整个表的数据删除呢？结果就是，<strong>所有的数据页都会被标记为可复用。但是磁盘上，文件不会变小。</strong></p>
<p>你现在知道了，<strong>delete命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过delete命令是不能回收表空间的</strong>。这些可以复用，<strong>而没有被使用的空间，看起来就像是“空洞”（怎么这里也出现了空洞）</strong>。</p>
<p>实际上，<strong>不止是删除数据会造成空洞，插入数据也会。</strong></p>
<p><strong>如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。</strong></p>
<p>假设图1中page A已经满了，这时我要再插入一行数据，会怎样呢？</p>
<p><img src="/2024/09/11/MySQL13/image-20240911145535516.png" alt="image-20240911145535516"></p>
<p><strong>可以看到，由于page A满了，再插入一个ID是550的数据时，就不得不再申请一个新的页面page B来保存数据了。页分裂完成后，page A的末尾就留下了空洞（注意：实际上，可能不止1个记录的位置是空洞）。</strong></p>
<p>另外，更新索引上的值，<strong>可以理解为删除一个旧的值，再插入一个新值。不难理解，这也是会造成空洞的</strong>。</p>
<p><strong>也就是说，经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。</strong></p>
<p><strong>而重建表，就可以达到这样的目的。（我记得前面有讲过，例子还是重建索引）</strong></p>
<h3 id="重建表"><a href="#重建表" class="headerlink" title="重建表"></a>重建表</h3><p>试想一下，如果你现在有一个表A，需要做空间收缩，为了把表中存在的空洞去掉，你可以怎么做呢？</p>
<p><strong>你可以新建一个与表A结构相同的表B，然后按照主键ID递增的顺序，把数据一行一行地从表A里读出来再插入到表B中。</strong></p>
<p>由于表B是新建的表，<strong>所以表A主键索引上的空洞，在表B中就都不存在了</strong>。显然地，表B的主键索引<strong>更紧凑</strong>，数据页的<strong>利用率也更高</strong>。如果我们把表B<strong>作为临时表，数据从表A导入表B的操作完成后，用表B替换A，从效果上看，就起到了收缩表A空间的作用</strong>。</p>
<p>这里，你可以使用<strong>alter table A engine&#x3D;InnoDB命令来重建表。在MySQL 5.5版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表B不需要你自己创建，MySQL会自动完成转存数据、交换表名、删除旧表的操作。</strong></p>
<p><img src="/2024/09/11/MySQL13/image-20240911145555956.png" alt="image-20240911145555956"></p>
<p>显然，花时间最多的步骤是往临时表插入数据的过程，如果在这个过程中，有新的数据要写入到表A的话，就会造成数据丢失。<strong>因此，在整个DDL过程中，表A中不能有更新。也就是说，这个DDL不是Online的。</strong></p>
<p>而在<strong>MySQL 5.6版本开始引入的Online DDL，对这个操作流程做了优化。</strong></p>
<p>我给你简单描述一下<strong>引入了Online DDL之后</strong>，重建表的流程：</p>
<ol>
<li><strong>建立一个临时文件</strong>，扫描表A主键的所有数据页；</li>
<li>用数据页中表A的记录<strong>生成B+树，存储到临时文件中；</strong></li>
<li>生成临时文件的过程中，<strong>将所有对A的操作记录在一个日志文件（row log）中，对应的是图中state2的状态；</strong></li>
<li>临时文件生成后，<strong>将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表A相同的数据文件，对应的就是图中state3的状态；</strong></li>
<li>用<strong>临时文件替换表A的数据文件。</strong></li>
</ol>
<p><img src="/2024/09/11/MySQL13/image-20240911145607197.png" alt="image-20240911145607197"></p>
<p>可以看到，与图3过程的不同之处在于，<strong>由于日志文件记录和重放操作这个功能的存在，这个方案在重建表的过程中，允许对表A做增删改操作。这也就是Online DDL名字的来源。</strong></p>
<p>DDL之类的忘记了建议看看这篇文章</p>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/391552199">SQL语言：DDL、DML、DQL、DCL详解 - 知乎 (zhihu.com)</a></p>
<p>简单的说，DDL修改表，DML修改行，DQL查询行，DCL权限控制</p>
</li>
</ul>
<p>我记得有同学在第6篇讲表锁的文章<a href="https://time.geekbang.org/column/article/69862">《全局锁和表锁 ：给表加个字段怎么索这么多阻碍？》</a>的评论区留言说，<strong>DDL之前是要拿MDL写锁的，这样还能叫Online DDL吗？</strong></p>
<p>确实，图4的流程中，<strong>alter语句在启动的时候需要获取MDL写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。</strong></p>
<p>为什么要退化呢？<strong>为了实现Online，MDL读锁不会阻塞增删改操作。</strong></p>
<p>那为什么不干脆直接解锁呢？为了保护自己，禁止其他线程对这个表同时做DDL。</p>
<p>而对于一个大表来说，<strong>Online DDL最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个DDL过程来说，锁的时间非常短。对业务来说，就可以认为是Online的。</strong></p>
<p>需要补充说明的是，<strong>上述的这些重建方法都会扫描原表数据和构建临时文件。对于很大的表来说，这个操作是很消耗IO和CPU资源的。</strong>因此，如果是线上服务，你要很小心地控制操作时间。如果想要比较安全的操作的话，我推荐你使用GitHub开源的gh-ost来做。</p>
<h3 id="Online-和-inplace"><a href="#Online-和-inplace" class="headerlink" title="Online 和 inplace"></a>Online 和 inplace</h3><p>说到Online，我还要再和你澄清一下它和另一个跟DDL有关的、容易混淆的概念inplace的区别。</p>
<p><img src="/2024/09/11/MySQL13/image-20240911154457500.png" alt="image-20240911154457500"></p>
<p>你可能注意到了，在图3中，<strong>我们把表A中的数据导出来的存放位置叫作tmp_table。这是一个临时表，是在server层创建的。</strong></p>
<p><img src="/2024/09/11/MySQL13/image-20240911154505918.png" alt="image-20240911154505918"></p>
<p>在图4中，根据表A重建出来的数据是放在“tmp_file”里的，这个临时文件是<strong>InnoDB在内部创建出来的。整个DDL过程都在InnoDB内部完成</strong>。<strong>对于server层来说，没有把数据挪动到临时表，是一个“原地”操作，这就是“inplace”名称的来源。</strong></p>
<p><strong>所以，我现在问你，如果你有一个1TB的表，现在磁盘间是1.2TB，能不能做一个inplace的DDL呢？</strong></p>
<p><strong>答案是不能。因为，tmp_file也是要占用临时空间的。</strong></p>
<p>我们<strong>重建表的这个语句alter table t engine&#x3D;InnoDB，其实隐含的意思是：（InnoDB内部）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table t engine=innodb,ALGORITHM=inplace;</span><br></pre></td></tr></table></figure>

<p><strong>跟inplace对应的就是拷贝表（server层）的方式了</strong>，用法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table t engine=innodb,ALGORITHM=copy;</span><br></pre></td></tr></table></figure>

<p>当你使用ALGORITHM&#x3D;copy的时候，表示的是<strong>强制拷贝表</strong>，对应的流程就是图3的操作过程。</p>
<p>但我这样说你可能会觉得，inplace跟Online是不是就是一个意思？</p>
<p>其实不是的，只是在重建表这个逻辑中刚好是这样而已。</p>
<p>比如，如果我要给InnoDB表的一个字段加全文索引，写法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table t add FULLTEXT(field_name);</span><br></pre></td></tr></table></figure>

<p><strong>这个过程是inplace的，但会阻塞增删改操作，是非Online的。</strong></p>
<p>如果说这两个逻辑之间的关系是什么的话，可以概括为：</p>
<ol>
<li>DDL过程<strong>如果是Online的，就一定是inplace的</strong>；</li>
<li>反过来未必，也就是说inplace的DDL，有可能不是Online的。截止到MySQL 8.0，添加全文索引（FULLTEXT index）和空间索引(SPATIAL index)就属于这种情况。</li>
</ol>
<p>最后，我们再延伸一下。</p>
<p>在第10篇文章<a href="https://time.geekbang.org/column/article/71173">《MySQL为什么有时候会选错索引》</a>的评论区中，有同学问到使用optimize table、analyze table和alter table这三种方式重建表的区别。这里，我顺便再简单和你解释一下。</p>
<ul>
<li>从MySQL 5.6版本开始，<strong>alter table t engine &#x3D; InnoDB（也就是recreate）默认的就是上面图4的流程了；</strong></li>
<li>analyze table t <strong>其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了MDL读锁；</strong></li>
<li>optimize table t <strong>等于recreate+analyze。</strong></li>
</ul>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天这篇文章，我和你讨论了数据库中收缩表空间的方法。</p>
<p><strong>现在你已经知道了，如果要收缩一个表，只是delete掉表里面不用的数据的话，表文件的大小是不会变的，你还要通过alter table命令重建表，才能达到表文件变小的目的</strong>。我跟你介绍了<strong>重建表的两种实现方式，Online DDL的方式是可以考虑在业务低峰期使用的，而MySQL 5.5及之前的版本，这个命令是会阻塞DML的，这个你需要特别小心</strong>。</p>
<p>假设现在有人碰到了一个“想要收缩表空间，结果适得其反”的情况，看上去是这样的：</p>
<ol>
<li>一个表t文件大小为1TB；</li>
<li>对这个表执行 alter table t engine&#x3D;InnoDB；</li>
<li>发现执行完成后，空间不仅没变小，还稍微大了一点儿，比如变成了1.01TB。</li>
</ol>
<p>你觉得可能是什么原因呢 ？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>在上期文章最后，我留给你的问题是，如果一个高配的机器，redo log设置太小，会发生什么情况。</p>
<p><strong>每次事务提交都要写redo log，如果设置太小，很快就会被写满，也就是下面这个图的状态，这个“环”将很快被写满，write pos一直追着CP。</strong></p>
<p><img src="/2024/09/11/MySQL13/image-20240911145857781.png" alt="image-20240911145857781"></p>
<p><strong>这时候系统不得不停止所有更新，去推进checkpoint。</strong></p>
<p>这时，你看到的现象就是<strong>磁盘压力很小，但是数据库出现间歇性的性能下跌。</strong></p>
<blockquote>
<p>在这种情况下，<strong>连change buffer的优化也失效了。</strong></p>
<p><strong>因为checkpoint一直要往前推，这个操作就会触发merge操作，然后又进一步地触发刷脏页操作；</strong></p>
<p> 有几个同学提到了<strong>内存淘汰脏页，对应的redo log的操作</strong>，这个我们会在后面的文章中展开，大家可以先看一下 </p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/13-wei-shi-mo-biao-shu-ju-shan-diao-yi-ban-biao-wen-jian-da-xiao-bu-bian#xiao-jie">13 为什么表数据删掉一半，表文件大小不变？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>count(*)这么慢，我该怎么办？</title>
    <url>/2024/09/11/MySQL14/</url>
    <content><![CDATA[<p>在开发系统的时候，<strong>你可能经常需要计算一个表的行数，比如一个交易系统的所有变更记录总数。这时候你可能会想，一条select count(*) from t 语句不就解决了吗？</strong></p>
<p>但是，<strong>你会发现随着系统中记录数越来越多，这条语句执行得也会越来越慢</strong>。然后你可能就想了，<strong>MySQL怎么这么笨啊，记个总数，每次要查的时候直接读出来，不就好了</strong>吗。</p>
<p>那么今天，我们就来<strong>聊聊count(*)语句到底是怎样实现的，以及MySQL为什么会这么实现</strong>。然后，<strong>我会再和你说说，如果应用中有这种频繁变更并需要统计表行数的需求</strong>，业务设计上可以怎么做。</p>
<h3 id="count-的实现方式"><a href="#count-的实现方式" class="headerlink" title="count(*)的实现方式"></a>count(*)的实现方式</h3><p>你首先要明确的是，<strong>在不同的MySQL引擎中，count(*)有不同的实现方式。</strong></p>
<ul>
<li><p><strong>MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高。</strong></p>
</li>
<li><p><strong>而InnoDB引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。</strong></p>
</li>
</ul>
<p>这里需要注意的是，<strong>我们在这篇文章里讨论的是没有过滤条件的count(*)，如果加了where 条件的话，MyISAM表也是不能返回得这么快的。</strong></p>
<p>在前面的文章中，<strong>我们一起分析了为什么要使用InnoDB，因为不论是在事务支持、并发能力还是在数据安全方面，InnoDB都优于MyISAM</strong>。我猜你的表也一定是用了InnoDB引擎。<strong>这就是当你的记录数越来越多的时候，计算一个表的总行数会越来越慢的原因</strong>。</p>
<p>那<strong>为什么InnoDB不跟MyISAM一样，也把数字存起来呢？</strong></p>
<p><strong>这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的。</strong>这里，我用一个算count(*)的例子来为你解释一下。</p>
<p>假设表t中现在有10000条记录，我们设计了三个用户并行的会话。</p>
<ul>
<li>会话A先启动事务并查询一次表的总行数；</li>
<li>会话B启动事务，插入一行后记录后，查询表的总行数；</li>
<li>会话C先启动一个单独的语句，插入一行记录后，查询表的总行数。</li>
</ul>
<p>我们假设从上到下是按照时间顺序执行的，同一行语句是在同一时刻执行的。</p>
<p><img src="/2024/09/11/MySQL14/image-20240911184247099.png" alt="image-20240911184247099"></p>
<p><strong>你会看到，在最后一个时刻，三个会话A、B、C会同时查询表t的总行数，但拿到的结果却不同。</strong></p>
<p><strong>这和InnoDB的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是MVCC来实现的。</strong>每一行记录<strong>都要判断自己是否对这个会话可见</strong>，因此对于count(<em>)请求来说，*<em>InnoDB只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。</em></em></p>
<blockquote>
<p>备注：<strong>如果你对MVCC记忆模糊了，可以再回顾下第3篇文章<a href="https://time.geekbang.org/column/article/68963">《事务隔离：为什么你改了我还看不见？》</a>和第8篇文章<a href="https://time.geekbang.org/column/article/70562">《事务到底是隔离的还是不隔离的？》</a>中的相关内容。</strong></p>
</blockquote>
<p>当然，<strong>现在这个看上去笨笨的MySQL，在执行count(*)操作的时候还是做了优化的</strong>。</p>
<p>你知道的，<strong>InnoDB是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值</strong>。所以，<strong>普通索引树比主键索引树小很多。对于count(*)这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的</strong>。因此，MySQL优化器会找到<strong>最小的那棵树</strong>来遍历。<strong>在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。</strong></p>
<p><strong>如果你用过show table status 命令的话，就会发现这个命令的输出结果里面也有一个TABLE_ROWS用于显示这个表当前有多少行，这个命令执行挺快的，那这个TABLE_ROWS能代替count(*)吗？</strong></p>
<p>关于这个命令，你可以看看这篇文章</p>
<ul>
<li><p><a href="https://blog.csdn.net/longool/article/details/140136206">mysql 命令 —— 查看表信息（show table status）-CSDN博客</a></p>
<p>这个命令是用来<strong>查询表信息的</strong>，<strong>比如整个表的数据量大小、表的索引占用空间大小</strong>等</p>
</li>
</ul>
<p>你可能还记得在第10篇文章<a href="https://time.geekbang.org/column/article/71173">《 MySQL为什么有时候会选错索引？》</a>中我提到过，<strong>索引统计的值是通过采样来估算的</strong>。实际上，TABLE_ROWS就是从这个采样估算得来的，<strong>因此它也很不准</strong>。有多不准呢，官方文档说误差<strong>可能达到40%到50%<strong>。</strong>所以，show table status命令显示的行数也不能直接使用。</strong></p>
<p>到这里我们小结一下：</p>
<ul>
<li><strong>MyISAM表虽然count(*)很快，但是不支持事务；</strong></li>
<li><strong>show table status命令虽然返回很快，但是不准确；</strong></li>
<li><strong>InnoDB表直接count(*)会遍历全表，虽然结果准确，但会导致性能问题。</strong></li>
</ul>
<p>那么，回到文章开头的问题，如果你现在有一个页面经常要显示交易系统的操作记录总数，到底应该怎么办呢？答案是，我们只能自己计数。</p>
<p>接下来，我们讨论一下，看看自己计数有哪些方法，以及每种方法的优缺点有哪些。</p>
<p>这里，我先和你说一下这些方法的基本思路：你需要自己找一个地方，把操作记录表的行数存起来。</p>
<h3 id="用缓存系统保存计数"><a href="#用缓存系统保存计数" class="headerlink" title="用缓存系统保存计数"></a>用缓存系统保存计数</h3><p>对于更新很频繁的库来说，你可能会第一时间想到，用缓存系统来支持。</p>
<p><strong>你可以用一个Redis服务来保存这个表的总行数。这个表每被插入一行Redis计数就加1，每被删除一行Redis计数就减1。这种方式下，读和更新操作都很快，但你再想一下这种方式存在什么问题吗？</strong></p>
<p>没错，<strong>缓存系统可能会丢失更新</strong>。</p>
<p>Redis的数据不能永久地留在内存里，所以你会找一个地方把这个值定期地持久化存储起来。<strong>但即使这样，仍然可能丢失更新。试想如果刚刚在数据表中插入了一行，Redis中保存的值也加了1，然后Redis异常重启了，重启后你要从存储redis数据的地方把这个值读回来，而刚刚加1的这个计数操作却丢失了。</strong></p>
<p>当然了，这还是有解的。<strong>比如，Redis异常重启以后，到数据库里面单独执行一次count(*)获取真实的行数，再把这个值写回到Redis里就可以了。异常重启毕竟不是经常出现的情况，这一次全表扫描的成本，还是可以接受的。</strong></p>
<p>但实际上，<strong>将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使Redis正常工作，这个值还是逻辑上不精确的。</strong></p>
<p>你可以设想一下有这么一个页面，要显示操作记录的总数，同时还要显示最近操作的100条记录。那么，这个页面的逻辑就需要先到Redis里面取出计数，再到数据表里面取数据记录。</p>
<p><strong>我们是这么定义不精确的：</strong></p>
<ol>
<li><strong>一种是，查到的100行结果里面有最新插入记录，而Redis的计数里还没加1；</strong></li>
<li><strong>另一种是，查到的100行结果里没有最新插入的记录，而Redis的计数里已经加了1。</strong></li>
</ol>
<p><strong>这两种情况，都是逻辑不一致的。</strong></p>
<p>我们一起来看看这个时序图。</p>
<p><img src="/2024/09/11/MySQL14/image-20240911184506659.png" alt="image-20240911184506659"></p>
<p>图2中，会话A是一个插入交易记录的逻辑，往数据表里插入一行R，然后Redis计数加1；会话B就是查询页面显示时需要的数据。</p>
<p>在图2的这个时序里，<strong>在T3时刻会话B来查询的时候，会显示出新插入的R这个记录，但是Redis的计数还没加1。这时候，就会出现我们说的数据不一致。</strong></p>
<p>你一定会说，这是因为我们执行新增记录逻辑时候，是先写数据表，再改Redis计数。而读的时候是先读Redis，再读数据表，这个顺序是相反的。那么，如果保持顺序一样的话，是不是就没问题了？我们现在把会话A的更新顺序换一下，再看看执行结果。</p>
<p><img src="/2024/09/11/MySQL14/image-20240911184533306.png" alt="image-20240911184533306"></p>
<p>你会发现，这时候反过来了，<strong>会话B在T3时刻查询的时候，Redis计数加了1了，但还查不到新插入的R这一行，也是数据不一致的情况。</strong></p>
<p><strong>在并发系统里面，我们是无法精确控制不同线程的执行时刻的，因为存在图中的这种操作序列，所以，我们说即使Redis正常工作，这个计数值还是逻辑上不精确的。</strong></p>
<h3 id="在数据库保存计数"><a href="#在数据库保存计数" class="headerlink" title="在数据库保存计数"></a>在数据库保存计数</h3><p>根据上面的分析，<strong>用缓存系统保存计数有丢失数据和计数不精确的问题</strong>。那么，<strong>如果我们把这个计数直接放到数据库里单独的一张计数表C中，又会怎么样呢？</strong></p>
<p>首先，<strong>这解决了崩溃丢失的问题，InnoDB是支持崩溃恢复不丢数据的</strong>。</p>
<blockquote>
<p><strong>备注：关于InnoDB的崩溃恢复，你可以再回顾一下第2篇文章<a href="https://time.geekbang.org/column/article/68633">《日志系统：一条SQL更新语句是如何执行的？》</a>中的相关内容。</strong></p>
</blockquote>
<p>然后，我们再看看能不能解决计数不精确的问题。</p>
<p>你会说，这不一样吗？<strong>无非就是把图3中对Redis的操作，改成了对计数表C的操作。只要出现图3的这种执行序列，这个问题还是无解的吧？</strong></p>
<p>这个问题还真不是无解的。</p>
<p>我们这篇文章要解决的问题，<strong>都是由于InnoDB要支持事务，从而导致InnoDB表不能把count(*)直接存起来，然后查询的时候直接返回形成的。</strong></p>
<p>所谓以子之矛攻子之盾，<strong>现在我们就利用“事务”这个特性，把问题解决掉</strong>。</p>
<p><img src="/2024/09/11/MySQL14/image-20240911184611901.png" alt="image-20240911184611901"></p>
<p>我们来看下现在的执行结果。<strong>虽然会话B的读操作仍然是在T3执行的，但是因为这时候更新事务还没有提交，所以计数值加1这个操作对会话B还不可见。</strong></p>
<p>因此，会话B看到的结果里， <strong>查计数值和“最近100条记录”看到的结果，逻辑上就是一致的。</strong></p>
<h3 id="不同的count用法"><a href="#不同的count用法" class="headerlink" title="不同的count用法"></a>不同的count用法</h3><p>在前面文章的评论区，有同学留言问到：在select count(?) from t这样的查询语句里面，count(<em>)、count(主键id)、count(字段)和count(1)等不同用法的性能，有哪些差别。今天谈到了count(</em>)的性能问题，我就借此机会和你详细说明一下这几种用法的性能差别。</p>
<p><strong>需要注意的是，下面的讨论还是基于InnoDB引擎的。</strong></p>
<p>这里，首先你要弄清楚count()的语义。<strong>count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值。</strong></p>
<p>所以，<strong>count(*)、count(主键id)和count(1)</strong> 都表示返回满足条件的结果集的<strong>总行数</strong>；<strong>而count(字段），则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数</strong>。</p>
<p><strong>至于分析性能差别的时候</strong>，你可以记住这么几个原则：</p>
<ol>
<li><p><strong>server层要什么就给什么；（注意这个count是在server层的功能）</strong></p>
</li>
<li><p><strong>InnoDB只给必要的值；</strong></p>
</li>
<li><p>现在的优化器只优化了count(*)的语义为“取行数”，其他“显而易见”的优化并没有做。</p>
</li>
</ol>
<p>这是什么意思呢？接下来，我们就一个个地来看看。</p>
<p><strong>对于count(主键id)来说</strong>，InnoDB引擎会遍历整张表，<strong>把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空的，就按行累加。</strong></p>
<p><strong>对于count(1)来说</strong>，InnoDB引擎遍历整张表，<strong>但不取值。server层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。</strong></p>
<p>单看这两个用法的差别的话，<strong>你能对比出来，count(1)执行得要比count(主键id)快。因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作。</strong></p>
<p><strong>对于count(字段)来说</strong>：</p>
<ol>
<li><p><strong>如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加；</strong></p>
</li>
<li><p><strong>如果这个“字段”定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。</strong></p>
</li>
</ol>
<p>也就是前面的第一条原则，<strong>server层要什么字段，InnoDB就返回什么字段。</strong></p>
<p><strong>但是count(*)是例外</strong>，并不会把全部字段取出来，<strong>而是专门做了优化，不取值。count(*)肯定不是null，按行累加</strong>。</p>
<p>看到这里，你一定会说，优化器就不能自己判断一下吗，主键id肯定非空啊，为什么不能按照count(*)来处理，多么简单的优化啊。</p>
<p>当然，MySQL专门针对这个语句进行优化，也不是不可以。但是这种需要专门优化的情况太多了，<strong>而且MySQL已经优化过count(*)了，你直接使用这种用法就可以了</strong>。</p>
<p>所以结论是：按照效率排序的话，**count(字段)&lt;count(主键id)&lt;count(1)≈count(<code>*</code>)，所以我建议你，尽量使用count(<code>*</code>)**。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p><strong>今天，我和你聊了聊MySQL中获得表行数的两种方法。</strong>我们提到了<strong>在不同引擎中count(*)的实现方式是不一样的</strong>，也分析了用<strong>缓存系统来存储计数值存在的问题</strong>。</p>
<p>其实，把计数放在Redis里面，不能够保证计数和MySQL表里的数据精确一致的原因，是<strong>这两个不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图。</strong>而把计数值也放在MySQL中，就解决了一致性视图的问题。</p>
<p>InnoDB引擎支持事务，<strong>我们利用好事务的原子性和隔离性，就可以简化在业务开发时的逻辑。这也是InnoDB引擎备受青睐的原因之一。</strong></p>
<p>在刚刚讨论的方案中，我们用了事务来确保计数准确。由于<strong>事务可以保证中间结果不被别的事务读到，因此修改计数值和插入新记录的顺序是不影响逻辑结果的。但是，从并发系统性能的角度考虑，你觉得在这个事务序列里，应该先插入操作记录，还是应该先更新计数表呢？</strong></p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期我给你留的问题是，什么时候使用alter table t engine&#x3D;InnoDB会让一个表占用的空间反而变大。</p>
<p>在这篇文章的评论区里面，大家都提到了一个点，<strong>就是这个表，本身就已经没有空洞的了，比如说刚刚做过一次重建表操作。</strong></p>
<p><strong>在DDL期间，如果刚好有外部的DML在执行，这期间可能会引入一些新的空洞。</strong></p>
<p><strong>在重建表的时候，InnoDB不会把整张表占满，每个页留了1&#x2F;16给后续的更新用。也就是说，其实重建表之后不是“最”紧凑的。</strong></p>
<p>假如是这么一个过程：</p>
<ol>
<li><strong>将表t重建一次；</strong></li>
<li><strong>插入一部分数据，但是插入的这些数据，用掉了一部分的预留空间；</strong></li>
<li><strong>这种情况下，再重建一次表t，就可能会出现问题中的现象。（简单的说，就是要保证一定的预留空间，用了的都是要还回来的）</strong></li>
</ol>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/14-count-zhe-mo-man-wo-gai-zen-mo-ban">14 count(*)这么慢，我该怎么办？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>“order by”是怎么工作的？</title>
    <url>/2024/09/11/MySQL16/</url>
    <content><![CDATA[<p>在你开发应用的时候，<strong>一定会经常碰到需要根据指定的字段排序来显示结果的需求</strong>。还是以我们前面举例用过的市民表为例，假设你要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前1000个人的姓名、年龄。</p>
<p>假设这个表的部分定义是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `t` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `city` varchar(16) NOT NULL,</span><br><span class="line">  `name` varchar(16) NOT NULL,</span><br><span class="line">  `age` int(11) NOT NULL,</span><br><span class="line">  `addr` varchar(128) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `city` (`city`)</span><br><span class="line">) ENGINE=InnoDB;</span><br></pre></td></tr></table></figure>

<p>这时，你的SQL语句可以这么写：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select city,name,age from t where city=&#x27;杭州&#x27; order by name limit 1000  ;</span><br></pre></td></tr></table></figure>

<p><strong>这个语句看上去逻辑很清晰，但是你了解它的执行流程吗？今天，我就和你聊聊这个语句是怎么执行的，以及有什么参数会影响执行的行为。</strong></p>
<h3 id="全字段排序"><a href="#全字段排序" class="headerlink" title="全字段排序"></a>全字段排序</h3><p>前面我们介绍过索引，<strong>所以你现在就很清楚了，为避免全表扫描，我们需要在city字段加上索引。</strong></p>
<p>在city字段上<strong>创建索引之后，我们用explain命令来看看这个语句的执行情况。</strong></p>
<p><img src="/2024/09/11/MySQL16/image-20240911231416888.png" alt="image-20240911231416888"></p>
<p>Extra<strong>这个字段中的“Using filesort”表示的就是需要排序，MySQL会给每个线程分配一块内存用于排序，称为sort_buffer。</strong></p>
<p>为了说明这个SQL查询语句的执行过程，我们先来看一下city这个索引的示意图。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231428669.png" alt="image-20240911231428669"></p>
<p>从图中可以看到，满足city&#x3D;’杭州’条件的行，是从ID_X到ID_(X+N)的这些记录。</p>
<p>通常情况下，这个语句执行流程如下所示 ：</p>
<ol>
<li><strong>初始化sort_buffer，确定放入name、city、age这三个字段；</strong></li>
<li><strong>从索引city找到第一个满足city&#x3D;’杭州’条件的主键id，也就是图中的ID_X；</strong></li>
<li><strong>到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中；</strong></li>
<li>从索引city取下一个记录的主键id；</li>
<li><strong>重复步骤3、4直到city的值不满足查询条件为止，对应的主键id也就是图中的ID_Y；</strong></li>
<li><strong>对sort_buffer中的数据按照字段name做快速排序；</strong></li>
<li><strong>按照排序结果取前1000行返回给客户端。</strong></li>
</ol>
<p>我们暂且把这个排序过程，<strong>称为全字段排序，执行流程的示意图如下所示</strong>，下一篇文章中我们还会用到这个排序。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231444741.png" alt="image-20240911231444741"></p>
<p><strong>图中“按name排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数sort_buffer_size。</strong></p>
<p>sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。<strong>如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。</strong></p>
<p>你可以用下面介绍的方法，来确定一个排序语句是否使用了临时文件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/* 打开optimizer_trace，只对本线程有效 */</span><br><span class="line">SET optimizer_trace=&#x27;enabled=on&#x27;; </span><br><span class="line"></span><br><span class="line">/* @a保存Innodb_rows_read的初始值 */</span><br><span class="line">select VARIABLE_VALUE into @a from  performance_schema.session_status where variable_name = &#x27;Innodb_rows_read&#x27;;</span><br><span class="line"></span><br><span class="line">/* 执行语句 */</span><br><span class="line">select city, name,age from t where city=&#x27;杭州&#x27; order by name limit 1000; </span><br><span class="line"></span><br><span class="line">/* 查看 OPTIMIZER_TRACE 输出 */</span><br><span class="line">SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G</span><br><span class="line"></span><br><span class="line">/* @b保存Innodb_rows_read的当前值 */</span><br><span class="line">select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = &#x27;Innodb_rows_read&#x27;;</span><br><span class="line"></span><br><span class="line">/* 计算Innodb_rows_read差值 */</span><br><span class="line">select @b-@a;</span><br></pre></td></tr></table></figure>

<p><strong>这个方法是通过查看 OPTIMIZER_TRACE 的结果来确认的，你可以从 number_of_tmp_files中看到是否使用了临时文件。</strong></p>
<p><img src="/2024/09/11/MySQL16/image-20240911231514278.png" alt="image-20240911231514278"></p>
<p>number_of_tmp_files表示的是，<strong>排序过程中使用的临时文件数。你一定奇怪，为什么需要12个文件？内存放不下时，就需要使用外部排序，外部排序一般使用归并排序算法</strong>。可以这么简单理解，<strong>MySQL将需要排序的数据分成12份，每一份单独排序后存在这些临时文件中。然后把这12个有序文件再合并成一个有序的大文件。</strong></p>
<p><strong>如果sort_buffer_size超过了需要排序的数据量的大小，number_of_tmp_files就是0，表示排序可以直接在内存中完成。</strong></p>
<p><strong>否则就需要放在临时文件中排序</strong>。<strong>sort_buffer_size越小，需要分成的份数越多，number_of_tmp_files的值就越大</strong>。</p>
<p>接下来，我再和你解释一下上图中其他两个值的意思。</p>
<p><strong>我们的示例表中有4000条满足city&#x3D;’杭州’的记录，所以你可以看到 examined_rows&#x3D;4000，表示参与排序的行数是4000行。</strong></p>
<p><strong>sort_mode 里面的packed_additional_fields的意思是，排序过程对字符串做了“紧凑”处理。即使name字段的定义是varchar(16)，在排序过程中还是要按照实际长度来分配空间的。</strong></p>
<p>同时，最后一个查询语句select @b-@a 的返回结果是4000，表示整个执行过程只扫描了4000行。</p>
<p>这里需要注意的是，为了避免对结论造成干扰，我把internal_tmp_disk_storage_engine设置成MyISAM。否则，select @b-@a的结果会显示为4001。</p>
<p>这是因为查询OPTIMIZER_TRACE这个表时，需要用到临时表，而internal_tmp_disk_storage_engine的默认值是InnoDB。如果使用的是InnoDB引擎的话，把数据从临时表取出来的时候，会让Innodb_rows_read的值加1。</p>
<h3 id="rowid排序"><a href="#rowid排序" class="headerlink" title="rowid排序"></a>rowid排序</h3><p>在上面这个算法过程里面，<strong>只对原表的数据读了一遍，剩下的操作都是在sort_buffer和临时文件中执行的</strong>。但这个算法有一个问题，<strong>就是如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差</strong>。</p>
<p>所以如果<strong>单行很大</strong>，这个方法效率不够好。</p>
<p>那么，<strong>如果MySQL认为排序的单行长度太大会怎么做呢？</strong></p>
<p>接下来，我来修改一个参数，让MySQL采用另外一种算法。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SET max_length_for_sort_data = 16;</span><br></pre></td></tr></table></figure>

<p>max_length_for_sort_data，<strong>是MySQL中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL就认为单行太大，要换一个算法</strong>。</p>
<p>city、name、age 这三个字段的定义总长度是36，我把max_length_for_sort_data设置为16，我们再来看看计算过程有什么改变。</p>
<p>新的算法放入sort_buffer的字段，<strong>只有要排序的列（即name字段）和主键id。</strong></p>
<p><strong>但这时，排序的结果就因为少了city和age字段的值，不能直接返回了，整个执行流程就变成如下所示的样子：</strong></p>
<ol>
<li><strong>初始化sort_buffer，确定放入两个字段，即name和id；</strong></li>
<li><strong>从索引city找到第一个满足city&#x3D;’杭州’条件的主键id，也就是图中的ID_X；</strong></li>
<li><strong>到主键id索引取出整行，取name、id这两个字段，存入sort_buffer中；</strong></li>
<li>从索引city取下一个记录的主键id；</li>
<li><strong>重复步骤3、4直到不满足city&#x3D;’杭州’条件为止，也就是图中的ID_Y；</strong></li>
<li><strong>对sort_buffer中的数据按照字段name进行排序；</strong></li>
<li><strong>遍历排序结果，取前1000行，并按照id的值回到原表中取出city、name和age三个字段返回给客户端。</strong></li>
</ol>
<p>这个执行流程的示意图如下，我把它称为rowid排序。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231614916.png" alt="image-20240911231614916"></p>
<p>对比图3的全字段排序流程图你会发现，<strong>rowid排序多访问了一次表t的主键索引</strong>，就是步骤7。</p>
<p>需要说明的是，<strong>最后的“结果集”是一个逻辑概念，实际上MySQL服务端从排序后的sort_buffer中依次取出id，然后到原表查到city、name和age这三个字段的结果，不需要在服务端再耗费内存存储结果，是直接返回给客户端的</strong>。</p>
<p>根据这个说明过程和图示，你可以想一下，这个时候执行select @b-@a，结果会是多少呢？</p>
<p>现在，我们就来看看结果有什么不同。</p>
<p>首先，图中的examined_rows的值还是4000，表示用于排序的数据是4000行。但是select @b-@a这个语句的值变成5000了。</p>
<p>因为这时候除了排序过程外，在排序完成后，<strong>还要根据id去原表取值。由于语句是limit 1000，因此会多读1000行</strong>。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231627370.png" alt="image-20240911231627370"></p>
<p>从OPTIMIZER_TRACE的结果中，你还能看到另外两个信息也变了。</p>
<ul>
<li>sort_mode变成了&lt;sort_key, rowid&gt;<strong>，表示参与排序的只有name和id这两个字段。</strong></li>
<li>number_of_tmp_files变成10了，<strong>是因为这时候参与排序的行数虽然仍然是4000行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了。</strong></li>
</ul>
<h3 id="全字段排序-VS-rowid排序"><a href="#全字段排序-VS-rowid排序" class="headerlink" title="全字段排序 VS rowid排序"></a>全字段排序 VS rowid排序</h3><p>我们来分析一下，从这两个执行流程里，还能得出什么结论。</p>
<p><strong>如果MySQL实在是担心排序内存太小，会影响排序效率，才会采用rowid排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。</strong></p>
<p>如果MySQL认为内存足够大，<strong>会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。</strong></p>
<p>这也就体现了MySQL的一个设计思想：<strong>如果内存够，就要多利用内存，尽量减少磁盘访问。</strong></p>
<p>对于InnoDB表来说，<strong>rowid排序会要求回表多造成磁盘读，因此不会被优先选择。</strong></p>
<p>这个结论看上去有点废话的感觉，但是你要记住它，下一篇文章我们就会用到。</p>
<p>看到这里，你就了解了，<strong>MySQL做排序是一个成本比较高的操作。那么你会问，是不是所有的order by都需要排序操作呢？如果不排序就能得到正确的结果，那对系统的消耗会小很多，语句的执行时间也会变得更短。</strong></p>
<p>其实，<strong>并不是所有的order by语句，都需要排序操作的</strong>。从上面分析的执行过程，我们可以看到，MySQL之所以需要生成临时表，并且在临时表上做排序操作，<strong>其原因是原来的数据都是无序的。</strong></p>
<p>你可以设想下，<strong>如果能够保证从city这个索引上取出来的行，天然就是按照name递增排序的话，是不是就可以不用再排序了呢？</strong></p>
<p>确实是这样的。</p>
<p>所以，我们可以在这个市民表上<strong>创建一个city和name的联合索引</strong>，对应的SQL语句是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table t add index city_user(city, name);</span><br></pre></td></tr></table></figure>

<p>作为与city索引的对比，我们来看看这个索引的示意图。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231703196.png" alt="image-20240911231703196"></p>
<p>在这个索引里面，<strong>我们依然可以用树搜索的方式定位到第一个满足city&#x3D;’杭州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要city的值是杭州，name的值就一定是有序的。</strong></p>
<p>这样整个查询过程的流程就变成了：</p>
<ol>
<li><strong>从索引(city,name)找到第一个满足city&#x3D;’杭州’条件的主键id；</strong></li>
<li><strong>到主键id索引取出整行</strong>，取name、city、age三个字段的值，<strong>作为结果集的一部分直接返回</strong>；</li>
<li>从索引(city,name)<strong>取下一个记录主键id；</strong></li>
<li><strong>重复步骤2、3，直到查到第1000条记录，或者是不满足city&#x3D;’杭州’条件时循环结束。</strong></li>
</ol>
<p><img src="/2024/09/11/MySQL16/image-20240911231714138.png" alt="image-20240911231714138"></p>
<p>可以看到，这个查询过程<strong>不需要临时表，也不需要排序</strong>。接下来，我们用explain的结果来印证一下。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231726013.png" alt="image-20240911231726013"></p>
<p>从图中可以看到，Extra字段中<strong>没有Using filesort</strong>了，也就是<strong>不需要排序了</strong>。<strong>而且由于(city,name)这个联合索引本身有序，所以这个查询也不用把4000行全都读一遍，只要找到满足条件的前1000条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描1000次。</strong></p>
<p>既然说到这里了，我们再往前讨论，<strong>这个语句的执行流程有没有可能进一步简化呢？</strong>不知道你还记不记得，我在第5篇文章<a href="https://time.geekbang.org/column/article/69636">《 深入浅出索引（下）》</a>中，和你介绍的<strong>覆盖索引</strong>。</p>
<p>这里我们可以再稍微复习一下。<strong>覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。</strong></p>
<p>按照覆盖索引的概念，我们可以<strong>再优化一下这个查询语句的执行流程。</strong></p>
<p>针对这个查询，我们可以创建一个<strong>city、name和age的联合索引</strong>，对应的SQL语句就是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table t add index city_user_age(city, name, age);</span><br></pre></td></tr></table></figure>

<p>这时，对于city字段的值相同的行来说，还是按照name字段的值递增排序的，此时的查询语句也就不再需要排序了。这样整个查询语句的执行流程就变成了：</p>
<ol>
<li><strong>从索引(city,name,age)找到第一个满足city&#x3D;’杭州’条件的记录，取出其中的city、name和age这三个字段的值，作为结果集的一部分直接返回；</strong></li>
<li><strong>从索引(city,name,age)取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回；</strong></li>
<li><strong>重复执行步骤2，直到查到第1000条记录，或者是不满足city&#x3D;’杭州’条件时循环结束。</strong></li>
</ol>
<p><img src="/2024/09/11/MySQL16/image-20240911231747107.png" alt="image-20240911231747107"></p>
<p>然后，我们再来看看explain的结果。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231800250.png" alt="image-20240911231800250"></p>
<p>可以看到，<strong>Extra字段里面多了“Using index”，表示的就是使用了覆盖索引，性能上会快很多</strong>。</p>
<p>当然，<strong>这里并不是说要为了每个查询能用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价的。这是一个需要权衡的决定。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天这篇文章，我和你介绍了MySQL里面<strong>order by语句的几种算法流程</strong>。</p>
<p>在开发系统的时候，<strong>你总是不可避免地会使用到order by语句。你心里要清楚每个语句的排序逻辑是怎么实现的，还要能够分析出在最坏情况下，每个语句的执行对系统资源的消耗，这样才能做到下笔如有神，不犯低级错误</strong>。</p>
<p>假设你的表里面已经有了city_name(city, name)这个联合索引，然后你要查杭州和苏州两个城市中所有的市民的姓名，并且按名字排序，显示前100条记录。如果SQL查询语句是这么写的 ：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t where city in (&#x27;杭州&#x27;,&quot;苏州&quot;) order by name limit 100;</span><br></pre></td></tr></table></figure>

<p>那么，这个语句执行的时候会有排序过程吗，为什么？</p>
<p>如果业务端代码由你来开发，需要实现一个在数据库端不需要排序的方案，你会怎么实现呢？</p>
<p>进一步地，如果有分页需求，要显示第101页，也就是说语句最后要改成 “limit 10000,100”， 你的实现方法又会是什么呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期的问题是，<strong>当MySQL去更新一行，但是要修改的值跟原来的值是相同的，这时候MySQL会真的去执行一次修改吗？还是看到值相同就直接返回呢？</strong></p>
<p>第一个选项是，<strong>MySQL读出数据，发现值与原来相同，不更新，直接返回，执行结束</strong>。这里我们可以用一个锁实验来确认。</p>
<p>假设，当前表t里的值是(1,2)。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231831064.png" alt="image-20240911231831064"></p>
<p>session B的update 语句被blocked了，加锁这个动作是InnoDB才能做的，所以排除选项1。</p>
<p>第二个选项是，<strong>MySQL调用了InnoDB引擎提供的接口，但是引擎发现值与原来相同，不更新，直接返回</strong>。有没有这种可能呢？这里我用一个可见性实验来确认。</p>
<p>假设当前表里的值是(1,2)。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231841641.png" alt="image-20240911231841641"></p>
<p>session A的第二个select 语句是一致性读（快照读)，它是不能看见session B的更新的。</p>
<p>现在它返回的是(1,3)，表示它看见了某个新的版本，这个版本只能是session A自己的update语句做更新的时候生成。（如果你对这个逻辑有疑惑的话，可以回顾下第8篇文章<a href="https://time.geekbang.org/column/article/70562">《事务到底是隔离的还是不隔离的？》</a>中的相关内容）</p>
<p>所以，我们上期思考题的答案应该是选项3，即：<strong>InnoDB认真执行了“把这个值修改成(1,2)”这个操作，该加锁的加锁，该更新的更新。</strong></p>
<p>然后你会说，<strong>MySQL怎么这么笨，就不会更新前判断一下值是不是相同吗？如果判断一下，不就不用浪费InnoDB操作，多去更新一次了？</strong></p>
<p>其实MySQL是确认了的。<strong>只是在这个语句里面，MySQL认为读出来的值，只有一个确定的 (id&#x3D;1), 而要写的是(a&#x3D;3)，只从这两个信息是看不出来“不需要修改”的。</strong></p>
<p>作为验证，你可以看一下下面这个例子。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231857418.png" alt="image-20240911231857418"></p>
<p><strong>补充说明：</strong></p>
<p>上面我们的验证结果都是在binlog_format&#x3D;statement格式下进行的。</p>
<p>如果是binlog_format&#x3D;row 并且binlog_row_image&#x3D;FULL的时候，由于MySQL需要在binlog里面记录所有的字段，所以在读数据的时候就会把所有数据都读出来了。</p>
<p>根据上面说的规则，“既然读了数据，就会判断”， 因此在这时候，select * from t where id&#x3D;1，结果就是“返回 (1,2)”。</p>
<p>同理，如果是binlog_row_image&#x3D;NOBLOB, 会读出除blob 外的所有字段，在我们这个例子里，结果还是“返回 (1,2)”。</p>
<p>对应的代码如图15所示。这是MySQL 5.6版本引入的，在此之前我没有看过。所以，特此说明。</p>
<p><img src="/2024/09/11/MySQL16/image-20240911231910743.png" alt="image-20240911231910743"></p>
<p>如果表中有timestamp字段而且设置了自动更新的话，那么更新“别的字段”的时候，MySQL会读入所有涉及的字段，这样通过判断，就会发现不需要修改。</p>
<p>这两个点我会在后面讲更新性能的文章中再展开。</p>
<blockquote>
<p>锁验证法； </p>
<p>结论是这样的： </p>
<p>第一，hexdump看出来没改应该是WAL机制生效了，要过一会儿，或者把库shutdown看看。</p>
<p> 第二，binlog没写是MySQL Server层知道行的值没变，所以故意不写的，这个是在row格式下的策略。你可以把binlog_format 改成statement再验证下。</p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/16-order-by-shi-zen-mo-gong-zuo-de">16 “order by”是怎么工作的？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么我只查一行的语句，也执行这么慢？</title>
    <url>/2024/09/12/MySQL19/</url>
    <content><![CDATA[<p>一般情况下，<strong>如果我跟你说查询性能优化，你首先会想到一些复杂的语句，想到查询需要返回大量的数据。但有些情况下，“查一行”，也会执行得特别慢</strong>。今天，我就跟你聊聊这个有趣的话题，看看什么情况下，会出现这个现象。</p>
<p>需要说明的是，<strong>如果MySQL数据库本身就有很大的压力，导致数据库服务器CPU占用率很高或ioutil（IO利用率）很高，这种情况下所有语句的执行都有可能变慢</strong>，不属于我们今天的讨论范围。</p>
<p>为了便于描述，我还是构造一个表，基于这个表来说明今天的问题。这个表有两个字段id和c，并且我在里面插入了10万行记录。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE `t` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `c` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB;</span><br><span class="line"></span><br><span class="line">delimiter ;;</span><br><span class="line">create procedure idata()</span><br><span class="line">begin</span><br><span class="line">  declare i int;</span><br><span class="line">  set i=1;</span><br><span class="line">  while(i&lt;=100000)do</span><br><span class="line">    insert into t values(i,i);</span><br><span class="line">    set i=i+1;</span><br><span class="line">  end while;</span><br><span class="line">end;;</span><br><span class="line">delimiter ;</span><br><span class="line"></span><br><span class="line">call idata();</span><br></pre></td></tr></table></figure>

<p>接下来，我会用几个不同的场景来举例，有些是前面的文章中我们已经介绍过的知识点，你看看能不能一眼看穿，来检验一下吧。</p>
<h3 id="第一类：查询长时间不返回"><a href="#第一类：查询长时间不返回" class="headerlink" title="第一类：查询长时间不返回"></a>第一类：查询长时间不返回</h3><p>如图1所示，在表t执行下面的SQL语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t where id=1;</span><br></pre></td></tr></table></figure>

<p>查询结果长时间不返回。</p>
<p><img src="/2024/09/12/MySQL19/image-20240912084741861.png" alt="image-20240912084741861"></p>
<p><strong>一般碰到这种情况的话，大概率是表t被锁住了。接下来分析原因的时候，一般都是首先执行一下show processlist命令，看看当前语句处于什么状态。</strong></p>
<p>对于不太了解的可以看看这几篇文章：</p>
<ul>
<li><p><a href="https://blog.csdn.net/chen_jimo_c/article/details/104921212">MySQL中 show processlist命令详解_mysql show processlist-CSDN博客</a></p>
<p>show processlist：通过查看mysql的官网，可以发现，<strong>其主要是查询数据库中哪些线程正在执行，针对比较慢的线程（time的数值比较大的线程）我们可以将其kill掉</strong>。此外，show full processlist 返回的结果是实时变化的。</p>
</li>
<li><p><a href="https://www.cnblogs.com/libaiyun/p/16462461.html">mysql之show processlist详解 - 云白Li - 博客园 (cnblogs.com)</a></p>
<p>这个讲的挺详细的，可以好好看看。</p>
</li>
</ul>
<p>然后我们再针对每种状态，去分析它们产生的原因、如何复现，以及如何处理。</p>
<h3 id="等MDL锁"><a href="#等MDL锁" class="headerlink" title="等MDL锁"></a>等MDL锁</h3><p>如图2所示，就是使用show processlist命令查看Waiting for table metadata lock的示意图。</p>
<p><img src="/2024/09/12/MySQL19/image-20240912084802650.png" alt="image-20240912084802650"></p>
<p>出现<strong>这个状态表示的是，现在有一个线程正在表t上请求或者持有MDL写锁，把select语句堵住了。</strong></p>
<p>在第6篇文章<a href="https://time.geekbang.org/column/article/69862">《全局锁和表锁 ：给表加个字段怎么有这么多阻碍？》</a>中，我给你介绍过一种复现方法。<strong>但需要说明的是，那个复现过程是基于MySQL 5.6版本的。而MySQL 5.7版本修改了MDL的加锁策略，所以就不能复现这个场景了。</strong></p>
<p>不过，在MySQL 5.7版本下复现这个场景，也很容易。如图3所示，我给出了简单的复现步骤。</p>
<p><img src="/2024/09/12/MySQL19/image-20240912084821808.png" alt="image-20240912084821808"></p>
<p>session A 通过lock table命令持有表t的MDL写锁，而session B的查询需要获取MDL读锁。<strong>所以，session B进入等待状态。</strong></p>
<p>这类问题的处理方式，<strong>就是找到谁持有MDL写锁，然后把它kill掉。</strong></p>
<p>但是，由于在show processlist的结果里面，session A的Command列是“Sleep”，导致查找起来很不方便。不过有了performance_schema和sys系统库以后，就方便多了。<strong>（MySQL启动时需要设置performance_schema&#x3D;on，相比于设置为off会有10%左右的性能损失)</strong></p>
<p>通过查询sys.schema_table_lock_waits这张表，<strong>我们就可以直接找出造成阻塞的process id，把这个连接用kill 命令断开即可。</strong></p>
<p><img src="/2024/09/12/MySQL19/image-20240912084834562.png" alt="image-20240912084834562"></p>
<h3 id="等flush"><a href="#等flush" class="headerlink" title="等flush"></a>等flush</h3><p>接下来，我给你举另外一种查询被堵住的情况。</p>
<p>我在表t上，执行下面的SQL语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from information_schema.processlist where id=1;</span><br></pre></td></tr></table></figure>

<p>这里，我先卖个关子。</p>
<p>你可以看一下图5。我查出来这个线程的状态是Waiting for table flush，你可以设想一下这是什么原因。</p>
<p><img src="/2024/09/12/MySQL19/image-20240912090014801.png" alt="image-20240912090014801"></p>
<p>这个状态表示的是，<strong>现在有一个线程正要对表t做flush操作。MySQL里面对表做flush操作的用法，一般有以下两个：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">flush tables t with read lock;</span><br><span class="line"></span><br><span class="line">flush tables with read lock;</span><br></pre></td></tr></table></figure>

<p><strong>这两个flush语句，如果指定表t的话，代表的是只关闭表t；如果没有指定具体的表名，则表示关闭MySQL里所有打开的表。</strong></p>
<p>但是正常这<strong>两个语句执行起来都很快，除非它们也被别的线程堵住了。</strong></p>
<p>所以，出现Waiting for table flush状态的可能情况是：<strong>有一个flush tables命令被别的语句堵住了，然后它又堵住了我们的select语句。</strong></p>
<p>现在，我们一起来复现一下这种情况，<strong>复现步骤</strong>如图6所示：</p>
<p><img src="/2024/09/12/MySQL19/image-20240912090103176.png" alt="image-20240912090103176"></p>
<p>在session A中，<strong>我故意每行都调用一次sleep(1)，这样这个语句默认要执行10万秒，在这期间表t一直是被session A“打开”着。然后，session B的flush tables t命令再要去关闭表t，就需要等session A的查询结束。这样，session C要再次查询的话，就会被flush 命令堵住了。</strong></p>
<p>图7是这个复现步骤的show processlist结果。这个例子的排查也很简单，<strong>你看到这个show processlist的结果，肯定就知道应该怎么做了。</strong></p>
<p><img src="/2024/09/12/MySQL19/image-20240912090115314.png" alt="image-20240912090115314"></p>
<h3 id="等行锁"><a href="#等行锁" class="headerlink" title="等行锁"></a>等行锁</h3><p>现在，<strong>经过了表级锁的考验，我们的select 语句终于来到引擎里了。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t where id=1 lock in share mode; </span><br></pre></td></tr></table></figure>

<p>上面这条语句的用法你也很熟悉了，我们在第8篇<a href="https://time.geekbang.org/column/article/70562">《事务到底是隔离的还是不隔离的？》</a>文章介绍当前读时提到过。</p>
<p>由于访问id&#x3D;1这个记录时要<strong>加读锁，如果这时候已经有一个事务在这行记录上持有一个写锁，我们的select语句就会被堵住。</strong></p>
<p>复现步骤和现场如下：</p>
<p><img src="/2024/09/12/MySQL19/image-20240912090144226.png" alt="image-20240912090144226"></p>
<p><img src="/2024/09/12/MySQL19/image-20240912090158681.png" alt="image-20240912090158681"></p>
<p>显然，session A启动了事务，<strong>占有写锁，还不提交，是导致session B被堵住的原因。</strong></p>
<p>这个问题并不难分析，但问题是怎么查出是谁占着这个写锁。如果你用的是MySQL 5.7版本，可以通过sys.innodb_lock_waits 表查到。</p>
<p>查询方法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t sys.innodb_lock_waits where locked_table=`&#x27;test&#x27;.&#x27;t&#x27;`\G</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/12/MySQL19/image-20240912090236152.png" alt="image-20240912090236152"></p>
<p>可以看到，这个信息很全，4号线程是造成堵塞的罪魁祸首。而干掉这个罪魁祸首的方式，就是KILL QUERY 4或KILL 4。</p>
<p>不过，<strong>这里不应该显示“KILL QUERY 4”。这个命令表示停止4号线程当前正在执行的语句，而这个方法其实是没有用的。因为占有行锁的是update语句，这个语句已经是之前执行完成了的，现在执行KILL QUERY，无法让这个事务去掉id&#x3D;1上的行锁。</strong></p>
<p>实际上，<strong>KILL 4才有效，也就是说直接断开这个连接。这里隐含的一个逻辑就是，连接被断开的时候，会自动回滚这个连接里面正在执行的线程</strong>，也就<strong>释放了id&#x3D;1上的行锁</strong>。</p>
<h3 id="第二类：查询慢"><a href="#第二类：查询慢" class="headerlink" title="第二类：查询慢"></a>第二类：查询慢</h3><p>经过了重重封“锁”，我们再来看看一些查询慢的例子。</p>
<p>先来看一条你一定知道原因的SQL语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t where c=50000 limit 1;</span><br></pre></td></tr></table></figure>

<p><strong>由于字段c上没有索引，这个语句只能走id主键顺序扫描，因此需要扫描5万行。</strong></p>
<p>作为确认，你可以看一下慢查询日志。注意，这里为了把所有语句记录到slow log里，<strong>我在连接后先执行了 set long_query_time&#x3D;0，将慢查询日志的时间阈值设置为0。</strong></p>
<p><img src="/2024/09/12/MySQL19/image-20240912090313828.png" alt="image-20240912090313828"></p>
<p>Rows_examined显示扫描了50000行。你可能会说，不是很慢呀，11.5毫秒就返回了，我们线上一般都配置超过1秒才算慢查询。但你要记住：<strong>坏查询不一定是慢查询</strong>。我们这个例子里面只有10万行记录，<strong>数据量大起来的话，执行时间就线性涨上去了</strong>。</p>
<p>扫描行数多，所以执行慢，这个很好理解。</p>
<p>但是接下来，我们再看一个只扫描一行，但是执行很慢的语句。</p>
<p>如图12所示，是这个例子的slow log。可以看到，执行的语句是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t where id=1；</span><br></pre></td></tr></table></figure>

<p>虽然扫描行数是1，但执行时间却长达800毫秒。</p>
<p><img src="/2024/09/12/MySQL19/image-20240912090348548.png" alt="image-20240912090348548"></p>
<p>是不是有点奇怪呢，这些时间都花在哪里了？</p>
<p>如果我把这个slow log的截图再往下拉一点，你可以看到下一个语句，select * from t where id&#x3D;1 lock in share mode，执行时扫描行数也是1行，执行时间是0.2毫秒。</p>
<p><img src="/2024/09/12/MySQL19/image-20240912090404478.png" alt="image-20240912090404478"></p>
<p>看上去是不是更奇怪了？<strong>按理说lock in share mode还要加锁，时间应该更长才对啊。</strong></p>
<p>可能有的同学已经有答案了。如果你还没有答案的话，我再给你一个提示信息，图14是这两个语句的执行输出结果。</p>
<p><img src="/2024/09/12/MySQL19/image-20240912090417710.png" alt="image-20240912090417710"></p>
<p>第一个语句的查询结果里c&#x3D;1，带lock in share mode的语句返回的是c&#x3D;1000001。看到这里应该有更多的同学知道原因了。如果你还是没有头绪的话，也别着急。我先跟你说明一下复现步骤，再分析原因。</p>
<p><img src="/2024/09/12/MySQL19/image-20240912090433278.png" alt="image-20240912090433278"></p>
<p>你看到了，<strong>session A先用start transaction with consistent snapshot命令启动了一个事务，之后session B才开始执行update 语句。</strong></p>
<p>session B执行完100万次update语句后，id&#x3D;1这一行处于什么状态呢？你可以从图16中找到答案。</p>
<p><img src="/2024/09/12/MySQL19/image-20240912090445372.png" alt="image-20240912090445372"></p>
<p><strong>session B更新完100万次，生成了100万个回滚日志(undo log)。</strong></p>
<p>带lock in share mode的SQL语句，<strong>是当前读</strong>，因此会直接读到1000001这个结果，所以速度很快；<strong>而select * from t where id&#x3D;1这个语句，是一致性读，因此需要从1000001开始，依次执行undo log，执行了100万次以后，才将1这个结果返回。</strong></p>
<p>注意，undo log里记录的其实是“把2改成1”，“把3改成2”这样的操作逻辑，画成减1的目的是方便你看图。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天我给你举了在一个简单的表上，执行“查一行”，可能会出现的<strong>被锁住和执行慢</strong>的例子。<strong>这其中涉及到了表锁、行锁和一致性读的概念。</strong></p>
<p>在实际使用中，碰到的场景会更复杂。但大同小异，你可以按照我在文章中介绍的定位方法，来定位并解决问题。</p>
<p>我们在举例加锁读的时候，用的是这个语句，select * from t where id&#x3D;1 lock in share mode。<strong>由于id上有索引，所以可以直接定位到id&#x3D;1这一行，因此读锁也是只加在了这一行上</strong>。</p>
<p>但如果是下面的SQL语句，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">begin;</span><br><span class="line">select * from t where c=5 for update;</span><br><span class="line">commit;</span><br></pre></td></tr></table></figure>

<p>这个语句序列是怎么加锁的呢？加的锁又是什么时候释放呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>在上一篇文章最后，我留给你的问题是，希望你可以分享一下之前碰到过的、与文章中类似的场景。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE `table_a` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `b` varchar(10) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `b` (`b`)</span><br><span class="line">) ENGINE=InnoDB;</span><br></pre></td></tr></table></figure>

<p>假设现在表里面，有100万行数据，其中有10万行数据的b的值是’1234567890’， 假设现在执行语句是这么写的:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from table_a where b=&#x27;1234567890abcd&#x27;;</span><br></pre></td></tr></table></figure>

<p>这时候，MySQL会怎么执行呢？</p>
<p>最理想的情况是，MySQL看到字段b定义的是varchar(10)，那肯定返回空呀。<strong>可惜，MySQL并没有这么做。</strong></p>
<p>那要不，<strong>就是把’1234567890abcd’拿到索引里面去做匹配，肯定也没能够快速判断出索引树b上并没有这个值，也很快就能返回空结果。</strong></p>
<p>但实际上，MySQL也不是这么做的。</p>
<p>这条SQL语句的执行很慢，流程是这样的：</p>
<ol>
<li>在传给引擎执行的时候，<strong>做了字符截断。因为引擎里面这个行只定义了长度是10，所以只截了前10个字节，就是’1234567890’进去做匹配；（这个字符截断很久以前还不太清楚，不过现在倒是清楚了）</strong></li>
<li>这样满足条件的数据有10万行；</li>
<li>因为是select *， <strong>所以要做10万次回表；</strong></li>
<li><strong>但是每次回表以后查出整行，到server层一判断，b的值都不是’1234567890abcd’;</strong></li>
<li>返回结果是空。</li>
</ol>
<p>这个例子，是我们文章内容的一个很好的补充。虽然执行过程中可能经过函数操作，但是最终在拿到结果后，server层还是要做一轮判断的。</p>
<blockquote>
<p>等号顺序问题，<strong>时间上MySQL优化器执行过程中，where 条件部分， a&#x3D;b和 b&#x3D;a的写法是一样的。</strong></p>
<p>相同的模板语句，<strong>但是匹配行数不同，语句执行时间相差很大。这种情况，在语句里面有order by这样的操作时会更明显。</strong></p>
<p>如果id 的类型是整数，<strong>传入的参数类型是字符串的时候，可以用上索引。</strong></p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/19-wei-shi-mo-wo-zhi-cha-yi-hang-de-yu-ju-ye-zhi-hang-zhe-mo-man">19 为什么我只查一行的语句，也执行这么慢？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>如何正确地显示随机消息？</title>
    <url>/2024/09/11/MySQL17/</url>
    <content><![CDATA[<p>我在上一篇文章，为你讲解完<strong>order by语句的几种执行模式后</strong>，就想到了之前一个做英语学习App的朋友碰到过的一个性能问题。今天这篇文章，我就从这个性能问题说起，和你说说MySQL中的另外一种排序需求，希望能够加深你对MySQL排序逻辑的理解。</p>
<p>这个英语学习App首页有一个<strong>随机显示单词的功能，也就是根据每个用户的级别有一个单词表，然后这个用户每次访问首页的时候，都会随机滚动显示三个单词</strong>。他们发现随着单词表变大，<strong>选单词这个逻辑变得越来越慢，甚至影响到了首页的打开速度</strong>。</p>
<p>现在，如果让你来设计这个SQL语句，你会怎么写呢？</p>
<p>为了便于理解，我对这个例子进行了简化：去掉每个级别的用户都有一个对应的单词表这个逻辑，直接就是从一个单词表中随机选出三个单词。这个表的建表语句和初始数据的命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE `words` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `word` varchar(64) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB;</span><br><span class="line"></span><br><span class="line">delimiter ;;</span><br><span class="line">create procedure idata()</span><br><span class="line">begin</span><br><span class="line">  declare i int;</span><br><span class="line">  set i=0;</span><br><span class="line">  while i&lt;10000 do</span><br><span class="line">    insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10))));</span><br><span class="line">    set i=i+1;</span><br><span class="line">  end while;</span><br><span class="line">end;;</span><br><span class="line">delimiter ;</span><br><span class="line"></span><br><span class="line">call idata();</span><br></pre></td></tr></table></figure>

<p><strong>为了便于量化说明，我在这个表里面插入了10000行记录。接下来，我们就一起看看要随机选择3个单词，有什么方法实现，存在什么问题以及如何改进。</strong></p>
<h3 id="内存临时表"><a href="#内存临时表" class="headerlink" title="内存临时表"></a>内存临时表</h3><p>首先，你会想到用**order by rand()**来实现这个逻辑。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select word from words order by rand() limit 3;</span><br></pre></td></tr></table></figure>

<p>这个语句的意思很直白，<strong>随机排序取前3个。虽然这个SQL语句写法很简单，但执行流程却有点复杂的</strong>。</p>
<p>我们先用explain命令来看看这个语句的执行情况。</p>
<p><img src="/2024/09/11/MySQL17/image-20240911234725834.png" alt="image-20240911234725834"></p>
<p>Extra字段显示<strong>Using temporary，表示的是需要使用临时表；Using filesort，表示的是需要执行排序操作</strong>。</p>
<p>因此这个Extra的意思就是，<strong>需要临时表，并且需要在临时表上排序</strong>。</p>
<p>这里，你可以先回顾一下<a href="https://time.geekbang.org/column/article/73479">上一篇文章</a>中全字段排序和rowid排序的内容。我把上一篇文章的两个流程图贴过来，方便你复习。</p>
<p><img src="/2024/09/11/MySQL17/image-20240911234734962.png" alt="image-20240911234734962"></p>
<p><img src="/2024/09/11/MySQL17/image-20240911234741776.png" alt="image-20240911234741776"></p>
<p>然后，我再问你一个问题，<strong>你觉得对于临时内存表的排序来说，它会选择哪一种算法呢</strong>？回顾一下上一篇文章的一个结论：<strong>对于InnoDB表来说</strong>，<strong>执行全字段排序会减少磁盘访问，因此会被优先选择。</strong></p>
<p>我强调了“InnoDB表”，你肯定想到了，<strong>对于内存表（如 MEMORY 表），回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘</strong>。优化器没有了这一层顾虑，<strong>那么它会优先考虑的，就是用于排序的行越少越好了，所以，MySQL这时就会选择rowid排序。</strong></p>
<p>理解了这个算法选择的逻辑，我们再来看看语句的执行流程。同时，通过今天的这个例子，我们来尝试分析一下语句的扫描行数。</p>
<p>这条语句的执行流程是这样的：</p>
<ol>
<li><strong>创建一个临时表。这个临时表使用的是memory引擎，表里有两个字段，第一个字段是double类型，为了后面描述方便，记为字段R，第二个字段是varchar(64)类型，记为字段W。并且，这个表没有建索引。</strong></li>
<li>从words表中，<strong>按主键顺序取出所有的word值。对于每一个word值，调用rand()函数生成一个大于0小于1的随机小数，并把这个随机小数和word分别存入临时表的R和W字段中，到此，扫描行数是10000。</strong></li>
<li>现在临时表有10000行数据了，<strong>接下来你要在这个没有索引的内存临时表上，按照字段R排序</strong>。</li>
<li><strong>初始化 sort_buffer。sort_buffer中有两个字段，一个是double类型，另一个是整型。</strong></li>
<li>从内存临时表中<strong>一行一行地取出R值和位置信息</strong>（我后面会和你解释这里为什么是“位置信息”），<strong>分别存入sort_buffer中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加10000，变成了20000。</strong></li>
<li>在sort_buffer中根据R的值进行排序。<strong>注意，这个过程没有涉及到表操作，所以不会增加扫描行数</strong>。</li>
<li><strong>排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出word值，返回给客户端</strong>。这个过程中，访问了表的三行数据，总扫描行数变成了20003。</li>
</ol>
<p>接下来，我们通过慢查询日志（slow log）来验证一下我们分析得到的扫描行数是否正确。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Query_time: 0.900376  Lock_time: 0.000347 Rows_sent: 3 Rows_examined: 20003</span><br><span class="line">SET timestamp=1541402277;</span><br><span class="line">select word from words order by rand() limit 3;</span><br></pre></td></tr></table></figure>

<p>其中，Rows_examined：20003就表示这个语句执行过程中扫描了20003行，也就验证了我们分析得出的结论。</p>
<p>这里插一句题外话，在平时学习概念的过程中，你可以经常这样做，<strong>先通过原理分析算出扫描行数，然后再通过查看慢查询日志，来验证自己的结论</strong>。我自己就是经常这么做，这个过程很有趣，分析对了开心，分析错了但是弄清楚了也很开心。</p>
<p>现在，我来把完整的排序执行流程图画出来。</p>
<p><img src="/2024/09/11/MySQL17/image-20240911234819941.png" alt="image-20240911234819941"></p>
<p>图中的pos就是位置信息，你可能会觉得奇怪，这里的“位置信息”是个什么概念？<strong>在上一篇文章中，我们对InnoDB表排序的时候，明明用的还是ID字段。</strong></p>
<p>这时候，我们就要回到一个基本概念：<strong>MySQL的表是用什么方法来定位“一行数据”的。</strong></p>
<p>在前面<a href="https://time.geekbang.org/column/article/69236">第4</a>和<a href="https://time.geekbang.org/column/article/69636">第5</a>篇介绍索引的文章中，有几位同学问到<strong>，如果把一个InnoDB表的主键删掉，是不是就没有主键，就没办法回表了？</strong></p>
<p>其实不是的。<strong>如果你创建的表没有主键，或者把一个表的主键删掉了，那么InnoDB会自己生成一个长度为6字节的rowid来作为主键。（这下看懂了，我记得之前我还了解过这个东西）</strong></p>
<p>这也就是排序模式里面，<strong>rowid名字的来历。实际上它表示的是：每个引擎用来唯一标识数据行的信息</strong>。</p>
<ul>
<li><strong>对于有主键的InnoDB表来说，这个rowid就是主键ID；</strong></li>
<li><strong>对于没有主键的InnoDB表来说，这个rowid就是由系统生成的；</strong></li>
<li>MEMORY引擎<strong>不是</strong>索引组织表。在这个例子里面，你可以认为它<strong>就是一个数组</strong>。因此，<strong>这个rowid其实就是数组的下标。</strong></li>
</ul>
<p>到这里，我来稍微小结一下：<strong>order by rand()使用了内存临时表，内存临时表排序的时候使用了rowid排序方法。</strong></p>
<h3 id="磁盘临时表"><a href="#磁盘临时表" class="headerlink" title="磁盘临时表"></a>磁盘临时表</h3><p>那么，是不是所有的临时表都是内存表呢？</p>
<p><strong>其实不是的。tmp_table_size这个配置限制了内存临时表的大小，默认值是16M。如果临时表大小超过了tmp_table_size，那么内存临时表就会转成磁盘临时表。（这个和Nginx后期配置有点差不多，当时也是这样学的，内存不够就磁盘）</strong></p>
<p>磁盘临时表使用的引擎默认是InnoDB，是由参数internal_tmp_disk_storage_engine控制的。</p>
<p>当使用磁盘临时表的时候，<strong>对应的就是一个没有显式索引的InnoDB表的排序过程</strong>。</p>
<p>为了复现这个过程，我把tmp_table_size设置成1024，把sort_buffer_size设置成 32768, 把 max_length_for_sort_data 设置成16。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set tmp_table_size=1024;</span><br><span class="line">set sort_buffer_size=32768;</span><br><span class="line">set max_length_for_sort_data=16;</span><br><span class="line">/* 打开 optimizer_trace，只对本线程有效 */</span><br><span class="line">SET optimizer_trace=&#x27;enabled=on&#x27;; </span><br><span class="line"></span><br><span class="line">/* 执行语句 */</span><br><span class="line">select word from words order by rand() limit 3;</span><br><span class="line"></span><br><span class="line">/* 查看 OPTIMIZER_TRACE 输出 */</span><br><span class="line">SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/11/MySQL17/image-20240911234847515.png" alt="image-20240911234847515"></p>
<p>然后，我们来看一下这次<strong>OPTIMIZER_TRACE</strong>的结果。</p>
<p>不太懂OPTIMIZER_TRACE的建议看看这篇文章：</p>
<ul>
<li><p><a href="https://blog.csdn.net/lilizhou2008/article/details/107554975">MySQL 调优 | OPTIMIZER_TRACE详解_access type: ref-CSDN博客</a></p>
<p>OPTIMIZER_TRACE是MySQL 5.6引入的一项跟踪功能，<strong>它可以跟踪优化器做出的各种决策（比如访问表的方法、各种开销计算、各种转换等），并将跟踪结果记录到</strong> <code>INFORMATION_SCHEMA.OPTIMIZER_TRACE</code> 表中。</p>
</li>
</ul>
<p>因为将max_length_for_sort_data设置成16，小于word字段的长度定义，<strong>所以我们看到sort_mode里面显示的是rowid排序，这个是符合预期的</strong>，参与排序的是随机值<strong>R字段和rowid字段组成的行</strong>。</p>
<p>这时候你可能心算了一下，发现不对。R字段存放的随机值就8个字节，rowid是6个字节（至于为什么是6字节，就留给你课后思考吧），数据总行数是10000，这样算出来就有140000字节，超过了sort_buffer_size 定义的 32768字节了。但是，number_of_tmp_files的值居然是0，难道不需要用临时文件吗？</p>
<p>这个SQL语句的排序<strong>确实没有用到临时文件</strong>，采用是MySQL 5.6版本引入的一个<strong>新的排序算法，即：优先队列排序算法</strong>。接下来，<strong>我们就看看为什么没有使用临时文件的算法，也就是归并排序算法，而是采用了优先队列排序算法。</strong></p>
<p>其实，我们现在的SQL语句，只需要取R值最小的3个rowid。但是，如果使用归并排序算法的话，<strong>虽然最终也能得到前3个值，但是这个算法结束后，已经将10000行数据都排好序了</strong>。</p>
<p><strong>也就是说，后面的9997行也是有序的了。但，我们的查询并不需要这些数据是有序的。所以，想一下就明白了，这浪费了非常多的计算量。</strong></p>
<p>而优先队列算法，<strong>就可以精确地只得到三个最小值</strong>，执行流程如下：</p>
<ol>
<li><strong>对于这10000个准备排序的(R,rowid)，先取前三行，构造成一个堆；</strong></li>
</ol>
<p>（对数据结构印象模糊的同学，可<strong>以先设想成这是一个由三个元素组成的数组</strong>）</p>
<ol>
<li>取下一个行(R’,rowid’)，跟当前堆里面最大的R比较，如果R’小于R，把这个(R,rowid)从堆中去掉，换成(R’,rowid’)；</li>
<li>重复第2步，直到第10000个(R’,rowid’)完成比较。</li>
</ol>
<p>这里我简单画了一个优先队列排序过程的示意图。</p>
<p><img src="/2024/09/11/MySQL17/image-20240911234907589.png" alt="image-20240911234907589"></p>
<p>图6是模拟6个(R,rowid)行，<strong>通过优先队列排序找到最小的三个R值的行的过程。整个排序过程中，为了最快地拿到当前堆的最大值，总是保持最大值在堆顶，因此这是一个最大堆。</strong></p>
<p>图5的OPTIMIZER_TRACE结果中，filesort_priority_queue_optimization这个部分的chosen&#x3D;true，就表示使用了优先队列排序算法，<strong>这个过程不需要临时文件，因此对应的number_of_tmp_files是0。</strong></p>
<p>这个流程结束后，我们构造的堆里面，就是这个10000行里面R值最小的三行。<strong>然后，依次把它们的rowid取出来，去临时表里面拿到word字段，这个过程就跟上一篇文章的rowid排序的过程一样了。</strong></p>
<p>我们再看一下上面一篇文章的SQL查询语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select city,name,age from t where city=&#x27;杭州&#x27; order by name limit 1000  ;</span><br></pre></td></tr></table></figure>

<p>你可能会问，这里也用到了limit，为什么没用优先队列排序算法呢？原因是，这条SQL语句是limit 1000，如果使用优先队列算法的话，<strong>需要维护的堆的大小就是1000行的(name,rowid)<strong>，</strong>超过了我设置的sort_buffer_size大小</strong>，所以<strong>只能使用归并排序算法</strong>。</p>
<p>总之，不论是使用哪种类型的临时表，<strong>order by rand()这种写法都会让计算过程非常复杂，需要大量的扫描行数，因此排序过程的资源消耗也会很大。</strong></p>
<p>再回到我们文章开头的问题，怎么正确地随机排序呢？</p>
<h3 id="随机排序方法"><a href="#随机排序方法" class="headerlink" title="随机排序方法"></a>随机排序方法</h3><p>我们先把问题简化一下，如果只随机选择1个word值，可以怎么做呢？思路上是这样的：</p>
<ol>
<li><strong>取得这个表的主键id的最大值M和最小值N;</strong></li>
<li>*<em>用随机函数生成一个最大值到最小值之间的数 X &#x3D; (M-N)<em>rand() + N;</em></em></li>
<li><strong>取不小于X的第一个ID的行。</strong></li>
</ol>
<p>我们把这个算法，暂时称作随机算法1。这里，我直接给你贴一下执行语句的序列:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select max(id),min(id) into @M,@N from t ;</span><br><span class="line">set @X= floor((@M-@N+1)*rand() + @N);</span><br><span class="line">select * from t where id &gt;= @X limit 1;</span><br></pre></td></tr></table></figure>

<p>这个方法效率很高，因为取max(<strong>id</strong>)和min(<strong>id</strong>)都是不需要扫描索引的，而第三步的select也可以用索引快速定位，可以认为就只扫描了3行。<strong>但实际上，这个算法本身并不严格满足题目的随机要求，因为ID中间可能有空洞，因此选择不同行的概率不一样，不是真正的随机。</strong></p>
<p>比如你有4个id，<strong>分别是1、2、4、5，如果按照上面的方法，那么取到 id&#x3D;4的这一行的概率是取得其他行概率的两倍</strong>。</p>
<p>如果这四行的id分别是1、2、40000、40001呢？<strong>这个算法基本就能当bug来看待了。</strong></p>
<p>所以，为了得到严格随机的结果，你可以用下面这个流程:</p>
<ol>
<li>取得整个表的<strong>行数</strong>，并记为C。</li>
<li>取得 Y &#x3D; floor(C * rand())。 floor函数在这里的作用，就是取整数部分。</li>
<li>再用limit Y,1 取得一行。</li>
</ol>
<p>我们把这个算法，称为随机算法2。下面这段代码，就是上面流程的执行语句的序列。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select count(*) into @C from t;</span><br><span class="line">set @Y = floor(@C * rand());</span><br><span class="line">set @sql = concat(&quot;select * from t limit &quot;, @Y, &quot;,1&quot;);</span><br><span class="line">prepare stmt from @sql;</span><br><span class="line">execute stmt;</span><br><span class="line">DEALLOCATE prepare stmt;</span><br></pre></td></tr></table></figure>

<p>由于limit 后面的参数不能直接跟变量，所以我在上面的代码中使用了prepare+execute的方法。你也可以把拼接SQL语句的方法写在应用程序中，会更简单些。</p>
<p>这个随机算法2，<strong>解决了算法1里面明显的概率不均匀问题</strong>。</p>
<p>MySQL处理limit Y,1 的做法就是按顺序一个一个地读出来，<strong>丢掉前Y个，然后把下一个记录作为返回结果，因此这一步需要扫描Y+1行。再加上，第一步扫描的C行，总共需要扫描C+Y+1行，执行代价比随机算法1的代价要高。</strong></p>
<p>当然，<strong>随机算法2跟直接order by rand()比起来，执行代价还是小很多的</strong>。</p>
<p>你可能问了，如果按照这个表有10000行来计算的话，C&#x3D;10000，要是随机到比较大的Y值，那扫描行数也跟20000差不多了，接近order by rand()的扫描行数，为什么说随机算法2的代价要小很多呢？我就把这个问题留给你去课后思考吧。</p>
<p>现在，我们再看看，如果我们按照随机算法2的思路，要随机取3个word值呢？你可以这么做：</p>
<ol>
<li>取得整个表的行数，记为C；</li>
<li><strong>根据相同的随机方法得到Y1、Y2、Y3；</strong></li>
<li><strong>再执行三个limit Y, 1语句得到三行数据。</strong></li>
</ol>
<p><strong>我们把这个算法，称作随机算法3。下面这段代码，就是上面流程的执行语句的序列。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select count(*) into @C from t;</span><br><span class="line">set @Y1 = floor(@C * rand());</span><br><span class="line">set @Y2 = floor(@C * rand());</span><br><span class="line">set @Y3 = floor(@C * rand());</span><br><span class="line">select * from t limit @Y1，1； //在应用代码里面取Y1、Y2、Y3值，拼出SQL后执行</span><br><span class="line">select * from t limit @Y2，1；</span><br><span class="line">select * from t limit @Y3，1；</span><br></pre></td></tr></table></figure>

<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天这篇文章，我是借着<strong>随机排序的需求，跟你介绍了MySQL对临时表排序的执行过程。</strong></p>
<p>如果你直接使用<strong>order by rand()，这个语句需要Using temporary 和 Using filesort</strong>，查询的执行代价往往是比较大的。<strong>所以，在设计的时候你要量避开这种写法。</strong></p>
<p>今天的例子里面，<strong>我们不是仅仅在数据库内部解决问题，还会让应用代码配合拼接SQL语句</strong>。在实际应用的过程中，比较规范的用法就是：<strong>尽量将业务逻辑写在业务代码中，让数据库只做“读写数据”的事情。因此，这类方法的应用还是比较广泛的。</strong></p>
<p>上面的随机算法3的总扫描行数是 C+(Y1+1)+(Y2+1)+(Y3+1)，实际上它还是可以继续优化，来进一步减少扫描行数的。</p>
<p>我的问题是，<strong>如果你是这个需求的开发人员，你会怎么做，来减少扫描行数呢？说说你的方案，并说明你的方案需要的扫描行数。</strong></p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我在上一篇文章最后留给你的问题是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t where city in (“杭州”,&quot; 苏州 &quot;) order by name limit 100;</span><br></pre></td></tr></table></figure>

<p>这个SQL语句是否需要排序？有什么方案可以避免排序？</p>
<p>虽然有(city,name)联合索引，对于单个city内部，name是递增的。<strong>但是由于这条SQL语句不是要单独地查一个city的值，而是同时查了”杭州”和” 苏州 “两个城市，因此所有满足条件的name就不是递增的了。也就是说，这条SQL语句需要排序。</strong></p>
<p>那怎么避免排序呢？</p>
<p>这里，<strong>我们要用到(city,name)联合索引</strong>的特性，把这一条语句拆成两条语句，执行流程如下：</p>
<ol>
<li>执行select * from t where city&#x3D;“杭州” order by name limit 100; 这个语句是不需要排序的，<strong>客户端用一个长度为100的内存数组A保存结果。</strong></li>
<li>执行select * from t where city&#x3D;“苏州” order by name limit 100; 用相同的方法，<strong>假设结果被存进了内存数组B。</strong></li>
<li><strong>现在A和B是两个有序数组，然后你可以用归并排序的思想，得到name最小的前100值，就是我们需要的结果了。（而且还是归并排序的最后一步。两个有序的数组合并为一个有序的数组）</strong></li>
</ol>
<p>如果把这条SQL语句里“limit 100”改成“limit 10000,100”的话，处理方式其实也差不多，即：要把上面的两条语句改成写：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t where city=&quot;杭州&quot; order by name limit 10100; </span><br></pre></td></tr></table></figure>

<p>和</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t where city=&quot;苏州&quot; order by name limit 10100。</span><br></pre></td></tr></table></figure>

<p>这时候数据量较大，可以同时起两个连接一行行读结果，用归并排序算法拿到这两个结果集里，按顺序取第10001~10100的name值，就是需要的结果了。</p>
<p><strong>当然这个方案有一个明显的损失，就是从数据库返回给客户端的数据量变大了。（将获取100条的业务逻辑写在业务代码中，在这里数据库只做“读写数据”的事情）</strong></p>
<p>所以，<strong>如果数据的单行比较大的话</strong>，可以考虑把这两条SQL语句改成下面这种写法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select id,name from t where city=&quot;杭州&quot; order by name limit 10100; </span><br></pre></td></tr></table></figure>

<p>和</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select id,name from t where city=&quot;苏州&quot; order by name limit 10100。</span><br></pre></td></tr></table></figure>

<p>然后，<strong>再用归并排序的方法</strong>取得按name顺序第10001~10100的name、id的值，<strong>然后拿着这100个id到数据库中去查出所有记录。</strong></p>
<p>上面这些方法，需要你根据性能需求和开发的复杂度做出权衡。</p>
<blockquote>
<p><strong>归并排序，</strong>是我们这个问题解法的核心思想； </p>
<p><strong>“从业务上砍掉功能”，</strong>这个也确实是在业务设计中可以考虑的一个方向； </p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/17-ru-he-zheng-que-di-xian-shi-sui-ji-xiao-xi">17 如何正确地显示随机消息？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么这些SQL语句逻辑相同，性能却差异巨大？</title>
    <url>/2024/09/12/MySQL18/</url>
    <content><![CDATA[<p>在MySQL中，有很多看上去<strong>逻辑相同，但性能却差异巨大的SQL语句</strong>。<strong>对这些语句使用不当的话，就会不经意间导致整个数据库的压力变大</strong>。</p>
<p>我今天挑选了三个这样的案例和你分享。希望再遇到相似的问题时，你可以做到举一反三、快速解决问题。</p>
<h3 id="案例一：条件字段函数操作"><a href="#案例一：条件字段函数操作" class="headerlink" title="案例一：条件字段函数操作"></a>案例一：条件字段函数操作</h3><p>假设你现在维护了一个交易系统，其中交易记录表tradelog包含交易流水号（tradeid）、交易员id（operator）、交易时间（t_modified）等字段。为了便于描述，我们先忽略其他字段。这个表的建表语句如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE `tradelog` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `tradeid` varchar(32) DEFAULT NULL,</span><br><span class="line">  `operator` int(11) DEFAULT NULL,</span><br><span class="line">  `t_modified` datetime DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `tradeid` (`tradeid`),</span><br><span class="line">  KEY `t_modified` (`t_modified`)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;</span><br></pre></td></tr></table></figure>

<p>假设，现在已经记录了从2016年初到2018年底的所有数据，运营部门有一个需求是，<strong>要统计发生在所有年份中7月份的交易记录总数。这个逻辑看上去并不复杂，你的SQL语句可能会这么写：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select count(*) from tradelog where month(t_modified)=7;</span><br></pre></td></tr></table></figure>

<p>由于t_modified字段上有索引，<strong>于是你就很放心地在生产库中执行了这条语句，但却发现执行了特别久，才返回了结果。</strong></p>
<p>如果你问DBA同事为什么会出现这样的情况，他大概会告诉你：<strong>如果对字段做了函数计算，就用不上索引了，这是MySQL的规定。</strong></p>
<p>现在你已经学过了InnoDB的索引结构了，可以再追问一句为什么？为什么条件是where t_modified&#x3D;’2018-7-1’的时候可以用上索引，而改成where month(t_modified)&#x3D;7的时候就不行了？</p>
<p>下面是这个t_modified索引的示意图。方框上面的数字就是month()函数对应的值。</p>
<p><img src="/2024/09/12/MySQL18/image-20240912080302192.png" alt="image-20240912080302192"></p>
<p>如果你的SQL语句条件用的是where t_modified&#x3D;’2018-7-1’的话，引擎就会按照上面绿色箭头的路线，快速定位到 t_modified&#x3D;’2018-7-1’需要的结果。</p>
<p><strong>实际上，B+树提供的这个快速定位能力，来源于同一层兄弟节点的有序性。</strong></p>
<p><strong>但是，如果计算month()函数的话，你会看到传入7的时候，在树的第一层就不知道该怎么办了。（我们建立索引之后日期是有序的，但是月份并不保证有序）</strong></p>
<p>也就是说，<strong>对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。</strong></p>
<p>需要注意的是，优化器并不是要放弃使用这个索引。</p>
<p>在这个例子里，放弃了树搜索功能，优化器可以选择遍历主键索引，也可以选择遍历索引t_modified，<strong>优化器对比索引大小后发现，索引t_modified更小，遍历这个索引比遍历主键索引来得更快。因此最终还是会选择索引t_modified。</strong></p>
<p>接下来，我们使用explain命令，查看一下这条SQL语句的执行结果。</p>
<p><img src="/2024/09/12/MySQL18/image-20240912080317773.png" alt="image-20240912080317773"></p>
<p><strong>key&#x3D;”t_modified”表示的是，使用了t_modified这个索引</strong>；我在测试表数据中插入了10万行数据，rows&#x3D;100335，说明这条语句<strong>扫描了整个索引的所有值</strong>；<strong>Extra字段的Using index，表示的是使用了覆盖索引。</strong></p>
<p>也就是说，由于在t_modified字段加了month()函数操作，导致了全索引扫描。<strong>为了能够用上索引的快速定位能力，我们就要把SQL语句改成基于字段本身的范围查询。按照下面这个写法，优化器就能按照我们预期的，用上t_modified索引的快速定位能力了。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select count(*) from tradelog where</span><br><span class="line">    -&gt; (t_modified &gt;= &#x27;2016-7-1&#x27; and t_modified&lt;&#x27;2016-8-1&#x27;) or</span><br><span class="line">    -&gt; (t_modified &gt;= &#x27;2017-7-1&#x27; and t_modified&lt;&#x27;2017-8-1&#x27;) or </span><br><span class="line">    -&gt; (t_modified &gt;= &#x27;2018-7-1&#x27; and t_modified&lt;&#x27;2018-8-1&#x27;);</span><br></pre></td></tr></table></figure>

<p>当然，如果你的系统上线时间更早，或者后面又插入了之后年份的数据的话，你就需要再把其他年份补齐。</p>
<p>到这里我给你说明了，<strong>由于加了month()函数操作，MySQL无法再使用索引快速定位功能，而只能使用全索引扫描。</strong></p>
<p>不过优化器在个问题上确实有“偷懒”行为，<strong>即使是对于不改变有序性的函数，也不会考虑使用索引。比如，对于select * from tradelog where id + 1 &#x3D; 10000这个SQL语句，这个加1操作并不会改变有序性，但是MySQL优化器还是不能用id索引快速定位到9999这一行。所以，需要你在写SQL语句的时候，手动改写成 where id &#x3D; 10000 -1才可以。（这也和MySQL索引使用的规则有关系。MySQL 的索引优化基于列的值，而不是基于表达式或计算结果。<code>id + 1</code> 是一个计算表达式，MySQL 不能将其直接映射到索引上的值。）</strong></p>
<h3 id="案例二：隐式类型转换"><a href="#案例二：隐式类型转换" class="headerlink" title="案例二：隐式类型转换"></a>案例二：隐式类型转换</h3><p>接下来我再跟你说一说，另一个经常让程序员掉坑里的例子。</p>
<p>我们一起看一下这条SQL语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from tradelog where tradeid=110717;</span><br></pre></td></tr></table></figure>

<p>交易编号tradeid这个字段上，<strong>本来就有索引，但是explain的结果却显示，这条语句需要走全表扫描。你可能也发现了，tradeid的字段类型是varchar(32)，而输入的参数却是整型，所以需要做类型转换。</strong></p>
<p>那么，现在这里就有两个问题：</p>
<ol>
<li>数据类型转换的规则是什么？</li>
<li>为什么有数据类型转换，就需要走全索引扫描？</li>
</ol>
<p>先来看第一个问题，你可能会说，数据库里面类型这么多，这种数据类型转换规则更多，我记不住，应该怎么办呢？</p>
<p>这里有一个简单的方法，看 select “10” &gt; 9的结果：</p>
<ol>
<li><strong>如果规则是“将字符串转成数字”，那么就是做数字比较，结果应该是1；</strong></li>
<li><strong>如果规则是“将数字转成字符串”，那么就是做字符串比较，结果应该是0。</strong></li>
</ol>
<p>验证结果如图3所示。</p>
<p><img src="/2024/09/12/MySQL18/image-20240912080358504.png" alt="image-20240912080358504"></p>
<p>从图中可知，select “10” &gt; 9返回的是1，所以你就能确认MySQL里的转换规则了：<strong>在MySQL中，字符串和数字做比较的话，是将字符串转换成数字（原来在MySQL中也有数据类型转换）</strong>。</p>
<p>这时，你再看这个全表扫描的语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from tradelog where tradeid=110717;</span><br></pre></td></tr></table></figure>

<p>就知道对于优化器来说，这个语句相当于：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from tradelog where  CAST(tradid AS signed int) = 110717;</span><br></pre></td></tr></table></figure>

<p>也就是说，<strong>这条语句触发了我们上面说到的规则：对索引字段做函数操作，优化器会放弃走树搜索功能</strong>。</p>
<p>现在，我留给你一个小问题，id的类型是int，如果执行下面这个语句，是否会导致全表扫描呢？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from tradelog where id=&quot;83126&quot;;</span><br></pre></td></tr></table></figure>

<p>你可以先自己分析一下，再到数据库里面去验证确认。</p>
<p>接下来，我们再来看一个稍微复杂点的例子。</p>
<h3 id="案例三：隐式字符编码转换"><a href="#案例三：隐式字符编码转换" class="headerlink" title="案例三：隐式字符编码转换"></a>案例三：隐式字符编码转换</h3><p>假设系统里还有另外一个表trade_detail，用于记录交易的操作细节。为了便于量化分析和复现，我往交易日志表tradelog和交易详情表trade_detail这两个表里插入一些数据。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE `trade_detail` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `tradeid` varchar(32) DEFAULT NULL,</span><br><span class="line">  `trade_step` int(11) DEFAULT NULL, /*操作步骤*/</span><br><span class="line">  `step_info` varchar(32) DEFAULT NULL, /*步骤信息*/</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `tradeid` (`tradeid`)</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br><span class="line"></span><br><span class="line">insert into tradelog values(1, &#x27;aaaaaaaa&#x27;, 1000, now());</span><br><span class="line">insert into tradelog values(2, &#x27;aaaaaaab&#x27;, 1000, now());</span><br><span class="line">insert into tradelog values(3, &#x27;aaaaaaac&#x27;, 1000, now());</span><br><span class="line"></span><br><span class="line">insert into trade_detail values(1, &#x27;aaaaaaaa&#x27;, 1, &#x27;add&#x27;);</span><br><span class="line">insert into trade_detail values(2, &#x27;aaaaaaaa&#x27;, 2, &#x27;update&#x27;);</span><br><span class="line">insert into trade_detail values(3, &#x27;aaaaaaaa&#x27;, 3, &#x27;commit&#x27;);</span><br><span class="line">insert into trade_detail values(4, &#x27;aaaaaaab&#x27;, 1, &#x27;add&#x27;);</span><br><span class="line">insert into trade_detail values(5, &#x27;aaaaaaab&#x27;, 2, &#x27;update&#x27;);</span><br><span class="line">insert into trade_detail values(6, &#x27;aaaaaaab&#x27;, 3, &#x27;update again&#x27;);</span><br><span class="line">insert into trade_detail values(7, &#x27;aaaaaaab&#x27;, 4, &#x27;commit&#x27;);</span><br><span class="line">insert into trade_detail values(8, &#x27;aaaaaaac&#x27;, 1, &#x27;add&#x27;);</span><br><span class="line">insert into trade_detail values(9, &#x27;aaaaaaac&#x27;, 2, &#x27;update&#x27;);</span><br><span class="line">insert into trade_detail values(10, &#x27;aaaaaaac&#x27;, 3, &#x27;update again&#x27;);</span><br><span class="line">insert into trade_detail values(11, &#x27;aaaaaaac&#x27;, 4, &#x27;commit&#x27;);</span><br></pre></td></tr></table></figure>

<p>这时候，如果要查询id&#x3D;2的交易的所有操作步骤信息，SQL语句可以这么写：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; /*语句Q1*/</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/12/MySQL18/image-20240912080532236.png" alt="image-20240912080532236"></p>
<p>我们一起来看下这个结果：</p>
<ol>
<li>第一行显示优化器会先在交易记录表tradelog上查到id&#x3D;2的行，这个步骤用上了<strong>主键索引</strong>，rows&#x3D;1表示只扫描一行；</li>
<li>第二行key&#x3D;NULL，表示没有用上交易详情表trade_detail上的tradeid索引，进行了全表扫描。</li>
</ol>
<p>在这个执行计划里，<strong>是从tradelog表中取tradeid字段，再去trade_detail表里查询匹配字段。因此，我们把tradelog称为驱动表，把trade_detail称为被驱动表，把tradeid称为关联字段。（这不就是关联表吗？）</strong></p>
<p>接下来，我们看下这个explain结果表示的执行流程：</p>
<p><img src="/2024/09/12/MySQL18/image-20240912080546148.png" alt="image-20240912080546148"></p>
<p>图中：</p>
<ul>
<li>第1步，是根据id在tradelog表里找到L2这一行；</li>
<li>第2步，是从L2中取出tradeid字段的值；</li>
<li>第3步，是根据tradeid值到trade_detail表中查找条件匹配的行。explain的结果里面第二行的key&#x3D;NULL表示的就是，<strong>这个过程是通过遍历主键索引的方式，一个一个地判断tradeid的值是否匹配。</strong></li>
</ul>
<p>进行到这里，你会发现第3步不符合我们的预期。因为表trade_detail里tradeid字段上是有索引的，我们本来是希望通过使用tradeid索引能够快速定位到等值的行。但，这里并没有。</p>
<p>如果你去问DBA同学，他们可能会告诉你，<strong>因为这两个表的字符集不同，一个是utf8，一个是utf8mb4，所以做表连接查询的时候用不上关联字段的索引。这个回答，也是通常你搜索这个问题时会得到的答案。（真的细节）</strong></p>
<p>但是你应该再追问一下，为什么字符集不同就用不上索引呢？</p>
<p>我们说问题是出在执行步骤的第3步，如果单独把这一步改成SQL语句的话，那就是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from trade_detail where tradeid=$L2.tradeid.value; </span><br></pre></td></tr></table></figure>

<p><strong>其中，$L2.tradeid.value的字符集是utf8mb4。</strong></p>
<p>参照前面的两个例子，你肯定就想到了，<strong>字符集utf8mb4是utf8的超集，所以当这两个类型的字符串在做比较的时候，MySQL内部的操作是，先把utf8字符串转成utf8mb4字符集，再做比较。</strong></p>
<blockquote>
<p><strong>这个设定很好理解，utf8mb4是utf8的超集。类似地，在程序设计语言里面，做自动类型转换的时候，为了避免数据在转换过程中由于截断导致数据错误，也都是“按数据长度增加的方向”进行转换的。</strong></p>
</blockquote>
<p>因此， 在执行上面这个语句的时候，<strong>需要将被驱动数据表里的字段一个个地转换成utf8mb4，再跟L2做比较</strong>。</p>
<p>也就是说，实际上这个语句等同于下面这个写法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from trade_detail  where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value; </span><br></pre></td></tr></table></figure>

<p><strong>CONVERT()函数，在这里的意思是把输入的字符串转成utf8mb4字符集。</strong></p>
<p><strong>这就再次触发了我们上面说到的原则：对索引字段做函数操作，优化器会放弃走树搜索功能。</strong></p>
<p>到这里，你终于明确了，<strong>字符集不同只是条件之一</strong>，<strong>连接过程中要求在被驱动表的索引字段上加函数操作</strong>，是直接导致对被驱动表做全表扫描的原因。</p>
<p>作为对比验证，我给你提另外一个需求，“查找trade_detail表里id&#x3D;4的操作，对应的操作者是谁”，再来看下这个语句和它的执行计划。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt;select l.operator from tradelog l , trade_detail d where d.tradeid=l.tradeid and d.id=4;</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/12/MySQL18/image-20240912080737873.png" alt="image-20240912080737873"></p>
<p>这个语句里trade_detail 表成了驱动表，但是explain结果的第二行显示，**这次的查询操作用上了被驱动表tradelog里的索引(tradeid)**，扫描行数是1。</p>
<p>这也是两个tradeid字段的join操作，为什么这次能用上被驱动表的tradeid索引呢？我们来分析一下。</p>
<p>假设驱动表trade_detail里id&#x3D;4的行记为R4，那么在连接的时候（图5的第3步），被驱动表tradelog上执行的就是类似这样的SQL 语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select operator from tradelog  where traideid =$R4.tradeid.value; </span><br></pre></td></tr></table></figure>

<p>这时候**$R4.tradeid.value的字符集是utf8, 按照字符集转换规则，要转成utf8mb4，所以这个过程就被改写成：**</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select operator from tradelog  where traideid =CONVERT($R4.tradeid.value USING utf8mb4); </span><br></pre></td></tr></table></figure>

<p>你看，<strong>这里的CONVERT函数是加在输入参数上的</strong>，这样就可以用上被驱动表的traideid索引。</p>
<p>理解了原理以后，就可以用来指导操作了。如果要优化语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2;</span><br></pre></td></tr></table></figure>

<p>的执行过程，有两种做法：</p>
<ul>
<li><p>比较常见的优化方法是，<strong>把trade_detail表上的tradeid字段的字符集也改成utf8mb4，这样就没有字符集转换的问题了。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table trade_detail modify tradeid varchar(32) CHARACTER SET utf8mb4 default null;</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果能够修改字段的字符集的话，是最好不过了。<strong>但如果数据量比较大， 或者业务上暂时不能做这个DDL的话，那就只能采用修改SQL语句的方法了。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; </span><br></pre></td></tr></table></figure></li>
</ul>
<p><img src="/2024/09/12/MySQL18/image-20240912080945459.png" alt="image-20240912080945459"></p>
<p>这里，<strong>我主动把 l.tradeid转成utf8（把传递过来的参数转化为这张表的字符编码，而不是将整张表都转换为传递过来参数的字符编码）</strong>，就避免了被驱动表上的字符编码转换，从explain结果可以看到，这次索引走对了。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天我给你举了三个例子，其实是在说同一件事儿，即：<strong>对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。</strong></p>
<p>第二个例子是<strong>隐式类型转换，第三个例子是隐式字符编码转换，它们都跟第一个例子一样，因为要求在索引字段上做函数操作而导致了全索引扫描</strong>。</p>
<p>MySQL的优化器确实有“偷懒”的嫌疑，即使简单地把where id+1&#x3D;1000改写成where id&#x3D;1000-1就能够用上索引快速查找，也不会主动做这个语句重写。</p>
<p>因此，<strong>每次你的业务代码升级时，把可能出现的、新的SQL语句explain一下，是一个很好的习惯（这个到时候我们可以具体去分析）</strong>。</p>
<p>你遇到过别的、类似今天我们提到的性能问题吗？你认为原因是什么，又是怎么解决的呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我在上篇文章的最后，留给你的问题是：我们文章中最后的一个方案是，通过三次limit Y,1 来得到需要的数据，你觉得有没有进一步的优化方法。</p>
<p>这里我给出一种方法，<strong>取Y1、Y2和Y3里面最大的一个数，记为M，最小的一个数记为N</strong>，然后执行下面这条SQL语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t limit N, M-N+1;</span><br></pre></td></tr></table></figure>

<p><strong>再加上取整个表总行数的C行，这个方案的扫描行数总共只需要C+M+1行。</strong></p>
<p><strong>当然也可以先取回id值，在应用中确定了三个id值以后，再执行三次where id&#x3D;X的语句也是可以的。</strong></p>
<blockquote>
<p>提出了重新整理的方法</p>
<p>用rowid的方法，是类似的思路，就是让表里面保存一个无空洞的自增值，这样就可以用我们的随机算法1来实现； </p>
<p>拿到第一个值以后，用id迭代往下找的方案，利用了主键索引的有序性。</p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/18-wei-shi-mo-zhe-xie-sql-yu-ju-luo-ji-xiang-tong-xing-neng-que-cha-yi-ju-da">18 为什么这些SQL语句逻辑相同，性能却差异巨大？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL有哪些“饮鸩止渴”提高性能的方法？</title>
    <url>/2024/09/12/MySQL22/</url>
    <content><![CDATA[<p>不知道你在实际运维过程中有没有碰到这样的情景：<strong>业务高峰期，生产环境的MySQL压力太大，没法正常响应，需要短期内、临时性地提升一些性能。</strong></p>
<p>我以前做业务护航的时候，就偶尔会碰上这种场景。用户的开发负责人说，不管你用什么方案，让业务先跑起来再说。</p>
<p>但，如果是无损方案的话，肯定不需要等到这个时候才上场。<strong>今天我们就来聊聊这些临时方案，并着重说一说它们可能存在的风险。</strong></p>
<h3 id="短连接风暴"><a href="#短连接风暴" class="headerlink" title="短连接风暴"></a>短连接风暴</h3><p><strong>正常的短连接模式就是连接到数据库后，执行很少的SQL语句就断开，下次需要的时候再重连。如果使用的是短连接，在业务高峰期的时候，就可能出现连接数突然暴涨的情况。</strong></p>
<p>我在第1篇文章<a href="https://time.geekbang.org/column/article/68319">《基础架构：一条SQL查询语句是如何执行的？》</a>中说过，<strong>MySQL建立连接的过程，成本是很高的。除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的数据读写权限。</strong></p>
<p>在数据库压力比较小的时候，这些额外的成本并不明显。</p>
<p>但是，短连接模型存在一个风险，<strong>就是一旦数据库处理得慢一些，连接数就会暴涨</strong>。max_connections参数，用来控制一个MySQL实例同时存在的连接数的上限，<strong>超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”</strong>。<strong>对于被拒绝连接的请求来说，从业务角度看就是数据库不可用</strong>。</p>
<p><strong>在机器负载比较高的时候，处理现有请求的时间变长，每个连接保持的时间也更长。这时，再有新建连接的话，就可能会超过max_connections的限制。</strong></p>
<p>碰到这种情况时，一个比较自然的想法，<strong>就是调高max_connections的值。但这样做是有风险的</strong>。因为设计max_connections这个参数的目的是想保护MySQL，<strong>如果我们把它改得太大，让更多的连接都可以进来，那么系统的负载可能会进一步加大，大量的资源耗费在权限验证等逻辑上，结果可能是适得其反</strong>，已经连接的线程<strong>拿不到CPU资源去执行业务的SQL请求</strong>。</p>
<p>那么这种情况下，你还有没有别的建议呢？我这里还有两种方法，但要注意，这些方法都是有损的。</p>
<p><strong>第一种方法：先处理掉那些占着连接但是不工作的线程。</strong></p>
<p>max_connections的计算，<strong>不是看谁在running，是只要连着就占用一个计数位置。对于那些不需要保持的连接，我们可以通过kill connection主动踢掉</strong>。这个行为跟事先设置wait_timeout的效果是一样的。<strong>设置wait_timeout参数表示的是，一个线程空闲wait_timeout这么多秒之后，就会被MySQL直接断开连接。</strong></p>
<p>但是需要注意，在show processlist的结果里，<strong>踢掉显示为sleep的线程，可能是有损的。</strong>我们来看下面这个例子。</p>
<p><img src="/2024/09/12/MySQL22/image-20240912115847527.png" alt="image-20240912115847527"></p>
<p>在上面这个例子里，如果断开session A的连接，<strong>因为这时候session A还没有提交</strong>，<strong>所以MySQL只能按照回滚事务来处理；而断开session B的连接，就没什么大影响。所以，如果按照优先级来说，你应该优先断开像session B这样的事务外空闲的连接。</strong></p>
<p>但是，<strong>怎么判断哪些是事务外空闲的呢？session C在T时刻之后的30秒执行show processlist，看到的结果是这样的。</strong></p>
<p><img src="/2024/09/12/MySQL22/image-20240912164135913.png" alt="image-20240912164135913"></p>
<p>图中id&#x3D;4和id&#x3D;5的两个会话都是Sleep 状态。<strong>而要看事务具体状态的话，你可以查information_schema库的innodb_trx表。</strong></p>
<p><img src="/2024/09/12/MySQL22/image-20240912174600352.png" alt="image-20240912174600352"></p>
<p><strong>这个结果里，trx_mysql_thread_id&#x3D;4，表示id&#x3D;4的线程还处在事务中。</strong></p>
<p><strong>因此，如果是连接数过多，你可以优先断开事务外空闲太久的连接；如果这样还不够，再考虑断开事务内空闲太久的连接。</strong></p>
<p>从服务端断开连接使用的是<strong>kill connection + id的命令</strong>， 一个客户端处于sleep状态时，<strong>它的连接被服务端主动断开后，这个客户端并不会马上知道</strong>。<strong>直到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。</strong></p>
<p>从数据库端<strong>主动断开连接可能是有损的，尤其是有的应用端收到这个错误后，不重新连接，而是直接用这个已经不能用的句柄重试查询</strong>。这会导致从应用端看上去，<strong>“MySQL一直没恢复”。</strong></p>
<p>你可能觉得这是一个冷笑话，但实际上我碰到过不下10次。</p>
<p>所以，如果你是一个支持业务的DBA，<strong>不要假设所有的应用代码都会被正确地处理。即使只是一个断开连接的操作，也要确保通知到业务开发团队。</strong></p>
<p><strong>第二种方法：减少连接过程的消耗。</strong></p>
<p>有的业务代码会在短时间内<strong>先大量申请数据库连接做备用，如果现在数据库确认是被连接行为打挂了，那么一种可能的做法，是让数据库跳过权限验证阶段。</strong></p>
<p>跳过权限验证的方法是：<strong>重启数据库，并使用–skip-grant-tables参数启动。这样，整个MySQL会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。</strong></p>
<p>但是，这种方法<strong>特别符合我们标题里说的“饮鸩止渴”</strong>，风险极高，是我特别不建议使用的方案。<strong>尤其你的库外网可访问的话，就更不能这么做了。</strong></p>
<p>在MySQL 8.0版本里，如果你启用–skip-grant-tables参数，MySQL会默认把 –skip-networking参数打开，<strong>表示这时候数据库只能被本地的客户端连接。可见，MySQL官方对skip-grant-tables这个参数的安全问题也很重视。</strong></p>
<p><strong>除了短连接数暴增可能会带来性能问题外，实际上，我们在线上碰到更多的是查询或者更新语句导致的性能问题。</strong>其中，查询问题比较典型的有两类：</p>
<ul>
<li>一类是由<strong>新出现的慢查询</strong>导致的</li>
<li>一类是由<strong>QPS（每秒查询数）突增</strong>导致的</li>
</ul>
<p>而关于更新语句导致的性能问题，我会在下一篇文章和你展开说明。</p>
<h3 id="慢查询性能问题"><a href="#慢查询性能问题" class="headerlink" title="慢查询性能问题"></a>慢查询性能问题</h3><p>在MySQL中，会引发性能问题的慢查询，大体有以下三种可能：</p>
<ol>
<li><strong>索引没有设计好；</strong></li>
<li><strong>SQL语句没写好；</strong></li>
<li><strong>MySQL选错了索引。</strong></li>
</ol>
<p>接下来，我们就具体分析一下这三种可能，以及对应的解决方案。</p>
<p><strong>导致慢查询的第一种可能是，索引没有设计好。</strong></p>
<p>这种场景一般就是通过<strong>紧急创建索引来解决。MySQL 5.6版本以后，创建索引都支持Online DDL了，对于那种高峰期数据库已经被这个语句打挂了的情况，最高效的做法就是直接执行alter table 语句。</strong></p>
<p>比较理想的是能够在备库先执行。假设你现在的服务是一主一备，主库A、备库B，这个方案的大致流程是这样的：</p>
<ol>
<li><strong>在备库B上执行 set sql_log_bin&#x3D;off，也就是不写binlog，然后执行alter table 语句加上索引；</strong></li>
<li>执行主备切换；</li>
<li><strong>这时候主库是B，备库是A。在A上执行 set sql_log_bin&#x3D;off，然后执行alter table 语句加上索引。</strong></li>
</ol>
<p>这是一个“古老”的DDL方案。平时在做变更的时候，你应该考虑类似gh-ost这样的方案，更加稳妥。但是在需要紧急处理时，上面这个方案的效率是最高的。</p>
<p><strong>导致慢查询的第二种可能是，语句没写好。</strong></p>
<p>比如，我们犯了在第18篇文章<a href="https://time.geekbang.org/column/article/74059">《为什么这些SQL语句逻辑相同，性能却差异巨大？》</a>中提到的那些错误，<strong>导致语句没有使用上索引。</strong></p>
<p>这时，我们可以通过改写SQL语句来处理。<strong>MySQL 5.7提供了query_rewrite功能，可以把输入的一种语句改写成另外一种模式。</strong></p>
<p>比如，语句被错误地写成了 select * from t where id + 1 &#x3D; 10000，你可以通过下面的方式，<strong>增加一个语句改写规则。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values (&quot;select * from t where id + 1 = ?&quot;, &quot;select * from t where id = ? - 1&quot;, &quot;db1&quot;);</span><br><span class="line"></span><br><span class="line">call query_rewrite.flush_rewrite_rules();</span><br></pre></td></tr></table></figure>

<p>这里，<strong>call query_rewrite.flush_rewrite_rules()这个存储过程，是让插入的新规则生效，也就是我们说的“查询重写”。你可以用图4中的方法来确认改写规则是否生效。</strong></p>
<p><img src="/2024/09/12/MySQL22/image-20240912174658072.png" alt="image-20240912174658072"></p>
<p><strong>导致慢查询的第三种可能，就是碰上了我们在第10篇文章</strong><a href="https://time.geekbang.org/column/article/71173"><strong>《MySQL为什么有时候会选错索引？》</strong></a><strong>中提到的情况，MySQL选错了索引。</strong></p>
<p>这时候，<strong>应急方案就是给这个语句加上force index。</strong></p>
<p>同样地，使用查询重写功能，<strong>给原来的语句加上force index，也可以解决这个问题。</strong></p>
<p>上面我和你讨论的<strong>由慢查询导致性能问题的三种可能情况，实际上出现最多的是前两种，即：索引没设计好和语句没写好。而这两种情况，恰恰是完全可以避免的。比如，通过下面这个过程，我们就可以预先发现问题。</strong></p>
<ol>
<li>上线前，在测试环境，<strong>把慢查询日志（slow log）打开，并且把long_query_time设置成0，确保每个语句都会被记录入慢查询日志；</strong></li>
<li>在测试表里<strong>插入模拟线上的数据，做一遍回归测试；</strong></li>
<li>观察慢查询日志里每类语句的输出，<strong>特别留意Rows_examined字段是否与预期一致。</strong>（我们在前面文章中已经多次用到过Rows_examined方法了，相信你已经动手尝试过了。如果还有不明白的，欢迎给我留言，我们一起讨论）。</li>
</ol>
<p>不要吝啬这段花在上线前的“额外”时间，<strong>因为这会帮你省下很多故障复盘的时间。</strong></p>
<p>如果新增的SQL语句不多，手动跑一下就可以。<strong>而如果是新项目的话，或者是修改了原有项目的 表结构设计，全量回归测试都是必要的</strong>。这时候，<strong>你需要工具帮你检查所有的SQL语句的返回结果。比如，你可以使用开源工具pt-query-digest</strong>(<a href="https://www.percona.com/doc/percona-toolkit/3.0/pt-query-digest.html)%E3%80%82">https://www.percona.com/doc/percona-toolkit/3.0/pt-query-digest.html)。</a></p>
<h3 id="QPS突增问题"><a href="#QPS突增问题" class="headerlink" title="QPS突增问题"></a>QPS突增问题</h3><p>有时候由于业务突然出现高峰，或者应用程序bug，导致某个语句的QPS突然暴涨，也可能导致MySQL压力过大，影响服务。</p>
<p>我之前碰到过一类情况，是由一个新功能的bug导致的。当然，<strong>最理想的情况是让业务把这个功能下掉，服务自然就会恢复。</strong></p>
<p>而下掉一个功能，<strong>如果从数据库端处理的话，对应于不同的背景，有不同的方法可用。</strong>我这里再和你展开说明一下。</p>
<ol>
<li>一种是由全新业务的bug导致的。假设你的DB运维是比较规范的，也就是说白名单是一个个加的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以从数据库端直接把白名单去掉。</li>
<li>如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后<strong>断开现有连接</strong>。<strong>这样，这个新功能的连接不成功，由它引发的QPS就会变成0。</strong></li>
<li>如果这个新增的功能跟主体功能是部署在一起的，那么我们只能通过处理语句来限制。<strong>这时，我们可以使用上面提到的查询重写功能，把压力最大的SQL语句直接重写成”select 1”返回。</strong></li>
</ol>
<p><strong>当然，这个操作的风险很高，需要你特别细致。它可能存在两个副作用：</strong></p>
<ol>
<li><strong>如果别的功能里面也用到了这个SQL语句模板，会有误伤；</strong></li>
<li>很多业务<strong>并不是靠这一个语句就能完成逻辑的</strong>，所以如果单独把这一个语句以select 1的结果返回的话，<strong>可能会导致后面的业务逻辑一起失败。</strong></li>
</ol>
<p>所以，<strong>方案3是用于止血的，跟前面提到的去掉权限验证一样，应该是你所有选项里优先级最低的一个方案</strong>。</p>
<p>同时你会发现，<strong>其实方案1和2都要依赖于规范的运维体系：虚拟化、白名单机制、业务账号分离。由此可见，更多的准备，往往意味着更稳定的系统。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天这篇文章，我以<strong>业务高峰期的性能问题为背景，和你介绍了一些紧急处理的手段</strong>。</p>
<p>这些处理手段中，<strong>既包括了粗暴地拒绝连接和断开连接，也有通过重写语句来绕过一些坑的方法</strong>；既有临时的高危方案，也有未雨绸缪的、相对安全的预案。</p>
<p>在实际开发中，<strong>我们也要尽量避免一些低效的方法，比如避免大量地使用短连接</strong>。同时，<strong>如果你做业务开发的话，要知道，连接异常断开是常有的事，你的代码里要有正确地重连并重试的机制。</strong></p>
<p>DBA虽然<strong>可以通过语句重写来暂时处理问题，但是这本身是一个风险高的操作，做好SQL审计可以减少需要这类操作的机会。</strong></p>
<p>其实，你可以看得出来，在这篇文章中<strong>我提到的解决方法主要集中在server层</strong>。在下一篇文章中，我会继续和你讨论一些跟InnoDB有关的处理方法。</p>
<p>今天，我留给你的课后问题是，你是否碰到过，在业务高峰期需要临时救火的场景？你又是怎么处理的呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>前两期我给你留的问题是，下面这个图的执行序列中，为什么session B的insert语句会被堵住。</p>
<p>我们用上一篇的加锁规则来分析一下，看看session A的select语句加了哪些锁：</p>
<p><img src="/2024/09/12/MySQL22/image-20240912174720833.png" alt="image-20240912174720833"></p>
<ol>
<li>由于是order by c desc，<strong>第一个要定位的是索引c上“最右边的”c&#x3D;20的行</strong>，所以会加上间隙锁(20,25)和next-key lock (15,20]。</li>
<li>在索引c上向左遍历，要扫描到c&#x3D;10才停下来，所以next-key lock会加到(5,10]，这正是阻塞session B的insert语句的原因。</li>
<li>在扫描过程中，c&#x3D;20、c&#x3D;15、c&#x3D;10这三行都存在值，由于是select *，所以会在主键id上加三个行锁。</li>
</ol>
<p>因此，session A 的select语句锁的范围就是：</p>
<ol>
<li><strong>索引c上 (5, 25)；</strong></li>
<li><strong>主键索引上id&#x3D;15、20两个行锁。</strong></li>
</ol>
<p>这里，我再啰嗦下，<strong>你会发现我在文章中，每次加锁都会说明是加在“哪个索引上”的。因为，锁就是加在索引上的，这是InnoDB的一个基础设定</strong>，需要你在分析问题的时候要一直记得。</p>
<blockquote>
<p>到底是间隙锁还是行锁？其实，这个问题，你要跟“执行过程”配合起来分析。<strong>在InnoDB要去找“第一个值”的时候，是按照等值去找的，用的是等值判断的规则；找到第一个值以后，要在索引内找“下一个值”，对应于我们规则中说的范围查找。</strong></p>
</blockquote>
<blockquote>
<p>要知道最终的加锁是根据实际执行情况来的。所以，如果一个select * from … for update 语句，<strong>优化器决定使用全表扫描，那么就会把主键索引上next-key lock全加上。</strong></p>
</blockquote>
<blockquote>
<p><strong>“有行”才会加行锁。如果查询条件没有命中行，那就加next-key lock。</strong>当然，<strong>等值判断的时候，需要加上优化2（即：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。）。</strong></p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/22-mysql-you-na-xie-yin-zhen-zhi-ke-ti-gao-xing-neng-de-fang-fa">22 MySQL有哪些“饮鸩止渴”提高性能的方法？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么我只改一行的语句，锁这么多？</title>
    <url>/2024/09/12/MySQL21/</url>
    <content><![CDATA[<p>在上一篇文章中，我和你介绍了间隙锁和next-key lock的概念，但是并没有说明加锁规则。间隙锁的概念理解起来确实有点儿难，尤其在配合上行锁以后，很容易在判断是否会出现锁等待的问题上犯错。</p>
<p>所以今天，我们就先从这个加锁规则开始吧。</p>
<p>首先说明一下，这些加锁规则我没在别的地方看到过有类似的总结，以前我自己判断的时候都是想着代码里面的实现来脑补的。这次为了总结成不看代码的同学也能理解的规则，是我又重新刷了代码临时总结出来的。所以，<strong>这个规则有以下两条前提说明：</strong></p>
<ol>
<li>MySQL后面的版本可能会改变加锁策略，所以这个规则只限于截止到现在的最新版本，<strong>即5.x系列&lt;&#x3D;5.7.24，8.0系列 &lt;&#x3D;8.0.13。</strong></li>
<li>如果大家在验证中有发现bad case的话，请提出来，我会再补充进这篇文章，使得一起学习本专栏的所有同学都能受益。</li>
</ol>
<p><strong>因为间隙锁在可重复读隔离级别下才有效，所以本篇文章接下来的描述，若没有特殊说明，默认是可重复读隔离级别。</strong></p>
<p><strong>我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。</strong></p>
<ol>
<li>原则1：<strong>加锁的基本单位是next-key lock。希望你还记得，next-key lock是前开后闭区间。</strong></li>
<li>原则2：<strong>查找过程中访问到的对象才会加锁。</strong></li>
<li>优化1：<strong>索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。</strong></li>
<li>优化2：<strong>索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。</strong></li>
<li>一个bug：<strong>唯一索引上的范围查询会访问到不满足条件的第一个值为止。</strong></li>
</ol>
<p>我还是以上篇文章的表t为例，和你解释一下这些规则。表t的建表语句和初始化语句如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `t` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `c` int(11) DEFAULT NULL,</span><br><span class="line">  `d` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `c` (`c`)</span><br><span class="line">) ENGINE=InnoDB;</span><br><span class="line"></span><br><span class="line">insert into t values(0,0,0),(5,5,5),</span><br><span class="line">(10,10,10),(15,15,15),(20,20,20),(25,25,25);</span><br></pre></td></tr></table></figure>

<p>接下来的例子基本都是配合着图片说明的，所以我建议你可以对照着文稿看，有些例子可能会“毁三观”，也建议你读完文章后亲手实践一下。</p>
<h3 id="案例一：等值查询间隙锁"><a href="#案例一：等值查询间隙锁" class="headerlink" title="案例一：等值查询间隙锁"></a>案例一：等值查询间隙锁</h3><p>第一个例子是关于等值条件操作间隙：</p>
<p><img src="/2024/09/12/MySQL21/image-20240912105649285.png" alt="image-20240912105649285"></p>
<p>由于表t中没有id&#x3D;7的记录，所以用我们上面提到的加锁规则判断一下的话：</p>
<ol>
<li>根据原则1，<strong>加锁单位是next-key lock，session A加锁范围就是(5,10]；</strong></li>
<li>同时根据优化2，这是一个等值查询(id&#x3D;7)，而id&#x3D;10不满足查询条件，<strong>next-key lock退化成间隙锁，因此最终加锁的范围是(5,10)。</strong></li>
</ol>
<p>所以，<strong>session B要往这个间隙里面插入id&#x3D;8的记录会被锁住，但是session C修改id&#x3D;10这行是可以的。</strong></p>
<h3 id="案例二：非唯一索引等值锁"><a href="#案例二：非唯一索引等值锁" class="headerlink" title="案例二：非唯一索引等值锁"></a>案例二：非唯一索引等值锁</h3><p>第二个例子是关于<strong>覆盖索引</strong>上的锁：</p>
<p><img src="/2024/09/12/MySQL21/image-20240912105706095.png" alt="image-20240912105706095"></p>
<p>看到这个例子，你是不是有一种“该锁的不锁，不该锁的乱锁”的感觉？我们来分析一下吧。</p>
<p>这里session A要给索引c上c&#x3D;5的这一行加上读锁。</p>
<ol>
<li>根据原则1，加锁单位是next-key lock，因此会给(0,5]加上next-key lock。</li>
<li><strong>要注意c是普通索引，因此仅访问c&#x3D;5这一条记录是不能马上停下来的，需要向右遍历，查到c&#x3D;10才放弃。根据原则2，访问到的都要加锁，因此要给(5,10]加next-key lock。</strong></li>
<li>但是同时这个符合优化2：<strong>等值判断，向右遍历，最后一个值不满足c&#x3D;5这个等值条件，因此退化成间隙锁(5,10)。</strong></li>
<li>根据原则2 ，<strong>只有访问到的对象才会加锁</strong>，这个查询使用覆盖索引，<strong>并不需要访问主键索引，所以主键索引上没有加任何锁</strong>，这就是为什么session B的update语句可以执行完成。</li>
</ol>
<p>但session C要插入一个(7,7,7)的记录，就会被session A的间隙锁(5,10)锁住。</p>
<p>需要注意，在这个例子中，lock in share mode<strong>只锁覆盖索引，但是如果是for update就不一样了。 执行 for update时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。</strong></p>
<p>这个例子说明，<strong>锁是加在索引上的（这个说的实在是太好了，我看到前面就有这样的感觉了）</strong>；同时，<strong>它给我们的指导是，如果你要用lock in share mode来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。</strong>比如，将session A的查询语句改成select d from t where c&#x3D;5 lock in share mode。你可以自己验证一下效果。</p>
<h3 id="案例三：主键索引范围锁"><a href="#案例三：主键索引范围锁" class="headerlink" title="案例三：主键索引范围锁"></a>案例三：主键索引范围锁</h3><p>第三个例子是关于范围查询的。</p>
<p>举例之前，你可以先思考一下这个问题：对于我们这个表t，下面这两条查询语句，加锁范围相同吗？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; select * from t where id=10 for update;</span><br><span class="line">mysql&gt; select * from t where id&gt;=10 and id&lt;11 for update;</span><br></pre></td></tr></table></figure>

<p>你可能会想，id定义为int类型，这两个语句就是等价的吧？其实，它们并不完全等价。</p>
<p>在逻辑上，这两条查语句肯定是等价的，但是它们的加锁规则不太一样。现在，我们就让session A执行第二个查询语句，来看看加锁效果。</p>
<p><img src="/2024/09/12/MySQL21/image-20240912105728204.png" alt="image-20240912105728204"></p>
<p>现在我们就用前面提到的加锁规则，来分析一下session A 会加什么锁呢？</p>
<ol>
<li><strong>开始执行的时候，要找到第一个id&#x3D;10的行，因此本该是next-key lock(5,10]。 根据优化1， 主键id上的等值条件，退化成行锁，只加了id&#x3D;10这一行的行锁。</strong></li>
<li><strong>范围查找就往后继续找，找到id&#x3D;15这一行停下来，因此需要加next-key lock(10,15]。</strong></li>
</ol>
<p>所以，session A这时候锁的范围就是<strong>主键索引上</strong>，**行锁id&#x3D;10和next-key lock(10,15]**。这样，session B和session C的结果你就能理解了。</p>
<p>这里你需要注意一点，<strong>首次session A定位查找id&#x3D;10的行的时候，是当做等值查询来判断的，而向右扫描到id&#x3D;15的时候，用的是范围查询判断。</strong></p>
<h3 id="案例四：非唯一索引范围锁"><a href="#案例四：非唯一索引范围锁" class="headerlink" title="案例四：非唯一索引范围锁"></a>案例四：非唯一索引范围锁</h3><p>接下来，我们再看两个<strong>范围查询加锁</strong>的例子，你可以对照着案例三来看。</p>
<p>需要注意的是，与案例三不同的是，案例四中查询语句的where部分用的是字段c。</p>
<p><img src="/2024/09/12/MySQL21/image-20240912105742861.png" alt="image-20240912105742861"></p>
<p>这次session A用字段c来判断，加锁规则跟案例三唯一的不同是：<strong>在第一次用c&#x3D;10定位记录的时候，索引c上加了(5,10]这个next-key lock后，由于索引c是非唯一索引，没有优化规则，也就是说不会蜕变为行锁，因此最终sesion A加的锁是，索引c上的(5,10] 和(10,15] 这两个next-key lock。</strong></p>
<p>所以从结果上来看，sesson B要插入（8,8,8)的这个insert语句时就被堵住了。</p>
<p>这里需要扫描到c&#x3D;15才停止扫描，<strong>是合理的，因为InnoDB要扫到c&#x3D;15，才知道不需要继续往后找了。</strong></p>
<h3 id="案例五：唯一索引范围锁bug"><a href="#案例五：唯一索引范围锁bug" class="headerlink" title="案例五：唯一索引范围锁bug"></a>案例五：唯一索引范围锁bug</h3><p>前面的四个案例，我们已经用到了加锁规则中的两个原则和两个优化，接下来再看一个关于加锁规则中bug的案例。</p>
<p><img src="/2024/09/12/MySQL21/image-20240912105756086.png" alt="image-20240912105756086"></p>
<p>session A是一个范围查询，<strong>按照原则1的话，应该是索引id上只加(10,15]这个next-key lock，并且因为id是唯一键，所以循环判断到id&#x3D;15这一行就应该停止了。</strong></p>
<p>但是实现上，<strong>InnoDB会往前扫描到第一个不满足条件的行为止，也就是id&#x3D;20。而且由于这是个范围扫描，因此索引id上的(15,20]这个next-key lock也会被锁上。</strong></p>
<p>所以你看到了，<strong>session B要更新id&#x3D;20这一行，是会被锁住的。同样地，session C要插入id&#x3D;16的一行，也会被锁住。</strong></p>
<p>照理说，这里锁住id&#x3D;20这一行的行为，其实是没有必要的。因为扫描到id&#x3D;15，<strong>就可以确定不用往后再找了。但实现上还是这么做了，因此我认为这是个bug。</strong></p>
<p>我也曾找社区的专家讨论过，<strong>官方bug系统上也有提到，但是并未被verified。所以，认为这是bug这个事儿，也只能算我的一家之言，如果你有其他见解的话，也欢迎你提出来。</strong></p>
<h3 id="案例六：非唯一索引上存在”等值”的例子"><a href="#案例六：非唯一索引上存在”等值”的例子" class="headerlink" title="案例六：非唯一索引上存在”等值”的例子"></a>案例六：非唯一索引上存在”等值”的例子</h3><p>接下来的例子，是为了更好地说明“间隙”这个概念。这里，我给表t插入一条新记录。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; insert into t values(30,10,30);</span><br></pre></td></tr></table></figure>

<p>新插入的这一行c&#x3D;10，也就是说现在表里有两个c&#x3D;10的行。那么，这时候索引c上的间隙是什么状态了呢？<strong>你要知道，由于非唯一索引上包含主键的值，所以是不可能存在“相同”的两行的。</strong></p>
<p><img src="/2024/09/12/MySQL21/image-20240912105814401.png" alt="image-20240912105814401"></p>
<p>可以看到，<strong>虽然有两个c&#x3D;10，但是它们的主键值id是不同的（分别是10和30），因此这两个c&#x3D;10的记录之间，也是有间隙的。</strong></p>
<p>图中我画出了索引c上的主键id。为了跟间隙锁的开区间形式进行区别，我用(c&#x3D;10,id&#x3D;30)这样的形式，来表示索引上的一行。</p>
<p>现在，我们来看一下案例六。</p>
<p>这次我们用delete语句来验证。注意，<strong>delete语句加锁的逻辑，其实跟select … for update 是类似的，也就是我在文章开始总结的两个“原则”、两个“优化”和一个“bug”。</strong></p>
<p><img src="/2024/09/12/MySQL21/image-20240912105823924.png" alt="image-20240912105823924"></p>
<p>这时，session A在遍历的时候，先访问第一个c&#x3D;10的记录。同样地，根据原则1，这里加的是(c&#x3D;5,id&#x3D;5)到(c&#x3D;10,id&#x3D;10)这个next-key lock。</p>
<p>然后，session A向右查找，直到碰到(c&#x3D;15,id&#x3D;15)这一行，循环才结束。根据优化2，<strong>这是一个等值查询，向右查找到了不满足条件的行，所以会退化成(c&#x3D;10,id&#x3D;10) 到 (c&#x3D;15,id&#x3D;15)的间隙锁。</strong></p>
<p>也就是说，这个delete语句在索引c上的加锁范围，就是下图中蓝色区域覆盖的部分。</p>
<p><img src="/2024/09/12/MySQL21/image-20240912105834105.png" alt="image-20240912105834105"></p>
<p>这个蓝色区域左右两边都是虚线，<strong>表示开区间，即(c&#x3D;5,id&#x3D;5)和(c&#x3D;15,id&#x3D;15)这两行上都没有锁。</strong></p>
<h3 id="案例七：limit-语句加锁"><a href="#案例七：limit-语句加锁" class="headerlink" title="案例七：limit 语句加锁"></a>案例七：limit 语句加锁</h3><p>例子6也有一个对照案例，场景如下所示：</p>
<p><img src="/2024/09/12/MySQL21/image-20240912105844034.png" alt="image-20240912105844034"></p>
<p>这个例子里，session A的delete语句加了 limit 2。<strong>你知道表t里c&#x3D;10的记录其实只有两条，因此加不加limit 2，删除的效果都是一样的，但是加锁的效果却不同。</strong>可以看到，session B的insert语句执行通过了，跟案例六的结果不同。</p>
<p>这是因为，案例七里的delete语句明确加了limit 2的限制，因此在遍历到(c&#x3D;10, id&#x3D;30)这一行之后，<strong>满足条件的语句已经有两条，循环就结束了。</strong></p>
<p><strong>因此，索引c上的加锁范围就变成了从（c&#x3D;5,id&#x3D;5)到（c&#x3D;10,id&#x3D;30)这个前开后闭区间，如下图所示：</strong></p>
<p><img src="/2024/09/12/MySQL21/image-20240912105855636.png" alt="image-20240912105855636"></p>
<p><strong>可以看到，(c&#x3D;10,id&#x3D;30）之后的这个间隙并没有在加锁范围里，因此insert语句插入c&#x3D;12是可以执行成功的。</strong></p>
<p>这个例子对我们实践的指导意义就是，<strong>在删除数据的时候尽量加limit</strong>。<strong>这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围（这个可以啊，以前完全没有了解过）</strong>。</p>
<h3 id="案例八：一个死锁的例子"><a href="#案例八：一个死锁的例子" class="headerlink" title="案例八：一个死锁的例子"></a>案例八：一个死锁的例子</h3><p>前面的例子中，我们在分析的时候，是按照next-key lock的逻辑来分析的，因为这样分析比较方便。最后我们再看一个案例，目的是说明：<strong>next-key lock实际上是间隙锁和行锁加起来的结果</strong>。</p>
<p>你一定会疑惑，这个概念不是一开始就说了吗？不要着急，我们先来看下面这个例子：</p>
<p><img src="/2024/09/12/MySQL21/image-20240912105909914.png" alt="image-20240912105909914"></p>
<p>现在，我们按时间顺序来分析一下为什么是这样的结果。</p>
<ol>
<li><strong>session A 启动事务后执行查询语句加lock in share mode，在索引c上加了next-key lock(5,10] 和间隙锁(10,15)；</strong></li>
<li>session B 的update语句也要在索引c上<strong>加next-key lock(5,10] ，进入锁等待；</strong></li>
<li>然后session A要再插入(8,8,8)这一行，<strong>被session B的间隙锁锁住。由于出现了死锁，InnoDB让session B回滚。</strong></li>
</ol>
<p>你可能会问，session B的next-key lock不是还没申请成功吗？</p>
<p>其实是这样的，<strong>session B的“加next-key lock(5,10] ”操作，实际上分成了两步，先是加(5,10)的间隙锁，加锁成功；然后加c&#x3D;10的行锁，这时候才被锁住的。</strong></p>
<p>也就是说，<strong>我们在分析加锁规则的时候可以用next-key lock来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的</strong>。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>这里我再次说明一下，<strong>我们上面的所有案例都是在可重复读隔离级别(repeatable-read)下验证的</strong>。同时，可重复读隔离级别<strong>遵守两阶段锁协议</strong>，<strong>所有加锁的资源，都是在事务提交或者回滚的时候才释放的</strong>。</p>
<p>在最后的案例中，<strong>你可以清楚地知道next-key lock实际上是由间隙锁加行锁实现的。如果切换到读提交隔离级别(read-committed)的话，就好理解了，过程中去掉间隙锁的部分，也就是只剩下行锁的部分</strong>。</p>
<p>其实<strong>读提交隔离级别在外键场景下还是有间隙锁，相对比较复杂</strong>，我们今天先不展开。</p>
<p>另外，<strong>在读提交隔离级别下还有一个优化，即：语句执行过程中加上的行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放了，不需要等到事务提交。</strong></p>
<p>也就是说，读提交隔离级别下，<strong>锁的范围更小，锁的时间更短，这也是不少业务都默认使用读提交隔离级别的原因</strong>。</p>
<p>不过，我希望你学过今天的课程以后，可以对next-key lock的概念有更清晰的认识，并且会用加锁规则去判断语句的加锁范围。</p>
<p>在业务<strong>需要使用可重复读隔离级别的时候，能够更细致地设计操作数据库的语句，解决幻读问题的同时，最大限度地提升系统并行处理事务的能力。</strong></p>
<p>经过这篇文章的介绍，你再看一下上一篇文章最后的思考题，再来尝试分析一次。</p>
<p>我把题目重新描述和简化一下：还是我们在文章开头初始化的表t，里面有6条记录，图12的语句序列中，为什么session B的insert操作，会被锁住呢？</p>
<p><img src="/2024/09/12/MySQL21/image-20240912105928814.png" alt="image-20240912105928814"></p>
<p>另外，如果你有兴趣多做一些实验的话，可以设计好语句序列，在执行之前先自己分析一下，然后实际地验证结果是否跟你的分析一致。</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期的问题，我在本期继续作为了课后思考题，所以会在下篇文章再一起公布“答案”。</p>
<p>这里，我展开回答一下评论区几位同学的问题。</p>
<ul>
<li>以前一直认为间隙锁只在二级索引上有。<strong>现在你知道了，有间隙的地方就可能有间隙锁。</strong></li>
<li>如果是varchar类型，加锁规则是什么样的。 回答：实际上在判断间隙的时候，<strong>varchar和int是一样的，排好序以后，相邻两个值之间就有间隙</strong>。</li>
<li>上一篇文章自己验证的结果跟案例一不同，就是在session A执行完这两个语句：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">begin;</span><br><span class="line">select * from t where d=5 for update; /*Q1*/</span><br></pre></td></tr></table></figure>

<p>以后，session B 的update 和session C的insert 都会被堵住。这是不是跟文章的结论矛盾？</p>
<p>其实不是的，这个例子用的是反证假设，就是假设不堵住，会出现问题；然后，<strong>推导出session A需要锁整个表所有的行和所有间隙。</strong></p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/21-wei-shi-mo-wo-zhi-gai-yi-hang-de-yu-ju-suo-zhe-mo-duo#an-li-er-fei-wei-yi-suo-yin-deng-zhi-suo">21 为什么我只改一行的语句，锁这么多？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>幻读是什么，幻读有什么问题？</title>
    <url>/2024/09/12/MySQL20/</url>
    <content><![CDATA[<p>在上一篇文章最后，我给你留了一个关于加锁规则的问题。今天，我们就从这个问题说起吧。</p>
<p>为了便于说明问题，这一篇文章，我们就先使用一个小一点儿的表。建表和初始化语句如下（为了便于本期的例子说明，我把上篇文章中用到的表结构做了点儿修改）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `t` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `c` int(11) DEFAULT NULL,</span><br><span class="line">  `d` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `c` (`c`)</span><br><span class="line">) ENGINE=InnoDB;</span><br><span class="line"></span><br><span class="line">insert into t values(0,0,0),(5,5,5),</span><br><span class="line">(10,10,10),(15,15,15),(20,20,20),(25,25,25);</span><br></pre></td></tr></table></figure>

<p>这个表除了主键id外，还有一个索引c，初始化语句在表中插入了6行数据。</p>
<p>上期我留给你的问题是，下面的语句序列，是怎么加锁的，加的锁又是什么时候释放的呢？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">begin;</span><br><span class="line">select * from t where d=5 for update;</span><br><span class="line">commit;</span><br></pre></td></tr></table></figure>

<p>比较好理解的是，<strong>这个语句会命中d&#x3D;5的这一行，对应的主键id&#x3D;5，因此在select 语句执行完成后，id&#x3D;5这一行会加一个写锁，而且由于两阶段锁协议，这个写锁会在执行commit语句的时候释放。</strong></p>
<p>由于字段d上没有索引，<strong>因此这条查询语句会做全表扫描。那么，其他被扫描到的，但是不满足条件的5行记录上，会不会被加锁呢？</strong></p>
<p>我们知道，InnoDB的默认事务隔离级别是可重复读，所以本文接下来没有特殊说明的部分，都是设定在可重复读隔离级别下。</p>
<h3 id="幻读是什么？"><a href="#幻读是什么？" class="headerlink" title="幻读是什么？"></a>幻读是什么？</h3><p>现在，我们就来分析一下，如果只在id&#x3D;5这一行加锁，而其他行的不加锁的话，会怎么样。</p>
<p>下面先来看一下这个场景（注意：这是我假设的一个场景）：</p>
<p><img src="/2024/09/12/MySQL20/image-20240912102746933.png" alt="image-20240912102746933"></p>
<p>可以看到，session A里执行了三次查询，分别是Q1、Q2和Q3。它们的SQL语句相同，都是select * from t where d&#x3D;5 for update。这个语句的意思你应该很清楚了，查所有d&#x3D;5的行，<strong>而且使用的是当前读</strong>，并且加上写锁。现在，我们来看一下这三条SQL语句，分别会返回什么结果。</p>
<ol>
<li>Q1只返回id&#x3D;5这一行；</li>
<li>在T2时刻，session B把id&#x3D;0这一行的d值改成了5，因此T3时刻Q2查出来的是id&#x3D;0和id&#x3D;5这两行；</li>
<li>在T4时刻，session C又插入一行（1,1,5），因此T5时刻Q3查出来的是id&#x3D;0、id&#x3D;1和id&#x3D;5的这三行。</li>
</ol>
<p>其中，Q3读到id&#x3D;1这一行的现象，<strong>被称为“幻读”。也就是说，幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行</strong>。</p>
<p>这里，我需要对“幻读”做一个说明：</p>
<ol>
<li>在可重复读隔离级别下，<strong>普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。</strong></li>
<li>上面session B的修改结果，被session A之后的select语句用“当前读”看到，<strong>不能称为幻读。幻读仅专指“新插入的行”。</strong></li>
</ol>
<p>如果只从第8篇文章<a href="https://time.geekbang.org/column/article/70562">《事务到底是隔离的还是不隔离的？》</a>我们学到的事务可见性规则来分析的话，上面这三条SQL语句的返回结果都没有问题。</p>
<p><strong>因为这三个查询都是加了for update，都是当前读。而当前读的规则，就是要能读到所有已经提交的记录的最新值。</strong>并且，session B和sessionC的两条语句，执行后就会提交，所以Q2和Q3就是应该看到这两个事务的操作效果，而且也看到了，这跟事务的可见性规则并不矛盾。</p>
<p>但是，这是不是真的没问题呢？</p>
<p>不，这里还真就有问题。</p>
<h3 id="幻读有什么问题？"><a href="#幻读有什么问题？" class="headerlink" title="幻读有什么问题？"></a>幻读有什么问题？</h3><p><strong>首先是语义上的。</strong>session A在T1时刻就声明了，<strong>“我要把所有d&#x3D;5的行锁住，不准别的事务进行读写操作”。而实际上，这个语义被破坏了。</strong></p>
<p>如果现在这样看感觉还不明显的话，我再往session B和session C里面分别加一条SQL语句，你再看看会出现什么现象。</p>
<p><img src="/2024/09/12/MySQL20/image-20240912102802316.png" alt="image-20240912102802316"></p>
<p>session B的第二条语句update t set c&#x3D;5 where id&#x3D;0，语义是“我把id&#x3D;0、d&#x3D;5这一行的c值，改成了5”。</p>
<p>由于在T1时刻，session A 还只是给id&#x3D;5这一行加了行锁， 并没有给id&#x3D;0这行加上锁。因此，session B在T2时刻，是可以执行这两条update语句的。<strong>这样，就破坏了 session A 里Q1语句要锁住所有d&#x3D;5的行的加锁声明。</strong></p>
<p><strong>session C也是一样的道理，对id&#x3D;1这一行的修改，也是破坏了Q1的加锁声明。</strong></p>
<p><strong>其次，是数据一致性的问题。</strong></p>
<p>我们知道<strong>，锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。</strong></p>
<p>为了说明这个问题，我给session A在T1时刻再加一个更新语句，即：update t set d&#x3D;100 where d&#x3D;5。</p>
<p><img src="/2024/09/12/MySQL20/image-20240912102818585.png" alt="image-20240912102818585"></p>
<p>update的加锁语义和select …for update 是一致的，所以这时候加上这条update语句也很合理。session A声明说“要给d&#x3D;5的语句加上锁”，就是为了要更新数据，<strong>新加的这条update语句就是把它认为加上了锁的这一行的d值修改成了100</strong>。</p>
<p>现在，我们来分析一下图3执行完成后，数据库里会是什么结果。</p>
<ol>
<li>经过T1时刻，id&#x3D;5这一行变成 (5,5,100)，当然这个结果最终是在T6时刻正式提交的;</li>
<li>经过T2时刻，id&#x3D;0这一行变成(0,5,5);</li>
<li>经过T4时刻，表里面多了一行(1,5,5);</li>
<li>其他行跟这个执行序列无关，保持不变。</li>
</ol>
<p>这样看，这些数据也没啥问题，但是我们再来看看这时候binlog里面的内容。</p>
<ol>
<li>T2时刻，session B事务提交，写入了两条语句；</li>
<li>T4时刻，session C事务提交，写入了两条语句；</li>
<li>T6时刻，session A事务提交，写入了update t set d&#x3D;100 where d&#x3D;5 这条语句。</li>
</ol>
<p>我统一放到一起的话，就是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">update t set d=5 where id=0; /*(0,0,5)*/</span><br><span class="line">update t set c=5 where id=0; /*(0,5,5)*/</span><br><span class="line"></span><br><span class="line">insert into t values(1,1,5); /*(1,1,5)*/</span><br><span class="line">update t set c=5 where id=1; /*(1,5,5)*/</span><br><span class="line"></span><br><span class="line">update t set d=100 where d=5;/*所有d=5的行，d改成100*/</span><br></pre></td></tr></table></figure>

<p>好，你应该看出问题了。这<strong>个语句序列，不论是拿到备库去执行，还是以后用binlog来克隆一个库，这三行的结果，都变成了 (0,5,100)、(1,5,100)和(5,5,100)。</strong></p>
<p><strong>也就是说，id&#x3D;0和id&#x3D;1这两行，发生了数据不一致。这个问题很严重，是不行的。</strong></p>
<p>到这里，我们再回顾一下，<strong>这个数据不一致到底是怎么引入的？</strong></p>
<p>我们分析一下可以知道，<strong>这是我们假设“select * from t where d&#x3D;5 for update这条语句只给d&#x3D;5这一行，也就是id&#x3D;5的这一行加锁”导致的。</strong></p>
<p>所以我们认为，上面的设定不合理，要改。</p>
<p>那怎么改呢？<strong>我们把扫描过程中碰到的行，也都加上写锁，再来看看执行效果。</strong></p>
<p><img src="/2024/09/12/MySQL20/image-20240912102845127.png" alt="image-20240912102845127"></p>
<p><strong>由于session A把所有的行都加了写锁，所以session B在执行第一个update语句的时候就被锁住了。需要等到T6时刻session A提交以后，session B才能继续执行。</strong></p>
<p>这样对于id&#x3D;0这一行，在数据库里的最终结果还是 (0,5,5)。在binlog里面，执行序列是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t values(1,1,5); /*(1,1,5)*/</span><br><span class="line">update t set c=5 where id=1; /*(1,5,5)*/</span><br><span class="line"></span><br><span class="line">update t set d=100 where d=5;/*所有d=5的行，d改成100*/</span><br><span class="line"></span><br><span class="line">update t set d=5 where id=0; /*(0,0,5)*/</span><br><span class="line">update t set c=5 where id=0; /*(0,5,5)*/</span><br></pre></td></tr></table></figure>

<p>可以看到，按照日志顺序执行，id&#x3D;0这一行的最终结果也是(0,5,5)。所以，id&#x3D;0这一行的问题解决了。</p>
<p>但同时你也可以看到，id&#x3D;1这一行，在数据库里面的结果是(1,5,5)，而根据binlog的执行结果是(1,5,100)，也就是说幻读的问题还是没有解决。<strong>为什么我们已经这么“凶残”地，把所有的记录都上了锁，还是阻止不了id&#x3D;1这一行的插入和更新呢？</strong></p>
<p><strong>原因很简单。在T3时刻，我们给所有行加锁的时候，id&#x3D;1这一行还不存在，不存在也就加不上锁。</strong></p>
<p><strong>也就是说，即使把所有的记录都加上锁，还是阻止不了新插入的记录，</strong>这也是为什么“幻读”会被单独拿出来解决的原因。</p>
<p>到这里，其实我们刚说明完文章的标题 ：幻读的定义和幻读有什么问题。</p>
<p>接下来，我们再看看InnoDB怎么解决幻读的问题。</p>
<h3 id="如何解决幻读？"><a href="#如何解决幻读？" class="headerlink" title="如何解决幻读？"></a>如何解决幻读？</h3><p><strong>现在你知道了，产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB只好引入新的锁，也就是间隙锁(Gap Lock)。</strong></p>
<p>顾名思义，间隙锁，<strong>锁的就是两个值之间的空隙</strong>。比如文章开头的表t，初始化插入了6个记录，这就产生了7个间隙。</p>
<p><img src="/2024/09/12/MySQL20/image-20240912102909138.png" alt="image-20240912102909138"></p>
<p>这样，当你执行 select * from t where d&#x3D;5 for update的时候，<strong>就不止是给数据库中已有的6个记录加上了行锁，还同时加了7个间隙锁。这样就确保了无法再插入新的记录</strong>。</p>
<p>也就是说这时候，在一行行扫描的过程中，不仅将给行加上了行锁，<strong>还给行两边的空隙，也加上了间隙锁</strong>。</p>
<p>现在你知道了，<strong>数据行是可以加上锁的实体，数据行之间的间隙，也是可以加上锁的实体。但是间隙锁跟我们之前碰到过的锁都不太一样</strong>。</p>
<p>比如行锁，分成读锁和写锁。下图就是这两种类型行锁的冲突关系。</p>
<p><img src="/2024/09/12/MySQL20/image-20240912102918732.png" alt="image-20240912102918732"></p>
<p>也就是说，跟行锁有冲突关系的是“另外一个行锁”。</p>
<p>但是间隙锁不一样，<strong>跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。</strong>间隙锁之间都<strong>不存在</strong>冲突关系。</p>
<p>这句话不太好理解，我给你举个例子：</p>
<p><img src="/2024/09/12/MySQL20/image-20240912102929394.png" alt="image-20240912102929394"></p>
<p>这里session B并不会被堵住。<strong>因为表t里并没有c&#x3D;7这个记录，因此session A加的是间隙锁(5,10)。而session B也是在这个间隙加的间隙锁。它们有共同的目标，即：保护这个间隙，不允许插入值。但，它们之间是不冲突的。</strong></p>
<p>间隙锁和行锁合称next-key lock，每个next-key lock是前开后闭区间。也就是说，我们的表t初始化以后，如果用select * from t for update要把整个表所有记录锁起来，就形成了7个next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。</p>
<blockquote>
<p>备注：这篇文章中，如果没有特别说明，<strong>我们把间隙锁记为开区间，把next-key lock记为前开后闭区间。</strong></p>
</blockquote>
<p>你可能会问说，这个supremum从哪儿来的呢？</p>
<p><strong>这是因为+∞是开区间。实现上，InnoDB给每个索引加了一个不存在的最大值supremum，这样才符合我们前面说的“都是前开后闭区间”。</strong></p>
<p><strong>间隙锁和next-key lock的引入，帮我们解决了幻读的问题，但同时也带来了一些“困扰”。</strong></p>
<p>在前面的文章中，就有同学提到了这个问题。我把他的问题转述一下，对应到我们这个例子的表来说，业务逻辑这样的：任意锁住一行，如果这一行不存在的话就插入，如果存在这一行就更新它的数据，代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">begin;</span><br><span class="line">select * from t where id=N for update;</span><br><span class="line"></span><br><span class="line">/*如果行不存在*/</span><br><span class="line">insert into t values(N,N,N);</span><br><span class="line">/*如果行存在*/</span><br><span class="line">update t set d=N set id=N;</span><br><span class="line"></span><br><span class="line">commit;</span><br></pre></td></tr></table></figure>

<p>可能你会说，<strong>这个不是insert … on duplicate key update 就能解决吗</strong>？但其实在有多个唯一键的时候，这个方法是不能满足这位提问同学的需求的。至于为什么，我会在后面的文章中再展开说明。</p>
<p>这里介绍一下<code>insert … on duplicate key update </code>是什么：</p>
<p><code>insert … on duplicate key update </code> 是 MySQL 中的一种语法，<strong>用于在插入数据时处理重复键的情况</strong>。其基本含义是：</p>
<ul>
<li><strong>如果插入的行在表中不存在，则执行插入操作。</strong></li>
<li><strong>如果插入的行在表中已存在（即出现了主键或唯一索引冲突），则执行更新操作。（我记得Mybatis-Plus里面有个API好像就是使用的这个SQL语句）</strong></li>
</ul>
<p>现在，我们就只讨论这个逻辑。</p>
<p>这个同学碰到的现象是，这个逻辑一旦有并发，就会碰到死锁。你一定也觉得奇怪，这个逻辑每次操作前用for update锁起来，已经是最严格的模式了，怎么还会有死锁呢？</p>
<p>这里，我用两个session来模拟并发，并假设N&#x3D;9。</p>
<p><img src="/2024/09/12/MySQL20/image-20240912102952820.png" alt="image-20240912102952820"></p>
<p>你看到了，其实都不需要用到后面的update语句，就已经形成死锁了。我们按语句执行顺序来分析一下：</p>
<ol>
<li><strong>session A 执行select … for update语句，由于id&#x3D;9这一行并不存在，因此会加上间隙锁(5,10);</strong></li>
<li><strong>session B 执行select … for update语句，同样会加上间隙锁(5,10)，间隙锁之间不会冲突，因此这个语句可以执行成功；</strong></li>
<li><strong>session B 试图插入一行(9,9,9)，被session A的间隙锁挡住了，只好进入等待；</strong></li>
<li><strong>session A试图插入一行(9,9,9)，被session B的间隙锁挡住了。</strong></li>
</ol>
<p>至此，两个session进入互相等待状态，形成死锁。<strong>当然，InnoDB的死锁检测马上就发现了这对死锁关系，让session A的insert语句报错返回了。</strong></p>
<p>你现在知道了，<strong>间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的</strong>。其实，这还只是一个简单的例子，在下一篇文章中我们还会碰到更多、更复杂的例子。</p>
<p>你可能会说，<strong>为了解决幻读的问题，我们引入了这么一大串内容，有没有更简单一点的处理方法呢。</strong></p>
<p>我在文章一开始就说过，如果没有特别说明，今天和你分析的问题都是在可重复读隔离级别下的<strong>，间隙锁是在可重复读隔离级别下才会生效的。所以，你如果把隔离级别设置为读提交的话，就没有间隙锁了</strong>。但同时，<strong>你要解决可能出现的数据和日志不一致问题，需要把binlog格式设置为row。这，也是现在不少公司使用的配置组合</strong>。</p>
<p>前面文章的评论区有同学留言说，他们公司就使用的是<strong>读提交隔离级别加binlog_format&#x3D;row的组合</strong>。他曾问他们公司的DBA说，你为什么要这么配置。DBA直接答复说，因为大家都这么用呀。</p>
<p>所以，这个同学在评论区就问说，这个配置到底合不合理。</p>
<p>关于这个问题本身的答案是，<strong>如果读提交隔离级别够用，也就是说，业务不需要可重复读的保证，这样考虑到读提交下操作数据的锁范围更小（没有间隙锁）</strong>，这个选择是合理的。</p>
<p>但其实我想说的是，<strong>配置是否合理，跟业务场景有关，需要具体问题具体分析。</strong></p>
<p>但是，如果DBA认为之所以这么用的原因是“大家都这么用”，那就有问题了，或者说，迟早会出问题。</p>
<p>比如说，大家都用读提交，可是逻辑备份的时候，mysqldump为什么要把备份线程设置成可重复读呢？（这个我在前面的文章中已经解释过了，你可以再回顾下第6篇文章<a href="https://time.geekbang.org/column/article/69862">《全局锁和表锁 ：给表加个字段怎么有这么多阻碍？》</a>的内容）</p>
<p>然后，在备份期间，<strong>备份线程用的是可重复读，而业务线程用的是读提交。同时存在两种事务隔离级别，会不会有问题？</strong></p>
<p>进一步地，这两个不同的隔离级别现象有什么不一样的，关于我们的业务，“用读提交就够了”这个结论是怎么得到的？</p>
<p>如果业务开发和运维团队这些问题都没有弄清楚，那么“没问题”这个结论，本身就是有问题的。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天我们从上一篇文章的课后问题说起，提到了全表扫描的加锁方式。<strong>我们发现即使给所有的行都加上行锁，仍然无法解决幻读问题，因此引入了间隙锁的概念。</strong></p>
<p>我碰到过很多对数据库有一定了解的业务开发人员，他们在设计数据表结构和业务SQL语句的时候，<strong>对行锁有很准确的认识，但却很少考虑到间隙锁。最后的结果，就是生产库上会经常出现由于间隙锁导致的死锁现象</strong>。</p>
<p><strong>行锁确实比较直观，判断规则也相对简单，间隙锁的引入会影响系统的并发度，也增加了锁分析的复杂度，但也有章可循。</strong>下一篇文章，我就会为你讲解InnoDB的加锁规则，帮你理顺这其中的“章法”。</p>
<p><img src="/2024/09/12/MySQL20/image-20240912103017577.png" alt="image-20240912103017577"></p>
<p>如果你之前没有了解过本篇文章的相关内容，一定觉得这三个语句简直是风马牛不相及。但实际上，这里session B和session C的insert 语句都会进入锁等待状态。</p>
<p>你可以试着分析一下，出现这种情况的原因是什么？</p>
<p>这里需要说明的是，这其实是我在下一篇文章介绍加锁规则后才能回答的问题，是留给你作为预习的，其中session C被锁住这个分析是有点难度的。如果你没有分析出来，也不要气馁，我会在下一篇文章和你详细说明。</p>
<p>你也可以说说，<strong>你的线上MySQL配置的是什么隔离级别，为什么会这么配置？你有没有碰到什么场景，是必须使用可重复读隔离级别的呢？</strong></p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我们在本文的开头回答了上期问题。有同学的回答中还说明了读提交隔离级别下，<strong>在语句执行完成后，是只有行锁的。而且语句执行完成后，InnoDB就会把不满足条件的行行锁去掉。</strong></p>
<p>当然了，c&#x3D;5这一行的行锁，还是会等到commit的时候才释放的。</p>
<blockquote>
<p><strong>在读提交隔离级别下，是只有行锁的。</strong></p>
<p>需要说明一下，需要在启动配置里面增加<strong>performance_schema&#x3D;on，才能用上这个功能，performance_schema库里的表才有数据。</strong></p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/20-huan-du-shi-shi-mo-huan-du-you-shi-mo-wen-ti">20 幻读是什么，幻读有什么问题？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL是怎么保证数据不丢的？</title>
    <url>/2024/09/13/MySQL23/</url>
    <content><![CDATA[<p>今天这篇文章，我会继续和你介绍<strong>在业务高峰期临时提升性能的方法</strong>。从文章标题“MySQL是怎么保证数据不丢的？”，你就可以看出来，今天我和你介绍的方法，跟<strong>数据的可靠性</strong>有关。</p>
<p>在专栏前面文章和答疑篇中，我都着重介绍了WAL机制（你可以再回顾下<a href="https://time.geekbang.org/column/article/68633">第2篇</a>、<a href="https://time.geekbang.org/column/article/70848">第9篇</a>、<a href="https://time.geekbang.org/column/article/71806">第12篇</a>和<a href="https://time.geekbang.org/column/article/73161">第15篇</a>文章中的相关内容），得到的结论是：<strong>只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启后，数据可以恢复。</strong></p>
<p>评论区有同学又继续追问，redo log的写入流程是怎么样的，如何保证redo log真实地写入了磁盘。那么今天，我们就再一起看看MySQL写入binlog和redo log的流程。</p>
<h3 id="binlog的写入机制"><a href="#binlog的写入机制" class="headerlink" title="binlog的写入机制"></a>binlog的写入机制</h3><p>其实，binlog的写入逻辑比较简单：<strong>事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中。</strong></p>
<p><strong>一个事务的binlog是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。</strong>这就涉及到了binlog cache的保存问题。</p>
<p><strong>系统给binlog cache分配了一片内存，每个线程一个，参数 binlog_cache_size用于控制单个线程内binlog cache所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。</strong></p>
<p>事务提交的时候，<strong>执行器把binlog cache里的完整事务写入到binlog中，并清空binlog cache。</strong>状态如图1所示。</p>
<p><img src="/2024/09/13/MySQL23/image-20240913074839744.png" alt="image-20240913074839744"></p>
<p>可以看到，<strong>每个线程有自己binlog cache，但是共用同一份binlog文件。</strong></p>
<ul>
<li>图中的write，<strong>指的就是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快。</strong></li>
<li>图中的fsync，<strong>才是将数据持久化到磁盘的操作。一般情况下，我们认为fsync才占磁盘的IOPS。</strong></li>
</ul>
<p>write 和fsync的时机，<strong>是由参数sync_binlog控制的：</strong></p>
<ol>
<li><strong>sync_binlog&#x3D;0的时候，表示每次提交事务都只write，不fsync；</strong></li>
<li><strong>sync_binlog&#x3D;1的时候，表示每次提交事务都会执行fsync；</strong></li>
<li><strong>sync_binlog&#x3D;N(N&gt;1)的时候，表示每次提交事务都write，但累积N个事务后才fsync。</strong></li>
</ol>
<p>因此，在出现IO瓶颈的场景里，<strong>将sync_binlog设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成0，比较常见的是将其设置为100~1000中的某个数值。</strong></p>
<p>但是，将sync_binlog设置为N，对应的风险是：<strong>如果主机发生异常重启，会丢失最近N个事务的binlog日志。</strong></p>
<h3 id="redo-log的写入机制"><a href="#redo-log的写入机制" class="headerlink" title="redo log的写入机制"></a>redo log的写入机制</h3><p>接下来，我们再说说redo log的写入机制。</p>
<p>在专栏的<a href="https://time.geekbang.org/column/article/73161">第15篇答疑文章</a>中，我给你介绍了redo log buffer。事务在执行过程中，<strong>生成的redo log是要先写到redo log buffer的</strong>。</p>
<p>然后就有同学问了，<strong>redo log buffer里面的内容，是不是每次生成后都要直接持久化到磁盘呢？</strong></p>
<p><strong>答案是，不需要。</strong></p>
<p>如果事务执行期间MySQL发生异常重启，<strong>那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。</strong></p>
<p>那么，另外一个问题是，<strong>事务还没提交的时候，redo log buffer中的部分日志有没有可能被持久化到磁盘呢？</strong></p>
<p><strong>答案是，确实会有。</strong></p>
<p>这个问题，要从redo log可能存在的三种状态说起。这三种状态，对应的就是图2 中的三个颜色块。</p>
<p><img src="/2024/09/13/MySQL23/image-20240913074857728.png" alt="image-20240913074857728"></p>
<p>这三种状态分别是：</p>
<ol>
<li><strong>存在redo log buffer中，物理上是在MySQL进程内存中</strong>，就是图中的红色部分；</li>
<li>写到磁盘(write)，<strong>但是没有持久化（fsync)<strong>，物理上是在</strong>文件系统的page cach</strong>e里面，也就是图中的黄色部分；</li>
<li><strong>持久化到磁盘，对应的是hard disk</strong>，也就是图中的绿色部分。</li>
</ol>
<p>日志写到redo log buffer是很快的，<strong>wirte到page cache也差不多，但是持久化到磁盘的速度就慢多了</strong>。</p>
<p>为了<strong>控制redo log的写入策略</strong>，InnoDB提供了innodb_flush_log_at_trx_commit参数，它有三种可能取值：</p>
<ol>
<li><strong>设置为0的时候，表示每次事务提交时都只是把redo log留在redo log buffer中;</strong></li>
<li><strong>设置为1的时候，表示每次事务提交时都将redo log直接持久化到磁盘；</strong></li>
<li><strong>设置为2的时候，表示每次事务提交时都只是把redo log写到page cache。</strong></li>
</ol>
<p>InnoDB有一个后台线程，<strong>每隔1秒，就会把redo log buffer中的日志，调用write写到文件系统的page cache，然后调用fsync持久化到磁盘。</strong></p>
<p>注意，事务执行<strong>中间过程</strong>的redo log也是直接写在redo log buffer中的，这些redo log也会<strong>被后台线程一起持久化到磁盘</strong>。也就是说，<strong>一个没有提交的事务的redo log，也是可能已经持久化到磁盘的</strong>。</p>
<p>实际上，<strong>除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的redo log写入到磁盘中</strong>。</p>
<ol>
<li><strong>一种是，redo log buffer占用的空间即将达到 innodb_log_buffer_size一半的时候，后台线程会主动写盘。</strong>注意，<strong>由于这个事务并没有提交，所以这个写盘动作只是write，而没有调用fsync，也就是只留在了文件系统的page cache</strong>。</li>
<li><strong>另一种是，并行的事务提交的时候，顺带将这个事务的redo log buffer持久化到磁盘。</strong>假设一个事务A执行到一半，已经写了一些redo log到buffer中，这时候有另外一个线程的事务B提交，如果innodb_flush_log_at_trx_commit设置的是1，那么按照这个参数的逻辑，<strong>事务B要把redo log buffer里的日志全部持久化到磁盘。这时候，就会带上事务A在redo log buffer里的日志一起持久化到磁盘</strong>。</li>
</ol>
<p>这里需要说明的是，我们介绍两阶段提交的时候说过，<strong>时序上redo log先prepare， 再写binlog，最后再把redo log commit。</strong></p>
<p>如果把innodb_flush_log_at_trx_commit设置成1，<strong>那么redo log在prepare阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖于prepare 的redo log</strong>，再加上binlog来恢复的。（如果你印象有点儿模糊了，可以再回顾下<a href="https://time.geekbang.org/column/article/73161">第15篇文章</a>中的相关内容）。</p>
<p>每秒一次后台轮询刷盘，<strong>再加上崩溃恢复这个逻辑，InnoDB就认为redo log在commit的时候就不需要fsync了，只会write到文件系统的page cache中就够了</strong>。</p>
<p>通常我们说MySQL的<strong>“双1”配置，指的就是sync_binlog（表示每次提交事务都会执行fsync）和innodb_flush_log_at_trx_commit（表示每次事务提交时都将redo log直接持久化到磁盘，两阶段提交）都设置成 1</strong>。也就是说，一个事务完整提交前，<strong>需要等待两次刷盘，一次是redo log（prepare 阶段），一次是binlog</strong>。</p>
<p>这时候，你可能有一个疑问，<strong>这意味着我从MySQL看到的TPS是每秒两万的话，每秒就会写四万次磁盘。但是，我用工具测试出来，磁盘能力也就两万左右，怎么能实现两万的TPS？</strong></p>
<p>不太懂TPS的看看这篇文章：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/396421383">MySQL数据库三个关键性能指标TPS\QPS\IOPS - 知乎 (zhihu.com)</a></li>
</ul>
<p>解释这个问题，就要用到<strong>组提交（group commit）机制</strong>了。</p>
<p>这里，我需要先和你介绍<strong>日志逻辑序列号（log sequence number，LSN）的概念。LSN是单调递增的，用来对应redo log的一个个写入点。每次写入长度为length的redo log， LSN的值就会加上length。</strong></p>
<p>LSN也会写到InnoDB的数据页中，<strong>来确保数据页不会被多次执行重复的redo log。关于LSN和redo log、checkpoint的关系，我会在后面的文章中详细展开</strong>。</p>
<p>如图3所示，是三个并发事务(trx1, trx2, trx3)在prepare 阶段，<strong>都写完redo log buffer，持久化到磁盘的过程，对应的LSN分别是50、120 和160</strong>。</p>
<p><img src="/2024/09/13/MySQL23/image-20240913074929396.png" alt="image-20240913074929396"></p>
<p>从图中可以看到，</p>
<ol>
<li>trx1是<strong>第一个</strong>到达的，会被选为这组的 <strong>leader</strong>；</li>
<li>等trx1要开始写盘的时候，<strong>这个组里面已经有了三个事务，这时候LSN也变成了160</strong>；</li>
<li>trx1去写盘的时候，<strong>带的就是LSN&#x3D;160，因此等trx1返回时，所有LSN小于等于160的redo log，都已经被持久化到磁盘；</strong></li>
<li>这时候trx2和trx3就可以<strong>直接返回了</strong>。</li>
</ol>
<p>所以，一次组提交里面，<strong>组员越多，节约磁盘IOPS的效果越好。但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了</strong>。</p>
<p>在并发更新场景下，<strong>第一个事务写完redo log buffer以后，接下来这个fsync越晚调用，组员可能越多，节约IOPS的效果就越好</strong>。</p>
<p>为了让一次fsync带的组员更多，<strong>MySQL有一个很有趣的优化：拖时间</strong>。在介绍两阶段提交的时候，我曾经给你画了一个图，现在我把它截过来。</p>
<p><img src="/2024/09/13/MySQL23/image-20240913075034423.png" alt="image-20240913075034423"></p>
<p>图中，我把“写binlog”当成一个动作。但实际上，写binlog是分成两步的：</p>
<ol>
<li><strong>先把binlog从binlog cache中写到磁盘上的binlog文件；</strong></li>
<li><strong>调用fsync持久化。</strong></li>
</ol>
<p>MySQL为了让组提交的效果更好，<strong>把redo log做fsync的时间拖到了步骤1之后</strong>。也就是说，上面的图变成了这样：</p>
<p><img src="/2024/09/13/MySQL23/image-20240913075100323.png" alt="image-20240913075100323"></p>
<p>这么一来，binlog也可以组提交了。在执行图5中第4步把binlog fsync到磁盘时，<strong>如果有多个事务的binlog已经写完了，也是一起持久化的，这样也可以减少IOPS的消耗</strong>。</p>
<p>不过通常情况下第3步执行得会很快，所以binlog的write和fsync间的间隔时间短，导致能集合到一起持久化的binlog比较少，<strong>因此binlog的组提交的效果通常不如redo log的效果那么好</strong>。</p>
<p>如果你想提升binlog组提交的效果，可以通过设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count来实现。</p>
<ol>
<li>binlog_group_commit_sync_delay参数，表示延迟多少微秒后才调用fsync;</li>
<li>binlog_group_commit_sync_no_delay_count参数，表示累积多少次以后才调用fsync。</li>
</ol>
<p>这两个条件是或的关系，也就是说只要有一个满足条件就会调用fsync。</p>
<p>所以，当binlog_group_commit_sync_delay设置为0的时候，binlog_group_commit_sync_no_delay_count也无效了。</p>
<p>之前有同学在评论区问到，<strong>WAL机制是减少磁盘写，可是每次提交事务都要写redo log和binlog，这磁盘读写次数也没变少呀？</strong></p>
<p>现在你就能理解了，<strong>WAL机制</strong>主要得益于两个方面：</p>
<ol>
<li><strong>redo log 和 binlog都是顺序写，磁盘的顺序写比随机写速度要快；</strong></li>
<li><strong>组提交机制，可以大幅度降低磁盘的IOPS消耗。</strong></li>
</ol>
<p>分析到这里，我们再来回答这个问题：<strong>如果你的MySQL现在出现了性能瓶颈，而且瓶颈在IO上，可以通过哪些方法来提升性能呢？</strong></p>
<p>针对这个问题，可以考虑以下三种方法：</p>
<ol>
<li><strong>设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count参数，减少binlog的写盘次数</strong>。这个方法是基于“额外的故意等待”来实现的，<strong>因此可能会增加语句的响应时间，但没有丢失数据的风险</strong>。</li>
<li>将sync_binlog <strong>设置为大于1的值（比较常见是100~1000）。这样做的风险是，主机掉电时会丢binlog日志</strong>。</li>
<li>将innodb_flush_log_at_trx_commit设置为2。<strong>这样做的风险是，主机掉电的时候会丢数据</strong>。</li>
</ol>
<p>我不建议你把innodb_flush_log_at_trx_commit 设置成0。因为把这个参数设置成0，表示redo log只保存在内存中，这样的话MySQL本身异常重启也会丢数据，风险太大。<strong>而redo log写到文件系统的page cache的速度也是很快的，所以将这个参数设置成2跟设置成0其实性能差不多，但这样做MySQL异常重启时就不会丢数据了，相比之下风险会更小。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在专栏的<a href="https://time.geekbang.org/column/article/68633">第2篇</a>和<a href="https://time.geekbang.org/column/article/73161">第15篇</a>文章中，我和你分析了，如果redo log和binlog是完整的，MySQL是如何保证crash-safe的。今天这篇文章，我着重和你介绍的是MySQL是“<strong>怎么保证redo log和binlog是完整的</strong>”。</p>
<p>希望这三篇文章串起来的内容，能够让你对crash-safe这个概念有更清晰的理解。</p>
<p>之前的第15篇答疑文章发布之后，有同学继续留言问到了一些跟日志相关的问题，这里为了方便你回顾、学习，我再集中回答一次这些问题。</p>
<p><strong>问题1：</strong>执行一个update语句以后，我再去执行hexdump命令直接查看ibd文件内容，为什么没有看到数据有改变呢？</p>
<p>回答：这可能是因为<strong>WAL机制</strong>的原因。update语句执行完成后，<strong>InnoDB只保证写完了redo log、内存，可能还没来得及将数据写到磁盘</strong>。</p>
<p><strong>问题2：</strong>为什么binlog cache是每个线程自己维护的，而redo log buffer是全局共用的？</p>
<p>回答：MySQL这么设计的主要原因是，<strong>binlog是不能“被打断的”。一个事务的binlog必须连续写，因此要整个事务完成后，再一起写到文件里</strong>。</p>
<p>而redo log并没有这个要求，<strong>中间有生成的日志可以写到redo log buffer中。redo log buffer中的内容还能“搭便车”，其他事务提交的时候可以被一起写到磁盘中</strong>。</p>
<p><strong>问题3：</strong>事务执行期间，还没到提交阶段，如果发生crash的话，redo log肯定丢了，这会不会导致主备不一致呢？</p>
<p>回答：不会。因为这时候binlog 也还在binlog cache里，没发给备库。<strong>crash以后redo log和binlog都没有了，从业务角度看这个事务也没有提交，所以数据是一致的</strong>。</p>
<p><strong>问题4：</strong>如果binlog写完盘以后发生crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是bug？</p>
<p>回答：不是。</p>
<p>你可以设想一下更极端的情况，整个事务都提交成功了，redo log commit完成了，备库也收到binlog并执行了。但是主库和客户端网络断开了，导致事务成功的包返回不回去，这时候客户端也会收到“网络断开”的异常。<strong>这种也只能算是事务成功的，不能认为是bug</strong>。</p>
<p>实际上数据库的crash-safe保证的是：</p>
<ol>
<li>如果客户端收到<strong>事务成功的消息，事务就一定持久化了；</strong></li>
<li>如果客户端收到<strong>事务失败（比如主键冲突、回滚等）的消息，事务就一定失败了；</strong></li>
<li>如果客户端收到<strong>“执行异常”</strong>的消息，应用需要重连后通过查询当前状态来继续后续的逻辑。<strong>此时数据库只需要保证内部（数据和日志之间，主库和备库之间）一致就可以了。</strong></li>
</ol>
<p>今天我留给你的思考题是：你的生产库设置的是“双1”吗？ 如果平时是的话，你有在什么场景下改成过“非双1”吗？你的这个操作又是基于什么决定的？</p>
<p>另外，我们都知道这些设置可能有损，如果发生了异常，你的止损方案是什么？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我在上篇文章最后，想要你分享的是线上“救火”的经验。</p>
<p>在留言中提到了几个很好的场景。</p>
<ul>
<li>“如果一个数据库是<strong>被客户端的压力打满导致无法响应的，重启数据库是没用的</strong>。”，说明他很好地思考了。 <strong>这个问题是因为重启之后，业务请求还会再发。而且由于是重启，buffer pool被清空，可能会导致语句执行得更慢。</strong></li>
<li>有时候一个表上会出现<strong>多个单字段索引</strong>（而且往往这是因为运维工程师对索引原理不够清晰做的设计），<strong>这样就可能出现优化器选择索引合并算法的现象。但实际上，索引合并算法的效率并不好。而通过将其中的一个索引改成联合索引的方法，是一个很好的应对方案。</strong></li>
</ul>
<p>还有其他几个同学提到的问题场景，也很好，很值得你一看。</p>
<blockquote>
<p>客户端程序的连接器，连接完成后会做一些诸如show columns的操作，在短连接模式下这个影响就非常大了。 这个提醒我们，在review项目的时候，不止要review我们自己业务的代码，也要review连接器的行为。一般做法就是在测试环境，把general_log打开，用业务行为触发连接，然后通过general log分析连接器的行为。</p>
</blockquote>
<blockquote>
<p>如果你的数据库请求模式直接对应于客户请求，这往往是一个危险的设计。因为客户行为不可控，可能突然因为你们公司的一个运营推广，压力暴增，这样很容易把数据库打挂。 在设计模型里面设计一层，专门负责管理请求和数据库服务资源，对于比较重要和大流量的业务，是一个好的设计方向。</p>
</blockquote>
<blockquote>
<p>用文中提到的DDL方案，会导致binlog里面少了这个DDL语句，后续影响备份恢复的功能。由于需要另一个知识点（主备同步协议），我放在后面的文章中说明。</p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/23-mysql-shi-zen-mo-bao-zheng-shu-ju-bu-diu-de">23 MySQL是怎么保证数据不丢的？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>为什么还有kill不掉的语句？</title>
    <url>/2024/09/13/MySQL32/</url>
    <content><![CDATA[<p>在MySQL中有两个kill命令：</p>
<ul>
<li><strong>一个是kill query +线程id，表示终止这个线程中正在执行的语句</strong></li>
<li><strong>一个是kill connection +线程id，这里connection可缺省，表示断开这个线程的连接，当然如果这个线程有语句正在执行，也是要先停止正在执行的语句的</strong></li>
</ul>
<p>不知道你在使用MySQL的时候，有没有遇到过这样的现象：使用了kill命令，却没能断开这个连接。再执行show processlist命令，看到这条语句的Command列显示的是Killed。</p>
<p><strong>你一定会奇怪，显示为Killed是什么意思，不是应该直接在show processlist的结果里看不到这个线程了吗？</strong></p>
<p>今天，我们就来讨论一下这个问题。</p>
<p>其实大多数情况下，kill query&#x2F;connection命令是有效的。比如，执行一个查询的过程中，<strong>发现执行时间太久，要放弃继续查询，这时我们就可以用kill query命令，终止这条查询语句。</strong></p>
<p>还有一种情况是，语句处于锁等待的时候，直接使用kill命令也是有效的。我们一起来看下这个例子：</p>
<p><img src="/2024/09/13/MySQL32/image-20240913084031223.png" alt="image-20240913084031223"></p>
<p>可以看到，session C 执行kill query以后，session B几乎同时就提示了语句被中断。这，就是我们预期的结果。</p>
<h3 id="收到kill以后，线程做什么？"><a href="#收到kill以后，线程做什么？" class="headerlink" title="收到kill以后，线程做什么？"></a>收到kill以后，线程做什么？</h3><p>但是，这里你要停下来想一下：<strong>session B是直接终止掉线程，什么都不管就直接退出吗？显然，这是不行的。</strong></p>
<p>我在<a href="https://time.geekbang.org/column/article/69862">第6篇文章</a>中讲过，<strong>当对一个表做增删改查操作时，会在表上加MDL读锁。所以，session B虽然处于blocked状态b，但还是拿着一个MDL读锁的</strong>。如果线程被kill的时候，<strong>就直接终止，那之后这个MDL读锁就没机会被释放了</strong>。</p>
<p>这样看来，kill并不是马上停止的意思，<strong>而是告诉执行线程说，这条语句已经不需要继续执行了，可以开始“执行停止的逻辑了”。</strong></p>
<blockquote>
<p>其实，这跟Linux的kill命令类似，<strong>kill -N pid并不是让进程直接停止，而是给进程发一个信号，然后进程处理这个信号，进入终止逻辑</strong>。只是对于MySQL的kill命令来说，<strong>不需要传信号量参数，就只有“停止”这个命令</strong>。</p>
</blockquote>
<p><strong>实现上，当用户执行kill query thread_id_B时，MySQL里处理kill命令的线程做了两件事：</strong></p>
<ol>
<li>把session B的运行状态改成THD::KILL_QUERY(将变量killed赋值为THD::KILL_QUERY)；</li>
<li>给session B的执行线程发一个信号。</li>
</ol>
<p>为什么要发信号呢？</p>
<p>因为像图1的我们例子里面，session B处于锁等待状态，<strong>如果只是把session B的线程状态设置THD::KILL_QUERY，线程B并不知道这个状态变化，还是会继续等待。发一个信号的目的，就是让session B退出等待，来处理这个THD::KILL_QUERY状态。</strong></p>
<p>上面的分析中，隐含了这么三层意思：</p>
<ol>
<li>一个语句执行过程中有<strong>多处“埋点”，在这些“埋点”的地方判断线程状态，如果发现线程状态是THD::KILL_QUERY，才开始进入语句终止逻辑</strong>；</li>
<li>如果处于等待状态，<strong>必须是一个可以被唤醒的等待，否则根本不会执行到“埋点”处</strong>；</li>
<li><strong>语句从开始进入终止逻辑，到终止逻辑完全完成，是有一个过程的</strong>。</li>
</ol>
<p>到这里你就知道了，原来<strong>不是“说停就停的”</strong>。</p>
<p>接下来，我们<strong>再看一个kill不掉的例子</strong>，也就是我们在前面<a href="https://time.geekbang.org/column/article/78134">第29篇文章</a>中提到的 innodb_thread_concurrency 不够用的例子。</p>
<p>首先，执行set global innodb_thread_concurrency&#x3D;2，将InnoDB的并发线程上限数设置为2；然后，执行下面的序列：</p>
<p><img src="/2024/09/13/MySQL32/image-20240913084049283.png" alt="image-20240913084049283"></p>
<p>可以看到：</p>
<ol>
<li>sesssion C执行的时候被堵住了；</li>
<li>但是session D执行的kill query C命令却没什么效果，</li>
<li>直到session E执行了kill connection命令，才断开了session C的连接，提示“Lost connection to MySQL server during query”，</li>
<li>但是这时候，如果在session E中执行show processlist，你就能看到下面这个图。</li>
</ol>
<p><img src="/2024/09/13/MySQL32/image-20240913084106657.png" alt="image-20240913084106657"></p>
<p>这时候，<strong>id&#x3D;12这个线程的Commnad列显示的是Killed。也就是说，客户端虽然断开了连接，但实际上服务端上这条语句还在执行过程中。</strong></p>
<p><strong>为什么在执行kill query命令时，这条语句不像第一个例子的update语句一样退出呢？</strong></p>
<p>在实现上，等行锁时，使用的是pthread_cond_timedwait函数，这个等待状态可以被唤醒。但是，在这个例子里，12号线程的等待逻辑是这样的：每10毫秒判断一下是否可以进入InnoDB执行，如果不行，<strong>就调用nanosleep函数进入sleep状态。</strong></p>
<p>也就是说，虽然12号线程的状态已经被设置成了KILL_QUERY，<strong>但是在这个等待进入InnoDB的循环过程中，并没有去判断线程的状态，因此根本不会进入终止逻辑阶段。</strong></p>
<p>而当session E执行kill connection 命令时，是这么做的，</p>
<ol>
<li>把12号线程状态设置为KILL_CONNECTION；</li>
<li>关掉12号线程的网络连接。因为有这个操作，所以你会看到，这时候session C收到了断开连接的提示。</li>
</ol>
<p>那为什么执行show processlist的时候，会看到Command列显示为killed呢？其实，这就是因为在执行show processlist的时候，有一个特别的逻辑：</p>
<ul>
<li>如果一个线程的状态是KILL_CONNECTION，就把Command列显示成Killed。</li>
</ul>
<p>所以其实，<strong>即使是客户端退出了，这个线程的状态仍然是在等待中</strong>。那这个线程什么时候会退出呢？</p>
<p>答案是，只有等到满足进入InnoDB的条件后，session C的查询语句继续执行，然后才有可能判断到线程状态已经变成了KILL_QUERY或者KILL_CONNECTION，再进入终止逻辑阶段。</p>
<p>到这里，我们来小结一下。</p>
<p><strong>这个例子是kill无效的第一类情况，即：线程没有执行到判断线程状态的逻辑。</strong>跟这种情况相同的，还有由于IO压力过大，读写IO的函数一直无法返回，导致不能及时判断线程的状态。</p>
<p><strong>另一类情况是，终止逻辑耗时较长。</strong>这时候，从show processlist结果上看也是Command&#x3D;Killed，需要等到终止逻辑完成，语句才算真正完成。这类情况，比较常见的场景有以下几种：</p>
<ol>
<li>超大事务执行期间被kill。<strong>这时候，回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长。</strong></li>
<li>大查询回滚。如果查询过程中<strong>生成了比较大的临时文件，加上此时文件系统压力大，删除临时文件可能需要等待IO资源，导致耗时较长。</strong></li>
<li>DDL命令执行到最后阶段，<strong>如果被kill，需要删除中间过程的临时文件，也可能受IO资源影响耗时较久。</strong></li>
</ol>
<p>之前有人问过我，如果直接在客户端通过Ctrl+C命令，是不是就可以直接终止线程呢？</p>
<p>答案是，<strong>不可以</strong>。</p>
<p>这里有一个误解，<strong>其实在客户端的操作只能操作到客户端的线程，客户端和服务端只能通过网络交互，是不可能直接操作服务端线程的</strong>。</p>
<p>而由于MySQL是停等协议，所以这个线程执行的语句还没有返回的时候，再往这个连接里面继续发命令也是没有用的。实际上，<strong>执行Ctrl+C的时候，是MySQL客户端另外启动一个连接，然后发送一个kill query 命令</strong>。</p>
<p>所以，<strong>你可别以为在客户端执行完Ctrl+C就万事大吉了。因为，要kill掉一个线程，还涉及到后端的很多操作</strong>。</p>
<h3 id="另外两个关于客户端的误解"><a href="#另外两个关于客户端的误解" class="headerlink" title="另外两个关于客户端的误解"></a>另外两个关于客户端的误解</h3><p>在实际使用中，我也经常会碰到一些同学对客户端的使用有误解。接下来，我们就来看看两个最常见的误解。</p>
<p><strong>第一个误解是：如果库里面的表特别多，连接就会很慢。</strong></p>
<p>有些线上的库，会包含很多表（我见过最多的一个库里有6万个表）。这时候，你就会发现，每次用客户端连接都会卡在下面这个界面上。</p>
<p><img src="/2024/09/13/MySQL32/image-20240913084122337.png" alt="image-20240913084122337"></p>
<p>而如果db1这个库里表很少的话，连接起来就会很快，可以很快进入输入命令的状态。因此，有同学会认为是表的数目影响了连接性能。</p>
<p>从<a href="https://time.geekbang.org/column/article/68319">第一篇文章</a>你就知道，每个客户端在和服务端建立连接的时候，需要做的事情就是TCP握手、用户校验、获取权限。但这几个操作，显然跟库里面表的个数无关。</p>
<p>但实际上，正如图中的文字提示所说的，当使用默认参数连接的时候，<strong>MySQL客户端会提供一个本地库名和表名补全的功能。为了实现这个功能，客户端在连接成功后，需要多做一些操作：</strong></p>
<ol>
<li><strong>执行show databases；</strong></li>
<li><strong>切到db1库，执行show tables；</strong></li>
<li><strong>把这两个命令的结果用于构建一个本地的哈希表。</strong></li>
</ol>
<p>在这些操作中，<strong>最花时间的就是第三步在本地构建哈希表的操作。所以，当一个库中的表个数非常多的时候，这一步就会花比较长的时间</strong>。</p>
<p>也就是说，<strong>我们感知到的连接过程慢，其实并不是连接慢，也不是服务端慢，而是客户端慢。</strong></p>
<p>图中的提示也说了，<strong>如果在连接命令中加上-A，就可以关掉这个自动补全的功能，然后客户端就可以快速返回了</strong>。</p>
<p>这里自动补全的效果就是，<strong>你在输入库名或者表名的时候，输入前缀，可以使用Tab键自动补全表名或者显示提示</strong>。</p>
<p>实际使用中，<strong>如果你自动补全功能用得并不多，我建议你每次使用的时候都默认加-A</strong>。</p>
<p>其实提示里面没有说，除了加-A以外，加–quick(或者简写为-q)参数，也可以跳过这个阶段。但是，这个<strong>–quick是一个更容易引起误会的参数，也是关于客户端常见的一个误解。</strong></p>
<p>你看到这个参数，<strong>是不是觉得这应该是一个让服务端加速的参数？但实际上恰恰相反，设置了这个参数可能会降低服务端的性能</strong>。为什么这么说呢？</p>
<p>MySQL客户端发送请求后，接收服务端返回结果的方式有两种：</p>
<ol>
<li><strong>一种是本地缓存，也就是在本地开一片内存，先把结果存起来</strong>。如果你用API开发，对应的就是mysql_store_result 方法。</li>
<li><strong>另一种是不缓存，读一个处理一个</strong>。如果你用API开发，对应的就是mysql_use_result方法。</li>
</ol>
<p>MySQL客户端默认采用第一种方式，<strong>而如果加上–quick参数，就会使用第二种不缓存的方式。</strong></p>
<p>采用不缓存的方式时，<strong>如果本地处理得慢，就会导致服务端发送结果被阻塞，因此会让服务端变慢</strong>。关于服务端的具体行为，我会在下一篇文章再和你展开说明。</p>
<p>那你会说，既然这样，为什么要给这个参数取名叫作quick呢？这是因为使用这个参数可以达到以下三点效果：</p>
<ul>
<li><strong>第一点，就是前面提到的，跳过表名自动补全功能。</strong></li>
<li>第二点，mysql_store_result需要申请本地内存来缓存查询结果，如果查询结果太大，会耗费较多的本地内存，<strong>可能会影响客户端本地机器的性能</strong>；</li>
<li><strong>第三点，是不会把执行命令记录到本地的命令历史文件。</strong></li>
</ul>
<p>所以你看到了，–quick参数的意思，<strong>是让客户端变得更快</strong>。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在今天这篇文章中，我首先和你介绍了MySQL中<strong>，有些语句和连接“kill不掉”的情况。</strong></p>
<p>这些“kill不掉”的情况，<strong>其实是因为发送kill命令的客户端，并没有强行停止目标线程的执行，而只是设置了个状态，并唤醒对应的线程</strong>。而被kill的线程，<strong>需要执行到判断状态的“埋点”，才会开始进入终止逻辑阶段。并且，终止逻辑本身也是需要耗费时间的。</strong></p>
<p>所以，如果你发现一个线程处于Killed状态，<strong>你可以做的事情就是，通过影响系统环境，让这个Killed状态尽快结束</strong>。</p>
<p>比如，如果是第一个例子里InnoDB并发度的问题，<strong>你就可以临时调大innodb_thread_concurrency的值，或者停掉别的线程，让出位子给这个线程执行。</strong></p>
<p>而如果是<strong>回滚逻辑由于受到IO资源限制执行得比较慢，就通过减少系统压力让它加速。</strong></p>
<p>做完这些操作后，<strong>其实你已经没有办法再对它做什么了，只能等待流程自己完成</strong>。</p>
<p>如果你碰到一个被killed的事务一直处于回滚状态，你认为是应该直接把MySQL进程强行重启，还是应该让它自己执行完成呢？为什么呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我在上一篇文章末尾，给你留下的问题是，希望你分享一下误删数据的处理经验。</p>
<p>运维的同学<strong>直接拷贝文本去执行，SQL语句截断，导致数据库执行出错。</strong></p>
<p><strong>从浏览器拷贝文本执行，是一个非常不规范的操作。</strong>除了这个例子里面说的<strong>SQL语句截断</strong>问题，<strong>还可能存在乱码问题。</strong></p>
<p>一般这种操作，<strong>如果脚本的开发和执行不是同一个人，需要开发同学把脚本放到git上，然后把git地址，以及文件的md5发给运维同学。</strong></p>
<p>这样就要求运维同学在执行命令之前，<strong>确认要执行的文件的md5，跟之前开发同学提供的md5相同才能继续执行</strong>。</p>
<p><strong>“四个脚本”的方法</strong>，我非常推崇。这四个脚本分别是：<strong>备份脚本、执行脚本、验证脚本和回滚脚本。如果能够坚持做到，即使出现问题，也是可以很快恢复的，一定能降低出现故障的概率。</strong></p>
<p>不过，这个方案最大的敌人是这样的思想：这是个小操作，不需要这么严格。</p>
<p>为了数据安全和服务稳定，<strong>多做点预防方案的设计讨论，总好过故障处理和事后复盘</strong>。方案设计讨论会和故障复盘会，这两种会议的会议室气氛完全不一样。<strong>经历过的同学一定懂的。</strong></p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/32-wei-shi-mo-huan-you-kill-bu-diao-de-yu-ju">32 为什么还有kill不掉的语句？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>我查这么多数据，会不会把数据库内存打爆？</title>
    <url>/2024/09/13/MySQL33/</url>
    <content><![CDATA[<p>我经常会被问到这样一个问题：<strong>我的主机内存只有100G，现在要对一个200G的大表做全表扫描，会不会把数据库主机的内存用光了？</strong></p>
<p>这个问题确实值得担心，被系统OOM（out of memory）可不是闹着玩的。<strong>但是，反过来想想，逻辑备份的时候，可不就是做整库扫描吗？如果这样就会把内存吃光，逻辑备份不是早就挂了？</strong></p>
<p>所以说，<strong>对大表做全表扫描，看来应该是没问题的。但是，这个流程到底是怎么样的呢</strong>？</p>
<h3 id="全表扫描对server层的影响"><a href="#全表扫描对server层的影响" class="headerlink" title="全表扫描对server层的影响"></a>全表扫描对server层的影响</h3><p>假设，我们现在要对一个200G的InnoDB表db1. t，执行一个全表扫描。当然，你要把扫描结果保存在客户端，会使用类似这样的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql -h$host -P$port -u$user -p$pwd -e &quot;select * from db1.t&quot; &gt; $target_file</span><br></pre></td></tr></table></figure>

<p>你已经知道了，<strong>InnoDB的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表t的主键索引</strong>。这条查询语句由于没有其他的判断条件，<strong>所以查到的每一行都可以直接放到结果集里面，然后返回给客户端</strong>。</p>
<p>那么，这个“结果集”存在哪里呢？</p>
<p>实际上，<strong>服务端并不需要保存一个完整的结果集</strong>。取数据和发数据的流程是这样的：</p>
<ol>
<li>获取一行，<strong>写到net_buffer中。这块内存的大小是由参数net_buffer_length定义的，默认是16k。</strong></li>
<li><strong>重复获取行，直到net_buffer写满，调用网络接口发出去。</strong></li>
<li>如果发送成功，<strong>就清空net_buffer，然后继续取下一行，并写入net_buffer</strong>。</li>
<li>如果发送函数返回EAGAIN或WSAEWOULDBLOCK，<strong>就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。</strong></li>
</ol>
<p>这个过程对应的流程图如下所示。</p>
<p><img src="/2024/09/13/MySQL33/image-20240913090432386.png" alt="image-20240913090432386"></p>
<p>从这个流程中，你可以看到：</p>
<ol>
<li><strong>一个查询在发送过程中，占用的MySQL内部的内存最大就是net_buffer_length这么大，并不会达到200G；</strong></li>
<li><strong>socket send buffer 也不可能达到200G（默认定义&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;wmem_default），如果socket send buffer被写满，就会暂停读数据的流程。</strong></li>
</ol>
<p>也就是说，<strong>MySQL是“边读边发的”</strong>，这个概念很重要。这就意味着，<strong>如果客户端接收得慢，会导致MySQL服务端由于结果发不出去，这个事务的执行时间变长</strong>。</p>
<p>比如下面这个状态，就是我故意让客户端不去读socket receive buffer中的内容，然后在服务端show processlist看到的结果。</p>
<p><img src="/2024/09/13/MySQL33/image-20240913090445493.png" alt="image-20240913090445493"></p>
<p>如果你看到State的值一直处于<strong>“Sending to client”</strong>，就表示<strong>服务器端的网络栈写满了</strong>。</p>
<p>我在上一篇文章中曾提到，<strong>如果客户端使用–quick参数，会使用mysql_use_result方法。这个方法是读一行处理一行。你可以想象一下，假设有一个业务的逻辑比较复杂，每读一行数据以后要处理的逻辑如果很慢，就会导致客户端要过很久才会去取下一行数据</strong>，可能就会出现如图2所示的这种情况。</p>
<p>因此，<strong>对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，我都建议你使用mysql_store_result这个接口，直接把查询结果保存到本地内存。</strong></p>
<p>当然前提是查询返回结果不多。在<a href="https://time.geekbang.org/column/article/78427">第30篇文章</a>评论区，有同学说到自己<strong>因为执行了一个大查询导致客户端占用内存近20G，这种情况下就需要改用mysql_use_result接口了。</strong></p>
<p>另一方面，<strong>如果你在自己负责维护的MySQL里看到很多个线程都处于“Sending to client”这个状态，就意味着你要让业务开发同学优化查询结果，并评估这么多的返回结果是否合理。</strong></p>
<p>而如果要<strong>快速减少处于这个状态的线程的话，将net_buffer_length参数设置为一个更大的值是一个可选方案。</strong></p>
<p>与“Sending to client”长相很类似的一个状态是<strong>“Sending data”</strong>，这是一个经常被误会的问题。有同学问我说，在自己维护的实例上看到很多查询语句的状态是“Sending data”，<strong>但查看网络也没什么问题啊，为什么Sending data要这么久</strong>？</p>
<p>实际上，一个查询语句的状态变化是这样的（注意：这里，我略去了其他无关的状态）：</p>
<ul>
<li>MySQL<strong>查询语句进入执行阶段后，首先把状态设置成“Sending data</strong>”；</li>
<li><strong>然后，发送执行结果的列相关的信息（meta data) 给客户端；</strong></li>
<li><strong>再继续执行语句的流程；</strong></li>
<li><strong>执行完成后，把状态设置成空字符串。</strong></li>
</ul>
<p>也就是说，<strong>“Sending data”并不一定是指“正在发送数据”，而可能是处于执行器过程中的任意阶段</strong>。比如，你可以构造一个锁等待的场景，就能看到Sending data状态。</p>
<p><img src="/2024/09/13/MySQL33/image-20240913090502397.png" alt="image-20240913090502397"></p>
<p><img src="/2024/09/13/MySQL33/image-20240913090510221.png" alt="image-20240913090510221"></p>
<p>可以看到，session B明显是在等锁，状态显示为Sending data。</p>
<p>也就是说，<strong>仅当一个线程处于“等待客户端接收结果”的状态，才会显示”Sending to client”；而如果显示成“Sending data”，它的意思只是“正在执行”</strong>。</p>
<p>现在你知道了，<strong>查询的结果是分段发给客户端的，因此扫描全表，查询返回大量的数据，并不会把内存打爆</strong>。</p>
<p>在server层的处理逻辑我们都清楚了，<strong>在InnoDB引擎里面又是怎么处理的呢？ 扫描全表会不会对引擎系统造成影响呢？</strong></p>
<h3 id="全表扫描对InnoDB的影响"><a href="#全表扫描对InnoDB的影响" class="headerlink" title="全表扫描对InnoDB的影响"></a>全表扫描对InnoDB的影响</h3><p>在<a href="https://time.geekbang.org/column/article/68633">第2</a>和<a href="https://time.geekbang.org/column/article/73161">第15篇</a>文章中，<strong>我介绍WAL机制的时候，和你分析了InnoDB内存的一个作用，是保存更新的结果，再配合redo log，就避免了随机写盘</strong>。</p>
<p><strong>内存的数据页是在Buffer Pool (BP)中管理的</strong>，<strong>在WAL里Buffer Pool 起到了加速更新的作用</strong>。<strong>而实际上，Buffer Pool 还有一个更重要的作用，就是加速查询</strong>。</p>
<p>在第2篇文章的评论区有同学问道，<strong>由于有WAL机制，当事务提交的时候，磁盘上的数据页是旧的，那如果这时候马上有一个查询要来读这个数据页，是不是要马上把redo log应用到数据页呢？</strong></p>
<p>答案是不需要。<strong>因为这时候内存数据页的结果是最新的，直接读内存页就可以了。你看，这时候查询根本不需要读磁盘，直接从内存拿结果，速度是很快的。所以说，Buffer Pool还有加速查询的作用。</strong></p>
<p>而Buffer Pool对查询的加速效果，<strong>依赖于一个重要的指标，即：内存命中率。</strong></p>
<p>你可以在show engine innodb status结果中，<strong>查看一个系统当前的BP命中率。一般情况下，一个稳定服务的线上系统，要保证响应时间符合要求的话，内存命中率要在99%以上。</strong></p>
<p>执行show engine innodb status ，可以看到“Buffer pool hit rate”字样，显示的就是当前的命中率。比如图5这个命中率，就是99.0%。</p>
<p><img src="/2024/09/13/MySQL33/image-20240913090525218.png" alt="image-20240913090525218"></p>
<p><strong>如果所有查询需要的数据页都能够直接从内存得到，那是最好的，对应的命中率就是100%。但，这在实际生产上是很难做到的。</strong></p>
<p>InnoDB Buffer Pool的大小是由参数 innodb_buffer_pool_size确定的，**一般建议设置成可用物理内存的60%~80%**。</p>
<p>在大约十年前，单机的数据量是上百个G，而物理内存是几个G；<strong>现在虽然很多服务器都能有128G甚至更高的内存，但是单机的数据量却达到了T级别。</strong></p>
<p>所以，<strong>innodb_buffer_pool_size小于磁盘的数据量是很常见的。如果一个 Buffer Pool满了，而又要从磁盘读入一个数据页，那肯定是要淘汰一个旧数据页的。</strong></p>
<p><strong>InnoDB内存管理用的是最近最少使用 (Least Recently Used, LRU)算法，这个算法的核心就是淘汰最久未使用的数据。</strong></p>
<p>下图是一个LRU算法的基本模型。</p>
<p><img src="/2024/09/13/MySQL33/image-20240913090536596.png" alt="image-20240913090536596"></p>
<p>InnoDB<strong>管理Buffer Pool的LRU算法，是用链表来实现的</strong>。</p>
<ol>
<li>在图6的状态1里，链表头部是P1，表示P1是最近刚刚被访问过的数据页；假设内存里只能放下这么多数据页；</li>
<li>这时候有一个读请求访问P3，因此变成状态2，P3被移到最前面；</li>
<li><strong>状态3表示，这次访问的数据页是不存在于链表中的，所以需要在Buffer Pool中新申请一个数据页Px，加到链表头部。但是由于内存已经满了，不能申请新的内存。于是，会清空链表末尾Pm这个数据页的内存，存入Px的内容，然后放到链表头部。</strong></li>
<li>从效果上看，就是最久没有被访问的数据页Pm，被淘汰了。</li>
</ol>
<p>这个算法乍一看上去没什么问题，但是如果考虑到要做一个全表扫描，会不会有问题呢？</p>
<p><strong>假设按照这个算法，我们要扫描一个200G的表，而这个表是一个历史数据表，平时没有业务访问它。</strong></p>
<p>那么，按照这个算法扫描的话，<strong>就会把当前的Buffer Pool里的数据全部淘汰掉，存入扫描过程中访问到的数据页的内容。也就是说Buffer Pool里面主要放的是这个历史数据表的数据。</strong></p>
<p>对于一个正在做业务服务的库，这可不妙。<strong>你会看到，Buffer Pool的内存命中率急剧下降，磁盘压力增加，SQL语句响应变慢。</strong></p>
<p>所以，<strong>InnoDB不能直接使用这个LRU算法。实际上，InnoDB对LRU算法做了改进</strong>。</p>
<p><img src="/2024/09/13/MySQL33/image-20240913090549951.png" alt="image-20240913090549951"></p>
<p><strong>在InnoDB实现上，按照5:3的比例把整个LRU链表分成了young区域和old区域</strong>。图中LRU_old指向的就是old区域的第一个位置，是整个链表的5&#x2F;8处。<strong>也就是说，靠近链表头部的5&#x2F;8是young区域，靠近链表尾部的3&#x2F;8是old区域（和JVM差不多了）</strong>。</p>
<p>改进后的LRU算法执行流程变成了下面这样。</p>
<ol>
<li>图7中状态1，要访问数据页P3，由于P3在young区域，因此和优化前的LRU算法一样，将其移到链表头部，变成状态2。</li>
<li><strong>之后要访问一个新的不存在于当前链表的数据页，这时候依然是淘汰掉数据页Pm，但是新插入的数据页Px，是放在LRU_old处。</strong></li>
<li>处于old区域的数据页，<strong>每次被访问的时候</strong>都要做下面这个判断：<ul>
<li><strong>若这个数据页在LRU链表中存在的时间超过了1秒，就把它移动到链表头部；</strong></li>
<li><strong>如果这个数据页在LRU链表中存在的时间短于1秒，位置保持不变。1秒这个时间，是由参数innodb_old_blocks_time控制的。其默认值是1000，单位毫秒。</strong></li>
</ul>
</li>
</ol>
<p>这个策略，就是为了处理类似全表扫描的操作量身定制的。还是以刚刚的扫描200G的历史数据表为例，我们看看改进后的LRU算法的操作逻辑：</p>
<ol>
<li>扫描过程中，<strong>需要新插入的数据页，都被放到old区域;</strong></li>
<li>一个数据页里面有多条记录，<strong>这个数据页会被多次访问到，但由于是顺序扫描，这个数据页第一次被访问和最后一次被访问的时间间隔不会超过1秒，因此还是会被保留在old区域；</strong></li>
<li>再继续扫描后续的数据，<strong>之前的这个数据页之后也不会再被访问到，于是始终没有机会移到链表头部（也就是young区域），很快就会被淘汰出去。</strong></li>
</ol>
<p>可以看到，<strong>这个策略最大的收益，就是在扫描这个大表的过程中，虽然也用到了Buffer Pool，但是对young区域完全没有影响，从而保证了Buffer Pool响应正常业务的查询命中率</strong>。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天，<strong>我用“大查询会不会把内存用光”这个问题，和你介绍了MySQL的查询结果，发送给客户端的过程</strong>。</p>
<p>由于MySQL采用的是<strong>边算边发</strong>的逻辑，因此<strong>对于数据量很大的查询结果来说，不会在server端保存完整的结果集</strong>。所以，如果客户端读结果不及时，<strong>会堵住MySQL的查询过程，但是不会把内存打爆</strong>。</p>
<p>而对于<strong>InnoDB引擎</strong>内部，由<strong>于有淘汰策略，大查询也不会导致内存暴涨。并且，由于InnoDB对LRU算法做了改进，冷数据的全表扫描，对Buffer Pool的影响也能做到可控</strong>。</p>
<p>当然，我们前面文章有说过，<strong>全表扫描还是比较耗费IO资源的，所以业务高峰期还是不能直接在线上主库执行全表扫描的。</strong></p>
<p>我在文章中说到，<strong>如果由于客户端压力太大，迟迟不能接收结果，会导致MySQL无法发送结果而影响语句执行</strong>。但，这还不是最糟糕的情况。</p>
<p>你可以设想出<strong>由于客户端的性能问题，对数据库影响更严重的例子</strong>吗？或者你是否经历过这样的场景？你又是怎么优化的？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期的问题是，<strong>如果一个事务被kill之后，持续处于回滚状态，从恢复速度的角度看，你是应该重启等它执行结束，还是应该强行重启整个MySQL进程</strong>。</p>
<p><strong>因为重启之后该做的回滚动作还是不能少的，所以从恢复速度的角度来说，应该让它自己结束</strong>。</p>
<p>当然，<strong>如果这个语句可能会占用别的锁，或者由于占用IO资源过多，从而影响到了别的语句执行的话，就需要先做主备切换，切到新主库提供服务。</strong></p>
<p><strong>切换之后别的线程都断开了连接，自动停止执行。</strong>接下来还是等它自己执行完成。<strong>这个操作属于我们在文章中说到的，减少系统压力，加速终止逻辑。</strong></p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/33-wo-cha-zhe-mo-duo-shu-ju-hui-bu-hui-ba-shu-ju-ku-nei-cun-da-bao">33 我查这么多数据，会不会把数据库内存打爆？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>到底可不可以使用join？</title>
    <url>/2024/09/13/MySQL34/</url>
    <content><![CDATA[<p>在实际生产中，<strong>关于join语句使用的问题，一般会集中在以下两类</strong>：</p>
<ol>
<li>我们DBA<strong>不让使用join，使用join有什么问题呢？</strong></li>
<li><strong>如果有两个大小不同的表做join，应该用哪个表做驱动表呢？</strong></li>
</ol>
<p>今天这篇文章，我就先跟你说说join语句到底是怎么执行的，然后再来回答这两个问题。</p>
<p>为了便于量化分析，我还是创建<strong>两个表t1和t2</strong>来和你说明。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `t2` (</span><br><span class="line">  `id` int(11) NOT NULL,</span><br><span class="line">  `a` int(11) DEFAULT NULL,</span><br><span class="line">  `b` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  KEY `a` (`a`)</span><br><span class="line">) ENGINE=InnoDB;</span><br><span class="line"></span><br><span class="line">drop procedure idata;</span><br><span class="line">delimiter ;;</span><br><span class="line">create procedure idata()</span><br><span class="line">begin</span><br><span class="line">  declare i int;</span><br><span class="line">  set i=1;</span><br><span class="line">  while(i&lt;=1000)do</span><br><span class="line">    insert into t2 values(i, i, i);</span><br><span class="line">    set i=i+1;</span><br><span class="line">  end while;</span><br><span class="line">end;;</span><br><span class="line">delimiter ;</span><br><span class="line">call idata();</span><br><span class="line"></span><br><span class="line">create table t1 like t2;</span><br><span class="line">insert into t1 (select * from t2 where id&lt;=100)</span><br></pre></td></tr></table></figure>

<p>可以看到，<strong>这两个表都有一个主键索引id和一个索引a，字段b上无索引</strong>。<strong>存储过程idata()往表t2里插入了1000行数据，在表t1里插入的是100行数据。</strong></p>
<h3 id="Index-Nested-Loop-Join"><a href="#Index-Nested-Loop-Join" class="headerlink" title="Index Nested-Loop Join"></a>Index Nested-Loop Join</h3><p>我们来看一下这个语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t1 straight_join t2 on (t1.a=t2.a);</span><br></pre></td></tr></table></figure>

<p>如果直接使用join语句，<strong>MySQL优化器可能会选择表t1或t2作为驱动表，这样会影响我们分析SQL语句的执行过程。所以，为了便于分析执行过程中的性能问题，我改用straight_join让MySQL使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去join。</strong>在这个语句里，t1 是驱动表，t2是被驱动表。</p>
<p>现在，我们来看一下这条语句的explain结果。</p>
<p><img src="/2024/09/13/MySQL34/image-20240913121700721.png" alt="image-20240913121700721"></p>
<p>可以看到，在这条语句里，被驱动表t2的字段a上有索引，join过程用上了这个索引，因此这个语句的执行流程是这样的：</p>
<ol>
<li>从表t1中读入一行数据 R；</li>
<li>从数据行R中，取出a字段到表t2里去查找；</li>
<li><strong>取出表t2中满足条件的行，跟R组成一行，作为结果集的一部分；</strong></li>
<li><strong>重复执行步骤1到3，直到表t1的末尾循环结束。</strong></li>
</ol>
<p>这个过程是先遍历表t1，然后根据从表t1中取出的每行数据中的a值，去表t2中查找满足条件的记录。<strong>在形式上，这个过程就跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称NLJ。</strong></p>
<p>它对应的流程图如下所示：</p>
<p><img src="/2024/09/13/MySQL34/image-20240913121711701.png" alt="image-20240913121711701"></p>
<p>在这个流程里：</p>
<ol>
<li>对驱动表t1做了全表扫描，这个过程需要扫描100行；</li>
<li><strong>而对于每一行R，根据a字段去表t2查找，走的是树搜索过程。由于我们构造的数据都是一一对应的，因此每次的搜索过程都只扫描一行，也是总共扫描100行；</strong></li>
<li>所以，整个执行流程，总扫描行数是200。</li>
</ol>
<p>现在我们知道了这个过程，再试着回答一下文章开头的两个问题。</p>
<p>先看第一个问题：<strong>能不能使用join?</strong></p>
<p>假设不使用join，<strong>那我们就只能用单表查询。我们看看上面这条语句的需求，用单表查询怎么实现。</strong></p>
<ol>
<li>执行<code>select * from t1</code>，查出表t1的所有数据，这里有100行；</li>
<li>循环遍历这100行数据：<ul>
<li>从每一行R取出字段a的值$R.a；</li>
<li>执行<code>select * from t2 where a=$R.a</code>；</li>
<li>把返回的结果和R构成结果集的一行。</li>
</ul>
</li>
</ol>
<p>可以看到，在这个查询过程，也是扫描了200行，<strong>但是总共执行了101条语句，比直接join多了100次交互。除此之外，客户端还要自己拼接SQL语句和结果。</strong></p>
<p>显然，这么做还不如直接join好。</p>
<p>我们再来看看第二个问题：<strong>怎么选择驱动表？</strong></p>
<p>在这个join语句执行过程中，<strong>驱动表是走全表扫描，而被驱动表是走树搜索（这个简直是太重要了，之前倒是没有这么清晰的认识）</strong>。</p>
<p>假设被驱动表的行数是M。<strong>每次在被驱动表查一行数据，要先搜索索引a，再搜索主键索引。每次搜索一棵树近似复杂度是以2为底的M的对数，记为log2M，所以在被驱动表上查一行的时间复杂度是 2*log2M。</strong></p>
<p><strong>假设驱动表的行数是N，执行过程就要扫描驱动表N行，然后对于每一行，到被驱动表上匹配一次。</strong></p>
<p><strong>因此整个执行过程，近似复杂度是 N + N<em>2</em>log2M。</strong></p>
<p>显然，<strong>N对扫描行数的影响更大，因此应该让小表来做驱动表。</strong></p>
<blockquote>
<p><strong>如果你没觉得这个影响有那么“显然”， 可以这么理解：N扩大1000倍的话，扫描行数就会扩大1000倍；而M扩大1000倍，扫描行数扩大不到10倍。</strong></p>
</blockquote>
<p>到这里小结一下，通过上面的分析我们得到了两个结论：</p>
<ol>
<li><strong>使用join语句，性能比强行拆成多个单表执行SQL语句的性能要好；</strong></li>
<li><strong>如果使用join语句的话，需要让小表做驱动表。</strong></li>
</ol>
<p>但是，你需要注意，<strong>这个结论的前提是“可以使用被驱动表的索引”。</strong></p>
<p>接下来，我们再看看被驱动表用不上索引的情况。</p>
<h3 id="Simple-Nested-Loop-Join"><a href="#Simple-Nested-Loop-Join" class="headerlink" title="Simple Nested-Loop Join"></a>Simple Nested-Loop Join</h3><p>现在，我们把SQL语句改成这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t1 straight_join t2 on (t1.a=t2.b);</span><br></pre></td></tr></table></figure>

<p><strong>由于表t2的字段b上没有索引，因此再用图2的执行流程时，每次到t2去匹配的时候，就要做一次全表扫描。</strong></p>
<p>你可以先设想一下这个问题，继续使用图2的算法，是不是可以得到正确的结果呢？如果只看结果的话，这个算法是正确的，而且这个算法也有一个名字，<strong>叫做“Simple Nested-Loop Join”。</strong></p>
<p>但是，这样算来，这个SQL请求就要扫描表t2多达100次，总共扫描100*1000&#x3D;10万行。</p>
<p>这还只是两个小表，<strong>如果t1和t2都是10万行的表（当然了，这也还是属于小表的范围），就要扫描100亿行，这个算法看上去太“笨重”了。</strong></p>
<p>当然，MySQL也没有使用这个Simple Nested-Loop Join算法，<strong>而是使用了另一个叫作“Block Nested-Loop Join”的算法，简称BNL。</strong></p>
<h3 id="Block-Nested-Loop-Join"><a href="#Block-Nested-Loop-Join" class="headerlink" title="Block Nested-Loop Join"></a>Block Nested-Loop Join</h3><p>这时候，被驱动表上没有可用的索引，算法的流程是这样的：</p>
<ol>
<li>*<em>把表t1的数据读入线程内存join_buffer中，由于我们这个语句中写的是select <em>，因此是把整个表t1放入了内存；</em></em></li>
<li>扫描表t2，<strong>把表t2中的每一行取出来，跟join_buffer中的数据做对比，满足join条件的，作为结果集的一部分返回。</strong></li>
</ol>
<p>这个过程的流程图如下：</p>
<p><img src="/2024/09/13/MySQL34/image-20240913121754310.png" alt="image-20240913121754310"></p>
<p>对应地，这条SQL语句的explain结果如下所示：</p>
<p><img src="/2024/09/13/MySQL34/image-20240913121813327.png" alt="image-20240913121813327"></p>
<p>可以看到，在这个过程中，对表t1和t2都做了一次全表扫描，因此总的扫描行数是1100。<strong>由于join_buffer是以无序数组的方式组织的，因此对表t2中的每一行，都要做100次判断，总共需要在内存中做的判断次数是：100*1000&#x3D;10万次。</strong></p>
<p>前面我们说过，如果使用Simple Nested-Loop Join算法进行查询，扫描行数也是10万行。因此，从时间复杂度上来说，这两个算法是一样的。<strong>但是，Block Nested-Loop Join算法的这10万次判断是内存操作，速度上会快很多，性能也更好。</strong></p>
<p>接下来，我们来看一下，在这种情况下，应该选择哪个表做驱动表。</p>
<p><strong>假设小表的行数是N，大表的行数是M，那么在这个算法里：</strong></p>
<ol>
<li><strong>两个表都做一次全表扫描，所以总的扫描行数是M+N；</strong></li>
<li><strong>内存中的判断次数是M*N。</strong></li>
</ol>
<p>可以看到，调换这两个算式中的M和N没差别，<strong>因此这时候选择大表还是小表做驱动表，执行耗时是一样的。</strong></p>
<p>然后，你可能马上就会问了，这个例子里表t1才100行，要是表t1是一个大表，join_buffer放不下怎么办呢？</p>
<p>join_buffer的大小是由参数join_buffer_size设定的，默认值是256k。<strong>如果放不下表t1的所有数据话，策略很简单，就是分段放。</strong>我把join_buffer_size改成1200，再执行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t1 straight_join t2 on (t1.a=t2.b);</span><br></pre></td></tr></table></figure>

<p>执行过程就变成了：</p>
<ol>
<li><strong>扫描表t1，顺序读取数据行放入join_buffer中，放完第88行join_buffer满了，继续第2步；</strong></li>
<li><strong>扫描表t2，把t2中的每一行取出来，跟join_buffer中的数据做对比，满足join条件的，作为结果集的一部分返回；</strong></li>
<li><strong>清空join_buffer；</strong></li>
<li><strong>继续扫描表t1，顺序读取最后的12行数据放入join_buffer中，继续执行第2步。</strong></li>
</ol>
<p>执行流程图也就变成这样：</p>
<p><img src="/2024/09/13/MySQL34/image-20240913121849541.png" alt="image-20240913121849541"></p>
<p>图中的步骤4和5，<strong>表示清空join_buffer再复用。</strong></p>
<p><strong>这个流程才体现出了这个算法名字中“Block”的由来，表示“分块去join”。</strong></p>
<p>可以看到，这时候由于表t1被分成了两次放入join_buffer中，导致表t2会被扫描两次。*<em>虽然分成两次放入join_buffer，但是判断等值条件的次数还是不变的，依然是(88+12)<em>1000&#x3D;10万次。</em></em></p>
<p>我们再来看下，在这种情况下驱动表的选择问题。</p>
<p>假设，驱动表的数据行数是N，<strong>需要分K段才能完成算法流程，被驱动表的数据行数是M。</strong></p>
<p>注意，这里的K不是常数<strong>，N越大K就会越大，因此把K表示为λ*N，显然λ的取值范围是(0,1)。</strong></p>
<p>所以，在这个算法的执行过程中：</p>
<ol>
<li><strong>扫描行数</strong>是 N+λ<em>N</em>M；</li>
<li>内存判断 N*M次。</li>
</ol>
<p><strong>显然，内存判断次数是不受选择哪个表作为驱动表影响的。而考虑到扫描行数，在M和N大小确定的情况下，N小一些，整个算式的结果会更小。</strong></p>
<p><strong>所以结论是，应该让小表当驱动表。</strong></p>
<p>当然，你会发现，<strong>在N+λ<em>N</em>M这个式子里，λ才是影响扫描行数的关键因素，这个值越小越好。</strong></p>
<p>刚刚我们说了N越大，分段数K越大。那么，N固定的时候，什么参数会影响K的大小呢？（也就是λ的大小）<strong>答案是join_buffer_size。join_buffer_size越大，一次可以放入的行越多，分成的段数也就越少，对被驱动表的全表扫描次数就越少。</strong></p>
<p>这就是为什么，你可能会看到一些建议告诉你，<strong>如果你的join语句很慢，就把join_buffer_size改大。</strong></p>
<p>理解了MySQL执行join的两种算法，现在我们再来试着<strong>回答文章开头的两个问题</strong>。</p>
<p>第一个问题：能不能使用join语句？</p>
<ol>
<li><strong>如果可以使用Index Nested-Loop Join算法，也就是说可以用上被驱动表上的索引，其实是没问题的；</strong></li>
<li><strong>如果使用Block Nested-Loop Join算法，扫描行数就会过多。尤其是在大表上的join操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种join尽量不要用。</strong></li>
</ol>
<p>所以你在判断要不要使用join语句时，<strong>就是看explain结果里面，Extra字段里面有没有出现“Block Nested Loop”字样。</strong></p>
<p>第二个问题是：<strong>如果要使用join，应该选择大表做驱动表还是选择小表做驱动表？</strong></p>
<ol>
<li><strong>如果是Index Nested-Loop Join算法，应该选择小表做驱动表；</strong></li>
<li><strong>如果是Block Nested-Loop Join算法：</strong><ul>
<li><strong>在join_buffer_size足够大的时候，是一样的；</strong></li>
<li><strong>在join_buffer_size不够大的时候（这种情况更常见），应该选择小表做驱动表。</strong></li>
</ul>
</li>
</ol>
<p>所以，这个问题的结论就是，<strong>总是应该使用小表做驱动表</strong>。</p>
<p>当然了，这里我需要说明下，<strong>什么叫作“小表”</strong>。</p>
<p>我们前面的例子是没有加条件的。如果我在语句的where条件加上 t2.id&lt;&#x3D;50这个限定条件，再来看下这两条语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t1 straight_join t2 on (t1.b=t2.b) where t2.id&lt;=50;</span><br><span class="line">select * from t2 straight_join t1 on (t1.b=t2.b) where t2.id&lt;=50;</span><br></pre></td></tr></table></figure>

<p>注意，<strong>为了让两条语句的被驱动表都用不上索引，所以join字段都使用了没有索引的字段b。</strong></p>
<p><strong>但如果是用第二个语句的话，join_buffer只需要放入t2的前50行，显然是更好的。所以这里，“t2的前50行”是那个相对小的表，也就是“小表”。</strong></p>
<p>我们再来看另外一组例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select t1.b,t2.* from  t1  straight_join t2 on (t1.b=t2.b) where t2.id&lt;=100;</span><br><span class="line">select t1.b,t2.* from  t2  straight_join t1 on (t1.b=t2.b) where t2.id&lt;=100;</span><br></pre></td></tr></table></figure>

<p>这个例子里，<strong>表t1 和 t2都是只有100行参加join。但是，这两条语句每次查询放入join_buffer中的数据是不一样的：</strong></p>
<ul>
<li><strong>表t1只查字段b，因此如果把t1放到join_buffer中，则join_buffer中只需要放入b的值；</strong></li>
<li><strong>表t2需要查所有的字段，因此如果把表t2放到join_buffer中的话，就需要放入三个字段id、a和b。</strong></li>
</ul>
<p>这里，<strong>我们应该选择表t1作为驱动表。也就是说在这个例子里，“只需要一列参与join的表t1”是那个相对小的表。</strong></p>
<p>所以，更准确地说，<strong>在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与join的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天，我和你介绍了<strong>MySQL执行join语句的两种可能算法，这两种算法是由能否使用被驱动表的索引决定的。</strong>而能否用上<strong>被驱动表的索引</strong>，对join语句的性能<strong>影响很大</strong>。</p>
<p>通过对Index Nested-Loop Join和Block Nested-Loop Join两个算法执行过程的分析，我们也得到了文章开头两个问题的答案：</p>
<ol>
<li><strong>如果可以使用被驱动表的索引，join语句还是有其优势的；</strong></li>
<li><strong>不能使用被驱动表的索引，只能使用Block Nested-Loop Join算法，这样的语句就尽量不要使用；</strong></li>
<li><strong>在使用join的时候，应该让小表做驱动表。</strong></li>
</ol>
<p>我们在上文说到，<strong>使用Block Nested-Loop Join算法，可能会因为join_buffer不够大，需要对被驱动表做多次全表扫描。</strong></p>
<p>我的问题是，如果被驱动表是一个大表，并且是一个冷数据表，除了查询过程中可能会导致IO压力大以外，你觉得对这个MySQL服务还有什么更严重的影响吗？（这个问题需要结合上一篇文章的知识点）</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我在上一篇文章最后留下的问题是，<strong>如果客户端由于压力过大，迟迟不能接收数据，会对服务端造成什么严重的影响</strong>。</p>
<p><strong>这个问题的核心是，造成了“长事务”。</strong></p>
<p>至于长事务的影响，就要结合我们前面文章中提到的锁、MVCC的知识点了。</p>
<ul>
<li>如果前面的语句有更新，意味着它们在占用着行锁，会导致别的语句更新被锁住；</li>
<li>当然读的事务也有问题，就是会导致undo log不能被回收，导致回滚段空间膨胀。</li>
</ul>
<blockquote>
<p><strong>同一个事务，更新之后要尽快提交，不要做没必要的查询，尤其是不要执行需要返回大量数据的查询；</strong> </p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/34-dao-di-ke-bu-ke-yi-shi-yong-join">34 到底可不可以使用join？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>join语句怎么优化？</title>
    <url>/2024/09/13/MySQL35/</url>
    <content><![CDATA[<p>在上一篇文章中，<strong>我和你介绍了join语句的两种算法，分别是Index Nested-Loop Join(NLJ)和Block Nested-Loop Join(BNL)。</strong></p>
<p>我们发现在使用NLJ算法的时候，其实效果还是不错的，<strong>比通过应用层拆分成多个语句然后再拼接查询结果更方便，而且性能也不会差。</strong></p>
<p><strong>但是，BNL算法在大表join的时候性能就差多了，比较次数等于两个表参与join的行数的乘积，很消耗CPU资源。</strong></p>
<p>当然了，<strong>这两个算法都还有继续优化的空间，我们今天就来聊聊这个话题。</strong></p>
<p>为了便于分析，我还是创建两个表t1、t2来和你展开今天的问题。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table t1(id int primary key, a int, b int, index(a));</span><br><span class="line">create table t2 like t1;</span><br><span class="line">drop procedure idata;</span><br><span class="line">delimiter ;;</span><br><span class="line">create procedure idata()</span><br><span class="line">begin</span><br><span class="line">  declare i int;</span><br><span class="line">  set i=1;</span><br><span class="line">  while(i&lt;=1000)do</span><br><span class="line">    insert into t1 values(i, 1001-i, i);</span><br><span class="line">    set i=i+1;</span><br><span class="line">  end while;</span><br><span class="line">  </span><br><span class="line">  set i=1;</span><br><span class="line">  while(i&lt;=1000000)do</span><br><span class="line">    insert into t2 values(i, i, i);</span><br><span class="line">    set i=i+1;</span><br><span class="line">  end while;</span><br><span class="line"></span><br><span class="line">end;;</span><br><span class="line">delimiter ;</span><br><span class="line">call idata();</span><br></pre></td></tr></table></figure>

<p>为了便于后面量化说明，<strong>我在表t1里，插入了1000行数据，每一行的a&#x3D;1001-id的值。也就是说，表t1中字段a是逆序的。同时，我在表t2中插入了100万行数据。</strong></p>
<h3 id="Multi-Range-Read优化"><a href="#Multi-Range-Read优化" class="headerlink" title="Multi-Range Read优化"></a>Multi-Range Read优化</h3><p>在介绍join语句的优化方案之前，<strong>我需要先和你介绍一个知识点，即：Multi-Range Read优化(MRR)。这个优化的主要目的是尽量使用顺序读盘。（这个我记得之前好像说过）</strong></p>
<p>在<a href="https://time.geekbang.org/column/article/69236">第4篇文章</a>中，<strong>我和你介绍InnoDB的索引结构时，提到了“回表”的概念</strong>。我们先来回顾一下这个概念。<strong>回表是指，InnoDB在普通索引a上查到主键id的值后，再根据一个个主键id的值到主键索引上去查整行数据的过程。</strong></p>
<p>然后，有同学在留言区问到，回表过程是一行行地查数据，还是批量地查数据？</p>
<p>我们先来看看这个问题。假设，我执行这个语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t1 where a&gt;=1 and a&lt;=100;</span><br></pre></td></tr></table></figure>

<p><strong>主键索引是一棵B+树，在这棵树上，每次只能根据一个主键id查到一行数据。因此，回表肯定是一行行搜索主键索引的，</strong>基本流程如图1所示。</p>
<p><img src="/2024/09/13/MySQL35/image-20240913125644269.png" alt="image-20240913125644269"></p>
<p>如果<strong>随着a的值递增顺序查询的话，id的值就变成随机的，那么就会出现随机访问，性能相对较差</strong>。虽然“按行查”这个机制不能改，但是调<strong>整查询的顺序，还是能够加速的</strong>。</p>
<p><strong>因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。</strong></p>
<p>这，就是MRR优化的设计思路。此时，语句的执行流程变成了这样：</p>
<ol>
<li>根据索引a，<strong>定位到满足条件的记录，将id值放入read_rnd_buffer中;</strong></li>
<li><strong>将read_rnd_buffer中的id进行递增排序；</strong></li>
<li><strong>排序后的id数组，依次到主键id索引中查记录，并作为结果返回。</strong></li>
</ol>
<p>这里，<strong>read_rnd_buffer的大小是由read_rnd_buffer_size参数控制的</strong>。<strong>如果步骤1中，read_rnd_buffer放满了，就会先执行完步骤2和3，然后清空read_rnd_buffer。之后继续找索引a的下个记录，并继续循环</strong>。</p>
<p>另外需要说明的是，<strong>如果你想要稳定地使用MRR优化的话，需要设置<code>set optimizer_switch=&quot;mrr_cost_based=off&quot;</code>。（官方文档的说法，是现在的优化器策略，判断消耗的时候，会更倾向于不使用MRR，把mrr_cost_based设置为off，就是固定使用MRR了。）</strong></p>
<p>下面两幅图就是使用了MRR优化后的执行流程和explain结果。</p>
<p><img src="/2024/09/13/MySQL35/image-20240913125702967.png" alt="image-20240913125702967"></p>
<p><img src="/2024/09/13/MySQL35/image-20240913125711867.png" alt="image-20240913125711867"></p>
<p>从图3的explain结果中，<strong>我们可以看到Extra字段多了Using MRR，表示的是用上了MRR优化</strong>。而且，由于我们在read_rnd_buffer中按照id做了排序，<strong>所以最后得到的结果集也是按照主键id递增顺序的，也就是与图1结果集中行的顺序相反。</strong></p>
<p>到这里，我们小结一下。</p>
<p><strong>MRR能够提升性能的核心</strong>在于，<strong>这条查询语句在索引a上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势。</strong></p>
<h3 id="Batched-Key-Access"><a href="#Batched-Key-Access" class="headerlink" title="Batched Key Access"></a>Batched Key Access</h3><p>理解了MRR性能提升的原理，我们就能理解MySQL在5.6版本后开始引入的Batched Key Acess(BKA)算法了。<strong>这个BKA算法，其实就是对NLJ算法的优化。</strong></p>
<p>我们再来看看上一篇文章中用到的NLJ算法的流程图：</p>
<p><img src="/2024/09/13/MySQL35/image-20240913125725131.png" alt="image-20240913125725131"></p>
<p>NLJ算法执行的逻辑是：<strong>从驱动表t1，一行行地取出a的值，再到被驱动表t2去做join。也就是说，对于表t2来说，每次都是匹配一个值。这时，MRR的优势就用不上了。</strong></p>
<p>那怎么才能一次性地多传些值给表t2呢？方法就是，<strong>从表t1里一次性地多拿些行出来，一起传给表t2。</strong></p>
<p>既然如此，我们就把表t1的数据取出来一部分，<strong>先放到一个临时内存。这个临时内存不是别人，就是join_buffer</strong>。</p>
<p>通过上一篇文章，<strong>我们知道join_buffer 在BNL算法里的作用，是暂存驱动表的数据</strong>。但是在<strong>NLJ算法</strong>里并没有用。<strong>那么，我们刚好就可以复用join_buffer到BKA算法中</strong>。</p>
<p>如图5所示，是上面的NLJ算法优化后的BKA算法的流程。</p>
<p><img src="/2024/09/13/MySQL35/image-20240913125742672.png" alt="image-20240913125742672"></p>
<p>图中，<strong>我在join_buffer中放入的数据是P1~P100，表示的是只会取查询需要的字段</strong>。当然，如果join buffer放不下P1~P100的所有数据，就会把这100行数据分成多段执行上图的流程。</p>
<p>那么，这个BKA算法到底要怎么启用呢？</p>
<p><strong>如果要使用BKA优化算法的话，你需要在执行SQL语句之前，先设置</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set optimizer_switch=&#x27;mrr=on,mrr_cost_based=off,batched_key_access=on&#x27;;</span><br></pre></td></tr></table></figure>

<p><strong>其中，前两个参数的作用是要启用MRR。这么做的原因是，BKA算法的优化要依赖于MRR。</strong></p>
<h3 id="BNL算法的性能问题"><a href="#BNL算法的性能问题" class="headerlink" title="BNL算法的性能问题"></a>BNL算法的性能问题</h3><p>说完了NLJ算法的优化，我们再来看BNL算法的优化。</p>
<p>我在上一篇文章末尾，给你留下的思考题是，使用Block Nested-Loop Join(BNL)算法时，<strong>可能会对被驱动表做多次扫描。如果这个被驱动表是一个大的冷数据表，除了会导致IO压力大以外，还会对系统有什么影响呢？</strong></p>
<p>在<a href="https://time.geekbang.org/column/article/79407">第33篇文章</a>中，<strong>我们说到InnoDB的LRU算法的时候提到，由于InnoDB对Bufffer Pool的LRU算法做了优化，即：第一次从磁盘读入内存的数据页，会先放在old区域。如果1秒之后这个数据页不再被访问了，就不会被移动到LRU链表头部，这样对Buffer Pool的命中率影响就不大</strong>。</p>
<p>但是，<strong>如果一个使用BNL算法的join语句，多次扫描一个冷表，而且这个语句执行时间超过1秒，就会在再次扫描冷表的时候，把冷表的数据页移到LRU链表头部</strong>。</p>
<p>这种情况对应的，<strong>是冷表的数据量小于整个Buffer Pool的3&#x2F;8，能够完全放入old区域的情况</strong>。</p>
<p>如果这个冷表很大，<strong>就会出现另外一种情况：业务正常访问的数据页，没有机会进入young区域</strong>。</p>
<p>由于优化机制的存在，<strong>一个正常访问的数据页，要进入young区域，需要隔1秒后再次被访问到</strong>。<strong>但是，由于我们的join语句在循环读磁盘和淘汰内存页，进入old区域的数据页，很可能在1秒之内就被淘汰了。这样，就会导致这个MySQL实例的Buffer Pool在这段时间内，young区域的数据页没有被合理地淘汰。</strong></p>
<p>也就是说，这两种情况都会影响Buffer Pool的正常运作。</p>
<p><strong>大表join操作虽然对IO有影响，但是在语句执行结束后，对IO的影响也就结束了。但是，对Buffer Pool的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。</strong></p>
<p>为了减少这种影响，<strong>你可以考虑增大join_buffer_size的值，减少对被驱动表的扫描次数</strong>。</p>
<p>也就是说，BNL算法对系统的影响主要包括三个方面：</p>
<ol>
<li><strong>可能会多次扫描被驱动表，占用磁盘IO资源；</strong></li>
<li><strong>判断join条件需要执行M*N次对比（M、N分别是两张表的行数），如果是大表就会占用非常多的CPU资源；</strong></li>
<li><strong>可能会导致Buffer Pool的热数据被淘汰，影响内存命中率。</strong></li>
</ol>
<p>我们执行语句之前，需要通过理论分析和查看explain结果的方式，<strong>确认是否要使用BNL算法。如果确认优化器会使用BNL算法，就需要做优化</strong>。<strong>优化的常见做法是，给被驱动表的join字段加上索引，把BNL算法转成BKA算法</strong>。</p>
<p>接下来，我们就具体看看，这个优化怎么做？</p>
<h3 id="BNL转BKA"><a href="#BNL转BKA" class="headerlink" title="BNL转BKA"></a>BNL转BKA</h3><p>一些情况下，<strong>我们可以直接在被驱动表上建索引，这时就可以直接转成BKA算法了。</strong></p>
<p>但是，有时候你确实会碰到一些不适合在被驱动表上建索引的情况。比如下面这个语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t1 join t2 on (t1.b=t2.b) where t2.b&gt;=1 and t2.b&lt;=2000;</span><br></pre></td></tr></table></figure>

<p>我们在文章开始的时候，在表t2中插入了100万行数据，但是经过where条件过滤后，需要参与join的只有2000行数据。<strong>如果这条语句同时是一个低频的SQL语句，那么再为这个语句在表t2的字段b上创建一个索引就很浪费了。</strong></p>
<p>但是，如果使用BNL算法来join的话，这个语句的执行流程是这样的：</p>
<ol>
<li><strong>把表t1的所有字段取出来，存入join_buffer中。这个表只有1000行，join_buffer_size默认值是256k，可以完全存入。</strong></li>
<li>扫描表t2，取出每一行数据跟join_buffer中的数据进行对比，<ul>
<li><strong>如果不满足t1.b&#x3D;t2.b，则跳过；</strong></li>
<li><strong>如果满足t1.b&#x3D;t2.b, 再判断其他条件，也就是是否满足t2.b处于[1,2000]的条件，如果是，就作为结果集的一部分返回，否则跳过。</strong></li>
</ul>
</li>
</ol>
<p>我在上一篇文章中说过，对于表t2的每一行，判断join是否满足的时候，<strong>都需要遍历join_buffer中的所有行。因此判断等值条件的次数是1000*100万&#x3D;10亿次，这个判断的工作量很大</strong>。</p>
<p><img src="/2024/09/13/MySQL35/image-20240913125831389.png" alt="image-20240913125831389"></p>
<p><img src="/2024/09/13/MySQL35/image-20240913125837238.png" alt="image-20240913125837238"></p>
<p>可以看到，explain结果里Extra字段显示使用了BNL算法。在我的测试环境里，这条语句需要执行1分11秒。</p>
<p>在表t2的字段b上创建索引会浪费资源，但是不创建索引的话这个语句的等值条件要判断10亿次，想想也是浪费。<strong>那么，有没有两全其美的办法呢？</strong></p>
<p><strong>这时候，我们可以考虑使用临时表。使用临时表的大致思路是：</strong></p>
<ol>
<li><strong>把表t2中满足条件的数据放在临时表tmp_t中；</strong></li>
<li><strong>为了让join使用BKA算法，给临时表tmp_t的字段b加上索引；</strong></li>
<li><strong>让表t1和tmp_t做join操作。</strong></li>
</ol>
<p>此时，对应的SQL语句的写法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create temporary table temp_t(id int primary key, a int, b int, index(b))engine=innodb;</span><br><span class="line">insert into temp_t select * from t2 where b&gt;=1 and b&lt;=2000;</span><br><span class="line">select * from t1 join temp_t on (t1.b=temp_t.b);</span><br></pre></td></tr></table></figure>

<p><img src="/2024/09/13/MySQL35/image-20240913125904227.png" alt="image-20240913125904227"></p>
<p>可以看到，整个过程3个语句执行时间的<strong>总和还不到1秒，相比于前面的1分11秒，性能得到了大幅提升。接下来，我们一起看一下这个过程的消耗：（我的天啊，这不就是将120秒的接口优化到了1秒了吗？）</strong></p>
<ol>
<li>执行insert语句构造temp_t表并插入数据的过程中，对表t2做了全表扫描，这里扫描行数是100万。</li>
<li><strong>之后的join语句，扫描表t1，这里的扫描行数是1000；join比较过程中，做了1000次带索引的查询。相比于优化前的join语句需要做10亿次条件判断来说，这个优化效果还是很明显的。</strong></li>
</ol>
<p>总体来看，<strong>不论是在原表上加索引，还是用有索引的临时表，我们的思路都是让join语句能够用上被驱动表上的索引，来触发BKA算法，提升查询性能。</strong></p>
<h3 id="扩展-hash-join"><a href="#扩展-hash-join" class="headerlink" title="扩展-hash join"></a>扩展-hash join</h3><p>看到这里你可能发现了，其实上面计算10亿次那个操作，看上去有点儿傻。<strong>如果join_buffer里面维护的不是一个无序数组，而是一个哈希表的话，那么就不是10亿次判断，而是100万次hash查找。这样的话，整条语句的执行速度就快多了吧？</strong></p>
<p>确实如此。</p>
<p>这，<strong>也正是MySQL的优化器和执行器一直被诟病的一个原因：不支持哈希join。并且，MySQL官方的roadmap，也是迟迟没有把这个优化排上议程</strong>。</p>
<p>实际上，这个优化思路，<strong>我们可以自己实现在业务端</strong>。实现流程大致如下：</p>
<ol>
<li><code>select * from t1;</code>取得表t1的全部1000行数据，<strong>在业务端存入一个hash结构</strong>，比如C++里的set、PHP的dict这样的数据结构。</li>
<li><code>select * from t2 where b&gt;=1 and b&lt;=2000;</code> 获取表t2中满足条件的2000行数据。</li>
<li>把这2000行数据，<strong>一行一行地取到业务端，到hash结构的数据表中寻找匹配的数据。满足匹配的条件的这行数据，就作为结果集的一行</strong>。</li>
</ol>
<p><strong>理论上，这个过程会比临时表方案的执行速度还要快一些。</strong>如果你感兴趣的话，可以自己验证一下。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天，我和你分享了<strong>Index Nested-Loop Join（NLJ）和Block Nested-Loop Join（BNL）</strong>的优化方法。</p>
<p>在这些优化方法中：</p>
<ol>
<li><strong>BKA优化是MySQL已经内置支持的，建议你默认使用；</strong></li>
<li><strong>BNL算法效率低，建议你都尽量转成BKA算法。优化的方向就是给被驱动表的关联字段加上索引；</strong></li>
<li><strong>基于临时表的改进方案，对于能够提前过滤出小数据的join语句来说，效果还是很好的；</strong></li>
<li><strong>MySQL目前的版本还不支持hash join，但你可以配合应用端自己模拟出来，理论上效果要好于临时表的方案。</strong></li>
</ol>
<p>我们在讲join语句的这两篇文章中，都只涉及到了两个表的join。那么，现在有一个三个表join的需求，假设这三个表的表结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `t1` (</span><br><span class="line"> `id` int(11) NOT NULL,</span><br><span class="line"> `a` int(11) DEFAULT NULL,</span><br><span class="line"> `b` int(11) DEFAULT NULL,</span><br><span class="line"> `c` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB;</span><br><span class="line"></span><br><span class="line">create table t2 like t1;</span><br><span class="line">create table t3 like t2;</span><br><span class="line">insert into ... //初始化三张表的数据</span><br></pre></td></tr></table></figure>

<p>语句的需求实现如下的join逻辑：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from t1 join t2 on(t1.a=t2.a) join t3 on (t2.b=t3.b) where t1.c&gt;=X and t2.c&gt;=Y and t3.c&gt;=Z;</span><br></pre></td></tr></table></figure>

<p>现在为了得到<strong>最快的执行速度</strong>，如果让你来设计表t1、t2、t3上的索引，来支持这个join语句，你会加哪些索引呢？</p>
<p>同时，如果我希望你用straight_join来重写这个语句，配合你创建的索引，你就需要安排连接顺序，你主要考虑的因素是什么呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我在上篇文章最后留给你的问题，已经在本篇文章中解答了。</p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/35-join-yu-ju-zen-mo-you-hua">35 join语句怎么优化？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>什么时候会使用内部临时表？</title>
    <url>/2024/09/13/MySQL37/</url>
    <content><![CDATA[<p>在<a href="https://time.geekbang.org/column/article/73479">第16</a>和<a href="https://time.geekbang.org/column/article/79700">第34</a>篇文章中，我分别和你介绍了sort buffer、内存临时表和join buffer。<strong>这三个数据结构都是用来存放语句执行过程中的中间数据，以辅助SQL语句的执行的。其中，我们在排序的时候用到了sort buffer，在使用join语句的时候用到了join buffer。</strong></p>
<p>然后，你可能会有这样的疑问，MySQL什么时候会使用内部临时表呢？</p>
<p>今天这篇文章，<strong>我就先给你举两个需要用到内部临时表的例子，来看看内部临时表是怎么工作的。然后，我们再来分析，什么情况下会使用内部临时表。</strong></p>
<h3 id="union-执行流程"><a href="#union-执行流程" class="headerlink" title="union 执行流程"></a>union 执行流程</h3><p>为了便于量化分析，我用下面的表t1来举例。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table t1(id int primary key, a int, b int, index(a));</span><br><span class="line">delimiter ;;</span><br><span class="line">create procedure idata()</span><br><span class="line">begin</span><br><span class="line">  declare i int;</span><br><span class="line"></span><br><span class="line">  set i=1;</span><br><span class="line">  while(i&lt;=1000)do</span><br><span class="line">    insert into t1 values(i, i, i);</span><br><span class="line">    set i=i+1;</span><br><span class="line">  end while;</span><br><span class="line">end;;</span><br><span class="line">delimiter ;</span><br><span class="line">call idata();</span><br></pre></td></tr></table></figure>

<p>然后，我们执行下面这条语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">(select 1000 as f) union (select id from t1 order by id desc limit 2);</span><br></pre></td></tr></table></figure>

<p>这条语句用到了union，它的语义是，<strong>取这两个子查询结果的并集。并集的意思就是这两个集合加起来，重复的行只保留一行。</strong></p>
<p>下图是这个语句的explain结果。</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173204559.png" alt="image-20240913173204559"></p>
<p>可以看到：</p>
<ul>
<li>第二行的key&#x3D;PRIMARY，<strong>说明第二个子句用到了索引id。</strong></li>
<li>第三行的Extra字段，<strong>表示在对子查询的结果集做union的时候，使用了临时表(Using temporary)。</strong></li>
</ul>
<p>这个语句的执行流程是这样的：</p>
<ol>
<li><strong>创建一个内存临时表，这个临时表只有一个整型字段f，并且f是主键字段</strong>。</li>
<li>执行第一个子查询，得到1000这个值，并存入临时表中。</li>
<li>执行第二个子查询：<ul>
<li><strong>拿到第一行id&#x3D;1000，试图插入临时表中。但由于1000这个值已经存在于临时表了，违反了唯一性约束，所以插入失败，然后继续执行；</strong></li>
<li><strong>取到第二行id&#x3D;999，插入临时表成功。</strong></li>
</ul>
</li>
<li><strong>从临时表中按行取出数据，返回结果，并删除临时表，结果中包含两行数据分别是1000和999。</strong></li>
</ol>
<p>这个过程的流程图如下所示：</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173231736.png" alt="image-20240913173231736"></p>
<p>可以看到，<strong>这里的内存临时表起到了暂存数据的作用，而且计算过程还用上了临时表主键id的唯一性约束，实现了union的语义。</strong></p>
<p>顺便提一下，<strong>如果把上面这个语句中的union改成union all的话，就没有了“去重”的语义。这样执行的时候，就依次执行子查询，得到的结果直接作为结果集的一部分，发给客户端。因此也就不需要临时表了。</strong></p>
<p><img src="/2024/09/13/MySQL37/image-20240913173244260.png" alt="image-20240913173244260"></p>
<p>可以看到，<strong>第二行的Extra字段显示的是Using index，表示只使用了覆盖索引，没有用临时表了。</strong></p>
<h3 id="group-by-执行流程"><a href="#group-by-执行流程" class="headerlink" title="group by 执行流程"></a>group by 执行流程</h3><p>另外一个<strong>常见的使用临时表的例子是group by</strong>，我们来看一下这个语句：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select id%10 as m, count(*) as c from t1 group by m;</span><br></pre></td></tr></table></figure>

<p>这个语句的逻辑是把表t1里的数据，<strong>按照 id%10 进行分组统计，并按照m的结果排序后输出</strong>。它的explain结果如下：</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173306798.png" alt="image-20240913173306798"></p>
<p>在Extra字段里面，我们可以看到三个信息：</p>
<ul>
<li><strong>Using index</strong>，表示这个语句<strong>使用了覆盖索引</strong>，选择了索引a，<strong>不需要回表</strong>；</li>
<li><strong>Using temporary</strong>，<strong>表示使用了临时表</strong>；</li>
<li><strong>Using filesort</strong>，<strong>表示需要排序</strong>。</li>
</ul>
<p>这个语句的执行流程是这样的：</p>
<ol>
<li><strong>创建内存临时表，表里有两个字段m和c，主键是m；</strong></li>
<li>扫描表t1的索引a，<strong>依次取出叶子节点上的id值，计算id%10的结果，记为x</strong>；<ul>
<li>如果临时表中没有主键为x的行，就插入一个记录(x,1);</li>
<li>如果表中有主键为x的行，就将x这一行的c值加1；</li>
</ul>
</li>
<li>遍历完成后，<strong>再根据字段m做排序</strong>，得到结果集返回给客户端。</li>
</ol>
<p>这个流程的执行图如下：</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173316396.png" alt="image-20240913173316396"></p>
<p>图中最后一步，<strong>对内存临时表的排序，在<a href="https://time.geekbang.org/column/article/73795">第17篇文章</a>中已经有过介绍，我把图贴过来</strong>，方便你回顾。</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173326878.png" alt="image-20240913173326878"></p>
<p>其中，临时表的排序过程就是图6中虚线框内的过程。</p>
<p>接下来，我们再看一下这条语句的执行结果：</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173340506.png" alt="image-20240913173340506"></p>
<p>如果<strong>你的需求并不需要对结果进行排序，那你可以在SQL语句末尾增加order by null</strong>，也就是改成：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select id%10 as m, count(*) as c from t1 group by m order by null;</span><br></pre></td></tr></table></figure>

<p>这样就跳过了最后排序的阶段，<strong>直接从临时表中取数据返回</strong>。返回的结果如图8所示。</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173404604.png" alt="image-20240913173404604"></p>
<p>由于表t1中的id值是从1开始的，因此返回的结果集中第一行是id&#x3D;1；<strong>扫描到id&#x3D;10的时候才插入m&#x3D;0这一行，因此结果集里最后一行才是m&#x3D;0</strong>。</p>
<p>这个例子里由于临时表只有10行，<strong>内存可以放得下，因此全程只使用了内存临时表。但是，内存临时表的大小是有限制的，参数tmp_table_size就是控制这个内存大小的，默认是16M。</strong></p>
<p>如果我执行下面这个语句序列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set tmp_table_size=1024;</span><br><span class="line">select id%100 as m, count(*) as c from t1 group by m order by null limit 10;</span><br></pre></td></tr></table></figure>

<p>把内存临时表的大小限制为最大1024字节，并把语句改成id % 100，这样返回结果里有100行数据。<strong>但是，这时的内存临时表大小不够存下这100行数据，也就是说，执行过程中会发现内存临时表大小到达了上限（1024字节）。</strong></p>
<p>那么，<strong>这时候就会把内存临时表转成磁盘临时表，磁盘临时表默认使用的引擎是InnoDB</strong>。 这时，返回的结果如图9所示。</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173427874.png" alt="image-20240913173427874"></p>
<p>如果这个表t1的数据量很大，<strong>很可能这个查询需要的磁盘临时表就会占用大量的磁盘空间</strong>。</p>
<h3 id="group-by-优化方法-–索引"><a href="#group-by-优化方法-–索引" class="headerlink" title="group by 优化方法 –索引"></a>group by 优化方法 –索引</h3><p>可以看到，<strong>不论是使用内存临时表还是磁盘临时表，group by逻辑都需要构造一个带唯一索引的表</strong>，执行代价都是比较高的。如果表的数据量比较大，上面这个group by语句执行起来就会很慢，我们有什么优化的方法呢？</p>
<p>要解决group by语句的优化问题，你可以先想一下这个问题：执行group by语句为什么需要临时表？</p>
<p>group by的语义逻辑，<strong>是统计不同的值出现的个数。但是，由于每一行的id%100的结果是无序的，所以我们就需要有一个临时表，来记录并统计结果。</strong></p>
<p>那么，<strong>如果扫描过程中可以保证出现的数据是有序的</strong>，是不是就简单了呢？</p>
<p>假设，现在有一个类似图10的这么一个数据结构，我们来看看group by可以怎么做。</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173439134.png" alt="image-20240913173439134"></p>
<p>可以看到，<strong>如果可以确保输入的数据是有序的，那么计算group by的时候，就只需要从左到右，顺序扫描，依次累加。也就是下面这个过程：</strong></p>
<ul>
<li>当碰到第一个1的时候，已经知道累积了X个0，结果集里的第一行就是(0,X);</li>
<li>当碰到第一个2的时候，已经知道累积了Y个1，结果集里的第一行就是(1,Y);</li>
</ul>
<p><strong>按照这个逻辑执行的话，扫描到整个输入的数据结束，就可以拿到group by的结果，不需要临时表，也不需要再额外排序。</strong></p>
<p>你一定想到了，<strong>InnoDB的索引，就可以满足这个输入有序的条件</strong>。</p>
<p>在MySQL 5.7版本支持了<strong>generated column机制，用来实现列数据的关联更新</strong>。你可以用下面的方法创建一个列z，<strong>然后在z列上创建一个索引（如果是MySQL 5.6及之前的版本，你也可以创建普通列和索引，来解决这个问题）。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table t1 add column z int generated always as(id % 100), add index(z);</span><br></pre></td></tr></table></figure>

<p>看不懂的话，建议看看这个解释：</p>
<ol>
<li>**<code>alter table t1</code>**：<ul>
<li>这是一个用于修改表结构的命令，指定要修改的表是 <code>t1</code>。</li>
</ul>
</li>
<li>**<code>add column z int generated always as (id % 100)</code>**：<ul>
<li>这部分表示添加一个新列 <code>z</code>。</li>
<li><code>z</code> 的数据类型是 <code>int</code>。</li>
<li><code>generated always as (id % 100)</code> 表示 <code>z</code> 列是一个计算列，它的值总是根据 <code>id</code> 列的值计算得出，即 <code>id</code> 对 100 取模的结果。</li>
<li>也就是说，<code>z</code> 的值将自动计算并存储为 <code>id</code> 除以 100 的余数。</li>
</ul>
</li>
<li>**<code>add index(z)</code>**：<ul>
<li>这部分表示为新添加的列 <code>z</code> 创建一个索引。</li>
<li>索引可以加快对 <code>z</code> 列的查询速度，尤其是在 <code>z</code> 列被用于过滤或排序时。</li>
</ul>
</li>
</ol>
<p>仔细一看的话和这张图差不多了（索引已经排好序了）</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173439134.png" alt="image-20240913173439134"></p>
<p>这样，索引z上的数据就是类似图10这样有序的了。<strong>上面的group by语句就可以改成：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select z, count(*) as c from t1 group by z;</span><br></pre></td></tr></table></figure>

<p>优化后的group by语句的explain结果，如下图所示：</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173508732.png" alt="image-20240913173508732"></p>
<p>从Extra字段可以看到，<strong>这个语句的执行不再需要临时表，也不需要排序了</strong>。</p>
<h3 id="group-by优化方法-–直接排序"><a href="#group-by优化方法-–直接排序" class="headerlink" title="group by优化方法 –直接排序"></a>group by优化方法 –直接排序</h3><p>所以，<strong>如果可以通过加索引来完成group by逻辑就再好不过了。但是，如果碰上不适合创建索引的场景，我们还是要老老实实做排序的</strong>。那么，这时候的group by要怎么优化呢？</p>
<p>如果我们明明知道，<strong>一个group by语句中需要放到临时表上的数据量特别大，却还是要按照“先放到内存临时表，插入一部分数据后，发现内存临时表不够用了再转成磁盘临时表”，看上去就有点儿傻。</strong></p>
<p>那么，我们就会想了，MySQL有没有让我们直接走磁盘临时表的方法呢？</p>
<p>答案是，有的。</p>
<p><strong>在group by语句中加入SQL_BIG_RESULT这个提示（hint），就可以告诉优化器：这个语句涉及的数据量很大，请直接用磁盘临时表。</strong></p>
<p>MySQL的优化器一看，<strong>磁盘临时表是B+树存储，存储效率不如数组来得高。所以，既然你告诉我数据量很大，那从磁盘空间考虑，还是直接用数组来存吧。</strong></p>
<p>因此，下面这个语句</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m;</span><br></pre></td></tr></table></figure>

<p>的执行流程就是这样的：</p>
<ol>
<li><strong>初始化sort_buffer</strong>，确定放入一个整型字段，记为m；</li>
<li>扫描表t1的索引a，依次取出里面的id值, 将 id%100的值存入sort_buffer中；</li>
<li>扫描完成后，对sort_buffer的字段m做排序<strong>（如果sort_buffer内存不够用，就会利用磁盘临时文件辅助排序）；</strong></li>
<li><strong>排序完成后，就得到了一个有序数组</strong>。</li>
</ol>
<p>根据有序数组，得到数组里面的不同值，以及每个值的出现次数。这一步的逻辑，你已经从前面的图10中了解过了。</p>
<p>下面两张图分别是执行流程图和执行explain命令得到的结果。</p>
<p><img src="/2024/09/13/MySQL37/image-20240913173534548.png" alt="image-20240913173534548"></p>
<p><img src="/2024/09/13/MySQL37/image-20240913173543600.png" alt="image-20240913173543600"></p>
<p>从Extra字段可以看到，<strong>这个语句的执行没有再使用临时表，而是直接用了排序算法</strong>。</p>
<p>基于上面的union、union all和group by语句的执行过程的分析，我们来回答文章开头的问题：MySQL什么时候会使用内部临时表？</p>
<ol>
<li><strong>如果语句执行过程可以一边读数据，一边直接得到结果，是不需要额外内存的，否则就需要额外的内存，来保存中间结果；（在MySQL 5.7版本支持了generated column机制）</strong></li>
<li><strong>join_buffer是无序数组，sort_buffer是有序数组，临时表是二维表结构；</strong></li>
<li><strong>如果执行逻辑需要用到二维表特性，就会优先考虑使用临时表。比如我们的例子中，union需要用到唯一索引约束， group by还需要用到另外一个字段来存累积计数。</strong></li>
</ol>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>通过今天这篇文章，我重点和你讲了group by的几种实现算法，从中可以总结一些使用的指导原则：</p>
<ol>
<li><strong>如果对group by语句的结果没有排序要求，要在语句后面加 order by null；</strong></li>
<li><strong>尽量让group by过程用上表的索引，确认方法是explain结果里没有Using temporary 和 Using filesort；</strong></li>
<li><strong>如果group by需要统计的数据量不大，尽量只使用内存临时表；也可以通过适当调大tmp_table_size参数，来避免用到磁盘临时表；</strong></li>
<li><strong>如果数据量实在太大，使用SQL_BIG_RESULT这个提示，来告诉优化器直接使用排序算法得到group by的结果。</strong></li>
</ol>
<p>文章中图8和图9都是order by null，为什么图8的返回结果里面，0是在结果集的最后一行，而图9的结果里面，0是在结果集的第一行？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期的问题是：为什么不能用rename修改临时表的改名。</p>
<p>在实现上，执行rename table语句的时候，<strong>要求按照“库名&#x2F;表名.frm”的规则去磁盘找文件，但是临时表在磁盘上的frm文件是放在tmpdir目录下的，并且文件名的规则是“#sql{进程id}_{线程id}_序列号.frm”，因此会报“找不到文件名”的错误</strong>。</p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/37-shi-mo-shi-hou-hui-shi-yong-nei-bu-lin-shi-biao#group-by-zhi-xing-liu-cheng">37 什么时候会使用内部临时表？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>自增主键为什么不是连续的？</title>
    <url>/2024/09/14/MySQL39/</url>
    <content><![CDATA[<p>在<a href="https://time.geekbang.org/column/article/69236">第4篇文章</a>中，<strong>我们提到过自增主键，由于自增主键可以让主键索引尽量地保持递增顺序插入，避免了页分裂（为了保证索引叶子下面元素的有序性，有时候会执行插入操作，如果是插入页之间会发生页分裂腾出空间），因此索引更紧凑。</strong></p>
<p><strong>之前我见过有的业务设计依赖于自增主键的连续性，也就是说，这个设计假设自增主键是连续的。但实际上，这样的假设是错的，因为自增主键不能保证连续递增。</strong></p>
<p>今天这篇文章，我们就来说说这个问题，<strong>看看什么情况下自增主键会出现 “空洞”？</strong></p>
<p>为了便于说明，我们创建一个表t，其中id是自增主键字段、c是唯一索引。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `t` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `c` int(11) DEFAULT NULL,</span><br><span class="line">  `d` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  UNIQUE KEY `c` (`c`)</span><br><span class="line">) ENGINE=InnoDB;</span><br></pre></td></tr></table></figure>

<h3 id="自增值保存在哪儿？"><a href="#自增值保存在哪儿？" class="headerlink" title="自增值保存在哪儿？"></a>自增值保存在哪儿？</h3><p>推荐先看看这篇文章：</p>
<ul>
<li><a href="https://blog.csdn.net/shunchang/article/details/106751943">（数据库-MySQL）查看表的结构、表的创建过程、表_show tables 遍历表 show create table-CSDN博客</a></li>
</ul>
<p>在这个空表t里面执行insert into t values(null, 1, 1);插入一行数据，再执行<strong>show create table命令</strong>，就可以看到如下图所示的结果：</p>
<p><img src="/2024/09/14/MySQL39/image-20240914114549239.png" alt="image-20240914114549239"></p>
<p>可以看到，<strong>表定义里面出现了一个AUTO_INCREMENT&#x3D;2，表示下一次插入数据时，如果需要自动生成自增值，会生成id&#x3D;2。</strong></p>
<p>其实，这个输出结果容易引起这样的误解：<strong>自增值是保存在表结构定义里的</strong>。实际上，<strong>表的结构定义存放在后缀名为.frm的文件中，但是并不会保存自增值。</strong></p>
<p>不同的引擎对于自增值的保存策略不同。</p>
<ul>
<li><strong>MyISAM引擎的自增值保存在数据文件中。</strong></li>
<li><strong>InnoDB引擎的自增值，其实是保存在了内存里</strong>，并且到了<strong>MySQL 8.0版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为MySQL重启前的值”，具体情况是：</strong><ul>
<li>在MySQL 5.7及之前的版本，<strong>自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值max(id)，然后将max(id)+1作为这个表当前的自增值。﻿ 举例来说，如果一个表当前数据行里最大的id是10，AUTO_INCREMENT&#x3D;11。这时候，我们删除id&#x3D;10的行，AUTO_INCREMENT还是11。但如果马上重启实例，重启后这个表的AUTO_INCREMENT就会变成10。﻿ 也就是说，MySQL重启可能会修改一个表的AUTO_INCREMENT的值。</strong></li>
<li>在MySQL 8.0版本，<strong>将自增值的变更记录在了redo log中，重启的时候依靠redo log恢复重启之前的值。</strong></li>
</ul>
</li>
</ul>
<p>理解了MySQL对自增值的保存策略以后，我们再看看自增值修改机制。</p>
<h3 id="自增值修改机制"><a href="#自增值修改机制" class="headerlink" title="自增值修改机制"></a>自增值修改机制</h3><p>在MySQL里面，<strong>如果字段id被定义为AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下：</strong></p>
<ol>
<li><strong>如果插入数据时id字段指定为0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT值填到自增字段；</strong></li>
<li><strong>如果插入数据时id字段指定了具体的值，就直接使用语句里指定的值。</strong></li>
</ol>
<p>根据要插入的值和当前自增值的大小关系，自增值的变更结果也会有所不同。<strong>假设，某次要插入的值是X-1，当前的自增值是Y-3。</strong></p>
<ol>
<li><strong>如果X-1&lt;Y-3，那么这个表的自增值不变；</strong></li>
<li><strong>如果X-3≥Y-1，就需要把当前自增值修改为新的自增值。</strong></li>
</ol>
<p><strong>新的自增值生成算法是</strong>：<strong>从auto_increment_offset开始，以auto_increment_increment为步长，持续叠加，直到找到第一个大于X的值，作为新的自增值。</strong></p>
<p>其中，auto_increment_offset 和 auto_increment_increment是两个系统参数，<strong>分别用来表示自增的初始值和步长，默认值都是1。</strong></p>
<blockquote>
<p>备注：<strong>在一些场景下，使用的就不全是默认值。比如，双M的主备结构里要求双写的时候，我们就可能会设置成auto_increment_increment&#x3D;2，让一个库的自增id都是奇数，另一个库的自增id都是偶数，避免两个库生成的主键发生冲突。</strong></p>
</blockquote>
<p>当auto_increment_offset和auto_increment_increment都是1的时候，新的自增值生成逻辑很简单，就是：</p>
<ol>
<li><strong>如果准备插入的值&gt;&#x3D;当前自增值，新的自增值就是“准备插入的值+1”；</strong></li>
<li><strong>否则，自增值不变。</strong></li>
</ol>
<p><strong>这就引入了我们文章开头提到的问题，在这两个参数都设置为1的时候，自增主键id却不能保证是连续的，这是什么原因呢？</strong></p>
<h3 id="自增值的修改时机"><a href="#自增值的修改时机" class="headerlink" title="自增值的修改时机"></a>自增值的修改时机</h3><p>要回答这个问题，我们就要看一下自增值的修改时机。</p>
<p>假设，表t里面已经有了(1,1,1)这条记录，这时我再执行一条插入数据命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t values(null, 1, 1); </span><br></pre></td></tr></table></figure>

<p>这个语句的执行流程就是：</p>
<ol>
<li><strong>执行器调用InnoDB引擎接口写入一行，传入的这一行的值是(0,1,1);</strong></li>
<li><strong>InnoDB发现用户没有指定自增id的值，获取表t当前的自增值2；</strong></li>
<li><strong>将传入的行的值改成(2,1,1);</strong></li>
<li><strong>将表的自增值改成3；</strong></li>
<li><strong>继续执行插入数据操作，由于已经存在c&#x3D;1的记录，所以报Duplicate key error，语句返回。</strong></li>
</ol>
<p>对应的执行流程图如下：</p>
<p><img src="/2024/09/14/MySQL39/image-20240914114647431.png" alt="image-20240914114647431"></p>
<p>可以看到，<strong>这个表的自增值改成3，是在真正执行插入数据的操作之前。这个语句真正执行的时候，因为碰到唯一键c冲突，所以id&#x3D;2这一行并没有插入成功，但也没有将自增值再改回去。</strong></p>
<p><strong>所以，在这之后，再插入新的数据行时，拿到的自增id就是3。也就是说，出现了自增主键不连续的情况。</strong></p>
<p>如图3所示就是完整的演示结果。</p>
<p><img src="/2024/09/14/MySQL39/image-20240914114709228.png" alt="image-20240914114709228"></p>
<p>可以看到，这个操作序列复现了一个自增主键id不连续的现场(没有id&#x3D;2的行）。可见，<strong>唯一键冲突是导致自增主键id不连续的第一种原因。</strong></p>
<p>同样地，事务<strong>回滚也会产生类似的现象，这就是第二种原因。</strong></p>
<p>下面这个语句序列<strong>就可以构造不连续的自增id，你可以自己验证一下。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t values(null,1,1);</span><br><span class="line">begin;</span><br><span class="line">insert into t values(null,2,2);</span><br><span class="line">rollback;</span><br><span class="line">insert into t values(null,2,2);</span><br><span class="line">//插入的行是(3,2,2)</span><br></pre></td></tr></table></figure>

<p>你可能会问，<strong>为什么在出现唯一键冲突或者回滚的时候，MySQL没有把表t的自增值改回去呢？如果把表t的当前自增值从3改回2，再插入新数据的时候，不就可以生成id&#x3D;2的一行数据了吗？</strong></p>
<p>其实，<strong>MySQL这么设计是为了提升性能</strong>。接下来，我就跟你分析一下这个设计思路，看看<strong>自增值为什么不能回退。</strong></p>
<p>假设有两个并行执行的事务，<strong>在申请自增值的时候，为了避免两个事务申请到相同的自增id，肯定要加锁，然后顺序申请。</strong></p>
<ol>
<li>假设事务A申请到了id&#x3D;2， 事务B申请到id&#x3D;3，那么这时候表t的自增值是4，之后继续执行。</li>
<li>事务B正确提交了，但事务A出现了唯一键冲突。</li>
<li><strong>如果允许事务A把自增id回退，也就是把表t的当前自增值改回2，那么就会出现这样的情况：表里面已经有id&#x3D;3的行，而当前的自增id值是2。</strong></li>
<li><strong>接下来，继续执行的其他事务就会申请到id&#x3D;2，然后再申请到id&#x3D;3。这时，就会出现插入语句报错“主键冲突”。</strong></li>
</ol>
<p>而为了解决这个主键冲突，有两种方法：</p>
<ol>
<li><strong>每次申请id之前，先判断表里面是否已经存在这个id。如果存在，就跳过这个id。但是，这个方法的成本很高。因为，本来申请id是一个很快的操作，现在还要再去主键索引树上判断id是否存在。</strong></li>
<li>把自增id的锁范围扩大，必须等到一个事务执行完成并提交，下一个事务才能再申请自增id。<strong>这个方法的问题，就是锁的粒度太大，系统并发能力大大下降。</strong></li>
</ol>
<p>可见，<strong>这两个方法都会导致性能问题。造成这些麻烦的罪魁祸首，就是我们假设的这个“允许自增id回退”的前提导致的。</strong></p>
<p>因此，InnoDB放弃了这个设计，<strong>语句执行失败也不回退自增id。也正是因为这样，所以才只保证了自增id是递增的，但不保证是连续的。</strong></p>
<h3 id="自增锁的优化"><a href="#自增锁的优化" class="headerlink" title="自增锁的优化"></a>自增锁的优化</h3><p>可以看到，<strong>自增id锁并不是一个事务锁，而是每次申请完就马上释放，以便允许别的事务再申请。其实，在MySQL 5.1版本之前，并不是这样的。</strong></p>
<p>接下来，我会先给你介绍下自增锁设计的历史，这样有助于你分析接下来的一个问题。</p>
<p>在MySQL 5.0版本的时候，<strong>自增锁的范围是语句级别。也就是说，如果一个语句申请了一个表自增锁，这个锁会等语句执行结束以后才释放。显然，这样设计会影响并发度。</strong></p>
<p>MySQL 5.1.22版本引入了一个新策略，<strong>新增参数innodb_autoinc_lock_mode，默认值是1。</strong></p>
<ol>
<li>这个参数的值被设置为0时，<strong>表示采用之前MySQL 5.0版本的策略，即语句执行结束后才释放锁；</strong></li>
<li>这个参数的值被设置为1时：<ul>
<li><strong>普通insert语句，自增锁在申请之后就马上释放；</strong></li>
<li><strong>类似insert … select这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放；</strong></li>
</ul>
</li>
<li><strong>这个参数的值被设置为2时，所有的申请自增主键的动作都是申请后就释放锁。</strong></li>
</ol>
<p>你一定有两个疑问：<strong>为什么默认设置下，insert … select 要使用语句级的锁？为什么这个参数的默认值不是2？</strong></p>
<p>答案是，这么设计还是为了数据的一致性。</p>
<p>我们一起来看一下这个场景：</p>
<p><img src="/2024/09/14/MySQL39/image-20240914114755762.png" alt="image-20240914114755762"></p>
<p>在这个例子里，我往表t1中插入了4行数据，然后创建了一个相同结构的表t2，然后两个session同时执行向表t2中插入数据的操作。</p>
<p>你可以设想一下，如果session B是申请了自增值以后马上就释放自增锁，那么就可能出现这样的情况：</p>
<ul>
<li>session B先插入了两个记录，(1,1,1)、(2,2,2)；</li>
<li>然后，session A来申请自增id得到id&#x3D;3，插入了（3,5,5)；</li>
<li>之后，session B继续执行，插入两条记录(4,3,3)、 (5,4,4)。</li>
</ul>
<p>你可能会说，这也没关系吧，毕竟session B的语义本身就没有要求表t2的所有行的数据都跟session A相同。</p>
<p>是的，从数据逻辑上看是对的。但是，如果我们现在的binlog_format&#x3D;statement，你可以设想下，binlog会怎么记录呢？</p>
<p>由于两个session是<strong>同时执行插入数据命令的，所以binlog里面对表t2的更新日志只有两种情况：要么先记session A的，要么先记session B的。</strong></p>
<p>但不论是哪一种，这个binlog拿去从库执行，或者用来恢复临时实例，备库和临时实例里面，session B这个语句执行出来，生成的结果里面，id都是连续的。这时，这个库就发生了数据不一致。</p>
<p>你可以分析一下，出现这个问题的原因是什么？</p>
<p>其实，这是因为原库session B的insert语句，生成的id不连续。这个不连续的id，用statement格式的binlog来串行执行，是执行不出来的。</p>
<p>而要解决这个问题，有两种思路：</p>
<ol>
<li>一种思路是，让原库的批量插入数据语句，固定生成连续的id值。所以，自增锁直到语句执行结束才释放，就是为了达到这个目的。</li>
<li>另一种思路是，在binlog里面把插入数据的操作都如实记录进来，到备库执行的时候，不再依赖于自增主键去生成。这种情况，其实就是innodb_autoinc_lock_mode设置为2，同时binlog_format设置为row。</li>
</ol>
<p>因此，<strong>在生产上，尤其是有insert … select这种批量插入数据的场景时，从并发插入数据性能的角度考虑，我建议你这样设置：innodb_autoinc_lock_mode&#x3D;2 ，并且 binlog_format&#x3D;row</strong>.这样做，既能提升并发性，又不会出现数据一致性问题。</p>
<p>需要注意的是，我这里说的<strong>批量插入数据，包含的语句类型是insert … select、replace … select和load data语句。</strong></p>
<p>但是，<strong>在普通的insert语句里面包含多个value值的情况下，即使innodb_autoinc_lock_mode设置为1，也不会等语句执行完成才释放锁。因为这类语句在申请自增id的时候，是可以精确计算出需要多少个id的，然后一次性申请，申请完成后锁就可以释放了。</strong></p>
<p>也就是说，<strong>批量插入数据的语句，之所以需要这么设置，是因为“不知道要预先申请多少个id”。</strong></p>
<p>既然预先不知道要申请多少个自增id，<strong>那么一种直接的想法就是需要一个时申请一个。但如果一个select … insert语句要插入10万行数据，按照这个逻辑的话就要申请10万次。显然，这种申请自增id的策略，在大批量插入数据的情况下，不但速度慢，还会影响并发插入的性能。</strong></p>
<p>因此，<strong>对于批量插入数据的语句，MySQL有一个批量申请自增id的策略：</strong></p>
<ol>
<li>语句执行过程中，第一次申请自增id，会分配1个；</li>
<li>1个用完以后，这个语句第二次申请自增id，会分配2个；</li>
<li>2个用完以后，还是这个语句，第三次申请自增id，会分配4个；</li>
<li><strong>依此类推，同一个语句去申请自增id，每次申请到的自增id个数都是上一次的两倍。</strong></li>
</ol>
<p>举个例子，我们一起看看下面的这个语句序列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t values(null, 1,1);</span><br><span class="line">insert into t values(null, 2,2);</span><br><span class="line">insert into t values(null, 3,3);</span><br><span class="line">insert into t values(null, 4,4);</span><br><span class="line">create table t2 like t;</span><br><span class="line">insert into t2(c,d) select c,d from t;</span><br><span class="line">insert into t2 values(null, 5,5);</span><br></pre></td></tr></table></figure>

<p><strong>insert…select，实际上往表t2中插入了4行数据。</strong>但是，这四行数据是分三次申请的自增id，第一次申请到了id&#x3D;1，第二次被分配了id&#x3D;2和id&#x3D;3， 第三次被分配到id&#x3D;4到id&#x3D;7。</p>
<p><strong>由于这条语句实际只用上了4个id，所以id&#x3D;5到id&#x3D;7就被浪费掉了。之后，再执行insert into t2 values(null, 5,5)，实际上插入的数据就是（8,5,5）。</strong></p>
<p><strong>批量插入数据是主键id出现自增id不连续的第三种原因。（唯一键冲突是导致自增主键id不连续的第一种原因，事务回滚也会产生类似的现象）</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天，我们从“自增主键为什么会出现不连续的值”这个问题开始，首先讨论了自增值的存储。</p>
<p><strong>在MyISAM引擎里面，自增值是被写在数据文件上的。而在InnoDB中，自增值是被记录在内存的。MySQL直到8.0版本，才给InnoDB表的自增值加上了持久化的能力，确保重启前后一个表的自增值不变。</strong></p>
<p>然后，<strong>我和你分享了在一个语句执行过程中，自增值改变的时机，分析了为什么MySQL在事务回滚的时候不能回收自增id。</strong></p>
<p>MySQL 5.1.22版本开始引入的参数innodb_autoinc_lock_mode，控制了自增值申请时的锁范围。<strong>从并发性能的角度考虑，我建议你将其设置为2，同时将binlog_format设置为row。我在前面的文章中其实多次提到，binlog_format设置为row，是很有必要的。</strong>今天的例子给这个结论多了一个理由。</p>
<p>在最后一个例子中，执行insert into t2(c,d) select c,d from t;这个语句的时候，如果隔离级别是可重复读（repeatable read），binlog_format&#x3D;statement。这个语句会对表t的所有记录和间隙加锁。</p>
<p>你觉得为什么需要这么做呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>上期的问题是，如果你维护的MySQL系统里有内存表，怎么避免内存表突然丢数据，然后导致主备同步停止的情况。</p>
<p>我们假设的是主库暂时不能修改引擎，那么就把备库的内存表引擎先都改成InnoDB。对于每个内存表，执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">set sql_log_bin=off;</span><br><span class="line">alter table tbl_name engine=innodb;</span><br></pre></td></tr></table></figure>

<p>这样就能避免备库重启的时候，数据丢失的问题。</p>
<p>由于主库重启后，会往binlog里面写“delete from tbl_name”，这个命令传到备库，备库的同名的表数据也会被清空。</p>
<p>因此，就不会出现主备同步停止的问题。</p>
<p>如果由于主库异常重启，触发了HA，这时候我们之前修改过引擎的备库变成了主库。而原来的主库变成了新备库，在新备库上把所有的内存表（这时候表里没数据）都改成InnoDB表。</p>
<p>所以，如果我们不能直接修改主库上的表引擎，可以配置一个自动巡检的工具，在备库上发现内存表就把引擎改了。</p>
<p>同时，跟业务开发同学约定好建表规则，避免创建新的内存表。</p>
<blockquote>
<p>将数据保存到InnoDB表用来持久化，也是一个方法。不过，我还是建议釜底抽薪，直接修改备库的内存表的引擎。 </p>
<p>主库异常重启的场景，这时候是不会报主备不一致的，因为主库重启的时候写了delete from tbl_name，主备的内存表都清空了。</p>
</blockquote>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/39-zi-zeng-zhu-jian-wei-shi-mo-bu-shi-lian-xu-de">39 自增主键为什么不是连续的？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>insert语句的锁为什么这么多？</title>
    <url>/2024/09/14/MySQL40/</url>
    <content><![CDATA[<p>在上一篇文章中，<strong>我提到MySQL对自增主键锁做了优化，尽量在申请到自增id以后，就释放自增锁</strong>。</p>
<p><strong>因此，insert语句是一个很轻量的操作</strong>。不过，这个结论对于“普通的insert语句”才有效。<strong>也就是说，还有些insert语句是属于“特殊情况”的，在执行过程中需要给其他资源加锁，或者无法在申请到自增id以后就立马释放自增锁</strong>。</p>
<p>那么，今天这篇文章，我们就一起来聊聊这个话题。</p>
<h3 id="insert-…-select-语句"><a href="#insert-…-select-语句" class="headerlink" title="insert … select 语句"></a>insert … select 语句</h3><p>我们先从昨天的问题说起吧。表t和t2的表结构、初始化数据语句如下，今天的例子我们还是针对这两个表展开。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE TABLE `t` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `c` int(11) DEFAULT NULL,</span><br><span class="line">  `d` int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (`id`),</span><br><span class="line">  UNIQUE KEY `c` (`c`)</span><br><span class="line">) ENGINE=InnoDB;</span><br><span class="line"></span><br><span class="line">insert into t values(null, 1,1);</span><br><span class="line">insert into t values(null, 2,2);</span><br><span class="line">insert into t values(null, 3,3);</span><br><span class="line">insert into t values(null, 4,4);</span><br><span class="line"></span><br><span class="line">create table t2 like t</span><br></pre></td></tr></table></figure>

<p>现在，我们一起来看看为什么在可重复读隔离级别下，binlog_format&#x3D;statement时执行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t2(c,d) select c,d from t;</span><br></pre></td></tr></table></figure>

<p>这个语句时，需要对表t的所有行和间隙加锁呢？</p>
<p>其实，这个问题我们需要考虑的还是日志和数据的一致性。我们看下这个执行序列：</p>
<p><img src="/2024/09/14/MySQL40/image-20240914130243546.png" alt="image-20240914130243546"></p>
<p>实际的执行效果是，<strong>如果session B先执行，由于这个语句对表t主键索引加了(-∞,1]这个next-key lock，会在语句执行完成后，才允许session A的insert语句执行。</strong></p>
<p>但如果没有锁的话，就可能出现session B的insert语句先执行，但是后写入binlog的情况。于是，在binlog_format&#x3D;statement的情况下，binlog里面就记录了这样的语句序列：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t values(-1,-1,-1);</span><br><span class="line">insert into t2(c,d) select c,d from t;</span><br></pre></td></tr></table></figure>

<p>这个语句到了备库执行，就会把id&#x3D;-1这一行也写到表t2中，<strong>出现主备不一致。</strong></p>
<h3 id="insert-循环写入"><a href="#insert-循环写入" class="headerlink" title="insert 循环写入"></a>insert 循环写入</h3><p>当然了，<strong>执行insert … select 的时候，对目标表也不是锁全表，而是只锁住需要访问的资源。</strong></p>
<p>如果现在有这么一个需求：要往表t2中插入一行数据，这一行的c值是表t中c值的最大值加1。</p>
<p>此时，我们可以这么写这条SQL语句 ：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t2(c,d)  (select c+1, d from t force index(c) order by c desc limit 1);</span><br></pre></td></tr></table></figure>

<p><strong>这个语句的加锁范围，就是表t索引c上的(3,4]和(4,supremum]这两个next-key lock，以及主键索引上id&#x3D;4这一行。</strong></p>
<p>它的执行流程也比较简单，<strong>从表t中按照索引c倒序，扫描第一行，拿到结果写入到表t2中。</strong></p>
<p><strong>因此整条语句的扫描行数是1。</strong></p>
<p>这个语句执行的慢查询日志（slow log），如下图所示：</p>
<p><img src="/2024/09/14/MySQL40/image-20240914130324728.png" alt="image-20240914130324728"></p>
<p><strong>通过这个慢查询日志，我们看到Rows_examined&#x3D;1，正好验证了执行这条语句的扫描行数为1。</strong></p>
<p>那么，如果我们是要把这样的一行数据插入到表t中的话：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t(c,d)  (select c+1, d from t force index(c) order by c desc limit 1);</span><br></pre></td></tr></table></figure>

<p>语句的执行流程是怎样的？扫描行数又是多少呢？</p>
<p>这时候，我们再看慢查询日志就会发现不对了。</p>
<p><img src="/2024/09/14/MySQL40/image-20240914130343354.png" alt="image-20240914130343354"></p>
<p>可以看到，这时候的Rows_examined的值是5。</p>
<p>我在前面的文章中提到过，希望你都能够学会用explain的结果来“脑补”整条语句的执行过程。今天，我们就来一起试试。</p>
<p>如图4所示就是这条语句的explain结果。</p>
<p><img src="/2024/09/14/MySQL40/image-20240914130353291.png" alt="image-20240914130353291"></p>
<p><strong>从Extra字段可以看到“Using temporary”字样，表示这个语句用到了临时表。也就是说，执行过程中，需要把表t的内容读出来，写入临时表。</strong></p>
<p>图中rows显示的是1，我们不妨先对这个语句的执行流程做一个猜测：如果说是把子查询的结果读出来（扫描1行），写入临时表，然后再从临时表读出来（扫描1行），写回表t中。那么，这个语句的扫描行数就应该是2，而不是5。</p>
<p>所以，这个猜测不对。<strong>实际上，Explain结果里的rows&#x3D;1是因为受到了limit 1 的影响。</strong></p>
<p>从另一个角度考虑的话，我们可以看看InnoDB扫描了多少行。如图5所示，是在执行这个语句前后查看Innodb_rows_read的结果。</p>
<p><img src="/2024/09/14/MySQL40/image-20240914130412603.png" alt="image-20240914130412603"></p>
<p>可以看到，这个语句执行前后，<strong>Innodb_rows_read的值增加了4。因为默认临时表是使用Memory引擎的，所以这4行查的都是表t，也就是说对表t做了全表扫描</strong>。</p>
<p>这样，我们就把整个执行过程理清楚了：</p>
<ol>
<li><strong>创建临时表，表里有两个字段c和d。</strong></li>
<li><strong>按照索引c扫描表t，依次取c&#x3D;4、3、2、1，然后回表，读到c和d的值写入临时表。这时，Rows_examined&#x3D;4。</strong></li>
<li><strong>由于语义里面有limit 1，所以只取了临时表的第一行，再插入到表t中。这时，Rows_examined的值加1，变成了5。（这下看懂了）</strong></li>
</ol>
<p>也就是说，<strong>这个语句会导致在表t上做全表扫描，并且会给索引c上的所有间隙都加上共享的next-key lock。所以，这个语句执行期间，其他事务不能在这个表上插入数据。</strong></p>
<p>至于这个语句的执行为什么需要临时表，<strong>原因是这类一边遍历数据，一边更新数据的情况，如果读出来的数据直接写回原表，就可能在遍历过程中，读到刚刚插入的记录，新插入的记录如果参与计算逻辑，就跟语义不符</strong>。</p>
<p>由于实现上这个语句<strong>没有在子查询中就直接使用limit 1</strong>，<strong>从而导致了这个语句的执行需要遍历整个表t</strong>。它的优化方法也比较简单，就是用前面介绍的方法，<strong>先insert into到临时表temp_t，这样就只需要扫描一行；然后再从表temp_t里面取出这行数据插入表t1。</strong></p>
<p>当然，<strong>由于这个语句涉及的数据量很小，你可以考虑使用内存临时表来做这个优化。使用内存临时表优化时，语句序列的写法如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create temporary table temp_t(c int,d int) engine=memory;</span><br><span class="line">insert into temp_t  (select c+1, d from t force index(c) order by c desc limit 1);</span><br><span class="line">insert into t select * from temp_t;</span><br><span class="line">drop table temp_t;</span><br></pre></td></tr></table></figure>

<h3 id="insert-唯一键冲突"><a href="#insert-唯一键冲突" class="headerlink" title="insert 唯一键冲突"></a>insert 唯一键冲突</h3><p>前面的两个例子是使用insert … select的情况，接下来我要介绍的这个例子就是<strong>最常见的insert语句出现唯一键冲突的情况。</strong></p>
<p>对于有唯一键的表，插入数据时出现唯一键冲突也是常见的情况了。我先给你举一个简单的唯一键冲突的例子。</p>
<p><img src="/2024/09/14/MySQL40/image-20240914130447229.png" alt="image-20240914130447229"></p>
<p>这个例子也是在可重复读（repeatable read）隔离级别下执行的。可以看到，<strong>session B要执行的insert语句进入了锁等待状态。</strong></p>
<p><strong>也就是说，session A执行的insert语句，发生唯一键冲突的时候，并不只是简单地报错返回，还在冲突的索引上加了锁。</strong>我们前面说过，<strong>一个next-key lock就是由它右边界的值定义的。这时候，session A持有索引c上的(5,10]共享next-key lock（读锁）。</strong></p>
<p>至于为什么要加这个读锁，其实我也没有找到合理的解释。从作用上来看，这样做可以避免这一行被别的事务删掉。</p>
<p>这里<a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-locks-set.html">官方文档</a>有一个描述错误，认为如果冲突的是主键索引，就加记录锁，唯一索引才加next-key lock。但实际上，这两类索引冲突加的都是next-key lock。</p>
<p>有同学在前面文章的评论区问到，<strong>在有多个唯一索引的表中并发插入数据时，会出现死锁。但是，由于他没有提供复现方法或者现场，我也无法做分析。</strong>所以，我建议你在评论区发问题的时候，尽量同时附上复现方法，或者现场信息，这样我才好和你一起分析问题。</p>
<p>这里，我就先和你分享一个经典的死锁场景，如果你还遇到过其他唯一键冲突导致的死锁场景，也欢迎给我留言。</p>
<p><img src="/2024/09/14/MySQL40/image-20240914130502121.png" alt="image-20240914130502121"></p>
<p>在session A执行rollback语句回滚的时候，session C几乎同时发现死锁并返回。</p>
<p>这个死锁产生的逻辑是这样的：</p>
<ol>
<li>在T1时刻，启动session A，并执行insert语句，此时在索引c的c&#x3D;5上加了记录锁。<strong>注意，这个索引是唯一索引，因此退化为记录锁</strong>（如果你的印象模糊了，可以回顾下<a href="https://time.geekbang.org/column/article/75659">第21篇文章</a>介绍的加锁规则）。</li>
<li>在T2时刻，<strong>session B要执行相同的insert语句，发现了唯一键冲突，加上读锁；同样地，session C也在索引c上，c&#x3D;5这一个记录上，加了读锁。</strong></li>
<li>T3时刻，session A回滚。这时候，<strong>session B和session C都试图继续执行插入操作，都要加上写锁。两个session都要等待对方的行锁，所以就出现了死锁。</strong></li>
</ol>
<p>这个流程的状态变化图如下所示。</p>
<p><img src="/2024/09/14/MySQL40/image-20240914130511803.png" alt="image-20240914130511803"></p>
<h3 id="insert-into-…-on-duplicate-key-update"><a href="#insert-into-…-on-duplicate-key-update" class="headerlink" title="insert into … on duplicate key update"></a>insert into … on duplicate key update</h3><p>上面这个例子是<strong>主键冲突后直接报错，如果是改写成</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">insert into t values(11,10,10) on duplicate key update d=100; </span><br></pre></td></tr></table></figure>

<p>的话，<strong>就会给索引c上(5,10] 加一个排他的next-key lock（写锁）。</strong></p>
<p><strong>insert into … on duplicate key update 这个语义的逻辑是，插入一行数据，如果碰到唯一键约束，就执行后面的更新语句。</strong></p>
<p>注意，如果有多个列违反了唯一性约束，就会按照索引的顺序，修改跟第一个索引冲突的行。</p>
<p>现在表t里面已经有了(1,1,1)和(2,2,2)这两行，我们再来看看下面这个语句执行的效果：</p>
<p><img src="/2024/09/14/MySQL40/image-20240914130534655.png" alt="image-20240914130534655"></p>
<p>可以看到，主键id是先判断的，MySQL认为这个语句跟id&#x3D;2这一行冲突，<strong>所以修改的是id&#x3D;2的行</strong>。</p>
<p>需要注意的是，<strong>执行这条语句的affected rows返回的是2，很容易造成误解。实际上，真正更新的只有一行，只是在代码实现上，insert和update都认为自己成功了，update计数加了1， insert计数也加了1。</strong></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天这篇文章，我和你介绍了几种特殊情况下的insert语句。</p>
<p><strong>insert … select 是很常见的在两个表之间拷贝数据的方法。你需要注意，在可重复读隔离级别下，这个语句会给select的表里扫描到的记录和间隙加读锁。</strong></p>
<p>而如果insert和select的对象是同一个表，则有可能会造成<strong>循环写入</strong>。这种情况下，我们需要<strong>引入用户临时表来做优化</strong>。</p>
<p>insert 语句如果<strong>出现唯一键冲突，会在冲突的唯一值上加共享的next-key lock(S锁)。因此，碰到由于唯一键约束导致报错后，要尽快提交或回滚事务，避免加锁时间过长</strong>。</p>
<p>你平时在两个表之间拷贝数据用的是什么方法，有什么注意事项吗？在你的应用场景里，这个方法，相较于其他方法的优势是什么呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我们已经在文章中回答了上期问题。</p>
<p>有同学提到，如果在insert … select 执行期间有其他线程操作原表，会导致逻辑错误。<strong>其实，这是不会的，如果不加锁，就是快照读。</strong></p>
<p>一条语句执行期间，<strong>它的一致性视图是不会修改的，所以即使有其他事务修改了原表的数据，也不会影响这条语句看到的数据。</strong></p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/40-insert-yu-ju-de-suo-wei-shi-mo-zhe-mo-duo">40 insert语句的锁为什么这么多？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么最快地复制一张表？</title>
    <url>/2024/09/14/MySQL41/</url>
    <content><![CDATA[<p>我在上一篇文章最后，给你留下的问题是怎么在两张表中拷贝数据。<strong>如果可以控制对源表的扫描行数和加锁范围很小的话，我们简单地使用insert … select 语句即可实现。</strong></p>
<p>当然，<strong>为了避免对源表加读锁，更稳妥的方案是先将数据写到外部文本文件，然后再写回目标表</strong>。这时，<strong>有两种常用的方法</strong>。接下来的内容，我会和你详细展开一下这两种方法。</p>
<p>为了便于说明，我还是先创建一个表db1.t，并插入1000行数据，同时创建一个相同结构的表db2.t。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create database db1;</span><br><span class="line">use db1;</span><br><span class="line"></span><br><span class="line">create table t(id int primary key, a int, b int, index(a))engine=innodb;</span><br><span class="line">delimiter ;;</span><br><span class="line">  create procedure idata()</span><br><span class="line">  begin</span><br><span class="line">    declare i int;</span><br><span class="line">    set i=1;</span><br><span class="line">    while(i&lt;=1000)do</span><br><span class="line">      insert into t values(i,i,i);</span><br><span class="line">      set i=i+1;</span><br><span class="line">    end while;</span><br><span class="line">  end;;</span><br><span class="line">delimiter ;</span><br><span class="line">call idata();</span><br><span class="line"></span><br><span class="line">create database db2;</span><br><span class="line">create table db2.t like db1.t</span><br></pre></td></tr></table></figure>

<p>假设，我们要把db1.t里面a&gt;900的数据行导出来，插入到db2.t中。</p>
<h3 id="mysqldump方法"><a href="#mysqldump方法" class="headerlink" title="mysqldump方法"></a>mysqldump方法</h3><p>一种方法是，<strong>使用mysqldump命令将数据导出成一组INSERT语句</strong>。你可以使用下面的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysqldump -h$host -P$port -u$user --add-locks=0 --no-create-info --single-transaction  --set-gtid-purged=OFF db1 t --where=&quot;a&gt;900&quot; --result-file=/client_tmp/t.sql</span><br></pre></td></tr></table></figure>

<p>把结果输出到临时文件。</p>
<p>这条命令中，主要参数含义如下：</p>
<ol>
<li><strong>–single-transaction的作用是，在导出数据的时候不需要对表db1.t加表锁，而是使用START TRANSACTION WITH CONSISTENT SNAPSHOT的方法；</strong></li>
<li><strong>–add-locks设置为0，表示在输出的文件结果里，不增加” LOCK TABLES <code>t</code> WRITE;” ；</strong></li>
<li><strong>–no-create-info的意思是，不需要导出表结构；</strong></li>
<li>–set-gtid-purged&#x3D;off表示的是，不输出跟GTID相关的信息；</li>
<li><strong>–result-file指定了输出文件的路径，其中client表示生成的文件是在客户端机器上的。</strong></li>
</ol>
<p>通过这条mysqldump命令生成的t.sql文件中就包含了如图1所示的INSERT语句。</p>
<p><img src="/2024/09/14/MySQL41/image-20240914134120878.png" alt="image-20240914134120878"></p>
<p>可以看到，<strong>一条INSERT语句里面会包含多个value对，这是为了后续用这个文件来写入数据的时候，执行速度可以更快</strong>。</p>
<p>如果你希望生成的文件中一条INSERT语句只插入一行数据的话，<strong>可以在执行mysqldump命令时，加上参数–skip-extended-insert。</strong></p>
<p>然后，你可以通过下面这条命令，将这些INSERT语句放到db2库里去执行。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql -h127.0.0.1 -P13000  -uroot db2 -e &quot;source /client_tmp/t.sql&quot;</span><br></pre></td></tr></table></figure>

<p>需要说明的是，<strong>source并不是一条SQL语句，而是一个客户端命令。mysql客户端执行这个命令的流程是这样的：</strong></p>
<ol>
<li><strong>打开文件，默认以分号为结尾读取一条条的SQL语句；</strong></li>
<li><strong>将SQL语句发送到服务端执行。</strong></li>
</ol>
<p>也就是说，<strong>服务端执行的并不是这个“source t.sql”语句，而是INSERT语句</strong>。所以，<strong>不论是在慢查询日志（slow log），还是在binlog，记录的都是这些要被真正执行的INSERT语句。</strong></p>
<h3 id="导出CSV文件"><a href="#导出CSV文件" class="headerlink" title="导出CSV文件"></a>导出CSV文件</h3><p><strong>另一种方法是直接将结果导出成.csv文件</strong>。<strong>MySQL提供了下面的语法，用来将查询结果导出到服务端本地目录：（这个我之前没有怎么使用过，一般而言，前面的一个已经很好用了）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from db1.t where a&gt;900 into outfile &#x27;/server_tmp/t.csv&#x27;;</span><br></pre></td></tr></table></figure>

<p>我们在使用这条语句时，需要注意如下几点。</p>
<ol>
<li><strong>这条语句会将结果保存在服务端。如果你执行命令的客户端和MySQL服务端不在同一个机器上，客户端机器的临时目录下是不会生成t.csv文件的。</strong></li>
<li>into outfile指定了文件的生成位置（&#x2F;server_tmp&#x2F;），<strong>这个位置必须受参数secure_file_priv的限制。参数secure_file_priv的可选值和作用分别是：</strong><ul>
<li><strong>如果设置为empty，表示不限制文件生成的位置，这是不安全的设置；</strong></li>
<li><strong>如果设置为一个表示路径的字符串，就要求生成的文件只能放在这个指定的目录，或者它的子目录；</strong></li>
<li><strong>如果设置为NULL，就表示禁止在这个MySQL实例上执行select … into outfile 操作。</strong></li>
</ul>
</li>
<li>这条命令<strong>不会帮你覆盖文件</strong>，因此你需要确保&#x2F;server_tmp&#x2F;t.csv这个文件<strong>不存在</strong>，<strong>否则执行语句时就会因为有同名文件的存在而报错</strong>。</li>
<li>这条命令生成的文本文件中，<strong>原则上一个数据行对应文本文件的一行。但是，如果字段中包含换行符，在生成的文本中也会有换行符。不过类似换行符、制表符这类符号，前面都会跟上“\”这个转义符，这样就可以跟字段之间、数据行之间的分隔符区分开。</strong></li>
</ol>
<p>得到.csv导出文件后，<strong>你就可以用下面的load data命令将数据导入到目标表db2.t中</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">load data infile &#x27;/server_tmp/t.csv&#x27; into table db2.t;</span><br></pre></td></tr></table></figure>

<p>这条语句的执行流程如下所示。</p>
<ol>
<li><strong>打开文件&#x2F;server_tmp&#x2F;t.csv，以制表符(\t)作为字段间的分隔符，以换行符（\n）作为记录之间的分隔符，进行数据读取；</strong></li>
<li>启动事务。</li>
<li><strong>判断每一行的字段数与表db2.t是否相同：</strong><ul>
<li><strong>若不相同，则直接报错，事务回滚；</strong></li>
<li><strong>若相同，则构造成一行，调用InnoDB引擎接口，写入到表中。</strong></li>
</ul>
</li>
<li>重复步骤3，直到&#x2F;server_tmp&#x2F;t.csv整个文件读入完成，提交事务。</li>
</ol>
<p>你可能有一个疑问，<strong>如果binlog_format&#x3D;statement，这个load语句记录到binlog里以后，怎么在备库重放呢？</strong></p>
<p>由于&#x2F;server_tmp&#x2F;t.csv文件只保存在主库所在的主机上，如果只是把这条语句原文写到binlog中，在备库执行的时候，备库的本地机器上没有这个文件，就会导致主备同步停止。</p>
<p>所以，这条语句执行的完整流程，其实是下面这样的。</p>
<ol>
<li>主库执行完成后，将&#x2F;server_tmp&#x2F;t.csv文件的内容直接写到binlog文件中。</li>
<li>往binlog文件中写入语句load data local infile ‘&#x2F;tmp&#x2F;SQL_LOAD_MB-1-0’ INTO TABLE <code>db2</code>.<code>t</code>。</li>
<li>把这个binlog日志传到备库。</li>
<li>备库的apply线程在执行这个事务日志时： a. 先将binlog中t.csv文件的内容读出来，写入到本地临时目录&#x2F;tmp&#x2F;SQL_LOAD_MB-1-0 中； b. 再执行load data语句，往备库的db2.t表中插入跟主库相同的数据。</li>
</ol>
<p>执行流程如图2所示：</p>
<p><img src="/2024/09/14/MySQL41/image-20240914134222408.png" alt="image-20240914134222408"></p>
<p>注意，这里备库执行的load data语句里面，多了一个“local”。它的意思是“将执行这条命令的客户端所在机器的本地文件&#x2F;tmp&#x2F;SQL_LOAD_MB-1-0的内容，加载到目标表db2.t中”。</p>
<p>也就是说，<strong>load data命令有两种用法</strong>：</p>
<ol>
<li><strong>不加“local”，是读取服务端的文件，这个文件必须在secure_file_priv指定的目录或子目录下；</strong></li>
<li><strong>加上“local”，读取的是客户端的文件，只要mysql客户端有访问这个文件的权限即可。这时候，MySQL客户端会先把本地文件传给服务端，然后执行上述的load data流程。</strong></li>
</ol>
<p>另外需要注意的是，<strong>select …into outfile方法不会生成表结构文件</strong>, 所以我们导数据时还需要单独的命令得到表结构定义。<strong>mysqldump提供了一个–tab参数，可以同时导出表结构定义文件和csv数据文件。这条命令的使用方法如下：（这个可以耶，之前导出的时候没有表的结构还单独导出了）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysqldump -h$host -P$port -u$user ---single-transaction  --set-gtid-purged=OFF db1 t --where=&quot;a&gt;900&quot; --tab=$secure_file_priv</span><br></pre></td></tr></table></figure>

<p>这条命令会在$secure_file_priv定义的目录下，<strong>创建一个t.sql文件保存建表语句</strong>，<strong>同时创建一个t.txt文件保存CSV数据。</strong></p>
<h3 id="物理拷贝方法"><a href="#物理拷贝方法" class="headerlink" title="物理拷贝方法"></a>物理拷贝方法</h3><p><strong>前面我们提到的mysqldump方法和导出CSV文件的方法，都是逻辑导数据的方法，也就是将数据从表db1.t中读出来，生成文本，然后再写入目标表db2.t中。</strong></p>
<p>你可能会问，有物理导数据的方法吗？比如，直接把db1.t表的.frm文件和.ibd文件拷贝到db2目录下，是否可行呢？</p>
<p><strong>答案是不行的。</strong></p>
<p>因为，<strong>一个InnoDB表，除了包含这两个物理文件外，还需要在数据字典中注册</strong>。直接拷贝这两个文件的话，<strong>因为数据字典中没有db2.t这个表，系统是不会识别和接受它们的</strong>。</p>
<p>不过，在MySQL 5.6版本引入了<strong>可传输表空间</strong>(transportable tablespace)的方法，<strong>可以通过导出+导入表空间的方式，实现物理拷贝表的功能</strong>。</p>
<p>假设我们现在的目标是在db1库下，复制一个跟表t相同的表r，具体的执行步骤如下：</p>
<ol>
<li><strong>执行 create table r like t，创建一个相同表结构的空表；</strong></li>
<li><strong>执行alter table r discard tablespace，这时候r.ibd文件会被删除；</strong></li>
<li><strong>执行flush table t for export，这时候db1目录下会生成一个t.cfg文件；</strong></li>
<li><strong>在db1目录下执行cp t.cfg r.cfg; cp t.ibd r.ibd；这两个命令（这里需要注意的是，拷贝得到的两个文件，MySQL进程要有读写权限）；</strong></li>
<li><strong>执行unlock tables，这时候t.cfg文件会被删除；</strong></li>
<li><strong>执行alter table r import tablespace，将这个r.ibd文件作为表r的新的表空间，由于这个文件的数据内容和t.ibd是相同的，所以表r中就有了和表t相同的数据。</strong></li>
</ol>
<p>至此，拷贝表数据的操作就完成了。这个流程的执行过程图如下：</p>
<p><img src="/2024/09/14/MySQL41/image-20240914134252175.png" alt="image-20240914134252175"></p>
<p>关于拷贝表的这个流程，有以下几个注意点：</p>
<ol>
<li><strong>在第3步执行完flsuh table命令之后，db1.t整个表处于只读状态，直到执行unlock tables命令后才释放读锁；</strong></li>
<li><strong>在执行import tablespace的时候，为了让文件里的表空间id和数据字典中的一致，会修改r.ibd的表空间id。而这个表空间id存在于每一个数据页中。因此，如果是一个很大的文件（比如TB级别），每个数据页都需要修改，所以你会看到这个import语句的执行是需要一些时间的。当然，如果是相比于逻辑导入的方法，import语句的耗时是非常短的。</strong></li>
</ol>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天这篇文章，我和你介绍了三种将一个表的数据导入到另外一个表中的方法。</p>
<p>我们来对比一下这三种方法的优缺点。</p>
<ol>
<li><strong>物理拷贝的方式速度最快，尤其对于大表拷贝来说是最快的方法。</strong>如果出现误删表的情况，用备份恢复出误删之前的临时库，然后再把临时库中的表拷贝到生产库上，是恢复数据最快的方法。但是，这种方法的使用也有一定的局限性：<ul>
<li><strong>必须是全表拷贝，不能只拷贝部分数据；</strong></li>
<li><strong>需要到服务器上拷贝数据，在用户无法登录数据库主机的场景下无法使用；</strong></li>
<li><strong>由于是通过拷贝物理文件实现的，源表和目标表都是使用InnoDB引擎时才能使用。</strong></li>
</ul>
</li>
<li><strong>用mysqldump生成包含INSERT语句文件的方法，可以在where参数增加过滤条件，来实现只导出部分数据。</strong>这个方式的不足之一是，<strong>不能使用join这种比较复杂的where条件写法。</strong></li>
<li><strong>用select … into outfile的方法是最灵活的，支持所有的SQL写法。</strong>但，<strong>这个方法的缺点之一就是，每次只能导出一张表的数据，而且表结构也需要另外的语句单独备份。</strong></li>
</ol>
<p><strong>后两种方式都是逻辑备份方式，是可以跨引擎使用的。</strong></p>
<p>我们前面介绍binlog_format&#x3D;statement的时候，binlog记录的load data命令是带local的。既然这条命令是发送到备库去执行的，那么备库执行的时候也是本地执行，为什么需要这个local呢？如果写到binlog中的命令不带local，又会出现什么问题呢？</p>
<h3 id="上期问题时间"><a href="#上期问题时间" class="headerlink" title="上期问题时间"></a>上期问题时间</h3><p>我在上篇文章最后给你留下的思考题，已经在今天这篇文章的正文部分做了回答。</p>
<p>如果sessionA拿到c&#x3D;5的记录锁是写锁，那为什么sessionB和sessionC还能加c&#x3D;5的读锁呢？</p>
<ul>
<li>这是因为<strong>next-key lock是先加间隙锁，再加记录锁的</strong>。加间隙锁成功了，加记录锁就会被堵住。如果你对这个过程有疑问的话，可以再复习一下<a href="https://time.geekbang.org/column/article/78427">第30篇文章</a>中的相关内容。</li>
</ul>
<p><strong>在MySQL 8.0版本中，已经能够用临时表处理insert … select写入原表的语句了。</strong></p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/41-zen-mo-zui-kuai-di-fu-zhi-yi-zhang-biao">41 怎么最快地复制一张表？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>自增id用完怎么办？</title>
    <url>/2024/09/14/MySQL45/</url>
    <content><![CDATA[<p><strong>MySQL里有很多自增的id，每个自增id都是定义了初始值，然后不停地往上加步长。虽然自然数是没有上限的，但是在计算机里，只要定义了表示这个数的字节长度，那它就有上限。比如，无符号整型(unsigned int)是4个字节，上限就是232-1。</strong></p>
<p>既然自增id有上限，就有可能被用完。但是，自增id用完了会怎么样呢？</p>
<p>今天这篇文章，我们就来看看MySQL里面的几种自增id，一起分析一下它们的值达到上限以后，会出现什么情况。</p>
<h3 id="表定义自增值id"><a href="#表定义自增值id" class="headerlink" title="表定义自增值id"></a>表定义自增值id</h3><p>说到自增id，你第一个想到的应该就是表结构定义里的自增字段，也就是我在第39篇文章<a href="https://time.geekbang.org/column/article/80531">《自增主键为什么不是连续的？》</a>中和你介绍过的自增主键id。</p>
<p><strong>表定义的自增值达到上限后的逻辑是：再申请下一个id时，得到的值保持不变。</strong></p>
<p>我们可以通过下面这个语句序列验证一下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table t(id int unsigned auto_increment primary key) auto_increment=4294967295;</span><br><span class="line">insert into t values(null);</span><br><span class="line">//成功插入一行 4294967295</span><br><span class="line">show create table t;</span><br><span class="line">/* CREATE TABLE `t` (</span><br><span class="line">  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=4294967295;</span><br><span class="line">*/</span><br><span class="line"></span><br><span class="line">insert into t values(null);</span><br><span class="line">//Duplicate entry &#x27;4294967295&#x27; for key &#x27;PRIMARY&#x27;</span><br></pre></td></tr></table></figure>

<p><strong>可以看到，第一个insert语句插入数据成功后，这个表的AUTO_INCREMENT没有改变（还是4294967295），就导致了第二个insert语句又拿到相同的自增id值，再试图执行插入语句，报主键冲突错误。</strong></p>
<p><strong>232-1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创建成8个字节的bigint unsigned。</strong></p>
<h3 id="InnoDB系统自增row-id"><a href="#InnoDB系统自增row-id" class="headerlink" title="InnoDB系统自增row_id"></a>InnoDB系统自增row_id</h3><p><strong>如果你创建的InnoDB表没有指定主键，那么InnoDB会给你创建一个不可见的，长度为6个字节的row_id。InnoDB维护了一个全局的dict_sys.row_id值，所有无主键的InnoDB表，每插入一行数据，都将当前的dict_sys.row_id值作为要插入数据的row_id，然后把dict_sys.row_id的值加1。</strong></p>
<p>实际上，<strong>在代码实现时row_id是一个长度为8字节的无符号长整型(bigint unsigned)。但是，InnoDB在设计时，给row_id留的只是6个字节的长度，这样写到数据表中时只放了最后6个字节</strong>，所以row_id能写到数据表中的值，就有两个特征：</p>
<ol>
<li><strong>row_id写入表中的值范围，是从0到248-1；</strong></li>
<li><strong>当dict_sys.row_id&#x3D;248时，如果再有插入数据的行为要来申请row_id，拿到以后再取最后6个字节的话就是0。</strong></li>
</ol>
<p>也就是说，<strong>写入表的row_id是从0开始到248-1。达到上限后，下一个值就是0，然后继续循环。</strong></p>
<p>当然，248-1这个值本身已经很大了，但是如果一个MySQL实例跑得足够久的话，还是可能达到这个上限的。<strong>在InnoDB逻辑里，申请到row_id&#x3D;N后，就将这行数据写入表中；如果表中已经存在row_id&#x3D;N的行，新写入的行就会覆盖原有的行。</strong></p>
<p>要验证这个结论的话，你可以通过gdb修改系统的自增row_id来实现。注意，用gdb改变量这个操作是为了便于我们复现问题，只能在测试环境使用。</p>
<p><img src="/2024/09/14/MySQL45/image-20240914144114042.png" alt="image-20240914144114042"></p>
<p><img src="/2024/09/14/MySQL45/image-20240914144121612.png" alt="image-20240914144121612"></p>
<p><strong>可以看到，在我用gdb将dict_sys.row_id设置为248之后，再插入的a&#x3D;2的行会出现在表t的第一行，因为这个值的row_id&#x3D;0。之后再插入的a&#x3D;3的行，由于row_id&#x3D;1，就覆盖了之前a&#x3D;1的行，因为a&#x3D;1这一行的row_id也是1。</strong></p>
<p>从这个角度看，<strong>我们还是应该在InnoDB表中主动创建自增主键。因为，表自增id到达上限后，再插入数据时报主键冲突错误，是更能被接受的。</strong></p>
<p><strong>毕竟覆盖数据，就意味着数据丢失，影响的是数据可靠性；报主键冲突，是插入失败，影响的是可用性。而一般情况下，可靠性优先于可用性。</strong></p>
<h3 id="Xid"><a href="#Xid" class="headerlink" title="Xid"></a>Xid</h3><p>在第15篇文章<a href="https://time.geekbang.org/column/article/73161">《答疑文章（一）：日志和索引相关问题》</a>中，<strong>我和你介绍redo log和binlog相配合的时候，提到了它们有一个共同的字段叫作Xid。它在MySQL中是用来对应事务的。（这个当时在学Seata的时候也遇到过）</strong></p>
<p>那么，Xid在MySQL内部是怎么生成的呢？</p>
<p><strong>MySQL内部维护了一个全局变量global_query_id，每次执行语句的时候将它赋值给Query_id，然后给这个变量加1。如果当前语句是这个事务执行的第一条语句，那么MySQL还会同时把Query_id赋值给这个事务的Xid。</strong></p>
<p><strong>而global_query_id是一个纯内存变量，重启之后就清零了。所以你就知道了，在同一个数据库实例中，不同事务的Xid也是有可能相同的。</strong></p>
<p><strong>但是MySQL重启之后会重新生成新的binlog文件，这就保证了，同一个binlog文件里，Xid一定是惟一的。</strong></p>
<p><strong>虽然MySQL重启不会导致同一个binlog里面出现两个相同的Xid，但是如果global_query_id达到上限后，就会继续从0开始计数。从理论上讲，还是就会出现同一个binlog里面出现相同Xid的场景。</strong></p>
<p>因为global_query_id定义的长度是8个字节，这个自增值的上限是264-1。要出现这种情况，必须是下面这样的过程：</p>
<ol>
<li>执行一个事务，假设Xid是A；</li>
<li>接下来执行264次查询语句，让global_query_id回到A；</li>
<li>再启动一个事务，这个事务的Xid也是A。</li>
</ol>
<p>不过，<strong>264这个值太大了，大到你可以认为这个可能性只会存在于理论上。</strong></p>
<h3 id="Innodb-trx-id"><a href="#Innodb-trx-id" class="headerlink" title="Innodb trx_id"></a>Innodb trx_id</h3><p><strong>Xid和InnoDB的trx_id是两个容易混淆的概念。</strong></p>
<p><strong>Xid是由server层维护的。InnoDB内部使用Xid，就是为了能够在InnoDB事务和server之间做关联。但是，InnoDB自己的trx_id，是另外维护的。</strong></p>
<p>其实，你应该非常熟悉这个trx_id。它就是在我们在第8篇文章<a href="https://time.geekbang.org/column/article/70562">《事务到底是隔离的还是不隔离的？》</a>中讲事务可见性时，用到的<strong>事务id（transaction id）</strong>。</p>
<p>InnoDB内部维护了一个<strong>max_trx_id全局变量，每次需要申请一个新的trx_id时，就获得max_trx_id的当前值，然后并将max_trx_id加1。</strong></p>
<p>InnoDB数据可见性的核心思想是：<strong>每一行数据都记录了更新它的trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的trx_id做对比。</strong></p>
<p>对于正在执行的事务，你可以从information_schema.innodb_trx表中看到事务的trx_id。</p>
<p>我在上一篇文章的末尾留给你的思考题，就是关于从innodb_trx表里面查到的trx_id的。现在，我们一起来看一个事务现场：</p>
<p><img src="/2024/09/14/MySQL45/image-20240914144145115.png" alt="image-20240914144145115"></p>
<p>session B里，我从innodb_trx表里查出的这两个字段，第二个字段trx_mysql_thread_id就是线程id。显示线程id，是为了说明这两次查询看到的事务对应的线程id都是5，也就是session A所在的线程。</p>
<p>可以看到，T2时刻显示的trx_id是一个很大的数；T4时刻显示的trx_id是1289，看上去是一个比较正常的数字。这是什么原因呢？</p>
<p>实际上，在T1时刻，session A还没有涉及到更新，是一个只读事务。而对于只读事务，InnoDB并不会分配trx_id。也就是说：</p>
<ol>
<li>在T1时刻，trx_id的值其实就是0。而这个很大的数，只是显示用的。一会儿我会再和你说说这个数据的生成逻辑。</li>
<li>直到session A 在T3时刻执行insert语句的时候，InnoDB才真正分配了trx_id。所以，T4时刻，session B查到的这个trx_id的值就是1289。</li>
</ol>
<p>需要注意的是，除了显而易见的修改类语句外，如果在select 语句后面加上for update，这个事务也不是只读事务。</p>
<p>在上一篇文章的评论区，有同学提出，实验的时候发现不止加1。这是因为：</p>
<ol>
<li>update 和 delete语句除了事务本身，还涉及到标记删除旧数据，也就是要把数据放到purge队列里等待后续物理删除，这个操作也会把max_trx_id+1， 因此在一个事务中至少加2；</li>
<li>InnoDB的后台操作，比如表的索引信息统计这类操作，也是会启动内部事务的，因此你可能看到，trx_id值并不是按照加1递增的。</li>
</ol>
<p>那么，<strong>T2时刻查到的这个很大的数字是怎么来的呢？</strong></p>
<p>其实，这个数字是每次查询的时候由系统临时计算出来的。它的算法是：把当前事务的trx变量的指针地址转成整数，再加上248。使用这个算法，就可以保证以下两点：</p>
<ol>
<li>因为同一个只读事务在执行期间，它的指针地址是不会变的，所以不论是在 innodb_trx还是在innodb_locks表里，同一个只读事务查出来的trx_id就会是一样的。</li>
<li>如果有并行的多个只读事务，每个事务的trx变量的指针地址肯定不同。这样，不同的并发只读事务，查出来的trx_id就是不同的。</li>
</ol>
<p>那么，<strong>为什么还要再加上248呢？</strong></p>
<p>在显示值里面加上248，目的是要保证只读事务显示的trx_id值比较大，正常情况下就会区别于读写事务的id。但是，trx_id跟row_id的逻辑类似，定义长度也是8个字节。因此，在理论上还是可能出现一个读写事务与一个只读事务显示的trx_id相同的情况。不过这个概率很低，并且也没有什么实质危害，可以不管它。</p>
<p>另一个问题是，<strong>只读事务不分配trx_id，有什么好处呢？</strong></p>
<ul>
<li>一个好处是，这样做可以减小事务视图里面活跃事务数组的大小。因为当前正在运行的只读事务，是不影响数据的可见性判断的。所以，在创建事务的一致性视图时，InnoDB就只需要拷贝读写事务的trx_id。</li>
<li>另一个好处是，可以减少trx_id的申请次数。在InnoDB里，即使你只是执行一个普通的select语句，在执行过程中，也是要对应一个只读事务的。所以只读事务优化后，普通的查询语句不需要申请trx_id，就大大减少了并发事务申请trx_id的锁冲突。</li>
</ul>
<p>由于只读事务不分配trx_id，一个自然而然的结果就是trx_id的增加速度变慢了。</p>
<p>但是，max_trx_id会持久化存储，重启也不会重置为0，那么从理论上讲，只要一个MySQL服务跑得足够久，就可能出现max_trx_id达到248-1的上限，然后从0开始的情况。</p>
<p>当达到这个状态后，MySQL就会持续出现一个脏读的bug，我们来复现一下这个bug。</p>
<p>首先我们需要把当前的max_trx_id先修改成248-1。注意：这个case里使用的是可重复读隔离级别。具体的操作流程如下：</p>
<p><img src="/2024/09/14/MySQL45/image-20240914144204473.png" alt="image-20240914144204473"></p>
<p><img src="/2024/09/14/MySQL45/image-20240914144215508.png" alt="image-20240914144215508"></p>
<p>由于我们已经把系统的max_trx_id设置成了248-1，所以在session A启动的事务TA的低水位就是248-1。</p>
<p>在T2时刻，session B执行第一条update语句的事务id就是248-1，而第二条update语句的事务id就是0了，这条update语句执行后生成的数据版本上的trx_id就是0。</p>
<p>在T3时刻，session A执行select语句的时候，判断可见性发现，c&#x3D;3这个数据版本的trx_id，小于事务TA的低水位，因此认为这个数据可见。</p>
<p>但，这个是脏读。</p>
<p>由于低水位值会持续增加，而事务id从0开始计数，就导致了系统在这个时刻之后，所有的查询都会出现脏读的。</p>
<p>并且，MySQL重启时max_trx_id也不会清0，也就是说重启MySQL，这个bug仍然存在。</p>
<p>那么，<strong>这个bug也是只存在于理论上吗？</strong></p>
<p>假设一个MySQL实例的TPS是每秒50万，持续这个压力的话，在17.8年后，就会出现这个情况。如果TPS更高，这个年限自然也就更短了。但是，从MySQL的真正开始流行到现在，恐怕都还没有实例跑到过这个上限。不过，这个bug是只要MySQL实例服务时间够长，就会必然出现的。</p>
<p>当然，这个例子更现实的意义是，可以加深我们对低水位和数据可见性的理解。你也可以借此机会再回顾下第8篇文章<a href="https://time.geekbang.org/column/article/70562">《事务到底是隔离的还是不隔离的？》</a>中的相关内容。</p>
<h3 id="thread-id"><a href="#thread-id" class="headerlink" title="thread_id"></a>thread_id</h3><p><strong>接下来，我们再看看线程id（thread_id）。其实，线程id才是MySQL中最常见的一种自增id。平时我们在查各种现场的时候，show processlist里面的第一列，就是thread_id。</strong></p>
<p>thread_id的逻辑很好理解：<strong>系统保存了一个全局变量thread_id_counter，每新建一个连接，就将thread_id_counter赋值给这个新连接的线程变量。</strong></p>
<p>thread_id_counter定义的大小是4个字节，因此达到232-1后，它就会重置为0，然后继续增加。<strong>但是，你不会在show processlist里看到两个相同的thread_id</strong>。</p>
<p><strong>这是因为MySQL设计了一个唯一数组的逻辑，给新线程分配thread_id的时候，逻辑代码是这样的</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">do &#123;</span><br><span class="line">  new_id= thread_id_counter++;</span><br><span class="line">&#125; while (!thread_ids.insert_unique(new_id).second);</span><br></pre></td></tr></table></figure>

<p>这个代码逻辑简单而且实现优雅，相信你一看就能明白。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>今天这篇文章，<strong>我给你介绍了MySQL不同的自增id达到上限以后的行为。数据库系统作为一个可能需要7*24小时全年无休的服务，考虑这些边界是非常有必要的。</strong></p>
<p>每种自增id有各自的应用场景，在达到上限后的表现也不同：</p>
<ol>
<li><strong>表的自增id达到上限后，再申请时它的值就不会改变，进而导致继续插入数据时报主键冲突的错误。</strong></li>
<li><strong>row_id达到上限后，则会归0再重新递增，如果出现相同的row_id，后写的数据会覆盖之前的数据。</strong></li>
<li><strong>Xid只需要不在同一个binlog文件中出现重复值即可。</strong>虽然理论上<strong>会出现重复值，但是概率极小，可以忽略不计。</strong></li>
<li>InnoDB的max_trx_id 递增值每次MySQL重启都会被保存起来，所以我们文章中提到的脏读的例子就是一个必现的bug，好在留给我们的时间还很充裕。</li>
<li><strong>thread_id是我们使用中最常见的，而且也是处理得最好的一个自增id逻辑了。</strong></li>
</ol>
<p><strong>当然，在MySQL里还有别的自增id，比如table_id、binlog文件序号等</strong>，就留给你去验证和探索了。</p>
<p><strong>不同的自增id有不同的上限值，上限值的大小取决于声明的类型长度</strong>。而我们专栏声明的上限id就是45，所以今天这篇文章也是我们的最后一篇技术文章了。</p>
<p>参考文章：<a href="https://jums.gitbook.io/mysql-shi-zhan-45-jiang/45-zi-zeng-id-yong-wan-zen-mo-ban">45 自增id用完怎么办？ | MySql实战45讲 (gitbook.io)</a></p>
]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>基本架构：一个键值数据库包含什么？</title>
    <url>/2024/09/14/Redis01/</url>
    <content><![CDATA[<p>我们知道，Redis 是典型的键值数据库，所以今天，我准备手把手地带你构建一个简单的键值数据库。为啥要这么做呢？</p>
<p>还记得我在开篇词说过吗？<strong>Redis 本身比较复杂，如果我们一上来就直接研究一个个具体的技术点，比如“单线程”“缓存”等，虽然可以直接学习到具体的内容，甚至立马就能解决一些小问题，但是这样学，很容易迷失在细枝末节里。</strong></p>
<p>从我自己的经验来看，更好的学习方式就是先建立起“<strong>系统观</strong>”。这也就是说，<strong>如果我们想要深入理解和优化 Redis，就必须要对它的总体架构和关键模块有一个全局的认知，然后再深入到具体的技术点。这也是我们这门课坚持的一种讲课方式。</strong></p>
<p>我相信，经过这样一个过程，我们在实践中定位和解决问题时，就会轻松很多，而且你还可以把这个学习方式迁移到其他的学习活动上。我希望你能彻底掌握这个学习思路，让自己的学习、工作效率更高。</p>
<p>说远了，还是回到我们今天的课程主题上。<strong>今天，在构造这个简单的键值数据库时，我们只需要关注整体架构和核心模块。</strong>这就相当于医学上在正式解剖人体之前，会先解剖一只小白鼠。我们通过剖析这个最简单的键值数据库，来迅速抓住学习和调优 Redis 的关键。</p>
<p>我把这个简单的键值数据库称为 SimpleKV。需要注意的是，GitHub 上也有一个名为 SimpleKV 的项目，这跟我说的 SimpleKV 不是一回事，我说的只是一个具有关键组件的键值数据库架构。</p>
<p>好了，你是不是已经准备好了，那我们就一起来构造 SimpleKV 吧。</p>
<p>开始构造 SimpleKV 时，<strong>首先就要考虑里面可以存什么样的数据，对数据可以做什么样的操作，也就是数据模型和操作接口</strong>。它们看似简单，<strong>实际上却是我们理解 Redis 经常被用于缓存、秒杀、分布式锁等场景的重要基础</strong>。</p>
<p>理解了数据模型，你就会明白，为什么在有些场景下，<strong>原先使用关系型数据库保存的数据，也可以用键值数据库保存。例如，用户信息（用户 ID、姓名、年龄、性别等）通常用关系型数据库保存，在这个场景下，一个用户 ID 对应一个用户信息集合，这就是键值数据库的一种数据模型，它同样能完成这一存储需求。</strong></p>
<p>但是，<strong>如果你只知道数据模型，而不了解操作接口的话，可能就无法理解，为什么在有些场景中，使用键值数据库又不合适了</strong>。例如，同样是在上面的场景中，<strong>如果你要对多个用户的年龄计算均值，键值数据库就无法完成了。因为它只提供简单的操作接口，无法支持复杂的聚合计算。</strong></p>
<p>那么，对于 Redis 来说，它到底能做什么，不能做什么呢？<strong>只有先搞懂它的数据模型和操作接口，我们才能真正把“这块好钢用在刀刃上”。</strong></p>
<p>接下来，我们就先来看可以存哪些数据。</p>
<h2 id="可以存哪些数据？"><a href="#可以存哪些数据？" class="headerlink" title="可以存哪些数据？"></a>可以存哪些数据？</h2><p>对于键值数据库而言，基本的数据模型是 key-value 模型。 例如，“hello”: “world”就是一个基本的 KV 对，其中，“hello”是 key，“world”是 value。SimpleKV 也不例外。<strong>在 SimpleKV 中，key 是 String 类型，而 value 是基本数据类型，例如 String、整型等。</strong></p>
<p>但是，SimpleKV 毕竟是一个简单的键值数据库，<strong>对于实际生产环境中的键值数据库来说，value 类型还可以是复杂类型。</strong></p>
<p>不同键值数据库支持的 key 类型一般差异不大，而 value 类型则有较大差别。<strong>我们在对键值数据库进行选型时，一个重要的考虑因素是它支持的 value 类型。例如，Memcached 支持的 value 类型仅为 String 类型，而 Redis 支持的 value 类型包括了 String、哈希表、列表、集合等。Redis 能够在实际业务场景中得到广泛的应用，就是得益于支持多样化类型的 value。（这下看懂了为什么经常提到Memcached，但是使用的频率比较低）</strong></p>
<p>从使用的角度来说，<strong>不同 value 类型的实现，不仅可以支撑不同业务的数据需求，而且也隐含着不同数据结构在性能、空间效率等方面的差异，从而导致不同的 value 操作之间存在着差异。</strong></p>
<p>只有深入地理解了这背后的原理，我们才能在选择 <strong>Redis value 类型和优化 Redis 性能时，做到游刃有余。</strong></p>
<h2 id="可以对数据做什么操作？"><a href="#可以对数据做什么操作？" class="headerlink" title="可以对数据做什么操作？"></a>可以对数据做什么操作？</h2><p>知道了数据模型，接下来，我们就要看它对数据的基本操作了。<strong>SimpleKV 是一个简单的键值数据库，因此，基本操作无外乎增删改查。</strong></p>
<p>我们先来了解下 SimpleKV 需要支持的 3 种基本操作，即 PUT、GET 和 DELETE。</p>
<ul>
<li>PUT：新写入或更新一个 key-value 对；</li>
<li>GET：根据一个 key 读取相应的 value 值；</li>
<li>DELETE：根据一个 key 删除整个 key-value 对。</li>
</ul>
<p>需要注意的是，<strong>有些键值数据库的新写 &#x2F; 更新操作叫 SET</strong>。<strong>新写入和更新虽然是用一个操作接口，但在实际执行时，会根据 key 是否存在而执行相应的新写或更新流程。</strong></p>
<p>在实际的业务场景中，我们经常会碰到这种情况：查询一个用户在一段时间内的访问记录。这种操作在键值数据库中属于 SCAN 操作，即<strong>根据一段 key 的范围返回相应的 value 值</strong>。因此，<strong>PUT&#x2F;GET&#x2F;DELETE&#x2F;SCAN 是一个键值数据库的基本操作集合</strong>。</p>
<p>此外，实际业务场景通常还有更加丰富的需求，例如，在黑白名单应用中，需要判断某个用户是否存在。<strong>如果将该用户的 ID 作为 key，那么，可以增加 EXISTS 操作接口，用于判断某个 key 是否存在。</strong>对于一个具体的键值数据库而言，你可以通过查看操作文档，了解其详细的操作接口。</p>
<p><strong>当然，当一个键值数据库的 value 类型多样化时，就需要包含相应的操作接口。</strong>例如，Redis 的 value 有列表类型，因此它的接口就要包括对列表 value 的操作。后面我也会具体介绍，不同操作对 Redis 访问效率的影响。</p>
<p>说到这儿呢，数据模型和操作接口我们就构造完成了，这是我们的基础工作。接下来呢，我们就要更进一步，考虑一个非常重要的设计问题：<strong>键值对保存在内存还是外存？</strong></p>
<p>保存在内存的好处是读写很快，毕竟内存的访问速度一般都在百 ns 级别（<strong>延迟在百纳秒（nanoseconds，ns）级别的范围</strong>）。但是，潜在的风险是一旦掉电，所有的数据都会丢失。</p>
<p>保存在外存，<strong>虽然可以避免数据丢失，但是受限于磁盘的慢速读写（通常在几 ms 级别），键值数据库的整体性能会被拉低。</strong></p>
<p>因此，<strong>如何进行设计选择，我们通常需要考虑键值数据库的主要应用场景</strong>。比如，<strong>缓存场景下的数据需要能快速访问但允许丢失，那么，用于此场景的键值数据库通常采用内存保存键值数据。Memcached 和 Redis 都是属于内存键值数据库。对于 Redis 而言，缓存是非常重要的一个应用场景。</strong>后面我会重点介绍 Redis 作为缓存使用的关键机制、优势，以及常见的优化方法。</p>
<p>为了和 Redis 保持一致，<strong>我们的 SimpleKV 就采用内存保存键值数据。接下来，我们来了解下 SimpleKV 的基本组件。</strong></p>
<p>大体来说，一个键值数据库包括了<strong>访问框架、索引模块、操作模块和存储模块</strong>四部分（见下图）。接下来，我们就从这四个部分入手，继续构建我们的 SimpleKV。</p>
<p><img src="/2024/09/14/Redis01/image-20240914154156077.png" alt="image-20240914154156077"></p>
<h2 id="采用什么访问模式？"><a href="#采用什么访问模式？" class="headerlink" title="采用什么访问模式？"></a>采用什么访问模式？</h2><p>访问模式通常有两种：一种是<strong>通过函数库调用的方式供外部应用使用</strong>，比如，上图中的 libsimplekv.so，就是以动态链接库的形式链接到我们自己的程序中，提供键值存储功能；另一种是<strong>通过网络框架以 Socket 通信的形式对外提供键值对操作</strong>，这种形式可以提供广泛的键值存储服务。在上图中，我们可以看到，网络框架中包括 Socket Server 和协议解析。</p>
<p><strong>不同的键值数据库服务器和客户端交互的协议并不相同，我们在对键值数据库进行二次开发、新增功能时，必须要了解和掌握键值数据库的通信协议，这样才能开发出兼容的客户端。</strong></p>
<p>实际的键值数据库也基本采用上述两种方式，例如，<strong>RocksDB 以动态链接库的形式使用，而 Memcached 和 Redis 则是通过网络框架访问。后面我还会给你介绍 Redis 现有的客户端和通信协议。</strong></p>
<p>通过<strong>网络框架提供键值存储服务，一方面扩大了键值数据库的受用面，但另一方面，也给键值数据库的性能、运行模型提供了不同的设计选择，带来了一些潜在的问题。</strong></p>
<p>举个例子，当客户端发送一个如下的命令后，该命令会被封装在网络包中发送给键值数据库：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PUT hello world</span><br></pre></td></tr></table></figure>

<p>键值数据库网络框架接收到网络包，并按照相应的协议进行解析之后，就可以知道，客户端想写入一个键值对，并开始实际的写入流程。<strong>此时，我们会遇到一个系统设计上的问题，简单来说，就是网络连接的处理、网络请求的解析，以及数据存取的处理，是用一个线程、多个线程，还是多个进程来交互处理呢</strong>？该如何进行设计和取舍呢？<strong>我们一般把这个问题称为 I&#x2F;O 模型设计。不同的 I&#x2F;O 模型对键值数据库的性能和可扩展性会有不同的影响。</strong></p>
<p>举个例子，如果一个线程既要处理网络连接、解析请求，又要完成数据存取，<strong>一旦某一步操作发生阻塞，整个线程就会阻塞住，这就降低了系统响应速度</strong>。如果我们采用<strong>不同线程处理不同操作，那么，某个线程被阻塞时，其他线程还能正常运行</strong>。但是，<strong>不同线程间如果需要访问共享资源，那又会产生线程竞争，也会影响系统效率，这又该怎么办呢</strong>？所以，这的确是个“两难”选择，需要我们进行精心的设计。</p>
<p>你可能经常听说 <strong>Redis 是单线程，那么，Redis 又是如何做到“单线程，高性能”的呢？后面我再和你好好聊一聊。</strong></p>
<h2 id="如何定位键值对的位置？"><a href="#如何定位键值对的位置？" class="headerlink" title="如何定位键值对的位置？"></a>如何定位键值对的位置？</h2><p>当 SimpleKV 解析了客户端发来的请求，知道了要进行的键值对操作，此时，SimpleKV 需要查找所要操作的键值对是否存在，这依赖于键值数据库的索引模块。<strong>索引的作用是让键值数据库根据 key 找到相应 value 的存储位置，进而执行操作</strong>。</p>
<p>索引的类型有很多，<strong>常见的有哈希表、B+ 树、字典树等。不同的索引结构在性能、空间消耗、并发控制等方面具有不同的特征</strong>。如果你看过其他键值数据库，就会发现，不同键值数据库采用的索引并不相同，<strong>例如，Memcached 和 Redis 采用哈希表作为 key-value 索引，而 RocksDB 则采用跳表作为内存中 key-value 的索引。</strong></p>
<p>一般而言，<strong>内存键值数据库（例如 Redis）采用哈希表作为索引，很大一部分原因在于，其键值数据基本都是保存在内存中的，而内存的高性能随机访问特性可以很好地与哈希表 O(1) 的操作复杂度相匹配。</strong></p>
<p>SimpleKV 的索引根据 key 找到 value 的存储位置即可。<strong>但是，和 SimpleKV 不同，对于 Redis 而言，很有意思的一点是，它的 value 支持多种类型，当我们通过索引找到一个 key 所对应的 value 后，仍然需要从 value 的复杂结构（例如集合和列表）中进一步找到我们实际需要的数据，这个操作的效率本身就依赖于它们的实现结构。</strong></p>
<p>Redis <strong>采用一些常见的高效索引结构作为某些 value 类型的底层数据结构，这一技术路线为 Redis 实现高性能访问提供了良好的支撑。</strong></p>
<h2 id="不同操作的具体逻辑是怎样的？"><a href="#不同操作的具体逻辑是怎样的？" class="headerlink" title="不同操作的具体逻辑是怎样的？"></a>不同操作的具体逻辑是怎样的？</h2><p>SimpleKV 的索引模块<strong>负责根据 key 找到相应的 value 的存储位置</strong>。对于不同的操作来说，找到存储位置之后，需要进一步执行的操作的具体逻辑会有所差异。SimpleKV 的操作模块就实现了不同操作的具体逻辑：</p>
<ul>
<li>对于 GET&#x2F;SCAN 操作而言，<strong>此时根据 value 的存储位置返回 value 值即可；</strong></li>
<li>对于 PUT 一个新的键值对数据而言，<strong>SimpleKV 需要为该键值对分配内存空间；</strong></li>
<li>对于 DELETE 操作，<strong>SimpleKV 需要删除键值对，并释放相应的内存空间，这个过程由分配器完成。</strong></li>
</ul>
<p>不知道你注意到没有，对于 PUT 和 DELETE 两种操作来说，<strong>除了新写入和删除键值对，还需要分配和释放内存。这就不得不提 SimpleKV 的存储模块了。</strong></p>
<h2 id="如何实现重启后快速提供服务？"><a href="#如何实现重启后快速提供服务？" class="headerlink" title="如何实现重启后快速提供服务？"></a>如何实现重启后快速提供服务？</h2><p><strong>SimpleKV 采用了常用的内存分配器 glibc 的 malloc 和 free，因此，SimpleKV 并不需要特别考虑内存空间的管理问题。但是，键值数据库的键值对通常大小不一，glibc 的分配器在处理随机的大小内存块分配时，表现并不好。一旦保存的键值对数据规模过大，就可能会造成较严重的内存碎片问题。</strong></p>
<p>因此，分配器是键值数据库中的一个关键因素。<strong>对于以内存存储为主的 Redis 而言，这点尤为重要。Redis 的内存分配器提供了多种选择，分配效率也不一样，后面我会具体讲一讲这个问题。</strong></p>
<p>SimpleKV <strong>虽然依赖于内存保存数据，提供快速访问，但是，我也希望 SimpleKV 重启后能快速重新提供服务，所以，我在 SimpleKV 的存储模块中增加了持久化功能。</strong></p>
<p>不过，<strong>鉴于磁盘管理要比内存管理复杂，SimpleKV 就直接采用了文件形式，将键值数据通过调用本地文件系统的操作接口保存在磁盘上。此时，SimpleKV 只需要考虑何时将内存中的键值数据保存到文件中，就可以了。</strong></p>
<p>一种方式是，<strong>对于每一个键值对，SimpleKV 都对其进行落盘保存，这虽然让 SimpleKV 的数据更加可靠，但是，因为每次都要写盘，SimpleKV 的性能会受到很大影响。</strong></p>
<p>另一种方式是，<strong>SimpleKV 只是周期性地把内存中的键值数据保存到文件中，这样可以避免频繁写盘操作的性能影响。但是，一个潜在的代价是 SimpleKV 的数据仍然有丢失的风险。</strong></p>
<p>和 SimpleKV 一样，Redis 也提供了持久化功能。<strong>不过，为了适应不同的业务场景，Redis 为持久化提供了诸多的执行机制和优化改进，后面我会和你逐一介绍 Redis 在持久化机制中的关键设计考虑。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>至此，我们构造了一个简单的键值数据库 SimpleKV。可以看到，<strong>前面两步我们是从应用的角度进行设计的，也就是应用视角；后面四步其实就是 SimpleKV 完整的内部构造，可谓是麻雀虽小，五脏俱全。</strong></p>
<p>SimpleKV 包含了一个键值数据库的基本组件，对这些组件有了了解之后，后面在学习 Redis 这个丰富版的 SimpleKV 时，就会轻松很多。</p>
<p>为了支持更加丰富的业务场景，Redis 对这些组件或者功能进行了扩展，或者说是进行了精细优化，从而满足了功能和性能等方面的要求。</p>
<p><img src="/2024/09/14/Redis01/image-20240914154336064.png" alt="image-20240914154336064"></p>
<p>从这张对比图中，我们可以看到，从 SimpleKV 演进到 Redis，有以下几个重要变化：</p>
<p><strong>Redis 主要通过网络框架进行访问，而不再是动态库了，这也使得 Redis 可以作为一个基础性的网络服务进行访问，扩大了 Redis 的应用范围。</strong></p>
<p><strong>Redis 数据模型中的 value 类型很丰富</strong>，因此也带来了更多的操作接口，例如面向列表的 LPUSH&#x2F;LPOP，面向集合的 SADD&#x2F;SREM 等。在下节课，我将和你聊聊这些 value 模型背后的数据结构和操作效率，以及它们对 Redis 性能的影响。</p>
<p><strong>Redis 的持久化模块能支持两种方式：日志（AOF）和快照（RDB），这两种持久化方式具有不同的优劣势，影响到 Redis 的访问性能和可靠性。</strong></p>
<p><strong>SimpleKV 是个简单的单机键值数据库，但是，Redis 支持高可靠集群和高可扩展集群，因此，Redis 中包含了相应的集群功能支撑模块。</strong></p>
<p>通过这节课 SimpleKV 的构建，我相信你已经对键值数据库的基本结构和重要模块有了整体认知和深刻理解，这其实也是 Redis 单机版的核心基础。针对刚刚提到的几点 Redis 的重大演进，在接下来的课程中，我会依次进行重点讲解。与此同时，我还会结合实战场景，让你不仅能够理解原理，还能真正学以致用，提升实战能力。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>给你留个小问题：和你了解的 Redis 相比，你觉得，SimpleKV 里面还缺少什么功能组件或模块吗？</p>
<ul>
<li><strong>【数据结构】上缺乏广泛的数据结构支持：比如支持范围查询的SkipList，和Stream等等数据结构</strong></li>
<li><strong>【高可用】上缺乏，哨兵或者master-slaver模式的高可用设计</strong></li>
<li><strong>【横向扩展】上缺乏集群和分片功能</strong></li>
<li><strong>【在内存安全性】上，缺乏内存过载时候的key淘汰算法的支持</strong></li>
<li><strong>【内存利用率】没有充分对数据结构优化提高内存利用率，例如使用压缩性的数据结构</strong></li>
<li><strong>【功能扩展】需要具备后续功能的拓展</strong></li>
<li><strong>【不具备事务性】无法保证多个操作的原子性</strong></li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/wb4pg4">基础篇 - 01 | 基本架构：一个键值数据库包含什么？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构：快速的Redis有哪些慢操作？</title>
    <url>/2024/09/15/Redis02/</url>
    <content><![CDATA[<p>一提到 Redis，我们的脑子里马上就会出现一个词：“快。”但是你有没有想过，Redis 的快，到底是快在哪里呢？实际上，这里有一个重要的表现：它接收到一个键值对操作后，能以<strong>微秒级别</strong>的速度找到数据，并快速完成操作。</p>
<p>数据库这么多，为啥 Redis 能有这么突出的表现呢？</p>
<p><strong>一方面，这是因为它是内存数据库，所有操作都在内存上完成，内存的访问速度本身就很快。</strong></p>
<p>另一方面，这要归功于它的数据结构。<strong>这是因为，键值对是按一定的数据结构来组织的，操作键值对最终就是对数据结构进行增删改查操作，所以高效的数据结构是 Redis 快速处理数据的基础。这节课，我就来和你聊聊数据结构。</strong></p>
<p>说到这儿，你肯定会说：“这个我知道，不就是 <strong>String（字符串）、List（列表）、Hash（哈希）、Set（集合）和 Sorted Set（有序集合）</strong>吗？”<strong>其实，这些只是 Redis 键值对中值的数据类型，也就是数据的保存形式</strong>。而这里，<strong>我们说的数据结构，是要去看看它们的底层实现。</strong></p>
<p>简单来说，<strong>底层数据结构一共有 6 种，分别是简单动态字符串、双向链表、压缩列表、哈希表、跳表和整数数组</strong>。它们和数据类型的对应关系如下图所示：</p>
<p><img src="/2024/09/15/Redis02/8219f7yy651e566d47cc9f661b399f01.jpg" alt="img"></p>
<p>可以看到，<strong>String 类型的底层实现只有一种数据结构，也就是简单动态字符串。而 List、Hash、Set 和 Sorted Set 这四种数据类型，都有两种底层实现结构</strong>。通常情况下，我们会把这四种类型称为集合类型，它们的特点是<strong>一个键对应了一个集合的数据</strong>。</p>
<p>看到这里，其实有些问题已经值得我们去考虑了：</p>
<ul>
<li><strong>这些数据结构都是值的底层实现，键和值本身之间用什么结构组织？</strong></li>
<li><strong>为什么集合类型有那么多的底层结构，它们都是怎么组织数据的，都很快吗？</strong></li>
<li><strong>什么是简单动态字符串，和常用的字符串是一回事吗？</strong></li>
</ul>
<p>接下来，我就和你聊聊前两个问题。这样，你不仅可以知道 Redis“快”的基本原理，还可以借此理解 Redis 中有哪些潜在的“慢操作”，最大化 Redis 的性能优势。而关于简单动态字符串，我会在后面的课程中再和你讨论。</p>
<p>我们先来看看键和值之间是用什么结构组织的。</p>
<h2 id="键和值用什么结构组织？"><a href="#键和值用什么结构组织？" class="headerlink" title="键和值用什么结构组织？"></a>键和值用什么结构组织？</h2><p>为了实现从键到值的快速访问，<strong>Redis 使用了一个哈希表来保存所有键值对。</strong></p>
<p><strong>一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶。所以，我们常说，一个哈希表是由多个哈希桶组成的，每个哈希桶中保存了键值对数据。</strong></p>
<p>看到这里，你可能会问了：“<strong>如果值是集合类型的话，作为数组元素的哈希桶怎么来保存呢？”其实，哈希桶中的元素保存的并不是值本身，而是指向具体值的指针。这也就是说，不管值是 String，还是集合类型，哈希桶中的元素都是指向它们的指针。</strong></p>
<p>在下图中，可以看到，<strong>哈希桶中的 entry 元素中保存了<code>*key和*value</code>指针，分别指向了实际的键和值，这样一来，即使值是一个集合，也可以通过<code>*value</code>指针被查找到。</strong></p>
<p><img src="/2024/09/15/Redis02/1cc8eaed5d1ca4e3cdbaa5a3d48dfb5f.jpg" alt="img"></p>
<p>因为这个哈希表保存了所有的键值对，所以，我也把它称为<strong>全局哈希表</strong>。<strong>哈希表的最大好处很明显，就是让我们可以用 O(1) 的时间复杂度来快速查找到键值对——我们只需要计算键的哈希值，就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素。</strong></p>
<p>你看，<strong>这个查找过程主要依赖于哈希计算，和数据量的多少并没有直接关系。也就是说，不管哈希表里有 10 万个键还是 100 万个键，我们只需要一次计算就能找到相应的键。</strong></p>
<p>但是，如果你只是了解了哈希表的 O(1) 复杂度和快速查找特性，那么，当你往 Redis 中写入大量数据后，<strong>就可能发现操作有时候会突然变慢了。这其实是因为你忽略了一个潜在的风险点</strong>，那就是<strong>哈希表的冲突问题和 rehash 可能带来的操作阻塞。</strong></p>
<h3 id="为什么哈希表操作变慢了？"><a href="#为什么哈希表操作变慢了？" class="headerlink" title="为什么哈希表操作变慢了？"></a>为什么哈希表操作变慢了？</h3><p>当你往哈希表中写入更多数据时，<strong>哈希冲突是不可避免的问题。这里的哈希冲突，也就是指，两个 key 的哈希值和哈希桶计算对应关系时，正好落在了同一个哈希桶中。</strong></p>
<p>毕竟，<strong>哈希桶的个数通常要少于 key 的数量，这也就是说，难免会有一些 key 的哈希值对应到了同一个哈希桶中。</strong></p>
<p>Redis 解决哈希冲突的方式，就是<strong>链式哈希</strong>。链式哈希也很容易理解，就是指<strong>同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接</strong>。</p>
<p>如下图所示：<strong>entry1、entry2 和 entry3 都需要保存在哈希桶 3 中，导致了哈希冲突。此时，entry1 元素会通过一个<em>next指针指向 entry2，同样，entry2 也会通过</em>next指针指向 entry3。这样一来，即使哈希桶 3 中的元素有 100 个，我们也可以通过 entry 元素中的指针，把它们连起来。这就形成了一个链表，也叫作哈希冲突链。</strong></p>
<p><img src="/2024/09/15/Redis02/8ac4cc6cf94968a502161f85d072e428.jpg" alt="img"></p>
<p>但是，这里依然存在一个问题，<strong>哈希冲突链上的元素只能通过指针逐一查找再操作。如果哈希表里写入的数据越来越多，哈希冲突可能也会越来越多，这就会导致某些哈希冲突链过长，进而导致这个链上的元素查找耗时长，效率降低</strong>。对于追求“快”的 Redis 来说，这是不太能接受的。</p>
<p>所以，<strong>Redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突。那具体怎么做呢？</strong></p>
<p>其实，为了使 rehash 操作更高效，<strong>Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行 rehash</strong>，这个过程分为三步：</p>
<ul>
<li><strong>给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍；</strong></li>
<li><strong>把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中；</strong></li>
<li><strong>释放哈希表 1 的空间。</strong></li>
</ul>
<p><strong>到此，我们就可以从哈希表 1 切换到哈希表 2，用增大的哈希表 2 保存更多数据，而原来的哈希表 1 留作下一次 rehash 扩容备用。</strong></p>
<p><strong>这个过程看似简单，但是第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都迁移完，会造成 Redis 线程阻塞，无法服务其他请求。此时，Redis 就无法快速访问数据了。</strong></p>
<p>为了避免这个问题，Redis 采用了<strong>渐进式 rehash</strong>。</p>
<p>简单来说就是在第二步拷贝数据时，<strong>Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries。如下图所示：</strong></p>
<p><img src="/2024/09/15/Redis02/73fb212d0b0928d96a0d7d6ayy76da0c.jpg" alt="img"></p>
<p>渐进式rehash</p>
<p><strong>这样就巧妙地把一次性大量拷贝的开销，分摊到了多次处理请求的过程中，避免了耗时操作，保证了数据的快速访问。</strong></p>
<p>好了，到这里，你应该就能理解，Redis 的键和值是怎么通过哈希表组织的了。<strong>对于 String 类型来说，找到哈希桶就能直接增删改查了，所以，哈希表的 O(1) 操作复杂度也就是它的复杂度了。</strong></p>
<p>但是，<strong>对于集合类型来说，即使找到哈希桶了，还要在集合中再进一步操作。接下来，我们来看集合类型的操作效率又是怎样的。</strong></p>
<h2 id="集合数据操作效率"><a href="#集合数据操作效率" class="headerlink" title="集合数据操作效率"></a>集合数据操作效率</h2><p>和 String 类型不同，一个集合类型的值，<strong>第一步是通过全局哈希表找到对应的哈希桶位置，第二步是在集合中再增删改查。</strong>那么，集合的操作效率和哪些因素相关呢？</p>
<ul>
<li><strong>首先，与集合的底层数据结构有关</strong>。例如，使用哈希表实现的集合，要比使用链表实现的集合访问效率更高。</li>
<li><strong>其次，操作效率和这些操作本身的执行特点有关</strong>，比如读写一个元素的操作要比读写所有元素的效率高。</li>
</ul>
<p>接下来，我们就分别聊聊集合类型的底层数据结构和操作复杂度。</p>
<h3 id="有哪些底层数据结构？"><a href="#有哪些底层数据结构？" class="headerlink" title="有哪些底层数据结构？"></a>有哪些底层数据结构？</h3><p>刚才，我也和你介绍过，<strong>集合类型的底层数据结构主要有 5 种：整数数组、双向链表、哈希表、压缩列表和跳表</strong>。</p>
<p>其中，哈希表的操作特点我们刚刚已经学过了；整数数组和双向链表也很常见，它们的操作特征都是顺序读写，<strong>也就是通过数组下标或者链表的指针逐个元素访问，操作复杂度基本是 O(N)，操作效率比较低</strong>；压缩列表和跳表我们平时接触得可能不多，但它们也是 Redis 重要的数据结构，所以我来重点解释一下。</p>
<p><strong>压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个 zlend，表示列表结束。</strong></p>
<p><img src="/2024/09/15/Redis02/9587e483f6ea82f560ff10484aaca4a0.jpg" alt="img"></p>
<p>在压缩列表中，<strong>如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是 O(N) 了。</strong></p>
<p>我们再来看下跳表。</p>
<p><strong>有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表</strong>。具体来说，跳表在链表的基础上，<strong>增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位</strong>，如下图所示：</p>
<p><img src="/2024/09/15/Redis02/1eca7135d38de2yy16681c2bbc4f3fb4.jpg" alt="img"></p>
<p>跳表的快速查找过程</p>
<p><strong>如果我们要在链表中查找 33 这个元素，只能从头开始遍历链表，查找 6 次，直到找到 33 为止。此时，复杂度是 O(N)，查找效率很低。</strong></p>
<p>为了提高查找速度，我们来增加一级索引：从第一个元素开始，<strong>每两个元素选一个出来作为索引。这些索引再通过指针指向原始的链表</strong>。例如，从前两个元素中抽取元素 1 作为一级索引，从第三、四个元素中抽取元素 11 作为一级索引。此时，我们只需要 4 次查找就能定位到元素 33 了。</p>
<p><strong>如果我们还想再快，可以再增加二级索引：从一级索引中，再抽取部分元素作为二级索引。</strong>例如，从一级索引中抽取 1、27、100 作为二级索引，二级索引指向一级索引。这样，我们只需要 3 次查找，就能定位到元素 33 了。</p>
<p>可以看到，<strong>这个查找过程就是在多级索引上跳来跳去，最后定位到元素。这也正好符合“跳”表的叫法。当数据量很大时，跳表的查找复杂度就是 O(logN)。</strong></p>
<p>好了，我们现在可以按照查找的时间复杂度给这些数据结构分下类了：</p>
<p><img src="/2024/09/15/Redis02/fb7e3612ddee8a0ea49b7c40673a0cf0.jpg" alt="img"></p>
<h3 id="不同操作的复杂度"><a href="#不同操作的复杂度" class="headerlink" title="不同操作的复杂度"></a>不同操作的复杂度</h3><p><strong>集合类型的操作类型很多，有读写单个集合元素的，例如 HGET、HSET，也有操作多个元素的，例如 SADD，还有对整个集合进行遍历操作的，例如 SMEMBERS。这么多操作，它们的复杂度也各不相同。而复杂度的高低又是我们选择集合类型的重要依据。</strong></p>
<p>我总结了一个“四句口诀”，希望能帮助你快速记住集合常见操作的复杂度。这样你在使用过程中，就可以提前规避高复杂度操作了。</p>
<ul>
<li><strong>单元素操作是基础；</strong></li>
<li><strong>范围操作非常耗时；</strong></li>
<li><strong>统计操作通常高效；</strong></li>
<li><strong>例外情况只有几个。</strong></li>
</ul>
<p>第一，<strong>单元素操作，是指每一种集合类型对单个数据实现的增删改查操作</strong>。</p>
<p><strong>例如，Hash 类型的 HGET、HSET 和 HDEL，Set 类型的 SADD、SREM、SRANDMEMBER 等。</strong>这些操作的复杂度由集合采用的数据结构决定，<strong>例如，HGET、HSET 和 HDEL 是对哈希表做操作，所以它们的复杂度都是 O(1)；Set 类型用哈希表作为底层数据结构时，它的 SADD、SREM、SRANDMEMBER 复杂度也是 O(1)。</strong></p>
<p>这里，有个地方你需要注意一下，<strong>集合类型支持同时对多个元素进行增删改查</strong>，例如 Hash 类型的 HMGET 和 HMSET，Set 类型的 SADD 也支持同时增加多个元素。<strong>此时，这些操作的复杂度，就是由单个元素操作复杂度和元素个数决定的。例如，HMSET 增加 M 个元素时，复杂度就从 O(1) 变成 O(M) 了。</strong></p>
<p>第二，<strong>范围操作，是指集合类型中的遍历操作，</strong>可以返回集合中的所有数据<strong>，比如 Hash 类型的 HGETALL 和 Set 类型的 SMEMBERS，或者返回一个范围内的部分数据，比如 List 类型的 LRANGE 和 ZSet 类型的 ZRANGE。</strong>这类操作的复杂度<strong>一般是 O(N)，比较耗时，我们应该尽量避免</strong>。</p>
<p>不过，<strong>Redis 从 2.8 版本开始提供了 SCAN 系列操作（包括 HSCAN，SSCAN 和 ZSCAN），这类操作实现了渐进式遍历，每次只返回有限数量的数据。这样一来，相比于 HGETALL、SMEMBERS 这类操作来说，就避免了一次性返回所有元素而导致的 Redis 阻塞。</strong></p>
<p>第三，统计操作，是指<strong>集合类型对集合中所有元素个数的记录</strong>，例如 LLEN 和 SCARD。<strong>这类操作复杂度只有 O(1)，这是因为当集合类型采用压缩列表、双向链表、整数数组这些数据结构时，这些结构中专门记录了元素的个数统计，因此可以高效地完成相关操作。</strong></p>
<p>第四，例外情况，是指某些数据结构的特殊记录，例如<strong>压缩列表和双向链表都会记录表头和表尾的偏移量</strong>。这样一来，<strong>对于 List 类型的 LPOP、RPOP、LPUSH、RPUSH 这四个操作来说，它们是在列表的头尾增删元素，这就可以通过偏移量直接定位，所以它们的复杂度也只有 O(1)，可以实现快速操作。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课，我们学习了 Redis 的底层数据结构，<strong>这既包括了 Redis 中用来保存每个键和值的全局哈希表结构，也包括了支持集合类型实现的双向链表、压缩列表、整数数组、哈希表和跳表这五大底层结构。</strong></p>
<p>Redis 之所以能快速操作键值对，<strong>一方面是因为 O(1) 复杂度的哈希表被广泛使用，包括 String、Hash 和 Set，它们的操作复杂度基本由哈希表决定</strong>，另一方面，Sorted Set 也采用了 O(logN) 复杂度的跳表。不过，集合类型的范围操作，因为要遍历底层数据结构，复杂度通常是 O(N)。这里，我的建议是：<strong>用其他命令来替代</strong>，例如<strong>可以用 SCAN 来代替，避免在 Redis 内部产生费时的全集合遍历操作</strong>。</p>
<p>当然，我们不能忘了复杂度较高的 List 类型，<strong>它的两种底层实现结构：双向链表和压缩列表的操作复杂度都是 O(N)<strong>。因此，我的建议是：</strong>因地制宜地使用 List 类型</strong>。例如，既然它的 POP&#x2F;PUSH 效率很高，<strong>那么就将它主要用于 FIFO 队列场景，而不是作为一个可以随机读写的集合。</strong></p>
<p>Redis 数据类型丰富，每个类型的操作繁多，我们通常无法一下子记住所有操作的复杂度。所以，最好的办法就是<strong>掌握原理，以不变应万变</strong>。这里，你可以看到，<strong>一旦掌握了数据结构基本原理，你可以从原理上推断不同操作的复杂度，即使这个操作你不一定熟悉</strong>。这样一来，你不用死记硬背，也能快速合理地做出选择了。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>整数数组和压缩列表在查找时间复杂度方面并没有很大的优势，那为什么 Redis 还会把它们作为底层数据结构呢？</p>
<ul>
<li>1、<strong>内存利用率</strong>。数组和压缩列表都是非常紧凑的数据结构，<strong>它比链表占用的内存要更少</strong>。Redis是内存数据库，大量数据存到内存中，此时需要做尽可能的优化，提高内存的利用率。<strong>这样节省空间而且也能避免一些内存碎片。</strong></li>
<li>2、<strong>数组对CPU高速缓存支持更友好</strong>。所以Redis在设计时，<strong>集合数据元素较少情况下，默认采用内存紧凑排列的方式存储，同时利用CPU高速缓存不会降低访问速度</strong>。<strong>当数据元素超过设定阈值后，避免查询时间复杂度太高，转为哈希和跳表数据结构存储，保证查询效率</strong>。</li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/db8smz">基础篇 - 02 | 数据结构：快速的Redis有哪些慢操作？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>内存快照：宕机后，Redis如何实现快速恢复？</title>
    <url>/2024/09/15/Redis05/</url>
    <content><![CDATA[<p>上节课，我们学习了 Redis 避免数据丢失的 AOF 方法。<strong>这个方法的好处，是每次执行只需要记录操作命令，需要持久化的数据量不大。一般而言，只要你采用的不是 always 的持久化策略，就不会对性能造成太大影响。</strong></p>
<p><strong>但是，也正因为记录的是操作命令，而不是实际的数据，所以，用 AOF 方法进行故障恢复的时候，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用</strong>。这当然不是理想的结果。那么，还有没有既可以保证可靠性，还能在宕机时实现快速恢复的其他方法呢？</p>
<p>当然有了，这就是我们今天要一起学习的另一种持久化方法：<strong>内存快照</strong>。所谓内存快照，<strong>就是指内存中的数据在某一个时刻的状态记录。这就类似于照片，当你给朋友拍照时，一张照片就能把朋友一瞬间的形象完全记下来。</strong></p>
<p>对 Redis 来说，<strong>它实现类似照片记录效果的方式，就是把某一时刻的状态以文件的形式写到磁盘上，也就是快照。这样一来，即使宕机，快照文件也不会丢失，数据的可靠性也就得到了保证。</strong>这个快照文件就称为 RDB 文件，其中，<strong>RDB 就是 Redis DataBase 的缩写。</strong></p>
<p>和 AOF 相比，<strong>RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我们可以直接把 RDB 文件读入内存，很快地完成恢复。听起来好像很不错，但内存快照也并不是最优选项。为什么这么说呢？</strong></p>
<p>我们还要考虑两个关键问题：</p>
<ul>
<li><strong>对哪些数据做快照？</strong>这关系到快照的执行效率问题；</li>
<li><strong>做快照时，数据还能被增删改吗？</strong>这关系到 Redis 是否被阻塞，能否同时正常处理请求。</li>
</ul>
<p>这么说可能你还不太好理解，我还是拿拍照片来举例子。我们在拍照时，通常要关注两个问题：</p>
<ul>
<li>如何取景？也就是说，我们打算把哪些人、哪些物拍到照片中；</li>
<li>在按快门前，要记着提醒朋友不要乱动，否则拍出来的照片就模糊了。</li>
</ul>
<p>你看，这两个问题是不是非常重要呢？那么，接下来，我们就来具体地聊一聊。先说“取景”问题，也就是我们对哪些数据做快照。</p>
<h2 id="给哪些内存数据做快照？"><a href="#给哪些内存数据做快照？" class="headerlink" title="给哪些内存数据做快照？"></a>给哪些内存数据做快照？</h2><p>Redis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是<strong>全量快照</strong>，也就是说，<strong>把内存中的所有数据都记录到磁盘中，这就类似于给 100 个人拍合影，把每一个人都拍进照片里。这样做的好处是，一次性记录了所有数据，一个都不少。</strong></p>
<p>当你给一个人拍照时，只用协调一个人就够了，<strong>但是，拍 100 人的大合影，却需要协调 100 个人的位置、状态，等等，这当然会更费时费力。同样，给内存的全量数据做快照，把它们全部写入磁盘也会花费很多时间。而且，全量数据越多，RDB 文件就越大，往磁盘上写数据的时间开销就越大。</strong></p>
<p>对于 Redis 而言，它的单线程模型就决定了，我们要尽量避免所有会阻塞主线程的操作，<strong>所以，针对任何操作，我们都会提一个灵魂之问：“它会阻塞主线程吗?”RDB 文件的生成是否会阻塞主线程，这就关系到是否会降低 Redis 的性能。</strong></p>
<p>Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。</p>
<ul>
<li><strong>save：在主线程中执行，会导致阻塞；</strong></li>
<li><strong>bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。</strong></li>
</ul>
<p>好了，这个时候，<strong>我们就可以通过 bgsave 命令来执行全量快照，这既提供了数据的可靠性保证，也避免了对 Redis 的性能影响。</strong></p>
<p>接下来，我们要关注的问题就是，在对内存数据做快照时，这些数据还能“动”吗? 也就是说，这些数据还能被修改吗？ </p>
<p>这个问题非常重要，这是因为，<strong>如果数据能被修改，那就意味着 Redis 还能正常处理写操作。否则，所有写操作都得等到快照完了才能执行，性能一下子就降低了。</strong></p>
<h2 id="快照时数据能修改吗"><a href="#快照时数据能修改吗" class="headerlink" title="快照时数据能修改吗?"></a>快照时数据能修改吗?</h2><p>在给别人拍照时，<strong>一旦对方动了，那么这张照片就拍糊了，我们就需要重拍，所以我们当然希望对方保持不动。对于内存快照而言，我们也不希望数据“动”。</strong></p>
<p>举个例子。我们在时刻 t 给内存做快照，假设内存数据量是 4GB，磁盘的写入带宽是 0.2GB&#x2F;s，简单来说，至少需要 20s（4&#x2F;0.2 &#x3D; 20）才能做完。如果在时刻 t+5s 时，一个还没有被写入磁盘的内存数据 A，被修改成了 A’，那么就会破坏快照的完整性，因为 A’不是时刻 t 时的状态。<strong>因此，和拍照类似，我们在做快照时也不希望数据“动”，也就是不能被修改。</strong></p>
<p>但是，如果快照执行期间数据不能被修改，是会有潜在问题的。<strong>对于刚刚的例子来说，在做快照的 20s 时间里，如果这 4GB 的数据都不能被修改，Redis 就不能处理对这些数据的写操作，那无疑就会给业务服务造成巨大的影响。</strong></p>
<p>你可能会想到，可以用 bgsave 避免阻塞啊。这里我就要说到一个常见的误区了，<strong>避免阻塞和正常处理写操作并不是一回事</strong>。此时，主线程的确没有阻塞，可以正常接收请求，<strong>但是，为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。</strong></p>
<p>为了快照而暂停写操作，肯定是不能接受的。<strong>所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。</strong></p>
<p>简单来说，<strong>bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。</strong></p>
<p>此时，<strong>如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。</strong></p>
<p><img src="/2024/09/15/Redis05/4dc5fb99a1c94f70957cce1ffef419cc.jpg" alt="img"></p>
<p>写时复制机制保证快照期间数据可修改</p>
<p><strong>这既保证了快照的完整性，也允许主线程同时对数据进行修改，避免了对正常业务的影响。</strong></p>
<p>到这里，我们就解决了对“哪些数据做快照”以及“做快照时数据能否修改”这两大问题：<strong>Redis 会使用 bgsave 对当前内存中的所有数据做快照，这个操作是子进程在后台完成的，这就允许主线程同时可以修改数据。</strong></p>
<p>现在，我们再来看另一个问题：多久做一次快照？我们在拍照的时候，还有项技术叫“连拍”，可以记录人或物连续多个瞬间的状态。那么，快照也适合“连拍”吗？</p>
<h2 id="可以每秒做一次快照吗？"><a href="#可以每秒做一次快照吗？" class="headerlink" title="可以每秒做一次快照吗？"></a>可以每秒做一次快照吗？</h2><p><strong>对于快照来说，所谓“连拍”就是指连续地做快照。这样一来，快照的间隔时间变得很短，即使某一时刻发生宕机了，因为上一时刻快照刚执行，丢失的数据也不会太多。但是，这其中的快照间隔时间就很关键了。</strong></p>
<p>如下图所示，我们先在 T0 时刻做了一次快照，然后又在 T0+t 时刻做了一次快照，在这期间，数据块 5 和 9 被修改了。如果在 t 这段时间内，机器宕机了，那么，只能按照 T0 时刻的快照进行恢复。此时，数据块 5 和 9 的修改值因为没有快照记录，就无法恢复了。</p>
<p><img src="/2024/09/15/Redis05/711c873a61bafde79b25c110735289ab.jpg" alt="img"></p>
<p>快照机制下的数据丢失</p>
<p>所以，要想尽可能恢复数据，<strong>t 值就要尽可能小，t 越小，就越像“连拍”。那么，t 值可以小到什么程度呢，比如说是不是可以每秒做一次快照？毕竟，每次快照都是由 bgsave 子进程在后台执行，也不会阻塞主线程。</strong></p>
<p>这种想法其实是错误的。虽然 bgsave 执行时不阻塞主线程，但是，<strong>如果频繁地执行全量快照，也会带来两方面的开销</strong>。</p>
<ul>
<li><strong>一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。</strong></li>
<li><strong>另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。</strong>虽然，子进程在创建后不会再阻塞主线程，<strong>但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。</strong>那么，有什么其他好方法吗？</li>
</ul>
<p>此时，<strong>我们可以做增量快照，所谓增量快照，就是指，做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销</strong>。</p>
<p>在第一次做完全量快照后，<strong>T1 和 T2 时刻如果再做快照，我们只需要将被修改的数据写入快照文件就行</strong>。但是，这么做的前提是，<strong>我们需要记住哪些数据被修改了</strong>。<strong>你可不要小瞧这个“记住”功能，它需要我们使用额外的元数据信息去记录哪些数据被修改了，这会带来额外的空间开销问题。</strong>如下图所示：</p>
<p><img src="/2024/09/15/Redis05/8a1d515269cd23595ee1813e8dff28a5.jpg" alt="img"></p>
<p>增量快照示意图</p>
<p>如果我们对每一个键值对的修改，都做个记录，<strong>那么，如果有 1 万个被修改的键值对，我们就需要有 1 万条额外的记录。而且，有的时候，键值对非常小，比如只有 32 字节，而记录它被修改的元数据信息，可能就需要 8 字节，这样的画，为了“记住”修改，引入的额外空间开销比较大。这对于内存资源宝贵的 Redis 来说，有些得不偿失。</strong></p>
<p>到这里，你可以发现，虽然跟 AOF 相比，<strong>快照的恢复速度快，但是，快照的频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销，那么，还有什么方法既能利用 RDB 的快速恢复，又能以较小的开销做到尽量少丢数据呢？</strong></p>
<p>Redis 4.0 中提出了一个<strong>混合使用 AOF 日志和内存快照</strong>的方法。简单来说，<strong>内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。</strong></p>
<p>这样一来，<strong>快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF 日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。</strong></p>
<p>如下图所示，T1 和 T2 时刻的修改，<strong>用 AOF 日志记录，等到第二次做全量快照时，就可以清空 AOF 日志，因为此时的修改都已经记录到快照中了，恢复时就不再用日志了。</strong></p>
<p><img src="/2024/09/15/Redis05/e4c5846616c19fe03dbf528437beb320.jpg" alt="img"></p>
<p>内存快照和AOF混合使用</p>
<p><strong>这个方法既能享受到 RDB 文件快速恢复的好处，又能享受到 AOF 只记录操作命令的简单优势，颇有点“鱼和熊掌可以兼得”的感觉，建议你在实践中用起来。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课，<strong>我们学习了 Redis 用于避免数据丢失的内存快照方法。这个方法的优势在于，可以快速恢复数据库，也就是只需要把 RDB 文件直接读入内存，这就避免了 AOF 需要顺序、逐一重新执行操作命令带来的低效性能问题。</strong></p>
<p>不过，内存快照也有它的局限性。<strong>它拍的是一张内存的“大合影”，不可避免地会耗时耗力。虽然，Redis 设计了 bgsave 和写时复制方式，尽可能减少了内存快照对正常读写的影响，但是，频繁快照仍然是不太能接受的。而混合使用 RDB 和 AOF，正好可以取两者之长，避两者之短，以较小的性能开销保证数据可靠性和性能。</strong></p>
<p>最后，关于 AOF 和 RDB 的选择问题，我想再给你提三点建议：</p>
<ul>
<li><strong>数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择；</strong></li>
<li><strong>如果允许分钟级别的数据丢失，可以只使用 RDB；</strong></li>
<li><strong>如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。</strong></li>
</ul>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>我曾碰到过这么一个场景：我们使用一个 2 核 CPU、4GB 内存、500GB 磁盘的云主机运行 Redis，Redis 数据库的数据量大小差不多是 2GB，我们使用了 RDB 做持久化保证。当时 Redis 的运行负载以修改操作为主，写读比例差不多在 8:2 左右，也就是说，如果有 100 个请求，80 个请求执行的是修改操作。<strong>你觉得，在这个场景下，用 RDB 做持久化有什么风险吗？</strong>你能帮着一起分析分析吗？</p>
<p>2核CPU、4GB内存、500G磁盘，Redis实例占用2GB，写读比例为8:2，此时做RDB持久化，产生的风险主要在于 CPU资源 和 内存资源 这2方面：</p>
<ul>
<li>a、内存资源风险：Redis fork子进程做RDB持久化，由于写的比例为80%，那么在持久化过程中，“写实复制”会重新分配整个实例80%的内存副本，大约需要重新分配1.6GB内存空间，这样整个系统的内存使用接近饱和，如果此时父进程又有大量新key写入，很快机器内存就会被吃光，如果机器开启了Swap机制，那么Redis会有一部分数据被换到磁盘上，当Redis访问这部分在磁盘上的数据时，性能会急剧下降，已经达不到高性能的标准（可以理解为武功被废）。如果机器没有开启Swap，会直接触发OOM，父子进程会面临被系统kill掉的风险。</li>
<li>b、CPU资源风险：虽然子进程在做RDB持久化，但生成RDB快照过程会消耗大量的CPU资源，虽然Redis处理处理请求是单线程的，但Redis Server还有其他线程在后台工作，例如AOF每秒刷盘、异步关闭文件描述符这些操作。由于机器只有2核CPU，这也就意味着父进程占用了超过一半的CPU资源，此时子进程做RDB持久化，可能会产生CPU竞争，导致的结果就是父进程处理请求延迟增大，子进程生成RDB快照的时间也会变长，整个Redis Server性能下降。</li>
<li>c、另外，可以再延伸一下，老师的问题没有提到Redis进程是否绑定了CPU，如果绑定了CPU，那么子进程会继承父进程的CPU亲和性属性，子进程必然会与父进程争夺同一个CPU资源，整个Redis Server的性能必然会受到影响！所以如果Redis需要开启定时RDB和AOF重写，进程一定不要绑定CPU。</li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/ldx9xs">基础篇 - 05 | 内存快照：宕机后，Redis如何实现快速恢复？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>高性能IO模型：为什么单线程Redis能那么快？</title>
    <url>/2024/09/15/Redis03/</url>
    <content><![CDATA[<p>今天，我们来探讨一个很多人都很关心的问题：“为什么单线程的 Redis 能那么快？”</p>
<p>首先，我要和你厘清一个事实，我们通常说，Redis 是单线程，主要是指 <strong>Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程</strong>。但 Redis 的其他功能，<strong>比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。</strong></p>
<p>所以，<strong>严格来说，Redis 并不是单线程，但是我们一般把 Redis 称为单线程高性能，这样显得“酷”些</strong>。接下来，<strong>我也会把 Redis 称为单线程模式。而且，这也会促使你紧接着提问：“为什么用单线程？为什么单线程能这么快？”</strong></p>
<p>要弄明白这个问题，我们就要深入地学习下 Redis 的单线程设计机制以及多路复用机制。<strong>之后你在调优 Redis 性能时，也能更有针对性地避免会导致 Redis 单线程阻塞的操作，例如执行复杂度高的命令。</strong></p>
<p>好了，话不多说，接下来，我们就先来学习下 Redis 采用单线程的原因。</p>
<h2 id="Redis-为什么用单线程？"><a href="#Redis-为什么用单线程？" class="headerlink" title="Redis 为什么用单线程？"></a>Redis 为什么用单线程？</h2><p>要更好地理解 Redis 为什么用单线程，我们就要先了解多线程的开销。</p>
<h3 id="多线程的开销"><a href="#多线程的开销" class="headerlink" title="多线程的开销"></a>多线程的开销</h3><p>日常写程序时，<strong>我们经常会听到一种说法：“使用多线程，可以增加系统吞吐率，或是可以增加系统扩展性。”</strong>的确，<strong>对于一个多线程的系统来说，在有合理的资源分配的情况下，可以增加系统中处理请求操作的资源实体，进而提升系统能够同时处理的请求数，即吞吐率</strong>。下面的左图是我们采用多线程时所期待的结果。</p>
<p>但是，请你注意，通常情况下，<strong>在我们采用多线程后，如果没有良好的系统设计，实际得到的结果，其实是右图所展示的那样。我们刚开始增加线程数时，系统吞吐率会增加，但是，再进一步增加线程时，系统吞吐率就增长迟缓了，有时甚至还会出现下降的情况。</strong></p>
<p><img src="/2024/09/15/Redis03/cbd394e62219cc5a6d9ae64035e51733.jpg" alt="img"></p>
<p>线程数与系统吞吐率</p>
<p>为什么会出现这种情况呢？<strong>一个关键的瓶颈在于，系统中通常会存在被多线程同时访问的共享资源，比如一个共享的数据结构</strong>。<strong>当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个额外的机制，就会带来额外的开销。</strong></p>
<p>拿 Redis 来说，在上节课中，我提到过，Redis 有 List 的数据类型，并提供出队（LPOP）和入队（LPUSH）操作。<strong>假设 Redis 采用多线程设计，如下图所示，现在有两个线程 A 和 B，线程 A 对一个 List 做 LPUSH 操作，并对队列长度加 1。同时，线程 B 对该 List 执行 LPOP 操作，并对队列长度减 1。为了保证队列长度的正确性，Redis 需要让线程 A 和 B 的 LPUSH 和 LPOP 串行执行，这样一来，Redis 可以无误地记录它们对 List 长度的修改</strong>。否则，我们可能就会得到错误的长度结果。这就是<strong>多线程编程模式面临的共享资源的并发访问控制问题</strong>。</p>
<p><img src="/2024/09/15/Redis03/303255dcce6d0837bf7e2440df0f8e08.jpg" alt="img"></p>
<p>多线程并发访问Redis</p>
<p><strong>并发访问控制一直是多线程开发中的一个难点问题，如果没有精细的设计，比如说，只是简单地采用一个粗粒度互斥锁，就会出现不理想的结果：即使增加了线程，大部分线程也在等待获取访问共享资源的互斥锁，并行变串行，系统吞吐率并没有随着线程的增加而增加。</strong></p>
<p>而且，<strong>采用多线程开发一般会引入同步原语来保护共享资源的并发访问，这也会降低系统代码的易调试性和可维护性。</strong>为了避免这些问题，Redis 直接采用了单线程模式。</p>
<p>讲到这里，你应该已经明白了“Redis 为什么用单线程”，那么，接下来，我们就来看看，为什么单线程 Redis 能获得高性能。</p>
<h2 id="单线程-Redis-为什么那么快？"><a href="#单线程-Redis-为什么那么快？" class="headerlink" title="单线程 Redis 为什么那么快？"></a>单线程 Redis 为什么那么快？</h2><p>通常来说，<strong>单线程的处理能力要比多线程差很多，但是 Redis 却能使用单线程模型达到每秒数十万级别的处理能力，这是为什么呢？其实，这是 Redis 多方面设计选择的一个综合结果。</strong></p>
<p>一方面，Redis 的<strong>大部分操作在内存上完成</strong>，再加上它<strong>采用了高效的数据结构</strong>，例如哈希表和跳表，这是它实现高性能的一个重要原因。</p>
<p>另一方面，就是 Redis 采用了<strong>多路复用机制</strong>，<strong>使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率</strong>。接下来，我们就重点学习下多路复用机制。</p>
<p>首先，<strong>我们要弄明白网络操作的基本 IO 模型和潜在的阻塞点。毕竟，Redis 采用单线程进行 IO，如果线程被阻塞了，就无法进行多路复用了。</strong></p>
<h3 id="基本-IO-模型与阻塞点"><a href="#基本-IO-模型与阻塞点" class="headerlink" title="基本 IO 模型与阻塞点"></a>基本 IO 模型与阻塞点</h3><p>你还记得我在第一节课介绍的具有网络框架的 SimpleKV 吗？</p>
<p>以 Get 请求为例，SimpleKV 为了处理一个 Get 请求，需要监听客户端请求（bind&#x2F;listen），和客户端建立连接（accept），从 socket 中读取请求（recv），解析客户端发送的请求（parse），根据请求类型读取键值数据（get），最后给客户端返回结果，即向 socket 中写回数据（send）。</p>
<p><strong>下图显示了这一过程，其中，bind&#x2F;listen、accept、recv、parse 和 send 属于网络 IO 处理，而 get 属于键值数据操作。既然 Redis 是单线程，那么，最基本的一种实现是在一个线程中依次执行上面说的这些操作。</strong></p>
<p><img src="/2024/09/15/Redis03/e18499ab244e4428a0e60b4da6575bc9.jpg" alt="img"></p>
<p>Redis基本IO模型</p>
<p>但是，在这里的网络 IO 操作中，<strong>有潜在的阻塞点，分别是 accept() 和 recv()。当 Redis 监听到一个客户端有连接请求，但一直未能成功建立起连接时，会阻塞在 accept() 函数这里，导致其他客户端无法和 Redis 建立连接</strong>。类似的，<strong>当 Redis 通过 recv() 从一个客户端读取数据时，如果数据一直没有到达，Redis 也会一直阻塞在 recv()。</strong></p>
<p><strong>这就导致 Redis 整个线程阻塞，无法处理其他客户端请求，效率很低</strong>。不过，幸运的是，<strong>socket 网络模型本身支持非阻塞模式</strong>。</p>
<h3 id="非阻塞模式"><a href="#非阻塞模式" class="headerlink" title="非阻塞模式"></a>非阻塞模式</h3><p><strong>Socket 网络模型的非阻塞模式设置</strong>，主要体现在三个关键的函数调用上，<strong>如果想要使用 socket 非阻塞模式，就必须要了解这三个函数的调用返回类型和设置模式。接下来，我们就重点学习下它们。</strong></p>
<p>在 socket 模型中，<strong>不同操作调用后会返回不同的套接字类型。socket() 方法会返回主动套接字，然后调用 listen() 方法，将主动套接字转化为监听套接字，此时，可以监听来自客户端的连接请求。最后，调用 accept() 方法接收到达的客户端连接，并返回已连接套接字。</strong></p>
<p><img src="/2024/09/15/Redis03/1ccc62ab3eb2a63c4965027b4248f34a.jpg" alt="img"></p>
<p>Redis套接字类型与非阻塞设置</p>
<p><strong>针对监听套接字，我们可以设置非阻塞模式：当 Redis 调用 accept() 但一直未有连接请求到达时，Redis 线程可以返回处理其他操作，而不用一直等待。但是，你要注意的是，调用 accept() 时，已经存在监听套接字了。</strong></p>
<p><strong>虽然 Redis 线程可以不用继续等待，但是总得有机制继续在监听套接字上等待后续连接请求，并在有请求时通知 Redis。</strong></p>
<p>类似的，<strong>我们也可以针对已连接套接字设置非阻塞模式：Redis 调用 recv() 后，如果已连接套接字上一直没有数据到达，Redis 线程同样可以返回处理其他操作。我们也需要有机制继续监听该已连接套接字，并在有数据达到时通知 Redis。</strong></p>
<p>这样才能保证 Redis 线程，<strong>既不会像基本 IO 模型中一直在阻塞点等待，也不会导致 Redis 无法处理实际到达的连接请求或数据。</strong></p>
<p>到此，Linux 中的 <strong>IO 多路复用机制</strong>就要登场了。</p>
<h3 id="基于多路复用的高性能-I-O-模型"><a href="#基于多路复用的高性能-I-O-模型" class="headerlink" title="基于多路复用的高性能 I&#x2F;O 模型"></a>基于多路复用的高性能 I&#x2F;O 模型</h3><p>Linux 中的 IO 多路复用机制是指<strong>一个线程处理多个 IO 流</strong>，就是我们经常听到的 <strong>select&#x2F;epoll 机制</strong>。简单来说，在 Redis 只运行单线程的情况下，<strong>该机制允许内核中，同时存在多个监听套接字和已连接套接字</strong>。<strong>内核会一直监听这些套接字上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。</strong></p>
<p>下图就是基于多路复用的 Redis IO 模型。<strong>图中的多个 FD 就是刚才所说的多个套接字（FD类似于Socket的省份证）</strong>。<strong>Redis 网络框架调用 epoll 机制，让内核监听这些套接字。此时，Redis 线程不会阻塞在某一个特定的监听或已连接套接字上，也就是说，不会阻塞在某一个特定的客户端请求处理上</strong>。正因为此，Redis 可以同时和多个客户端连接并处理请求，从而提升并发性。</p>
<p><img src="/2024/09/15/Redis03/00ff790d4f6225aaeeebba34a71d8bea.jpg" alt="img"></p>
<p>基于多路复用的Redis高性能IO模型</p>
<p>为了在请求到达时能通知到 Redis 线程，select&#x2F;epoll 提供了<strong>基于事件的回调机制</strong>，即<strong>针对不同事件的发生，调用相应的处理函数</strong>。</p>
<p>那么，回调机制是怎么工作的呢？<strong>其实，select&#x2F;epoll 一旦监测到 FD 上有请求到达时，就会触发相应的事件。</strong></p>
<p><strong>这些事件会被放进一个事件队列，Redis 单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费</strong>。同时，<strong>Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升 Redis 的响应性能。</strong></p>
<p>为了方便你理解，我再以<strong>连接请求和读数据请求</strong>为例，具体解释一下。</p>
<p>这两个请求分别对应 Accept 事件和 Read 事件，Redis 分别对这两个事件注册 accept 和 get 回调函数。<strong>当 Linux 内核监听到有连接请求或读数据请求时，就会触发 Accept 事件和 Read 事件，此时，内核就会回调 Redis 相应的 accept 和 get 函数进行处理。</strong></p>
<p>这就像病人去医院瞧病。在医生实际诊断前，<strong>每个病人（等同于请求）都需要先分诊、测体温、登记等。如果这些工作都由医生来完成，医生的工作效率就会很低。所以，医院都设置了分诊台，分诊台会一直处理这些诊断前的工作（类似于 Linux 内核监听请求），然后再转交给医生做实际诊断</strong>。这样即使一个医生（相当于 Redis 单线程），效率也能提升。</p>
<p>不过，需要注意的是，<strong>即使你的应用场景中部署了不同的操作系统，多路复用机制也是适用的。因为这个机制的实现有很多种，既有基于 Linux 系统下的 select 和 epoll 实现，也有基于 FreeBSD 的 kqueue 实现，以及基于 Solaris 的 evport 实现</strong>，这样，你可以根据 Redis <strong>实际运行的操作系统，选择相应的多路复用实现</strong>。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天，我们重点学习了 Redis 线程的三个问题：</p>
<ul>
<li>“Redis 真的只有单线程吗？”</li>
<li>“为什么用单线程？”</li>
<li>“单线程为什么这么快？”</li>
</ul>
<p>现在，我们知道了，<strong>Redis 单线程是指它对网络 IO 和数据读写的操作采用了一个线程</strong>，<strong>而采用单线程的一个核心原因是避免多线程开发的并发控制问题</strong>。<strong>单线程的 Redis 也能获得高性能，跟多路复用的 IO 模型密切相关，因为这避免了 accept() 和 send()&#x2F;recv() 潜在的网络 IO 操作阻塞点。</strong></p>
<p>另外，我也剧透下，可能你也注意到了，2020 年 5 月，Redis 6.0 的稳定版发布了，<strong>Redis  6.0 中提出了多线程模型</strong>。那么，这个多线程模型和这节课所说的 IO 模型有什么关联？会引入复杂的并发控制问题吗？会给 Redis 6.0 带来多大提升？关于这些问题，我会在后面的课程中和你具体介绍。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>这节课，我给你提个小问题，在“Redis 基本 IO 模型”图中，你觉得还有哪些潜在的性能瓶颈吗？</p>
<p>Redis<strong>单线程处理IO请求性能瓶颈</strong>主要包括2个方面：</p>
<p>1、<strong>任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。</strong>耗时的操作包括以下几种：</p>
<ul>
<li><strong>a、操作bigkey</strong>：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；</li>
<li><strong>b、使用复杂度过高的命令</strong>：例如SORT&#x2F;SUNION&#x2F;ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据；</li>
<li><strong>c、大量key集中过期</strong>：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长；</li>
<li><strong>d、淘汰策略</strong>：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长；</li>
<li><strong>e、AOF刷盘开启always机制</strong>：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能；</li>
<li><strong>f、主从全量同步生成RDB</strong>：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久；</li>
</ul>
<p>2、<strong>并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。</strong></p>
<p>针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。</p>
<p>针对问题2，Redis在6.0推出了多线程，<strong>可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的</strong>。</p>
<p>介绍一下select poll epoll的区别：</p>
<ul>
<li><strong>select和poll本质上没啥区别，就是文件描述符数量的限制，select根据不同的系统，文件描述符限制为1024或者2048，poll没有数量限制。</strong>他两都是把<strong>文件描述符集合保存在用户态，每次把集合传入内核态，内核态返回ready的文件描述符。</strong></li>
<li><strong>epoll是通过epoll_create和epoll_ctl和epoll_await三个系统调用完成的，每当接入一个文件描述符，通过ctl添加到内核维护的红黑树中，通过事件机制，当数据ready后，从红黑树移动到链表，通过await获取链表中准备好数据的fd，程序去处理。</strong></li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/hnnm2l">基础篇 - 03 | 高性能IO模型：为什么单线程Redis能那么快？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>哨兵机制：主库挂了，如何不间断服务？</title>
    <url>/2024/09/16/Redis07/</url>
    <content><![CDATA[<p>上节课，我们学习了主从库集群模式。<strong>在这个模式下，如果从库发生故障了，客户端可以继续向主库或其他从库发送请求，进行相关的操作，但是如果主库发生故障了，那就直接会影响到从库的同步，因为从库没有相应的主库可以进行数据复制操作了。</strong></p>
<p>而且，如果客户端发送的都是读操作请求，那还可以由从库继续提供服务，这在纯读的业务场景下还能被接受。<strong>但是，一旦有写操作请求了，按照主从库模式下的读写分离要求，需要由主库来完成写操作。此时，也没有实例可以来服务客户端的写操作请求了，如下图所示：</strong></p>
<p><img src="/2024/09/16/Redis07/d828d7eee133cec690dc140e99e26f20.jpg" alt="img"></p>
<p>主库故障后从库无法服务写操作</p>
<p><strong>无论是写服务中断，还是从库无法进行数据同步，都是不能接受的。所以，如果主库挂了，我们就需要运行一个新主库，比如说把一个从库切换为主库，把它当成主库。这就涉及到三个问题：</strong></p>
<ul>
<li>主库真的挂了吗？</li>
<li>该选择哪个从库作为主库？</li>
<li>怎么把新主库的相关信息通知给从库和客户端呢？</li>
</ul>
<p>这就要提到哨兵机制了。<strong>在 Redis 主从集群中，哨兵机制是实现主从库自动切换的关键机制，它有效地解决了主从复制模式下故障转移的这三个问题。</strong></p>
<p>接下来，我们就一起学习下哨兵机制。</p>
<h2 id="哨兵机制的基本流程"><a href="#哨兵机制的基本流程" class="headerlink" title="哨兵机制的基本流程"></a>哨兵机制的基本流程</h2><p><strong>哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知。</strong></p>
<p>我们先看监控。<strong>监控是指哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始自动切换主库的流程。</strong></p>
<p>这个流程首先是执行哨兵的第二个任务，选主。<strong>主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。这一步完成后，现在的集群里就有了新主库。</strong></p>
<p>然后，哨兵会执行最后一个任务：通知。<strong>在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。</strong></p>
<p>我画了一张图片，展示了这三个任务以及它们各自的目标。</p>
<p><img src="/2024/09/16/Redis07/efcfa517d0f09d057be7da32a84cf2a1.jpg" alt="img"></p>
<p>哨兵机制的三项任务与目标</p>
<p>在这三个任务中，<strong>通知任务相对来说比较简单，哨兵只需要把新主库信息发给从库和客户端，让它们和新主库建立连接就行，并不涉及决策的逻辑。但是，在监控和选主这两个任务中，哨兵需要做出两个决策：</strong></p>
<ul>
<li>在监控任务中，哨兵需要判断主库是否处于下线状态；</li>
<li>在选主任务中，哨兵也要决定选择哪个从库实例作为主库。</li>
</ul>
<p>接下来，我们就先说说如何判断主库的下线状态。</p>
<p>你首先要知道的是，<strong>哨兵对主库的下线判断有“主观下线”和“客观下线”两种。那么，为什么会存在两种判断呢？它们的区别和联系是什么呢？</strong></p>
<h2 id="主观下线和客观下线"><a href="#主观下线和客观下线" class="headerlink" title="主观下线和客观下线"></a>主观下线和客观下线</h2><p>我先解释下什么是“主观下线”。</p>
<p><strong>哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态</strong>。如果哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记为“主观下线”。</p>
<p><strong>如果检测的是从库，那么，哨兵简单地把它标记为“主观下线”就行了，因为从库的下线影响一般不太大，集群的对外服务不会间断。</strong></p>
<p><strong>但是，如果检测的是主库，那么，哨兵还不能简单地把它标记为“主观下线”，开启主从切换。因为很有可能存在这么一个情况：那就是哨兵误判了，其实主库并没有故障。</strong>可是，一旦启动了主从切换，<strong>后续的选主和通知操作都会带来额外的计算和通信开销。</strong></p>
<p>为了避免这些不必要的开销，我们要特别注意误判的情况。</p>
<p>首先，我们要知道啥叫误判。很简单，就是主库实际并没有下线，但是哨兵误以为它下线了。<strong>误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况下。</strong></p>
<p>一旦哨兵判断主库下线了，<strong>就会开始选择新主库，并让从库和新主库进行数据同步，这个过程本身就会有开销，例如，哨兵要花时间选出新主库，从库也需要花时间和新主库同步。而在误判的情况下，主库本身根本就不需要进行切换的，所以这个过程的开销是没有价值的。正因为这样，我们需要判断是否有误判，以及减少误判。</strong></p>
<p>那怎么减少误判呢？在日常生活中，当我们要对一些重要的事情做判断的时候，经常会和家人或朋友一起商量一下，然后再做决定。</p>
<p>哨兵机制也是类似的，它<strong>通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群</strong>。<strong>引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。</strong></p>
<p>这节课，你只需要先理解哨兵集群在减少误判方面的作用，就行了。至于具体的运行机制，下节课我们再重点学习。</p>
<p>在判断主库是否下线时，<strong>不能由一个哨兵说了算，只有大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线”，这个叫法也是表明主库下线成为一个客观事实了。这个判断原则就是：少数服从多数。同时，这会进一步触发哨兵开始主从切换流程。</strong></p>
<p>为了方便你理解，我再画一张图展示一下这里的逻辑。</p>
<p>如下图所示，Redis 主从集群有一个主库、三个从库，还有三个哨兵实例。在图片的左边，哨兵 2 判断主库为“主观下线”，但哨兵 1 和 3 却判定主库是上线状态，此时，主库仍然被判断为处于上线状态。<strong>在图片的右边，哨兵 1 和 2 都判断主库为“主观下线”，此时，即使哨兵 3 仍然判断主库为上线状态，主库也被标记为“客观下线”了。</strong></p>
<p><img src="/2024/09/16/Redis07/1945703abf16ee14e2f7559873e4e60d.jpg" alt="img"></p>
<p>客观下线的判断</p>
<p>简单来说，“客观下线”的标准就是，<strong>当有 N 个哨兵实例时，最好要有 N&#x2F;2 + 1 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以减少误判的概率，也能避免误判带来的无谓的主从库切换</strong>。（当然，有多少个实例做出“主观下线”的判断才可以，可以由 Redis 管理员自行设定）。</p>
<p>好了，到这里，<strong>你可以看到，借助于多个哨兵实例的共同判断机制，我们就可以更准确地判断出主库是否处于下线状态。如果主库的确下线了，哨兵就要开始下一个决策过程了，即从许多从库中，选出一个从库来做新主库。</strong></p>
<h2 id="如何选定新主库？"><a href="#如何选定新主库？" class="headerlink" title="如何选定新主库？"></a>如何选定新主库？</h2><p>一般来说，<strong>我把哨兵选择新主库的过程称为“筛选 + 打分”。简单来说，我们在多个从库中，先按照一定的筛选条件，把不符合条件的从库去掉。然后，我们再按照一定的规则，给剩下的从库逐个打分，将得分最高的从库选为新主库</strong>，如下图所示：</p>
<p><img src="/2024/09/16/Redis07/f2e9b8830db46d959daa6a39fbf4a14c.jpg" alt="img"></p>
<p>新主库的选择过程</p>
<p>在刚刚的这段话里，需要注意的是两个“一定”，现在，我们要考虑这里的“一定”具体是指什么。</p>
<p>首先来看筛选的条件。</p>
<p><strong>一般情况下，我们肯定要先保证所选的从库仍然在线运行。不过，在选主时从库正常在线，这只能表示从库的现状良好，并不代表它就是最适合做主库的。</strong></p>
<p>设想一下，如果在选主时，一个从库正常运行，我们把它选为新主库开始使用了。可是，很快它的网络出了故障，此时，我们就得重新选主了。这显然不是我们期望的结果。</p>
<p>所以，在选主时，<strong>除了要检查从库的当前在线状态，还要判断它之前的网络连接状态</strong>。如果从库总是和主库断连，<strong>而且断连次数超出了一定的阈值，我们就有理由相信，这个从库的网络状况并不是太好，就可以把这个从库筛掉了</strong>。</p>
<p>具体怎么判断呢？你使用配置项 down-after-milliseconds * 10。<strong>其中，down-after-milliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。</strong></p>
<p>好了，这样我们就过滤掉了不适合做主库的从库，完成了筛选工作。</p>
<p>接下来就要给剩余的从库打分了。我们可以分别按照三个规则依次进行三轮打分，这三个规则分别是<strong>从库优先级、从库复制进度以及从库 ID 号</strong>。<strong>只要在某一轮中，有从库得分最高，那么它就是主库了，选主过程到此结束。如果没有出现得分最高的从库，那么就继续进行下一轮。</strong></p>
<p><strong>第一轮：优先级最高的从库得分高。</strong></p>
<p>用户可以通过 slave-priority 配置项，<strong>给不同的从库设置不同优先级。比如，你有两个从库，它们的内存大小不一样，你可以手动给内存大的实例设置一个高优先级。在选主时，哨兵会给优先级高的从库打高分，如果有一个从库优先级最高，那么它就是新主库了。</strong>如果从库的优先级都一样，那么哨兵开始第二轮打分。</p>
<p><strong>第二轮：和旧主库同步程度最接近的从库得分高。</strong></p>
<p>这个规则的依据是，<strong>如果选择和旧主库同步最接近的那个从库作为主库，那么，这个新主库上就有最新的数据。</strong></p>
<p>如何判断从库和旧主库间的同步进度呢？</p>
<p>上节课我向你介绍过，主从库同步时有个命令传播的过程。在这个过程中，<strong>主库会用 master_repl_offset 记录当前的最新写操作在 repl_backlog_buffer 中的位置，而从库会用 slave_repl_offset 这个值记录当前的复制进度。</strong></p>
<p>此时，我们想要找的从库，<strong>它的 slave_repl_offset 需要最接近 master_repl_offset。如果在所有从库中，有从库的 slave_repl_offset 最接近 master_repl_offset，那么它的得分就最高，可以作为新主库。</strong></p>
<p>就像下图所示，旧主库的 master_repl_offset 是 1000，从库 1、2 和 3 的 slave_repl_offset 分别是 950、990 和 900，那么，从库 2 就应该被选为新主库。</p>
<p><img src="/2024/09/16/Redis07/626yy88853a2d15b5196b922367140df.jpg" alt="img"></p>
<p>基于复制进度的新主库选主原则</p>
<p><strong>当然，如果有两个从库的 slave_repl_offset 值大小是一样的（例如，从库 1 和从库 2 的 slave_repl_offset 值都是 990），我们就需要给它们进行第三轮打分了。</strong></p>
<p><strong>第三轮：ID 号小的从库得分高。</strong></p>
<p>每个实例都会有一个 ID，这个 ID 就类似于这里的从库的编号。目前，Redis 在选主库时，有一个默认的规定：<strong>在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库</strong>。</p>
<p><strong>到这里，新主库就被选出来了，“选主”这个过程就完成了。</strong></p>
<p><strong>我们再回顾下这个流程。首先，哨兵会按照在线状态、网络状态，筛选过滤掉一部分不符合要求的从库，然后，依次按照优先级、复制进度、ID 号大小再对剩余的从库进行打分，只要有得分最高的从库出现，就把它选为新主库。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课，我们一起学习了哨兵机制，它是实现 Redis 不间断服务的重要保证。<strong>具体来说，主从集群的数据同步，是数据可靠的基础保证；而在主库发生故障时，自动的主从切换是服务不间断的关键支撑。</strong></p>
<p>Redis 的哨兵机制自动完成了以下三大功能，从而实现了主从库的自动切换，可以降低 Redis 集群的运维开销：</p>
<ul>
<li>监控主库运行状态，并判断主库是否客观下线；</li>
<li>在主库客观下线后，选取新主库；</li>
<li>选出新主库后，通知从库和客户端。</li>
</ul>
<p><strong>为了降低误判率，在实际应用时，哨兵机制通常采用多实例的方式进行部署，多个哨兵实例通过“少数服从多数”的原则，来判断主库是否客观下线。</strong>一般来说，我们可以部署三个哨兵，如果有两个哨兵认定主库“主观下线”，就可以开始切换过程。<strong>当然，如果你希望进一步提升判断准确率，也可以再适当增加哨兵个数，比如说使用五个哨兵。</strong></p>
<p>但是，使用多个哨兵实例来降低误判率，其实相当于组成了一个哨兵集群，我们会因此面临着一些新的挑战，例如：</p>
<ul>
<li><strong>哨兵集群中有实例挂了，怎么办，会影响主库状态判断和选主吗？</strong></li>
<li><strong>哨兵集群多数实例达成共识，判断出主库“客观下线”后，由哪个实例来执行主从切换呢？</strong></li>
</ul>
<p>要搞懂这些问题，就不得不提哨兵集群了，下节课，我们来具体聊聊哨兵集群的机制和问题。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提个小问题。这节课，我提到，通过哨兵机制，可以实现主从库的自动切换，这是实现服务不间断的关键支撑，同时，我也提到了主从库切换是需要一定时间的。<strong>所以，请你考虑下，在这个切换过程中，客户端能否正常地进行请求操作呢？如果想要应用程序不感知服务的中断，还需要哨兵或需要客户端再做些什么吗？</strong></p>
<p>哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作？</p>
<ul>
<li><strong>如果客户端使用了读写分离，那么读请求可以在从库上正常执行，不会受到影响。但是由于此时主库已经挂了，而且哨兵还没有选出新的主库，所以在这期间写请求会失败，失败持续的时间 &#x3D; 哨兵切换主从的时间 + 客户端感知到新主库 的时间。</strong></li>
<li>如果不想让业务感知到异常，<strong>客户端只能把写失败的请求先缓存起来或写入消息队列中间件中，等哨兵切换完主从后，再把这些写请求发给新的主库，但这种场景只适合对写入请求返回值不敏感的业务，而且还需要业务层做适配，另外主从切换时间过长，也会导致客户端或消息队列中间件缓存写请求过多，切换完成之后重放这些请求的时间变长。</strong></li>
<li>哨兵检测主库多久没有响应就提升从库为新的主库，这个时间是可以配置的（down-after-milliseconds参数）。<strong>配置的时间越短，哨兵越敏感，哨兵集群认为主库在短时间内连不上就会发起主从切换，这种配置很可能因为网络拥塞但主库正常而发生不必要的切换，当然，当主库真正故障时，因为切换得及时，对业务的影响最小。如果配置的时间比较长，哨兵越保守，这种情况可以减少哨兵误判的概率，但是主库故障发生时，业务写失败的时间也会比较久，缓存写请求数据量越多。</strong></li>
</ul>
<p>应用程序不感知服务的中断，还需要哨兵和客户端做些什么？<strong>当哨兵完成主从切换后，客户端需要及时感知到主库发生了变更，然后把缓存的写请求写入到新库中，保证后续写请求不会再受到影响，具体做法如下：</strong></p>
<p>哨兵提升一个从库为新主库后，<strong>哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。</strong></p>
<p>如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，<strong>客户端也需要支持主动去获取最新主从的地址进行访问。</strong></p>
<p>所以，<strong>客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。</strong></p>
<p>一般Redis的SDK都提供了<strong>通过哨兵拿到实例地址，再访问实例的方式，我们直接使用即可，不需要自己实现这些逻辑</strong>。当然，<strong>对于只有主从实例的情况，客户端需要和哨兵配合使用，而在分片集群模式下，这些逻辑都可以做在proxy层，这样客户端也不需要关心这些逻辑了，Codis就是这么做的。</strong></p>
<p>另外再简单回答下哨兵相关的问题：</p>
<p>1、哨兵集群中有实例挂了，怎么办，会影响主库状态判断和选主吗？</p>
<p>这个属于分布式系统领域的问题了，指的是在分布式系统中，如果存在故障节点，整个集群是否还可以提供服务？而且提供的服务是正确的？</p>
<p>这是一个分布式系统容错问题，<strong>这方面最著名的就是分布式领域中的“拜占庭将军”问题了，“拜占庭将军问题”不仅解决了容错问题，还可以解决错误节点的问题，虽然比较复杂，但还是值得研究的，有兴趣的同学可以去了解下。</strong></p>
<p>简单说结论：<strong>存在故障节点时，只要集群中大多数节点状态正常，集群依旧可以对外提供服务</strong>。具体推导过程细节很多，大家去查前面的资料了解就好。</p>
<p>2、哨兵集群多数实例达成共识，判断出主库“客观下线”后，由哪个实例来执行主从切换呢？</p>
<p><strong>哨兵集群判断出主库“主观下线”后，会选出一个“哨兵领导者”，之后整个过程由它来完成主从切换。</strong></p>
<p>但是如何选出“哨兵领导者”？<strong>这个问题也是一个分布式系统中的问题，就是我们经常听说的共识算法，指的是集群中多个节点如何就一个问题达成共识。共识算法有很多种，例如Paxos、Raft，这里哨兵集群采用的类似于Raft的共识算法。</strong></p>
<p>简单来说就是<strong>每个哨兵设置一个随机超时时间，超时后每个哨兵会请求其他哨兵为自己投票，其他哨兵节点对收到的第一个请求进行投票确认，一轮投票下来后，首先达到多数选票的哨兵节点成为“哨兵领导者”，如果没有达到多数选票的哨兵节点，那么会重新选举，直到能够成功选出“哨兵领导者”。</strong></p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/exa2g1">基础篇 - 07 | 哨兵机制：主库挂了，如何不间断服务？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>AOF日志：宕机了，Redis如何避免数据丢失？</title>
    <url>/2024/09/15/Redis04/</url>
    <content><![CDATA[<p>如果有人问你：“你会把 Redis 用在什么业务场景下？”</p>
<p><strong>我想你大概率会说：“我会把它当作缓存使用，因为它把后端数据库中的数据存储在内存中，然后直接从内存中读取数据，响应速度会非常快。”</strong>没错，这确实是 Redis 的一个普遍使用场景，但是，这里也有一个绝对不能忽略的问题：<strong>一旦服务器宕机，内存中的数据将全部丢失。</strong></p>
<p>我们很容易想到的一个解决方案是，<strong>从后端数据库恢复这些数据，但这种方式存在两个问题：一是，需要频繁访问数据库，会给数据库带来巨大的压力；二是，这些数据是从慢速数据库中读取出来的，性能肯定比不上从 Redis 中读取，导致使用这些数据的应用程序响应变慢。所以，对 Redis 来说，实现数据的持久化，避免从后端数据库中进行恢复，是至关重要的。</strong></p>
<p>目前，<strong>Redis 的持久化主要有两大机制，即 AOF（Append Only File）日志和 RDB 快照</strong>。在接下来的两节课里，我们就分别学习一下吧。这节课，我们先重点学习下 AOF 日志。</p>
<h2 id="AOF-日志是如何实现的？"><a href="#AOF-日志是如何实现的？" class="headerlink" title="AOF 日志是如何实现的？"></a>AOF 日志是如何实现的？</h2><p>说到日志，<strong>我们比较熟悉的是数据库的写前日志（Write Ahead Log, WAL），也就是说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复。不过，AOF 日志正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入内存，然后才记录日志，如下图所示：</strong></p>
<p><img src="/2024/09/15/Redis04/407f2686083afc37351cfd9107319a1f.jpg" alt="img"></p>
<p>Redis AOF操作过程</p>
<p>那 AOF 为什么要先执行命令再记日志呢？要回答这个问题，我们要先知道 AOF 里记录了什么内容。</p>
<p><strong>传统数据库的日志，例如 redo log（重做日志），记录的是修改后的数据，而 AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。</strong></p>
<p>我们以 Redis 收到“set testkey testvalue”命令后记录的日志为例，看看 AOF 日志的内容。其中，“*3”表示当前命令有三个部分，每部分都是由<code>“$+数字”</code>开头，后面紧跟着具体的命令、键或值。这里，<strong>“数字”表示这部分中的命令、键或值一共有多少字节</strong>。例如，<code>“$3 set”</code>表示这部分有 3 个字节，也就是“set”命令。</p>
<p><img src="/2024/09/15/Redis04/4d120bee623642e75fdf1c0700623a9f.jpg" alt="img"></p>
<p>Redis AOF日志内容</p>
<p><strong>但是，为了避免额外的检查开销，Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查</strong>。<strong>所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。</strong></p>
<p><strong>而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。所以，Redis 使用写后日志这一方式的一大好处是，可以避免出现记录错误命令的情况。</strong></p>
<p>除此之外，AOF 还有一个好处：它是在命令执行后才记录日志，所以<strong>不会阻塞当前的写操作</strong>。</p>
<p>不过，AOF 也有两个潜在的风险。</p>
<ul>
<li><strong>首先，如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。</strong>如果此时 Redis 是用作缓存，还可以从后端数据库重新读入数据进行恢复，但是，如果 Redis 是直接用作数据库的话，此时，因为命令没有记入日志，所以就无法用日志进行恢复了。</li>
<li><strong>其次，AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。</strong>这是因为，<strong>AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。</strong></li>
</ul>
<p>仔细分析的话，你就会发现，<strong>这两个风险都是和 AOF 写回磁盘的时机相关的。这也就意味着，如果我们能够控制一个写命令执行完后 AOF 日志写回磁盘的时机，这两个风险就解除了。</strong></p>
<h2 id="三种写回策略"><a href="#三种写回策略" class="headerlink" title="三种写回策略"></a>三种写回策略</h2><p>其实，对于这个问题，AOF 机制给我们提供了三个选择，也就是 AOF 配置项 appendfsync 的三个可选值。</p>
<ul>
<li><strong>Always</strong>，同步写回：每个写命令执行完，<strong>立马同步地将日志写回磁盘</strong>；</li>
<li><strong>Everysec</strong>，每秒写回：每个写命令执行完，<strong>只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘</strong>；</li>
<li><strong>No</strong>，操作系统控制的写回：每个写命令执行完，<strong>只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘</strong>。</li>
</ul>
<p>针对避免主线程阻塞和减少数据丢失问题，这三种写回策略都无法做到两全其美。我们来分析下其中的原因。</p>
<ul>
<li>“同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，<strong>不可避免地会影响主线程性能；</strong></li>
<li>虽然“操作系统控制的写回”在写完缓冲区后，就可以继续执行后续的命令，<strong>但是落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了；</strong></li>
<li>“每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，<strong>虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。</strong>所以，这只能算是，<strong>在避免影响主线程性能和避免数据丢失两者间取了个折中。</strong></li>
</ul>
<p>我把这三种策略的写回时机，以及优缺点汇总在了一张表格里，以方便你随时查看。</p>
<p><img src="/2024/09/15/Redis04/72f547f18dbac788c7d11yy167d7ebf8.jpg" alt="img"></p>
<p>到这里，我们就可以根据系统对高性能和高可靠性的要求，来选择使用哪种写回策略了。<strong>总结一下就是：想要获得高性能，就选择 No 策略；如果想要得到高可靠性保证，就选择 Always 策略；如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择 Everysec 策略。</strong></p>
<p>但是，按照系统的性能需求选定了写回策略，并不是“高枕无忧”了。<strong>毕竟，AOF 是以文件的形式在记录接收到的所有写命令。随着接收的写命令越来越多，AOF 文件会越来越大。这也就意味着，我们一定要小心 AOF 文件过大带来的性能问题。</strong></p>
<p>这里的“性能问题”，主要在于以下三个方面：</p>
<ul>
<li><strong>一是，文件系统本身对文件大小有限制，无法保存过大的文件；</strong></li>
<li><strong>二是，如果文件太大，之后再往里面追加命令记录的话，效率也会变低；</strong></li>
<li><strong>三是，如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用。</strong></li>
</ul>
<p>所以，我们就要采取一定的控制手段，这个时候，<strong>AOF 重写机制</strong>就登场了。</p>
<h2 id="日志文件太大了怎么办？"><a href="#日志文件太大了怎么办？" class="headerlink" title="日志文件太大了怎么办？"></a>日志文件太大了怎么办？</h2><p>简单来说，<strong>AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入。比如说，当读取了键值对“testkey”: “testvalue”之后，重写机制会记录 set testkey testvalue 这条命令。这样，当需要恢复时，可以重新执行该命令，实现“testkey”: “testvalue”的写入。</strong></p>
<p>为什么重写机制可以把日志文件变小呢? 实际上，重写机制具有“多变一”功能。<strong>所谓的“多变一”，也就是说，旧日志文件中的多条命令，在重写后的新日志中变成了一条命令。</strong></p>
<p>我们知道，AOF 文件是以追加的方式，逐一记录接收到的写命令的。<strong>当一个键值对被多条写命令反复修改时，AOF 文件会记录相应的多条命令。但是，在重写的时候，是根据这个键值对当前的最新状态，为它生成对应的写入命令。这样一来，一个键值对在重写日志中只用一条命令就行了，而且，在日志恢复时，只用执行这条命令，就可以直接完成这个键值对的写入了。</strong></p>
<p>下面这张图就是一个例子：</p>
<p><img src="/2024/09/15/Redis04/6528c699fdcf40b404af57040bb8d208.jpg" alt="img"></p>
<p>AOF重写减少日志大小</p>
<p>当我们对一个列表先后做了 6 次修改操作后，列表的最后状态是[“D”, “C”, “N”]，此时，<strong>只用 LPUSH u:list “N”, “C”, “D”这一条命令就能实现该数据的恢复，这就节省了五条命令的空间。对于被修改过成百上千次的键值对来说，重写能节省的空间当然就更大了。</strong></p>
<p>不过，虽然 AOF 重写后，日志文件会缩小，<strong>但是，要把整个数据库的最新数据的操作日志都写回磁盘，仍然是一个非常耗时的过程。这时，我们就要继续关注另一个问题了：重写会不会阻塞主线程？</strong></p>
<h2 id="AOF-重写会阻塞吗"><a href="#AOF-重写会阻塞吗" class="headerlink" title="AOF 重写会阻塞吗?"></a>AOF 重写会阻塞吗?</h2><p>和 AOF 日志由主线程写回不同，<strong>重写过程是由后台子进程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。</strong></p>
<p>我把重写的过程总结为“<strong>一个拷贝，两处日志</strong>”。</p>
<ul>
<li>“一个拷贝”就是指，<strong>每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。</strong>然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，<strong>逐一把拷贝的数据写成操作，记入重写日志。</strong></li>
</ul>
<p>“两处日志”又是什么呢？</p>
<ul>
<li><strong>因为主线程未阻塞，仍然可以处理新来的操作。</strong>此时，如果有写操作，<strong>第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。</strong></li>
<li>而第二处日志，就是指新的 AOF 重写日志。<strong>这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。</strong>等到拷贝数据的所有操作记录重写完成后，<strong>重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。</strong></li>
</ul>
<p><img src="/2024/09/15/Redis04/6b054eb1aed0734bd81ddab9a31d0be8.jpg" alt="img"></p>
<p>AOF非阻塞的重写过程</p>
<p>总结来说，<strong>每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。</strong>而且，因为 Redis <strong>采用额外的线程进行数据重写</strong>，<strong>所以，这个过程并不会阻塞主线程。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课，我向你介绍了 Redis 用于避免数据丢失的 AOF 方法。<strong>这个方法通过逐一记录操作命令，在恢复时再逐一执行命令的方式，保证了数据的可靠性。</strong></p>
<p>这个方法看似“简单”，但也是充分考虑了对 Redis 性能的影响。总结来说，它提供了 AOF 日志的三种写回策略，<strong>分别是 Always、Everysec 和 No，这三种策略在可靠性上是从高到低，而在性能上则是从低到高。</strong></p>
<p>此外，为了避免日志文件过大，<strong>Redis 还提供了 AOF 重写机制，直接根据数据库里数据的最新状态，生成这些数据的插入命令，作为新日志。这个过程通过后台线程完成，避免了对主线程的阻塞。</strong></p>
<p>其中，<strong>三种写回策略体现了系统设计中的一个重要原则 ，即 trade-off，或者称为“取舍”，指的就是在性能和可靠性保证之间做取舍。我认为，这是做系统设计和开发的一个关键哲学，我也非常希望，你能充分地理解这个原则，并在日常开发中加以应用。（其实在MySQL中就有很充分的体现了）</strong></p>
<p>不过，<strong>你可能也注意到了，落盘时机和重写机制都是在“记日志”这一过程中发挥作用的</strong>。例如，落盘时机的选择可以避免记日志时阻塞主线程，重写可以避免日志文件过大。<strong>但是，在“用日志”的过程中，也就是使用 AOF 进行故障恢复时，我们仍然需要把所有的操作记录都运行一遍。再加上 Redis 的单线程设计，这些命令操作只能一条一条按顺序执行，这个“重放”的过程就会很慢了。</strong></p>
<p>那么，有没有既能<strong>避免数据丢失，又能更快地恢复</strong>的方法呢？当然有，那就是 RDB 快照了。下节课，我们就一起学习一下，敬请期待。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>这节课，我给你提两个小问题：</p>
<ul>
<li>AOF 日志重写的时候，是由 bgrewriteaof 子进程来完成的，不用主线程参与，我们今天说的非阻塞也是指子进程的执行不阻塞主线程。但是，你觉得，这个重写过程有没有其他潜在的阻塞风险呢？如果有的话，会在哪里阻塞？</li>
<li>AOF 重写也有一个重写日志，为什么它不共享使用 AOF 本身的日志呢？</li>
</ul>
<p>问题1，Redis采用fork子进程重写AOF文件时，潜在的阻塞风险包括：fork子进程 和 AOF重写过程中父进程产生写入的场景，下面依次介绍。</p>
<ul>
<li>a、fork子进程，fork这个瞬间一定是会阻塞主线程的（注意，fork时并不会一次性拷贝所有内存数据给子进程，老师文章写的是拷贝所有内存数据给子进程，我个人认为是有歧义的），<strong>fork采用操作系统提供的写实复制(Copy On Write)机制，就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞问题，但fork子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量CPU资源，拷贝完成之前整个进程是会阻塞的</strong>，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork阻塞时间越久。<strong>拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。</strong>那什么时候父子进程才会真正内存分离呢？“写实复制”顾名思义，就是在写发生时，才真正拷贝内存真正的数据，这个过程中，父进程也可能会产生阻塞的风险，就是下面介绍的场景。</li>
<li>b、<strong>fork出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行AOF重写，把内存中的所有数据写入到AOF文件中。</strong>但是此时父进程依旧是会有流量写入的，如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，<strong>这样逐渐地，父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。</strong>因为内存分配是以页为单位进行分配的，默认4k，<strong>如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险。</strong>另外，如果操作系统开启了内存大页机制(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间。</li>
</ul>
<p>问题2，AOF重写不复用AOF本身的日志，<strong>一个原因是父子进程写同一个文件必然会产生竞争问题，控制竞争就意味着会影响父进程的性能。二是如果AOF重写过程中失败了，那么原本的AOF文件相当于被污染了，无法做恢复使用。</strong>所以Redis AOF重写一个新文件，<strong>重写失败的话，直接删除这个文件就好了，不会对原先的AOF文件产生影响。等重写完成之后，直接替换旧文件即可。</strong></p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/km48w5">基础篇 - 04 | AOF日志：宕机了，Redis如何避免数据丢失？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>数据同步：主从库如何实现数据一致？</title>
    <url>/2024/09/15/Redis06/</url>
    <content><![CDATA[<p>前两节课，我们学习了 AOF 和 RDB，<strong>如果 Redis 发生了宕机，它们可以分别通过回放日志和重新读入 RDB 文件的方式恢复数据，从而保证尽量少丢失数据，提升可靠性。</strong></p>
<p>不过，即使用了这两种方法，也依然存在服务不可用的问题。<strong>比如说，我们在实际使用时只运行了一个 Redis 实例，那么，如果这个实例宕机了，它在恢复期间，是无法服务新来的数据存取请求的。</strong></p>
<p>那我们总说的 Redis 具有高可靠性，又是什么意思呢？其实，这里有两层含义：一是<strong>数据尽量少丢失</strong>，二是<strong>服务尽量少中断</strong>。AOF 和 RDB 保证了前者，而对于后者，Redis 的做法就是<strong>增加副本冗余量</strong>，将一份数据同时保存在多个实例上。<strong>即使有一个实例出现了故障，需要过一段时间才能恢复，其他实例也可以对外提供服务，不会影响业务使用。</strong></p>
<p>多实例保存同一份数据，听起来好像很不错，但是，我们必须要考虑一个问题：<strong>这么多副本，它们之间的数据如何保持一致呢？数据读写操作可以发给所有的实例吗？</strong></p>
<p><strong>实际上，Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。</strong></p>
<ul>
<li><strong>读操作</strong>：主库、从库都可以接收；</li>
<li><strong>写操作</strong>：首先到主库执行，然后，主库将写操作同步给从库。</li>
</ul>
<p><img src="/2024/09/15/Redis06/809d6707404731f7e493b832aa573a2f.jpg" alt="img"></p>
<p>Redis主从库和读写分离</p>
<p>那么，为什么要采用读写分离的方式呢？</p>
<p><strong>你可以设想一下，如果在上图中，不管是主库还是从库，都能接收客户端的写操作，那么，一个直接的问题就是：如果客户端对同一个数据（例如 k1）前后修改了三次，每一次的修改请求都发送到不同的实例上，在不同的实例上执行，那么，这个数据在这三个实例上的副本就不一致了（分别是 v1、v2 和 v3）。在读取这个数据的时候，就可能读取到旧的值。</strong></p>
<p>如果我们非要保持这个数据在三个实例上一致，<strong>就要涉及到加锁、实例间协商是否完成修改等一系列操作，但这会带来巨额的开销，当然是不太能接受的。</strong></p>
<p>而主从库模式一旦采用了读写分离，<strong>所有数据的修改只会在主库上进行，不用协调三个实例。主库有了最新的数据后，会同步给从库，这样，主从库的数据就是一致的。</strong></p>
<p><strong>那么，主从库同步是如何完成的呢？主库数据是一次性传给从库，还是分批同步？要是主从库间的网络断连了，数据还能保持一致吗？这节课，我就和你聊聊主从库同步的原理，以及应对网络断连风险的方案。</strong></p>
<p>好了，<strong>我们先来看看主从库间的第一次同步是如何进行的，这也是 Redis 实例建立主从库模式后的规定动作。</strong></p>
<h2 id="主从库间如何进行第一次同步？"><a href="#主从库间如何进行第一次同步？" class="headerlink" title="主从库间如何进行第一次同步？"></a>主从库间如何进行第一次同步？</h2><p>当我们启动多个 Redis 实例的时候，<strong>它们相互之间就可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。</strong></p>
<p>例如，现在有实例 1（ip：172.16.19.3）和实例 2（ip：172.16.19.5），我们在实例 2 上执行以下这个命令后，<strong>实例 2 就变成了实例 1 的从库，并从实例 1 上复制数据：</strong></p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line">replicaof  172.16.19.3  6379</span><br></pre></td></tr></table></figure>

<p>接下来，我们就要学习主从库间数据第一次同步的三个阶段了。你可以先看一下下面这张图，有个整体感知，接下来我再具体介绍。</p>
<p><img src="/2024/09/15/Redis06/63d18fd41efc9635e7e9105ce1c33da1.jpg" alt="img"></p>
<p>主从库第一次同步的流程</p>
<p>第一阶段是主从库间建立连接、协商同步的过程，主要是为<strong>全量复制</strong>做准备。在这一步，<strong>从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了</strong>。</p>
<p>具体来说，<strong>从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制</strong>。psync 命令包含了<strong>主库的 runID</strong> 和<strong>复制进度 offset</strong> 两个参数。</p>
<ul>
<li>runID，<strong>是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例</strong>。当从库和主库第一次复制时，<strong>因为不知道主库的 runID，所以将 runID 设为“？”。</strong></li>
<li>offset，<strong>此时设为 -1，表示第一次复制。</strong></li>
</ul>
<p>主库收到 psync 命令后，<strong>会用 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。</strong></p>
<p>这里有个地方需要注意，<strong>FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说，主库会把当前所有的数据都复制给从库</strong>。</p>
<p>在第二阶段，<strong>主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载</strong>。这个过程依赖于内存快照生成的 RDB 文件。</p>
<p>具体来说，<strong>主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件</strong>。<strong>这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空。（这个我之前好像在配置Redis集群的时候遇见过）</strong></p>
<p><strong>在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis 的服务就被中断了。</strong>但是，<strong>这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作。</strong></p>
<p>最后，也就是第三个阶段，<strong>主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作</strong>。这样一来，主从库就实现同步了。</p>
<h2 id="主从级联模式分担全量复制时的主库压力"><a href="#主从级联模式分担全量复制时的主库压力" class="headerlink" title="主从级联模式分担全量复制时的主库压力"></a>主从级联模式分担全量复制时的主库压力</h2><p>通过分析主从库间第一次数据同步的过程，你可以看到，<strong>一次全量复制中，对于主库来说，需要完成两个耗时的操作：生成 RDB 文件和传输 RDB 文件</strong>。</p>
<p>如果从库数量很多，而且都要和主库进行全量复制的话，<strong>就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量同步</strong>。fork 这个操作<strong>会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢</strong>。此外，<strong>传输 RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力</strong>。那么，有没有好的解决方法可以分担主库压力呢？</p>
<p><strong>其实是有的，这就是“主 - 从 - 从”模式。</strong></p>
<p>在刚才介绍的主从库模式中，所有的从库都是和主库连接，所有的全量复制也都是和主库进行的。现在，我们可以<strong>通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上</strong>。</p>
<p>简单来说，<strong>我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">replicaof  所选从库的IP 6379</span><br></pre></td></tr></table></figure>

<p>这样一来，这些从库就会知道，在进行同步时，不用再和主库进行交互了，只要和级联的从库进行写操作同步就行了，<strong>这就可以减轻主库上的压力</strong>，如下图所示：</p>
<p><img src="/2024/09/15/Redis06/403c2ab725dca8d44439f8994959af45.jpg" alt="img"></p>
<p>级联的“主-从-从”模式</p>
<p>好了，到这里，<strong>我们了解了主从库间通过全量复制实现数据同步的过程，以及通过“主 - 从 - 从”模式分担主库压力的方式。那么，一旦主从库完成了全量复制，它们之间就会一直维护一个网络连接</strong>，主库会通过这个连接将后续陆续收到的命令操作再同步给从库，这个过程也称为<strong>基于长连接的命令传播</strong>，<strong>可以避免频繁建立连接的开销。</strong></p>
<p>听上去好像很简单，但不可忽视的是，这个过程中存在着风险点，最常见的就是<strong>网络断连或阻塞</strong>。<strong>如果网络断连，主从库之间就无法进行命令传播了，从库的数据自然也就没办法和主库保持一致了，客户端就可能从从库读到旧数据。</strong></p>
<p>接下来，我们就来聊聊网络断连后的解决办法。</p>
<h2 id="主从库间网络断了怎么办？"><a href="#主从库间网络断了怎么办？" class="headerlink" title="主从库间网络断了怎么办？"></a>主从库间网络断了怎么办？</h2><p><strong>在 Redis 2.8 之前，如果主从库在命令传播时出现了网络闪断，那么，从库就会和主库重新进行一次全量复制，开销非常大。</strong></p>
<p><strong>从 Redis 2.8 开始，网络断了之后，主从库会采用增量复制的方式继续同步。听名字大概就可以猜到它和全量复制的不同：全量复制是同步所有数据，而增量复制只会把主从库网络断连期间主库收到的命令，同步给从库。</strong></p>
<p><strong>那么，增量复制时，主从库之间具体是怎么保持同步的呢？这里的奥妙就在于 repl_backlog_buffer 这个缓冲区。</strong>我们先来看下它是如何用于增量命令的同步的。</p>
<p><strong>当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。</strong></p>
<p>repl_backlog_buffer 是一个环形缓冲区，<strong>主库会记录自己写到的位置，从库则会记录自己已经读到的位置（哦，这玩意我之前学过，总记得Redis有个环什么的，原来是在这里）</strong>。</p>
<p>刚开始的时候，<strong>主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是 master_repl_offset。主库接收的新写操作越多，这个值就会越大。</strong></p>
<p>同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，<strong>此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏移量基本相等。</strong></p>
<p><img src="/2024/09/15/Redis06/13f26570a1b90549e6171ea24554b737.jpg" alt="img"></p>
<p>Redis repl_backlog_buffer的使用</p>
<p>主从库的连接恢复之后，<strong>从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距。</strong></p>
<p>在网络断连阶段，<strong>主库可能会收到新的写操作命令，所以，一般来说，master_repl_offset 会大于 slave_repl_offset。此时，主库只用把 master_repl_offset 和 slave_repl_offset 之间的命令操作同步给从库就行。</strong></p>
<p>就像刚刚示意图的中间部分，主库和从库之间相差了 put d e 和 put d f 两个操作，在增量复制时，主库只需要把它们同步给从库，就行了。</p>
<p>说到这里，我们再借助一张图，回顾下增量复制的流程。</p>
<p><img src="/2024/09/15/Redis06/20e233bd30c3dacb0221yy0c77780b16.jpg" alt="img"></p>
<p>Redis增量复制流程</p>
<p>不过，有一个地方我要强调一下，因为 repl_backlog_buffer 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。<strong>如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致</strong>。</p>
<p>因此，我们要想办法避免这一情况，一般而言，我们可以调整 <strong>repl_backlog_size</strong> 这个参数。<strong>这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：缓冲空间大小 &#x3D; 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size &#x3D; 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值。</strong></p>
<p>举个例子，如果主库每秒写入 2000 个操作，每个操作的大小为 2KB，网络每秒能传输 1000 个操作，那么，有 1000 个操作需要缓冲起来，<strong>这就至少需要 2MB 的缓冲空间。否则，新写的命令就会覆盖掉旧操作了。为了应对可能的突发压力，我们最终把 repl_backlog_size 设为 4MB。</strong></p>
<p>这样一来，增量复制时主从库的数据不一致风险就降低了。<strong>不过，如果并发请求量非常大，连两倍的缓冲空间都存不下新操作请求的话，此时，主从库数据仍然可能不一致。</strong></p>
<p>针对这种情况，<strong>一方面，你可以根据 Redis 所在服务器的内存资源再适当增加 repl_backlog_size 值，比如说设置成缓冲空间大小的 4 倍，另一方面，你可以考虑使用切片集群来分担单个主库的请求压力。关于切片集群，我会在第 9 讲具体介绍。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课，我们一起学习了 Redis 的主从库同步的基本原理，<strong>总结来说，有三种模式：全量复制、基于长连接的命令传播，以及增量复制。</strong></p>
<p>全量复制虽然耗时，但是对于从库来说，如果是第一次同步，全量复制是无法避免的，所以，我给你一个小建议：<strong>一个 Redis 实例的数据库不要太大</strong>，<strong>一个实例大小在几 GB 级别比较合适，这样可以减少 RDB 文件生成、传输和重新加载的开销。另外，为了避免多个从库同时和主库进行全量复制，给主库过大的同步压力，我们也可以采用“主 - 从 - 从”这一级联模式，来缓解主库的压力。</strong></p>
<p><strong>长连接复制是主从库正常运行后的常规同步阶段</strong>。在这个阶段中，主从库之间通过命令传播实现同步。<strong>不过，这期间如果遇到了网络断连，增量复制就派上用场了。我特别建议你留意一下 repl_backlog_size 这个配置参数。如果它配置得过小，在增量复制阶段，可能会导致从库的复制进度赶不上主库，进而导致从库重新进行全量复制。所以，通过调大这个参数，可以减少从库在网络断连时全量复制的风险。</strong></p>
<p>不过，<strong>主从库模式使用读写分离虽然避免了同时写多个实例带来的数据不一致问题，但是还面临主库故障的潜在风险。主库故障了从库该怎么办，数据还能保持一致吗，Redis 还能正常提供服务吗？在接下来的两节课里，我会和你具体聊聊主库故障后，保证服务可靠性的解决方案。</strong></p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提一个小问题。这节课，我提到，主从库间的数据复制同步使用的是 RDB 文件，前面我们学习过，AOF 记录的操作命令更全，相比于 RDB 丢失的数据更少。那么，为什么主从库间的复制不使用 AOF 呢？</p>
<p>主从全量同步使用RDB而不使用AOF的原因：</p>
<p><strong>1、RDB文件内容是经过压缩的二进制数据（不同数据类型数据做了针对性优化），文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗</strong>，<strong>从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量同步的成本最低。</strong></p>
<p>2、假设要使用AOF做全量同步，意味着必须打开AOF功能，<strong>打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量同步数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的。</strong></p>
<p>另外，需要指出老师文章的错误：“当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。”</p>
<p>1、主从库连接都断开了，哪里来replication buffer呢？</p>
<p>2、应该不是“主从库断连后”主库才把写操作写入repl_backlog_buffer，只要有从库存在，这个repl_backlog_buffer就会存在。<strong>主库的所有写命令除了传播给从库之外，都会在这个repl_backlog_buffer中记录一份</strong>，缓存起来，只有预先缓存了这些命令，当从库断连后，从库重新发送psync <code>$master_runid $offset</code>，主库才能通过<code>$offset</code>在repl_backlog_buffer中找到从库断开的位置，只发送<code>$offset</code>之后的增量数据给从库即可。</p>
<p>有同学对repl_backlog_buffer和replication buffer理解比较混淆，我大概解释一下：</p>
<ul>
<li>1、<strong>repl_backlog_buffer：就是上面我解释到的，它是为了从库断开之后，如何找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销。</strong>如果从库断开时间太久，<strong>repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量同步，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量同步的概率。</strong>而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer。</li>
<li>2、<strong>replication buffer：Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个 内存buffer进行数据交互，客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的</strong>：<strong>Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互</strong>。所以主从在增量同步时，<strong>从库作为一个client，也会分配一个buffer，只不过这个buffer专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer。</strong></li>
<li>3、再延伸一下，既然有这个内存buffer存在，那么这个buffer有没有限制呢？<strong>如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个buffer就会持续增长，消耗大量的内存资源，甚至OOM。所以Redis提供了client-output-buffer-limit参数限制这个buffer的大小，如果超过限制，主库会强制断开这个client的连接</strong>，也就是说从库处理慢导致主库内存buffer的积压达到限制后，主库会强制断开从库的连接，<strong>此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。</strong></li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/owmafw">基础篇 - 06 | 数据同步：主从库如何实现数据一致？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>哨兵集群：哨兵挂了，主从库还能切换吗？</title>
    <url>/2024/09/16/Redis08/</url>
    <content><![CDATA[<p>上节课，<strong>我们学习了哨兵机制，它可以实现主从库的自动切换。通过部署多个实例，就形成了一个哨兵集群。哨兵集群中的多个实例共同判断，可以降低对主库下线的误判率。</strong></p>
<p>但是，我们还是要考虑一个问题：如果有哨兵实例在运行时发生了故障，主从库还能正常切换吗？</p>
<p>实际上，一旦多个实例组成了<strong>哨兵集群</strong>，<strong>即使有哨兵实例出现故障挂掉了，其他哨兵还能继续协作完成主从库切换的工作，包括判定主库是不是处于下线状态，选择新主库，以及通知从库和客户端。</strong></p>
<p><strong>如果你部署过哨兵集群的话就会知道，在配置哨兵的信息时，我们只需要用到下面的这个配置项，设置主库的 IP 和端口，并没有配置其他哨兵的连接信息。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; </span><br></pre></td></tr></table></figure>

<p>这些哨兵实例既然都不知道彼此的地址，又是怎么组成集群的呢？要弄明白这个问题，我们就需要学习一下哨兵集群的组成和运行机制了。</p>
<h2 id="基于-pub-sub-机制的哨兵集群组成"><a href="#基于-pub-sub-机制的哨兵集群组成" class="headerlink" title="基于 pub&#x2F;sub 机制的哨兵集群组成"></a>基于 pub&#x2F;sub 机制的哨兵集群组成</h2><p><strong>哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub&#x2F;sub 机制，也就是发布 &#x2F; 订阅机制。</strong></p>
<p><strong>哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端口。</strong></p>
<p>除了哨兵实例，<strong>我们自己编写的应用程序也可以通过 Redis 进行消息的发布和订阅</strong>。所以，<strong>为了区分不同应用的消息，Redis 会以频道的形式，对这些消息进行分门别类的管理。所谓的频道，实际上就是消息的类别。当消息类别相同时，它们就属于同一个频道。反之，就属于不同的频道</strong>。<strong>只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换</strong>。</p>
<p>在主从集群中，主库上有一个名为“<strong>sentinel</strong>:hello”的频道，<strong>不同哨兵就是通过它来相互发现，实现互相通信的。（怪不得Redis里面会有这样类似消息队列的东西存在，原来是为了配合哨兵机制）</strong></p>
<p>我来举个例子，具体说明一下。在下图中，哨兵 1 把自己的 IP（172.16.19.3）和端口（26579）发布到“<strong>sentinel</strong>:hello”频道上，<strong>哨兵 2 和 3 订阅了该频道。那么此时，哨兵 2 和 3 就可以从这个频道直接获取哨兵 1 的 IP 地址和端口号。</strong></p>
<p><strong>然后，哨兵 2、3 可以和哨兵 1 建立网络连接。通过这个方式，哨兵 2 和 3 也可以建立网络连接，这样一来，哨兵集群就形成了。它们相互间可以通过网络连接进行通信，比如说对主库有没有下线这件事儿进行判断和协商。</strong></p>
<p><img src="/2024/09/16/Redis08/ca42698128aa4c8a374efbc575ea22b1.jpg" alt="img"></p>
<p><strong>哨兵除了彼此之间建立起连接形成集群外，还需要和从库建立连接</strong>。这是因为，在哨兵的监控任务中<strong>，它需要对主从库都进行心跳判断，而且在主从库切换完成后，它还需要通知从库，让它们和新主库进行同步。</strong></p>
<p>那么，<strong>哨兵是如何知道从库的 IP 地址和端口的呢？</strong></p>
<p><strong>这是由哨兵向主库发送 INFO 命令来完成的</strong>。就像下图所示，<strong>哨兵 2 给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵 1 和 3 可以通过相同的方法和从库建立连接</strong>。</p>
<p><img src="/2024/09/16/Redis08/88fdc68eb94c44efbdf7357260091de0.jpg" alt="img"></p>
<p><strong>你看，通过 pub&#x2F;sub 机制，哨兵之间可以组成集群，同时，哨兵又通过 INFO 命令，获得了从库连接信息，也能和从库建立连接，并进行监控了。</strong></p>
<p>但是，哨兵不能只和主、从库连接。因为，<strong>主从库切换后，客户端也需要知道新主库的连接信息，才能向新主库发送请求操作。所以，哨兵还需要完成把新主库的信息告诉客户端这个任务。</strong></p>
<p>而且，<strong>在实际使用哨兵时，我们有时会遇到这样的问题：如何在客户端通过监控了解哨兵进行主从切换的过程呢？比如说，主从切换进行到哪一步了？这其实就是要求，客户端能够获取到哨兵集群在监控、选主、切换这个过程中发生的各种事件。</strong></p>
<p>此时，我们仍然可以依赖 <strong>pub&#x2F;sub 机制</strong>，来帮助我们完成哨兵和客户端间的信息同步。</p>
<h2 id="基于-pub-sub-机制的客户端事件通知"><a href="#基于-pub-sub-机制的客户端事件通知" class="headerlink" title="基于 pub&#x2F;sub 机制的客户端事件通知"></a>基于 pub&#x2F;sub 机制的客户端事件通知</h2><p>从本质上说，<strong>哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务</strong>。所以，每个哨兵实例也提供 pub&#x2F;sub 机制，<strong>客户端可以从哨兵订阅消息</strong>。<strong>哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件</strong>。</p>
<p>频道有这么多，一下子全部学习容易丢失重点。为了减轻你的学习压力，我把重要的频道汇总在了一起，涉及几个关键事件，包括主库下线判断、新主库选定、从库重新配置。</p>
<p><img src="/2024/09/16/Redis08/4e9665694a9565abbce1a63cf111f725.jpg" alt="img"></p>
<p>知道了这些频道之后，你就可以<strong>让客户端从哨兵这里订阅消息</strong>了。<strong>具体的操作步骤是，客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后，我们可以在客户端执行订阅命令，来获取不同的事件消息。</strong></p>
<p>举个例子，你可以执行如下命令，<strong>来订阅“所有实例进入客观下线状态的事件”：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SUBSCRIBE +odown</span><br></pre></td></tr></table></figure>

<p>当然，你也可以执行如下命令，<strong>订阅所有的事件</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PSUBSCRIBE  *</span><br></pre></td></tr></table></figure>

<p><strong>当哨兵把新主库选择出来后，客户端就会看到下面的 switch-master 事件。这个事件表示主库已经切换了，新主库的 IP 地址和端口信息已经有了。这个时候，客户端就可以用这里面的新主库地址和端口进行通信了。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">switch-master &lt;master name&gt; &lt;oldip&gt; &lt;oldport&gt; &lt;newip&gt; &lt;newport&gt;</span><br></pre></td></tr></table></figure>

<p><strong>有了这些事件通知，客户端不仅可以在主从切换后得到新主库的连接信息，还可以监控到主从库切换过程中发生的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步了，有助于了解切换进度。</strong></p>
<p>好了，<strong>有了 pub&#x2F;sub 机制，哨兵和哨兵之间、哨兵和从库之间、哨兵和客户端之间就都能建立起连接了</strong>，再加上我们上节课介绍<strong>主库下线判断和选主依据，哨兵集群的监控、选主和通知三个任务就基本可以正常工作了</strong>。不过，我们还需要考虑一个问题：主库故障以后，哨兵集群有多个实例，那怎么确定由哪个哨兵来进行实际的主从切换呢？</p>
<h2 id="由哪个哨兵执行主从切换？"><a href="#由哪个哨兵执行主从切换？" class="headerlink" title="由哪个哨兵执行主从切换？"></a>由哪个哨兵执行主从切换？</h2><p>确定由哪个哨兵执行主从切换的过程，<strong>和主库“客观下线”的判断过程类似，也是一个“投票仲裁”的过程</strong>。在具体了解这个过程前，我们再来看下，判断“客观下线”的仲裁过程。</p>
<p>哨兵集群要判定主库“客观下线”，<strong>需要有一定数量的实例都认为该主库已经“主观下线”了</strong>。我在上节课向你介绍了判断“客观下线”的原则，接下来，我介绍下具体的判断过程。</p>
<p><strong>任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down-by-addr 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。</strong></p>
<p><img src="/2024/09/16/Redis08/e0832d432c14c98066a94e0ef86af384.jpg" alt="img"></p>
<p><strong>一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的。例如，现在有 5 个哨兵，quorum 配置的是 3，那么，一个哨兵需要 3 张赞成票，就可以标记主库为“客观下线”了。这 3 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。</strong></p>
<p>此时，<strong>这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader。</strong></p>
<p>在投票过程中，任何一个想成为 Leader 的哨兵，要满足两个条件：</p>
<ul>
<li>第一，<strong>拿到半数以上的赞成票</strong>；</li>
<li>第二，<strong>拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。以 3 个哨兵为例，假设此时的 quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以了。</strong></li>
</ul>
<p>这么说你可能还不太好理解，我再画一张图片，展示一下 3 个哨兵、quorum 为 2 的选举过程。</p>
<p><img src="/2024/09/16/Redis08/5f6ceeb9337e158cc759e23c0f375fd9.jpg" alt="img"></p>
<p>在 T1 时刻，<strong>S1 判断主库为“客观下线”，它想成为 Leader，就先给自己投一张赞成票，然后分别向 S2 和 S3 发送命令，表示要成为 Leader。</strong></p>
<p>在 T2 时刻，<strong>S3 判断主库为“客观下线”，它也想成为 Leader，所以也先给自己投一张赞成票，再分别向 S1 和 S2 发送命令，表示要成为 Leader。</strong></p>
<p>在 T3 时刻，S1 收到了 S3 的 Leader 投票请求。<strong>因为 S1 已经给自己投了一票 Y，所以它不能再给其他哨兵投赞成票了，所以 S1 回复 N 表示不同意</strong>。同时，<strong>S2 收到了 T2 时 S3 发送的 Leader 投票请求。因为 S2 之前没有投过票，它会给第一个向它发送投票请求的哨兵回复 Y，给后续再发送投票请求的哨兵回复 N，所以，在 T3 时，S2 回复 S3，同意 S3 成为 Leader。</strong></p>
<p>在 T4 时刻，<strong>S2 才收到 T1 时 S1 发送的投票命令。因为 S2 已经在 T3 时同意了 S3 的投票请求，此时，S2 给 S1 回复 N，表示不同意 S1 成为 Leader。发生这种情况，是因为 S3 和 S2 之间的网络传输正常，而 S1 和 S2 之间的网络传输可能正好拥塞了，导致投票请求传输慢了。</strong></p>
<p>最后，在 T5 时刻，<strong>S1 得到的票数是来自它自己的一票 Y 和来自 S2 的一票 N。而 S3 除了自己的赞成票 Y 以外，还收到了来自 S2 的一票 Y。此时，S3 不仅获得了半数以上的 Leader 赞成票，也达到预设的 quorum 值（quorum 为 2），所以它最终成为了 Leader。</strong>接着，S3 会开始执行选主操作，而且在选定新主库后，会给其他从库和客户端通知新主库的信息。</p>
<p><strong>如果 S3 没有拿到 2 票 Y，那么这轮投票就不会产生 Leader。哨兵集群会等待一段时间（也就是哨兵故障转移超时时间的 2 倍），再重新选举。</strong>这是因为，哨兵集群能够进行成功投票，<strong>很大程度上依赖于选举命令的正常网络传播。如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。</strong></p>
<p>需要注意的是，<strong>如果哨兵集群只有 2 个实例，此时，一个哨兵要想成为 Leader，必须获得 2 票，而不是 1 票。所以，如果有个哨兵挂掉了，那么，此时的集群是无法进行主从库切换的。因此，通常我们至少会配置 3 个哨兵实例。这一点很重要，你在实际应用时可不能忽略了。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>通常，我<strong>们在解决一个系统问题的时候，会引入一个新机制，或者设计一层新功能</strong>，就像我们在这两节课学习的内容：<strong>为了实现主从切换，我们引入了哨兵；为了避免单个哨兵故障后无法进行主从切换，以及为了减少误判率，又引入了哨兵集群；哨兵集群又需要有一些机制来支撑它的正常运行。</strong></p>
<p>这节课上，我就向你介绍了支持哨兵集群的这些关键机制，包括：</p>
<ul>
<li><strong>基于 pub&#x2F;sub 机制的哨兵集群组成过程；</strong></li>
<li><strong>基于 INFO 命令的从库列表，这可以帮助哨兵和从库建立连接；</strong></li>
<li><strong>基于哨兵自身的 pub&#x2F;sub 功能，这实现了客户端和哨兵之间的事件通知。</strong></li>
</ul>
<p>对于主从切换，<strong>当然不是哪个哨兵想执行就可以执行的，否则就乱套了。所以，这就需要哨兵集群在判断了主库“客观下线”后，经过投票仲裁，选举一个 Leader 出来，由它负责实际的主从切换，即由它来完成新主库的选择以及通知从库与客户端。</strong></p>
<p>最后，我想再给你分享一个经验：<strong>要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值 down-after-milliseconds</strong>。我们曾经就踩过一个“坑”。<strong>当时，在我们的项目中，因为这个值在不同的哨兵实例上配置不一致，导致哨兵集群一直没有对有故障的主库形成共识，也就没有及时切换主库，最终的结果就是集群服务不稳定。所以，你一定不要忽略这条看似简单的经验。</strong></p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>这节课上，我给你提一个小问题。</p>
<p>假设有一个 Redis 集群，是“一主四从”，同时配置了包含 5 个哨兵实例的集群，quorum 值设为 2。在运行过程中，<strong>如果有 3 个哨兵实例都发生故障了，此时，Redis 主库如果有故障，还能正确地判断主库“客观下线”吗？如果可以的话，还能进行主从库自动切换吗？此外，哨兵实例是不是越多越好呢，如果同时调大 down-after-milliseconds 值，对减少误判是不是也有好处呢？</strong></p>
<p>Redis 1主4从，5个哨兵，哨兵配置quorum为2，如果3个哨兵故障，当主库宕机时，哨兵能否判断主库“客观下线”？能否自动切换？</p>
<p>经过实际测试，我的结论如下：</p>
<ul>
<li>1、哨兵集群可以判定主库“主观下线”。由于quorum&#x3D;2，所以当一个哨兵判断主库“主观下线”后，<strong>询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，哨兵集群可以判定主库为“客观下线”。</strong></li>
<li>2、但哨兵不能完成主从切换。<strong>哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5&#x2F;2+1&#x3D;3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到多数选票的结果。</strong></li>
</ul>
<p>但是投票选举过程的细节并不是大家认为的：每个哨兵各自1票，这个情况是不一定的。下面具体说一下：</p>
<ul>
<li>场景a：<strong>哨兵A先判定主库“主观下线”，然后马上询问哨兵B（注意，此时哨兵B只是被动接受询问，并没有去询问哨兵A，也就是它还没有进入判定“客观下线”的流程），哨兵B回复主库已“主观下线”，达到quorum&#x3D;2后哨兵A此时可以判定主库“客观下线”。</strong>此时，哨兵A马上可以向其他哨兵发起成为“哨兵领导者”的投票，哨兵B收到投票请求后，由于自己还没有询问哨兵A进入判定“客观下线”的流程，所以哨兵B是可以给哨兵A投票确认的，这样哨兵A就已经拿到2票了。<strong>等稍后哨兵B也判定“主观下线”后想成为领导者时，因为它已经给别人投过票了，所以这一轮自己就不能再成为领导者了。</strong></li>
<li>场景b：哨兵A和哨兵B同时判定主库“主观下线”，然后同时询问对方后都得到可以“客观下线”的结论，<strong>此时它们各自给自己投上1票后，然后向其他哨兵发起投票请求，但是因为各自都给自己投过票了，因此各自都拒绝了对方的投票请求，这样2个哨兵各自持有1票。</strong></li>
</ul>
<p><strong>场景a是1个哨兵拿到2票，场景b是2个哨兵各自有1票，这2种情况都不满足大多数选票(3票)的结果，因此无法完成主从切换。</strong></p>
<p>经过测试发现，场景b发生的概率非常小，<strong>只有2个哨兵同时进入判定“主观下线”的流程时才可以发生。我测试几次后发现，都是复现的场景a。</strong></p>
<p>哨兵实例是不是越多越好？</p>
<p><strong>并不是，我们也看到了，哨兵在判定“主观下线”和选举“哨兵领导者”时，都需要和其他节点进行通信，交换信息，哨兵实例越多，通信的次数也就越多，而且部署多个哨兵时，会分布在不同机器上，节点越多带来的机器故障风险也会越大，这些问题都会影响到哨兵的通信和选举，出问题时也就意味着选举时间会变长，切换主从的时间变久。</strong></p>
<p>调大down-after-milliseconds值，对减少误判是不是有好处？</p>
<p><strong>是有好处的，适当调大down-after-milliseconds值，当哨兵与主库之间网络存在短时波动时，可以降低误判的概率。但是调大down-after-milliseconds值也意味着主从切换的时间会变长，对业务的影响时间越久，我们需要根据实际场景进行权衡，设置合理的阈值。</strong></p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/ogzb67">基础篇 - 08 | 哨兵集群：哨兵挂了，主从库还能切换吗？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>“万金油”的String，为什么不好用了？</title>
    <url>/2024/09/16/Redis11/</url>
    <content><![CDATA[<p>从今天开始，我们就要进入“实践篇”了。接下来，我们会用 5 节课的时间学习“数据结构”。<strong>我会介绍节省内存开销以及保存和统计海量数据的数据类型及其底层数据结构，还会围绕典型的应用场景（例如地址位置查询、时间序列数据库读写和消息队列存取），跟你分享使用 Redis 的数据类型和 module 扩展功能来满足需求的具体方案。</strong></p>
<p>今天，我们先了解下 <strong>String 类型的内存空间消耗问题，以及选择节省内存开销的数据类型的解决方案。</strong></p>
<p>先跟你分享一个我曾经遇到的需求。</p>
<p>当时，<strong>我们要开发一个图片存储系统，要求这个系统能快速地记录图片 ID 和图片在存储系统中保存时的 ID（可以直接叫作图片存储对象 ID）。同时，还要能够根据图片 ID 快速查找到图片存储对象 ID。</strong></p>
<p>因为图片数量巨大，<strong>所以我们就用 10 位数来表示图片 ID 和图片存储对象 ID，例如，图片 ID 为 1101000051，它在存储系统中对应的 ID 号是 3301000051。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">photo_id: 1101000051</span><br><span class="line"></span><br><span class="line">photo_obj_id: 3301000051</span><br></pre></td></tr></table></figure>

<p>可以看到<strong>，图片 ID 和图片存储对象 ID 正好一一对应，是典型的“键 - 单值”模式。所谓的“单值”，就是指键值对中的值就是一个值，而不是一个集合，这和 String 类型提供的“一个键对应一个值的数据”的保存形式刚好契合。</strong></p>
<p><strong>而且，String 类型可以保存二进制字节流，就像“万金油”一样，只要把数据转成二进制字节数组，就可以保存了。</strong></p>
<p>所以，<strong>我们的第一个方案就是用 String 保存数据。我们把图片 ID 和图片存储对象 ID 分别作为键值对的 key 和 value 来保存，其中，图片存储对象 ID 用了 String 类型。</strong></p>
<p>刚开始，我们保存了 1 亿张图片，大约用了 6.4GB 的内存。<strong>但是，随着图片数据量的不断增加，我们的 Redis 内存使用量也在增加，结果就遇到了大内存 Redis 实例因为生成 RDB 而响应变慢的问题。很显然，String 类型并不是一种好的选择，我们还需要进一步寻找能节省内存开销的数据类型方案。</strong></p>
<p>在这个过程中，<strong>我深入地研究了 String 类型的底层结构，找到了它内存开销大的原因，对“万金油”的 String 类型有了全新的认知：String 类型并不是适用于所有场合的，它有一个明显的短板，就是它保存数据时所消耗的内存空间较多。</strong></p>
<p>同时，<strong>我还仔细研究了集合类型的数据结构。我发现，集合类型有非常节省内存空间的底层实现结构，但是，集合类型保存的数据模式，是一个键对应一系列值，并不适合直接保存单值的键值对。所以，我们就使用二级编码的方法，实现了用集合类型保存单值键值对，Redis 实例的内存空间消耗明显下降了。</strong></p>
<p>这节课，我就把在解决这个问题时学到的经验和方法分享给你，<strong>包括 String 类型的内存空间消耗在哪儿了、用什么数据结构可以节省内存，以及如何用集合类型保存单值键值对</strong>。如果<strong>你在使用 String 类型时也遇到了内存空间消耗较多的问题，就可以尝试下今天的解决方案了。</strong></p>
<p>接下来，我们先来看看 String 类型的内存都消耗在哪里了。</p>
<h2 id="为什么-String-类型内存开销大？"><a href="#为什么-String-类型内存开销大？" class="headerlink" title="为什么 String 类型内存开销大？"></a>为什么 String 类型内存开销大？</h2><p>在刚才的案例中，我们保存了 1 亿张图片的信息，用了约 6.4GB 的内存，<strong>一个图片 ID 和图片存储对象 ID 的记录平均用了 64 字节。</strong></p>
<p>但问题是，<strong>一组图片 ID 及其存储对象 ID 的记录，实际只需要 16 字节就可以了。</strong></p>
<p>我们来分析一下。<strong>图片 ID 和图片存储对象 ID 都是 10 位数，我们可以用两个 8 字节的 Long 类型表示这两个 ID。因为 8 字节的 Long 类型最大可以表示 2 的 64 次方的数值，所以肯定可以表示 10 位数。但是，为什么 String 类型却用了 64 字节呢？</strong></p>
<p>其实，除了记录实际数据，<strong>String 类型还需要额外的内存空间记录数据长度、空间使用等信息，这些信息也叫作元数据</strong>。<strong>当实际保存的数据较小时，元数据的空间开销就显得比较大了，有点“喧宾夺主”的意思。</strong></p>
<p>那么，String 类型具体是怎么保存数据的呢？我来解释一下。</p>
<p>当你保存 64 位有符号整数时，<strong>String 类型会把它保存为一个 8 字节的 Long 类型整数，这种保存方式通常也叫作 int 编码方式。</strong></p>
<p>但是，<strong>当你保存的数据中包含字符时，String 类型就会用简单动态字符串（Simple Dynamic String，SDS）结构体来保存</strong>，如下图所示：</p>
<p><img src="/2024/09/16/Redis11/37c6a8d5abd65906368e7c4a6b938657.jpg" alt="img"></p>
<ul>
<li><strong>buf</strong>：字节数组，<strong>保存实际数据。为了表示字节数组的结束，Redis 会自动在数组最后加一个“\0”，这就会额外占用 1 个字节的开销。</strong></li>
<li><strong>len</strong>：占 4 个字节，<strong>表示 buf 的已用长度。</strong></li>
<li><strong>alloc</strong>：也占个 4 字节，<strong>表示 buf 的实际分配长度，一般大于 len。</strong></li>
</ul>
<p>可以看到，<strong>在 SDS 中，buf 保存实际数据，而 len 和 alloc 本身其实是 SDS 结构体的额外开销。</strong></p>
<p>另外，对于 String 类型来说，<strong>除了 SDS 的额外开销，还有一个来自于 RedisObject 结构体的开销。</strong></p>
<p>因为 Redis 的数据类型有很多，而且，<strong>不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以，Redis 会用一个 RedisObject 结构体来统一记录这些元数据，同时指向实际数据。</strong></p>
<p><strong>一个 RedisObject 包含了 8 字节的元数据和一个 8 字节指针，这个指针再进一步指向具体数据类型的实际数据所在，例如指向 String 类型的 SDS 结构所在的内存地址</strong>，可以看一下下面的示意图。关于 RedisObject 的具体结构细节，我会在后面的课程中详细介绍，现在你只要了解它的基本结构和元数据开销就行了。</p>
<p><img src="/2024/09/16/Redis11/3409948e9d3e8aa5cd7cafb9b66c2857.jpg" alt="img"></p>
<p><strong>为了节省内存空间，Redis 还对 Long 类型整数和 SDS 的内存布局做了专门的设计。</strong></p>
<ul>
<li>一方面，当保存的是 Long 类型整数时，<strong>RedisObject 中的指针就直接赋值为整数数据了，这样就不用额外的指针再指向整数了，节省了指针的空间开销。</strong></li>
<li>另一方面，当保存的是字符串数据，<strong>并且字符串小于等于 44 字节时，RedisObject 中的元数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片。这种布局方式也被称为 embstr 编码方式。</strong></li>
</ul>
<p>当然，<strong>当字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和 RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构。这种布局方式被称为 raw 编码模式。</strong></p>
<p>为了帮助你理解 int、embstr 和 raw 这三种编码模式，我画了一张示意图，如下所示：</p>
<p><img src="/2024/09/16/Redis11/ce83d1346c9642fdbbf5ffbe701bfbe3.jpg" alt="img"></p>
<p>好了，<strong>知道了 RedisObject 所包含的额外元数据开销，现在，我们就可以计算 String 类型的内存使用量了。</strong></p>
<p>因为 10 位数的图片 ID 和图片存储对象 ID 是 Long 类型整数，<strong>所以可以直接用 int 编码的 RedisObject 保存。每个 int 编码的 RedisObject 元数据部分占 8 字节，指针部分被直接赋值为 8 字节的整数了。此时，每个 ID 会使用 16 字节，加起来一共是 32 字节。但是，另外的 32 字节去哪儿了呢？</strong></p>
<p>我在第 2 讲中说过，<strong>Redis 会使用一个全局哈希表保存所有键值对，哈希表的每一项是一个 dictEntry 的结构体，用来指向一个键值对</strong>。<strong>dictEntry 结构中有三个 8 字节的指针，分别指向 key、value 以及下一个 dictEntry，三个指针共 24 字节，如下图所示：</strong></p>
<p><img src="/2024/09/16/Redis11/a6708594a86d2a49107f8b6cfc1a2b8a.jpg" alt="img"></p>
<p><strong>但是，这三个指针只有 24 字节，为什么会占用了 32 字节呢？这就要提到 Redis 使用的内存分配库 jemalloc 了</strong>。</p>
<p>jemalloc 在分配内存时，<strong>会根据我们申请的字节数 N，找一个比 N 大，但是最接近 N 的 2 的幂次数作为分配的空间，这样可以减少频繁分配的次数</strong>。</p>
<p><strong>举个例子。如果你申请 6 字节空间，jemalloc 实际会分配 8 字节空间；如果你申请 24 字节空间，jemalloc 则会分配 32 字节。所以，在我们刚刚说的场景里，dictEntry 结构就占用了 32 字节。</strong></p>
<p>好了，到这儿，你应该就能理解，为什么用 String 类型保存图片 ID 和图片存储对象 ID 时需要用 64 个字节了。</p>
<p><strong>你看，明明有效信息只有 16 字节，使用 String 类型保存时，却需要 64 字节的内存空间，有 48 字节都没有用于保存实际的数据。我们来换算下，如果要保存的图片有 1 亿张，那么 1 亿条的图片 ID 记录就需要 6.4GB 内存空间，其中有 4.8GB 的内存空间都用来保存元数据了，额外的内存空间开销很大。那么，有没有更加节省内存的方法呢？</strong></p>
<h2 id="用什么数据结构可以节省内存？"><a href="#用什么数据结构可以节省内存？" class="headerlink" title="用什么数据结构可以节省内存？"></a>用什么数据结构可以节省内存？</h2><p><strong>Redis 有一种底层数据结构，叫压缩列表（ziplist），这是一种非常节省内存的结构。</strong></p>
<p>我们先回顾下压缩列表的构成。<strong>表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量，以及列表中的 entry 个数。压缩列表尾还有一个 zlend，表示列表结束。</strong></p>
<p><img src="/2024/09/16/Redis11/f6d4df5f7d6e80de29e2c6446b02429f.jpg" alt="img"></p>
<p><strong>压缩列表之所以能节省内存，就在于它是用一系列连续的 entry 保存数据。每个 entry 的元数据包括下面几部分。</strong></p>
<ul>
<li><strong>prev_len</strong>，表示前一个 entry 的长度。<strong>prev_len 有两种取值情况：1 字节或 5 字节。取值 1 字节时，表示上一个 entry 的长度小于 254 字节。虽然 1 字节的值能表示的数值范围是 0 到 255，但是压缩列表中 zlend 的取值默认是 255，因此，就默认用 255 表示整个压缩列表的结束，其他表示长度的地方就不能再用 255 这个值了。所以，当上一个 entry 长度小于 254 字节时，prev_len 取值为 1 字节，否则，就取值为 5 字节。</strong></li>
<li><strong>len</strong>：表示<strong>自身长度</strong>，4 字节；</li>
<li><strong>encoding</strong>：表示<strong>编码方式</strong>，1 字节；</li>
<li><strong>content</strong>：保存<strong>实际数据。</strong></li>
</ul>
<p><strong>这些 entry 会挨个儿放置在内存中，不需要再用额外的指针进行连接，这样就可以节省指针所占用的空间。</strong></p>
<p>我们以保存图片存储对象 ID 为例，来分析一下压缩列表是如何节省内存空间的。</p>
<p><strong>每个 entry 保存一个图片存储对象 ID（8 字节），此时，每个 entry 的 prev_len 只需要 1 个字节就行，因为每个 entry 的前一个 entry 长度都只有 8 字节，小于 254 字节。这样一来，一个图片的存储对象 ID 所占用的内存大小是 14 字节（1+4+1+8&#x3D;14），实际分配 16 字节。</strong></p>
<p><strong>Redis 基于压缩列表实现了 List、Hash 和 Sorted Set 这样的集合类型，这样做的最大好处就是节省了 dictEntry 的开销。当你用 String 类型时，一个键值对就有一个 dictEntry，要用 32 字节空间。但采用集合类型时，一个 key 就对应一个集合的数据，能保存的数据多了很多，但也只用了一个 dictEntry，这样就节省了内存。</strong></p>
<p>这个方案听起来很好，但还存在一个问题：</p>
<p><strong>在用集合类型保存键值对时，一个键对应了一个集合的数据，但是在我们的场景中，一个图片 ID 只对应一个图片的存储对象 ID，我们该怎么用集合类型呢？换句话说，在一个键对应一个值（也就是单值键值对）的情况下，我们该怎么用集合类型来保存这种单值键值对呢？</strong></p>
<h2 id="如何用集合类型保存单值的键值对？"><a href="#如何用集合类型保存单值的键值对？" class="headerlink" title="如何用集合类型保存单值的键值对？"></a>如何用集合类型保存单值的键值对？</h2><p><strong>在保存单值的键值对时，可以采用基于 Hash 类型的二级编码方法。这里说的二级编码，就是把一个单值的数据拆分成两部分，前一部分作为 Hash 集合的 key，后一部分作为 Hash 集合的 value，这样一来，我们就可以把单值数据保存到 Hash 集合中了。</strong></p>
<p>以图片 ID 1101000060 和图片存储对象 ID 3302000080 为例，<strong>我们可以把图片 ID 的前 7 位（1101000）作为 Hash 类型的键，把图片 ID 的最后 3 位（060）和图片存储对象 ID 分别作为 Hash 类型值中的 key 和 value。</strong></p>
<p>按照这种设计方法，<strong>我在 Redis 中插入了一组图片 ID 及其存储对象 ID 的记录，并且用 info 命令查看了内存开销，我发现，增加一条记录后，内存占用只增加了 16 字节</strong>，如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; info memory</span><br><span class="line"></span><br><span class="line">\<span class="comment"># Memory</span></span><br><span class="line"></span><br><span class="line">used_memory:1039120</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; hset 1101000 060 3302000080</span><br><span class="line"></span><br><span class="line">(<span class="built_in">integer</span>) 1</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; info memory</span><br><span class="line"></span><br><span class="line">\<span class="comment"># Memory</span></span><br><span class="line"></span><br><span class="line">used_memory:1039136</span><br></pre></td></tr></table></figure>

<p><strong>在使用 String 类型时，每个记录需要消耗 64 字节，这种方式却只用了 16 字节，所使用的内存空间是原来的 1&#x2F;4，满足了我们节省内存空间的需求。</strong></p>
<p>不过，你可能也会有疑惑：“<strong>二级编码一定要把图片 ID 的前 7 位作为 Hash 类型的键，把最后 3 位作为 Hash 类型值中的 key 吗</strong>？”<strong>其实，二级编码方法中采用的 ID 长度是有讲究的</strong>。</p>
<p>在第 2 讲中，<strong>我介绍过 Redis Hash 类型的两种底层实现结构，分别是压缩列表和哈希表。</strong></p>
<p>那么，Hash 类型底层结构什么时候使用压缩列表，什么时候使用哈希表呢<strong>？其实，Hash 类型设置了用压缩列表保存数据时的两个阈值，一旦超过了阈值，Hash 类型就会用哈希表来保存数据了。</strong></p>
<p>这两个阈值分别对应以下两个配置项：</p>
<ul>
<li>hash-max-ziplist-entries：<strong>表示用压缩列表保存时哈希集合中的最大元素个数。</strong></li>
<li>hash-max-ziplist-value：<strong>表示用压缩列表保存时哈希集合中单个元素的最大长度。</strong></li>
</ul>
<p><strong>如果我们往 Hash 集合中写入的元素个数超过了 hash-max-ziplist-entries，或者写入的单个元素大小超过了 hash-max-ziplist-value，Redis 就会自动把 Hash 类型的实现结构由压缩列表转为哈希表。</strong></p>
<p><strong>一旦从压缩列表转为了哈希表，Hash 类型就会一直用哈希表进行保存，而不会再转回压缩列表了。在节省内存空间方面，哈希表就没有压缩列表那么高效了。</strong></p>
<p><strong>为了能充分使用压缩列表的精简内存布局，我们一般要控制保存在 Hash 集合中的元素个数</strong>。所以，在刚才的二级编码中，我们只用图片 ID 最后 3 位作为 Hash 集合的 key，<strong>也就保证了 Hash 集合的元素个数不超过 1000，同时，我们把 hash-max-ziplist-entries 设置为 1000，这样一来，Hash 集合就可以一直使用压缩列表来节省内存空间了。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><strong>这节课，我们打破了对 String 的认知误区，以前，我们认为 String 是“万金油”，什么场合都适用，但是，在保存的键值对本身占用的内存空间不大时（例如这节课里提到的的图片 ID 和图片存储对象 ID），String 类型的元数据开销就占据主导了，这里面包括了 RedisObject 结构、SDS 结构、dictEntry 结构的内存开销。</strong></p>
<p><strong>针对这种情况，我们可以使用压缩列表保存数据。</strong>当然，使用 Hash 这种集合类型保存单值键值对的数据时，<strong>我们需要将单值数据拆分成两部分，分别作为 Hash 集合的键和值，就像刚才案例中用二级编码来表示图片 ID，希望你能把这个方法用到自己的场景中。</strong></p>
<p>最后，我还想再给你提供一个小方法：<strong>如果你想知道键值对采用不同类型保存时的内存开销，可以在这个网址里输入你的键值对长度和使用的数据类型，这样就能知道实际消耗的内存大小了。建议你把这个小工具用起来，它可以帮助你充分地节省内存。</strong></p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，给你提个小问题：<strong>除了 String 类型和 Hash 类型，你觉得，还有其他合适的类型可以应用在这节课所说的保存图片的例子吗？</strong></p>
<p>保存图片的例子，除了用String和Hash存储之外，<strong>还可以用Sorted Set存储（勉强）。</strong></p>
<p><strong>Sorted Set与Hash类似，当元素数量少于zset-max-ziplist-entries，并且每个元素内存占用小于zset-max-ziplist-value时，默认也采用ziplist结构存储。我们可以把zset-max-ziplist-entries参数设置为1000，这样Sorted Set默认就会使用ziplist存储了，member和score也会紧凑排列存储，可以节省内存空间。</strong></p>
<p>使用zadd 1101000 3302000080 060命令存储图片ID和对象ID的映射关系，查询时使用zscore 1101000 060获取结果。</p>
<p><strong>但是Sorted Set使用ziplist存储时的缺点是，这个ziplist是需要按照score排序的（为了方便zrange和zrevrange命令的使用），所以在插入一个元素时，需要先根据score找到对应的位置，然后把member和score插入进去，这也意味着Sorted Set插入元素的性能没有Hash高（这也是前面说勉强能用Sorte Set存储的原因）。而Hash在插入元素时，只需要将新的元素插入到ziplist的尾部即可，不需要定位到指定位置。</strong></p>
<p>不管是使用Hash还是Sorted Set，<strong>当采用ziplist方式存储时，虽然可以节省内存空间，但是在查询指定元素时，都要遍历整个ziplist，找到指定的元素。所以使用ziplist方式存储时，虽然可以利用CPU高速缓存，但也不适合存储过多的数据（hash-max-ziplist-entries和zset-max-ziplist-entries不宜设置过大），否则查询性能就会下降比较厉害。整体来说，这样的方案就是时间换空间，我们需要权衡使用。</strong></p>
<p>当使用ziplist存储时，<strong>我们尽量存储int数据，ziplist在设计时每个entry都进行了优化，针对要存储的数据，会尽量选择占用内存小的方式存储（整数比字符串在存储时占用内存更小），这也有利于我们节省Redis的内存</strong>。还有，因为ziplist是每个元素紧凑排列，<strong>而且每个元素存储了上一个元素的长度，所以当修改其中一个元素超过一定大小时，会引发多个元素的级联调整（前面一个元素发生大的变动，后面的元素都要重新排列位置，重新分配内存），这也会引发性能问题，需要注意。</strong></p>
<p>另外，使用Hash和Sorted Set存储时，<strong>虽然节省了内存空间，但是设置过期变得困难（无法控制每个元素的过期，只能整个key设置过期，或者业务层单独维护每个元素过期删除的逻辑，但比较复杂）。而使用String虽然占用内存多，但是每个key都可以单独设置过期时间，还可以设置maxmemory和淘汰策略，以这种方式控制整个实例的内存上限。</strong></p>
<p><strong>所以在选用Hash和Sorted Set存储时，意味着把Redis当做数据库使用，这样就需要务必保证Redis的可靠性（做好备份、主从副本），防止实例宕机引发数据丢失的风险。</strong>而采用String存储时，<strong>可以把Redis当做缓存使用，每个key设置过期时间，同时设置maxmemory和淘汰策略，控制整个实例的内存上限，这种方案需要在数据库层（例如MySQL）也存储一份映射关系，当Redis中的缓存过期或被淘汰时，需要从数据库中重新查询重建缓存，同时需要保证数据库和缓存的一致性，这些逻辑也需要编写业务代码实现。</strong></p>
<p>总之，各有利弊，我们需要根据实际场景进行选择。</p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/caw7kq">数据结构 - 11 | “万金油”的String，为什么不好用了？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
<p>推荐GitHub文档：<a href="https://github.com/zpoint/Redis-Internals/blob/5.0/Object/hash/hash_cn.md">Redis-Internals&#x2F;Object&#x2F;hash&#x2F;hash_cn.md at 5.0 · zpoint&#x2F;Redis-Internals (github.com)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>GEO是什么？还可以定义新的数据类型吗？</title>
    <url>/2024/09/16/Redis13/</url>
    <content><![CDATA[<p>在第 2 讲中，我们学习了 <strong>Redis 的 5 大基本数据类型：String、List、Hash、Set 和 Sorted Set，它们可以满足大多数的数据存储需求，但是在面对海量数据统计时，它们的内存开销很大，而且对于一些特殊的场景，它们是无法支持的</strong>。所以，Redis <strong>还提供了 3 种扩展数据类型，分别是 Bitmap、HyperLogLog 和 GEO。前两种我在上节课已经重点介绍过了，今天，我再具体讲一讲 GEO。</strong></p>
<p>另外，<strong>我还会给你介绍开发自定义的新数据类型的基本步骤。掌握了自定义数据类型的开发方法，当你面临一些复杂的场景时，就不用受基本数据类型的限制，可以直接在 Redis 中增加定制化的数据类型，来满足你的特殊需求。</strong></p>
<p>接下来，我们就先来了解下扩展数据类型 GEO 的实现原理和使用方法。</p>
<h2 id="面向-LBS-应用的-GEO-数据类型"><a href="#面向-LBS-应用的-GEO-数据类型" class="headerlink" title="面向 LBS 应用的 GEO 数据类型"></a>面向 LBS 应用的 GEO 数据类型</h2><p>在日常生活中，我们越来越依赖搜索“附近的餐馆”、在打车软件上叫车，<strong>这些都离不开基于位置信息服务（Location-Based Service，LBS）的应用</strong>。<strong>LBS 应用访问的数据是和人或物关联的一组经纬度信息，而且要能查询相邻的经纬度范围，GEO 就非常适合应用在 LBS 服务的场景中，我们来看一下它的底层结构。</strong></p>
<h3 id="GEO-的底层结构"><a href="#GEO-的底层结构" class="headerlink" title="GEO 的底层结构"></a>GEO 的底层结构</h3><p><strong>一般来说，在设计一个数据类型的底层结构时，我们首先需要知道，要处理的数据有什么访问特点</strong>。所以，我们需要先搞清楚位置信息到底是怎么被存取的。</p>
<p>我以叫车服务为例，来分析下 LBS 应用中经纬度的存取特点。</p>
<p><strong>每一辆网约车都有一个编号（例如 33），网约车需要将自己的经度信息（例如 116.034579）和纬度信息（例如 39.000452 ）发给叫车应用。</strong></p>
<p><strong>用户在叫车的时候，叫车应用会根据用户的经纬度位置（例如经度 116.054579，纬度 39.030452），查找用户的附近车辆，并进行匹配。</strong></p>
<p>等把位置相近的用户和车辆匹配上以后，叫车应用就会根据车辆的编号，获取车辆的信息，并返回给用户。</p>
<p><strong>可以看到，一辆车（或一个用户）对应一组经纬度，并且随着车（或用户）的位置移动，相应的经纬度也会变化。</strong></p>
<p><strong>这种数据记录模式属于一个 key（例如车 ID）对应一个 value（一组经纬度）。当有很多车辆信息要保存时，就需要有一个集合来保存一系列的 key 和 value。Hash 集合类型可以快速存取一系列的 key 和 value，正好可以用来记录一系列车辆 ID 和经纬度的对应关系，所以，我们可以把不同车辆的 ID 和它们对应的经纬度信息存在 Hash 集合中，如下图所示：</strong></p>
<p><img src="/2024/09/16/Redis13/c8d3f1951874da0d916ed51ccdce9e0e.jpg" alt="img"></p>
<p>同时，Hash 类型的 HSET 操作命令，会根据 key 来设置相应的 value 值，所以，我们可以用它来快速地更新车辆变化的经纬度信息。</p>
<p>到这里，Hash 类型看起来是一个不错的选择。<strong>但问题是，对于一个 LBS 应用来说，除了记录经纬度信息，还需要根据用户的经纬度信息在车辆的 Hash 集合中进行范围查询。一旦涉及到范围查询，就意味着集合中的元素需要有序，但 Hash 类型的元素是无序的，显然不能满足我们的要求。</strong></p>
<p>我们再来看看使用 <strong>Sorted Set 类型</strong>是不是合适。</p>
<p><strong>Sorted Set 类型也支持一个 key 对应一个 value 的记录模式，其中，key 就是 Sorted Set 中的元素，而 value 则是元素的权重分数。更重要的是，Sorted Set 可以根据元素的权重分数排序，支持范围查询。这就能满足 LBS 服务中查找相邻位置的需求了。</strong></p>
<p>实际上，<strong>GEO 类型的底层数据结构就是用 Sorted Set 来实现的。咱们还是借着叫车应用的例子来加深下理解。</strong></p>
<p>用 Sorted Set 来保存车辆的经纬度信息时，<strong>Sorted Set 的元素是车辆 ID，元素的权重分数是经纬度信息，如下图所示：</strong></p>
<p><img src="/2024/09/16/Redis13/a9a6bc78ea3bb652ef1404020dd2934e.jpg" alt="img"></p>
<p>这时问题来了，<strong>Sorted Set 元素的权重分数是一个浮点数（float 类型），而一组经纬度包含的是经度和纬度两个值，是没法直接保存为一个浮点数的，那具体该怎么进行保存呢？</strong></p>
<p><strong>这就要用到 GEO 类型中的 GeoHash 编码了。</strong></p>
<h3 id="GeoHash-的编码方法"><a href="#GeoHash-的编码方法" class="headerlink" title="GeoHash 的编码方法"></a>GeoHash 的编码方法</h3><p><strong>为了能高效地对经纬度进行比较，Redis 采用了业界广泛使用的 GeoHash 编码方法，这个方法的基本原理就是“二分区间，区间编码”。</strong></p>
<p>当我们要对一组经纬度进行 GeoHash 编码时，<strong>我们要先对经度和纬度分别编码，然后再把经纬度各自的编码组合成一个最终编码。</strong></p>
<p>首先，我们来看下经度和纬度的单独编码过程。</p>
<p><strong>对于一个地理位置信息来说，它的经度范围是[-180,180]。GeoHash 编码会把一个经度值编码成一个 N 位的二进制值，我们来对经度范围[-180,180]做 N 次的二分区操作，其中 N 可以自定义。</strong></p>
<p><strong>在进行第一次二分区时，经度范围[-180,180]会被分成两个子区间：[-180,0) 和[0,180]（我称之为左、右分区）。此时，我们可以查看一下要编码的经度值落在了左分区还是右分区。如果是落在左分区，我们就用 0 表示；如果落在右分区，就用 1 表示。这样一来，每做完一次二分区，我们就可以得到 1 位编码值。</strong></p>
<p><strong>然后，我们再对经度值所属的分区再做一次二分区，同时再次查看经度值落在了二分区后的左分区还是右分区，按照刚才的规则再做 1 位编码。当做完 N 次的二分区后，经度值就可以用一个 N bit 的数来表示了。</strong></p>
<p><strong>举个例子，假设我们要编码的经度值是 116.37，我们用 5 位编码值（也就是 N&#x3D;5，做 5 次分区）。</strong></p>
<p>我们先做第一次二分区操作，把经度区间[-180,180]分成了左分区[-180,0) 和右分区[0,180]，此时，经度值 116.37 是属于右分区[0,180]，所以，<strong>我们用 1 表示第一次二分区后的编码值。</strong></p>
<p>接下来，我们做第二次二分区：把经度值 116.37 所属的[0,180]区间，分成[0,90) 和[90, 180]。此时，经度值 116.37 还是属于右分区[90,180]，<strong>所以，第二次分区后的编码值仍然为 1</strong>。等到第三次对[90,180]进行二分区，经度值 116.37 落在了分区后的左分区[90, 135) 中，所以，<strong>第三次分区后的编码值就是 0。</strong></p>
<p>按照这种方法，做完 5 次分区后，我们把经度值 116.37 定位在[112.5, 123.75]这个区间，并且得到了经度值的 5 位编码值，即 <strong>11010</strong>。这个编码过程如下表所示：</p>
<p><img src="/2024/09/16/Redis13/3cb007yy63c820d6dd2e4999608683f2.jpg" alt="img"></p>
<p>对纬度的编码方式，<strong>和对经度的一样，只是纬度的范围是[-90，90]，下面这张表显示了对纬度值 39.86 的编码过程。</strong></p>
<p><img src="/2024/09/16/Redis13/65f41469866cb94963b4c9afbf2b016d.jpg" alt="img"></p>
<p><strong>当一组经纬度值都编完码后，我们再把它们的各自编码值组合在一起，组合的规则是：最终编码值的偶数位上依次是经度的编码值，奇数位上依次是纬度的编码值，其中，偶数位从 0 开始，奇数位从 1 开始。</strong></p>
<p><strong>我们刚刚计算的经纬度（116.37，39.86）的各自编码值是 11010 和 10111，组合之后，第 0 位是经度的第 0 位 1，第 1 位是纬度的第 0 位 1，第 2 位是经度的第 1 位 1，第 3 位是纬度的第 1 位 0，以此类推，就能得到最终编码值 1110011101，如下图所示：</strong></p>
<p><img src="/2024/09/16/Redis13/4a8296e841f18ed4f3a554703ebd5887.jpg" alt="img"></p>
<p><strong>用了 GeoHash 编码后，原来无法用一个权重分数表示的一组经纬度（116.37，39.86）就可以用 1110011101 这一个值来表示，就可以保存为 Sorted Set 的权重分数了。</strong></p>
<p>当然，使用 GeoHash 编码后，我们相当于把整个地理空间划分成了一个个方格，每个方格对应了 GeoHash 中的一个分区。</p>
<p>举个例子。我们把经度区间[-180,180]做一次二分区，把纬度区间[-90,90]做一次二分区，就会得到 4 个分区。我们来看下它们的经度和纬度范围以及对应的 GeoHash 组合编码。</p>
<p>分区一：[-180,0) 和[-90,0)，编码 00；</p>
<p>分区二：[-180,0) 和[0,90]，编码 01；</p>
<p>分区三：[0,180]和[-90,0)，编码 10；</p>
<p>分区四：[0,180]和[0,90]，编码 11。</p>
<p><strong>这 4 个分区对应了 4 个方格，每个方格覆盖了一定范围内的经纬度值，分区越多，每个方格能覆盖到的地理空间就越小，也就越精准。我们把所有方格的编码值映射到一维空间时，相邻方格的 GeoHash 编码值基本也是接近的，</strong>如下图所示：</p>
<p><img src="/2024/09/16/Redis13/2a2a650086acf9700c0603a4be8ceb74.jpg" alt="img"></p>
<p><strong>所以，我们使用 Sorted Set 范围查询得到的相近编码值，在实际的地理空间上，也是相邻的方格，这就可以实现 LBS 应用“搜索附近的人或物”的功能了。</strong></p>
<p>不过，我要提醒你一句，<strong>有的编码值虽然在大小上接近，但实际对应的方格却距离比较远。例如，我们用 4 位来做 GeoHash 编码，把经度区间[-180,180]和纬度区间[-90,90]各分成了 4 个分区，一共 16 个分区，对应了 16 个方格。编码值为 0111 和 1000 的两个方格就离得比较远，如下图所示：</strong></p>
<p><img src="/2024/09/16/Redis13/0d64c9765ab72a50abef16a0275bc0ba.jpg" alt="img"></p>
<p><strong>所以，为了避免查询不准确问题，我们可以同时查询给定经纬度所在的方格周围的 4 个或 8 个方格。</strong></p>
<p>好了，到这里，我们就知道了，<strong>GEO 类型是把经纬度所在的区间编码作为 Sorted Set 中元素的权重分数</strong>，<strong>把和经纬度相关的车辆 ID 作为 Sorted Set 中元素本身的值保存下来，这样相邻经纬度的查询就可以通过编码值的大小范围查询来实现了。接下来，我们再来聊聊具体如何操作 GEO 类型。</strong></p>
<h3 id="如何操作-GEO-类型？"><a href="#如何操作-GEO-类型？" class="headerlink" title="如何操作 GEO 类型？"></a>如何操作 GEO 类型？</h3><p>在使用 GEO 类型时，我们经常会用到两个命令，分别是 GEOADD 和 GEORADIUS。</p>
<ul>
<li>GEOADD 命令：<strong>用于把一组经纬度信息和相对应的一个 ID 记录到 GEO 类型集合中；</strong></li>
<li>GEORADIUS 命令：<strong>会根据输入的经纬度位置，查找以这个经纬度为中心的一定范围内的其他元素。当然，我们可以自己定义这个范围。</strong></li>
</ul>
<p>我还是以叫车应用的车辆匹配场景为例，介绍下具体如何使用这两个命令。</p>
<p>假设车辆 ID 是 33，经纬度位置是（116.034579，39.030452），<strong>我们可以用一个 GEO 集合保存所有车辆的经纬度，集合 key 是 cars:locations。执行下面的这个命令，就可以把 ID 号为 33 的车辆的当前经纬度位置存入 GEO 集合中：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GEOADD cars:locations 116.034579 39.030452 33</span><br></pre></td></tr></table></figure>

<p>当用户想要寻找自己附近的网约车时，LBS 应用就可以使用 GEORADIUS 命令。</p>
<p><strong>例如，LBS 应用执行下面的命令时，Redis 会根据输入的用户的经纬度信息（116.054579，39.030452 ），查找以这个经纬度为中心的 5 公里内的车辆信息，并返回给 LBS 应用。当然， 你可以修改“5”这个参数，来返回更大或更小范围内的车辆信息。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GEORADIUS cars:locations 116.054579 39.030452 5 km ASC COUNT 10</span><br></pre></td></tr></table></figure>

<p>另外，我们还可以进一步限定返回的车辆信息。</p>
<p><strong>比如，我们可以使用 ASC 选项，让返回的车辆信息按照距离这个中心位置从近到远的方式来排序，以方便选择最近的车辆</strong>；<strong>还可以使用 COUNT 选项，指定返回的车辆信息的数量</strong>。<strong>毕竟，5 公里范围内的车辆可能有很多，如果返回全部信息，会占用比较多的数据带宽，这个选项可以帮助控制返回的数据量，节省带宽。</strong></p>
<p>可以看到，<strong>使用 GEO 数据类型可以非常轻松地操作经纬度这种信息。</strong></p>
<p><strong>虽然我们有了 5 种基本类型和 3 种扩展数据类型，但是有些场景下，我们对数据类型会有特殊需求，例如，我们需要一个数据类型既能像 Hash 那样支持快速的单键查询，又能像 Sorted Set 那样支持范围查询，此时，我们之前学习的这些数据类型就无法满足需求了。</strong>那么，接下来，我就再向你介绍下 Redis 扩展数据类型的终极版——<strong>自定义的数据类型</strong>。这样，你就可以定制符合自己需求的数据类型了，不管你的应用场景怎么变化，你都不用担心没有合适的数据类型。</p>
<h2 id="如何自定义数据类型？"><a href="#如何自定义数据类型？" class="headerlink" title="如何自定义数据类型？"></a>如何自定义数据类型？</h2><p><strong>为了实现自定义数据类型，首先，我们需要了解 Redis 的基本对象结构 RedisObject，因为 Redis 键值对中的每一个值都是用 RedisObject 保存的。</strong></p>
<p>我在第 11 讲中说过，<strong>RedisObject 包括元数据和指针。其中，元数据的一个功能就是用来区分不同的数据类型，指针用来指向具体的数据类型的值。所以，要想开发新数据类型，我们就先来了解下 RedisObject 的元数据和指针。</strong></p>
<h3 id="Redis-的基本对象结构"><a href="#Redis-的基本对象结构" class="headerlink" title="Redis 的基本对象结构"></a>Redis 的基本对象结构</h3><p><strong>RedisObject 的内部组成包括了 type、encoding、lru 和 refcount 4 个元数据，以及 1 个*ptr指针。</strong></p>
<ul>
<li>type：<strong>表示值的类型，涵盖了我们前面学习的五大基本类型；</strong></li>
<li>encoding：<strong>是值的编码方式，用来表示 Redis 中实现各个基本类型的底层数据结构，例如 SDS、压缩列表、哈希表、跳表等；</strong></li>
<li>lru：<strong>记录了这个对象最后一次被访问的时间，用于淘汰过期的键值对；</strong></li>
<li>refcount：<strong>记录了对象的引用计数；</strong></li>
<li><em>ptr：*<em>是指向数据的指针。</em></em></li>
</ul>
<p><img src="/2024/09/16/Redis13/05c2d546e507d8a863c002e2173c71af.jpg" alt="img"></p>
<p>RedisObject 结构借助<code>*ptr</code>指针，就可以指向不同的数据类型<strong>，例如，<code>*ptr</code>指向一个 SDS 或一个跳表，就表示键值对中的值是 String 类型或 Sorted Set 类型</strong>。<strong>所以，我们在定义了新的数据类型后，也只要在 RedisObject 中设置好新类型的 type 和 encoding，再用*ptr指向新类型的实现，就行了</strong>。</p>
<h3 id="开发一个新的数据类型"><a href="#开发一个新的数据类型" class="headerlink" title="开发一个新的数据类型"></a>开发一个新的数据类型</h3><p>了解了 RedisObject 结构后，定义一个新的数据类型也就不难了。<strong>首先，我们需要为新数据类型定义好它的底层结构、type 和 encoding 属性值，然后再实现新数据类型的创建、释放函数和基本命令。</strong></p>
<p>接下来，我以开发一个名字叫作 NewTypeObject 的新数据类型为例，来解释下具体的 4 个操作步骤。</p>
<p><img src="/2024/09/16/Redis13/88702464f8bc80ea11b26ab157926199.jpg" alt="img"></p>
<p><strong>第一步：定义新数据类型的底层结构</strong></p>
<p>我们用 newtype.h 文件来保存这个新类型的定义，具体定义的代码如下所示：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">NewTypeObject</span> &#123;</span></span><br><span class="line"></span><br><span class="line">     <span class="class"><span class="keyword">struct</span> <span class="title">NewTypeNode</span> *<span class="title">head</span>;</span> </span><br><span class="line"></span><br><span class="line">     <span class="type">size_t</span> len; </span><br><span class="line"></span><br><span class="line">&#125;NewTypeObject;</span><br></pre></td></tr></table></figure>

<p>其中，NewTypeNode 结构就是我们自定义的新类c型的底层结构。我们为底层结构设计两个成员变量：一个是 Long 类型的 value 值，用来保存实际数据；一个是*next指针，指向下一个 NewTypeNode 结构。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">NewTypeNode</span> &#123;</span></span><br><span class="line"></span><br><span class="line">     <span class="type">long</span> value;</span><br><span class="line"></span><br><span class="line">     <span class="class"><span class="keyword">struct</span> <span class="title">NewTypeNode</span> *<span class="title">next</span>;</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>从代码中可以看到，NewTypeObject 类型的底层结构其实就是一个 Long 类型的单向链表。当然，你还可以根据自己的需求，把 NewTypeObject 的底层结构定义为其他类型。例如，如果我们想要 NewTypeObject 的查询效率比链表高，就可以把它的底层结构设计成一颗 B+ 树。</p>
<p><strong>第二步：在 RedisObject 的 type 属性中，增加这个新类型的定义</strong></p>
<p>这个定义是在 Redis 的 server.h 文件中。比如，我们增加一个叫作 OBJ_NEWTYPE 的宏定义，用来在代码中指代 NewTypeObject 这个新类型。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">\<span class="meta">#<span class="keyword">define</span> OBJ_STRING 0    <span class="comment">/* String object. */</span></span></span><br><span class="line"></span><br><span class="line">\<span class="meta">#<span class="keyword">define</span> OBJ_LIST 1      <span class="comment">/* List object. */</span></span></span><br><span class="line"></span><br><span class="line">\<span class="meta">#<span class="keyword">define</span> OBJ_SET 2       <span class="comment">/* Set object. */</span></span></span><br><span class="line"></span><br><span class="line">\<span class="meta">#<span class="keyword">define</span> OBJ_ZSET 3      <span class="comment">/* Sorted set object. */</span></span></span><br><span class="line"></span><br><span class="line">…</span><br><span class="line"></span><br><span class="line">\<span class="meta">#<span class="keyword">define</span> OBJ_NEWTYPE 7</span></span><br></pre></td></tr></table></figure>

<p><strong>第三步：开发新类型的创建和释放函数</strong></p>
<p>Redis 把数据类型的创建和释放函数都定义在了 object.c 文件中。所以，我们可以在这个文件中增加 NewTypeObject 的创建函数 createNewTypeObject，如下所示：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">robj *<span class="title function_">createNewTypeObject</span><span class="params">(<span class="type">void</span>)</span>&#123;</span><br><span class="line"></span><br><span class="line">   NewTypeObject *h = newtypeNew(); </span><br><span class="line"></span><br><span class="line">   robj *o = createObject(OBJ_NEWTYPE,h);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> o;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>createNewTypeObject 分别调用了 newtypeNew 和 createObject 两个函数，我分别来介绍下。</p>
<p>先说 newtypeNew 函数。它是用来为新数据类型初始化内存结构的。这个初始化过程主要是用 zmalloc 做底层结构分配空间，以便写入数据。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">NewTypeObject *<span class="title function_">newtypeNew</span><span class="params">(<span class="type">void</span>)</span>&#123;</span><br><span class="line"></span><br><span class="line">     NewTypeObject *n = zmalloc(<span class="keyword">sizeof</span>(*n));</span><br><span class="line"></span><br><span class="line">     n-&gt;head = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">     n-&gt;len = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> n;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>newtypeNew 函数涉及到新数据类型的具体创建，而 Redis 默认会为每个数据类型定义一个单独文件，实现这个类型的创建和命令操作，例如，t_string.c 和 t_list.c 分别对应 String 和 List 类型。按照 Redis 的惯例，我们就把 newtypeNew 函数定义在名为 t_newtype.c 的文件中。</p>
<p>createObject 是 Redis 本身提供的 RedisObject 创建函数，它的参数是数据类型的 type 和指向数据类型实现的指针*ptr。</p>
<p>我们给 createObject 函数中传入了两个参数，分别是新类型的 type 值 OBJ_NEWTYPE，以及指向一个初始化过的 NewTypeObjec 的指针。这样一来，创建的 RedisObject 就能指向我们自定义的新数据类型了。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">robj *<span class="title function_">createObject</span><span class="params">(<span class="type">int</span> type, <span class="type">void</span> *ptr)</span> &#123;</span><br><span class="line"></span><br><span class="line">     robj *o = zmalloc(<span class="keyword">sizeof</span>(*o));</span><br><span class="line"></span><br><span class="line">     o-&gt;type = type;</span><br><span class="line"></span><br><span class="line">     o-&gt;ptr = ptr;</span><br><span class="line"></span><br><span class="line">     ...</span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> o;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于释放函数来说，它是创建函数的反过程，是用 zfree 命令把新结构的内存空间释放掉。</p>
<p><strong>第四步：开发新类型的命令操作</strong></p>
<p>简单来说，增加相应的命令操作的过程可以分成三小步：</p>
<p>\1. 在 t_newtype.c 文件中增加命令操作的实现。比如说，我们定义 ntinsertCommand 函数，由它实现对 NewTypeObject 单向链表的插入操作：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">ntinsertCommand</span><span class="params">(client *c)</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//基于客户端传递的参数，实现在NewTypeObject链表头插入元素</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>\2. 在 server.h 文件中，声明我们已经实现的命令，以便在 server.c 文件引用这个命令，例如：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">ntinsertCommand</span><span class="params">(client *c)</span></span><br></pre></td></tr></table></figure>

<p>\3. 在 server.c 文件中的 redisCommandTable 里面，把新增命令和实现函数关联起来。例如，新增的 ntinsert 命令由 ntinsertCommand 函数实现，我们就可以用 ntinsert 命令给 NewTypeObject 数据类型插入元素了。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">redisCommand</span> <span class="title">redisCommandTable</span>[] =</span> &#123; </span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&quot;ntinsert&quot;</span>,ntinsertCommand,<span class="number">2</span>,<span class="string">&quot;m&quot;</span>,...&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>此时，我们就完成了一个自定义的 NewTypeObject 数据类型，可以实现基本的命令操作了。当然，如果你还希望新的数据类型能被持久化保存，我们还需要在 Redis 的 RDB 和 AOF 模块中增加对新数据类型进行持久化保存的代码，我会在后面的加餐中再和你分享。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><strong>这节课，我们学习了 Redis 的扩展数据类型 GEO。GEO 可以记录经纬度形式的地理位置信息，被广泛地应用在 LBS 服务中。GEO 本身并没有设计新的底层数据结构，而是直接使用了 Sorted Set 集合类型。</strong></p>
<p><strong>GEO 类型使用 GeoHash 编码方法实现了经纬度到 Sorted Set 中元素权重分数的转换，这其中的两个关键机制就是对二维地图做区间划分，以及对区间进行编码。</strong>一组经纬度落在某个区间后，<strong>就用区间的编码值来表示，并把编码值作为 Sorted Set 元素的权重分数</strong>。这样一来，我<strong>们就可以把经纬度保存到 Sorted Set 中，利用 Sorted Set 提供的“按权重进行有序范围查找”的特性</strong>，实现 LBS 服务中频繁使用的“搜索附近”的需求。</p>
<p><strong>GEO 属于 Redis 提供的扩展数据类型。扩展数据类型有两种实现途径：一种是基于现有的数据类型，通过数据编码或是实现新的操作的方式，来实现扩展数据类型，例如基于 Sorted Set 和 GeoHash 编码实现 GEO，以及基于 String 和位操作实现 Bitmap；另一种就是开发自定义的数据类型，具体的操作是增加新数据类型的定义，实现创建和释放函数，实现新数据类型支持的命令操作，建议你尝试着把今天学到的内容灵活地应用到你的工作场景中。</strong></p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>到今天为止，<strong>我们已经学习 Redis 的 5 大基本数据类型和 3 个扩展数据类型</strong>，我想请你来聊一聊，你在日常的实践过程中，还用过 Redis 的其他数据类型吗？</p>
<p><strong>Redis也可以使用List数据类型当做队列使用，一个客户端使用rpush生产数据到Redis中，另一个客户端使用lpop取出数据进行消费，非常方便。</strong>但要注意的是，使用List当做队列，<strong>缺点是没有ack机制和不支持多个消费者</strong>。<strong>没有ack机制会导致从Redis中取出的数据后，如果客户端处理失败了，取出的这个数据相当于丢失了，无法重新消费。所以使用List用作队列适合于对于丢失数据不敏感的业务场景，但它的优点是，因为都是内存操作，所以非常快和轻量。</strong></p>
<p><strong>而Redis提供的PubSub，可以支持多个消费者进行消费，生产者发布一条消息，多个消费者同时订阅消费</strong>。<strong>但是它的缺点是，如果任意一个消费者挂了，等恢复过来后，在这期间的生产者的数据就丢失了。PubSub只把数据发给在线的消费者，消费者一旦下线，就会丢弃数据。另一个缺点是，PubSub中的数据不支持数据持久化，当Redis宕机恢复后，其他类型的数据都可以从RDB和AOF中恢复回来，但PubSub不行，它就是简单的基于内存的多播机制。</strong></p>
<p><strong>之后Redis 5.0推出了Stream数据结构，它借鉴了Kafka的设计思想，弥补了List和PubSub的不足</strong>。<strong>Stream类型数据可以持久化、支持ack机制、支持多个消费者、支持回溯消费，基本上实现了队列中间件大部分功能，比List和PubSub更可靠。</strong></p>
<p><strong>另一个经常使用的是基于Redis实现的布隆过滤器</strong>，<strong>其底层实现利用的是String数据结构和位运算，可以解决业务层缓存穿透的问题，而且内存占用非常小，操作非常高效。</strong></p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/bgeogc">数据结构 - 13 | GEO是什么？还可以定义新的数据类型吗？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>有一亿个keys要统计，应该用哪种集合？</title>
    <url>/2024/09/16/Redis12/</url>
    <content><![CDATA[<p>在 Web 和移动应用的业务场景中，我们经常需要保存这样一种信息：<strong>一个 key 对应了一个数据集合</strong>。我举几个例子。</p>
<ul>
<li>手机 App 中的每天的用户登录信息：<strong>一天对应一系列用户 ID 或移动设备 ID；</strong></li>
<li>电商网站上商品的用户评论列表：<strong>一个商品对应了一系列的评论；</strong></li>
<li>用户在手机 App 上的签到打卡信息：<strong>一天对应一系列用户的签到记录；</strong></li>
<li>应用网站上的网页访问信息：<strong>一个网页对应一系列的访问点击。</strong></li>
</ul>
<p>我们知道，<strong>Redis 集合类型的特点就是一个键对应一系列的数据，所以非常适合用来存取这些数据。但是，在这些场景中，除了记录信息，我们往往还需要对集合中的数据进行统计，例如：</strong></p>
<ul>
<li>在移动应用中，<strong>需要统计每天的新增用户数和第二天的留存用户数；</strong></li>
<li>在电商网站的商品评论中，<strong>需要统计评论列表中的最新评论；</strong></li>
<li>在签到打卡中，<strong>需要统计一个月内连续打卡的用户数；</strong></li>
<li>在网页访问记录中，<strong>需要统计独立访客（Unique Visitor，UV）量。</strong></li>
</ul>
<p>通常情况下，我们面临的用户数量以及访问量都是巨大的，比如百万、千万级别的用户数量，或者千万级别、甚至亿级别的访问信息。<strong>所以，我们必须要选择能够非常高效地统计大量数据（例如亿级）的集合类型。</strong></p>
<p><strong>要想选择合适的集合，我们就得了解常用的集合统计模式。</strong>这节课，我就给你介绍集合类型常见的四种统计模式，<strong>包括聚合统计、排序统计、二值状态统计和基数统计</strong>。我会以刚刚提到的这四个场景为例，<strong>和你聊聊在这些统计模式下，什么集合类型能够更快速地完成统计，而且还节省内存空间</strong>。掌握了今天的内容，<strong>之后再遇到集合元素统计问题时，你就能很快地选出合适的集合类型了</strong>。</p>
<h2 id="聚合统计"><a href="#聚合统计" class="headerlink" title="聚合统计"></a>聚合统计</h2><p>我们先来看集合元素统计的第一个场景：聚合统计。</p>
<p><strong>所谓的聚合统计，就是指统计多个集合元素的聚合结果，包括：统计多个集合的共有元素（交集统计）；把两个集合相比，统计其中一个集合独有的元素（差集统计）；统计多个集合的所有元素（并集统计）。</strong></p>
<p>在刚才提到的场景中，统计手机 App 每天的新增用户数和第二天的留存用户数，正好对应了聚合统计。</p>
<p><strong>要完成这个统计任务，我们可以用一个集合记录所有登录过 App 的用户 ID，同时，用另一个集合记录每一天登录过 App 的用户 ID。然后，再对这两个集合做聚合统计。我们来看下具体的操作。</strong></p>
<p><strong>记录所有登录过 App 的用户 ID 还是比较简单的，我们可以直接使用 Set 类型，把 key 设置为 user:id，表示记录的是用户 ID，value 就是一个 Set 集合，里面是所有登录过 App 的用户 ID，我们可以把这个 Set 叫作累计用户 Set</strong>，如下图所示：</p>
<p><img src="https://static001.geekbang.org/resource/image/99/ca/990e56babf199d9a7fa4c7343167ecca.jpg" alt="img"></p>
<p>需要注意的是，<strong>累计用户 Set 中没有日期信息，我们是不能直接统计每天的新增用户的。所以，我们还需要把每一天登录的用户 ID，记录到一个新集合中，我们把这个集合叫作每日用户 Set，它有两个特点：</strong></p>
<p>key 是 user:id 以及当天日期，例如 user<code>:id:</code>20200803；</p>
<p>value 是 Set 集合，记录当天登录的用户 ID。</p>
<p><img src="/2024/09/16/Redis12/a63dd95d5e44bf538fe960e67761b59e.jpg" alt="img"></p>
<p><strong>在统计每天的新增用户时，我们只用计算每日用户 Set 和累计用户 Set 的差集就行。</strong></p>
<p>我借助一个具体的例子来解释一下。</p>
<p>假设我们的手机 App 在 2020 年 8 月 3 日上线，那么，8 月 3 日前是没有用户的。<strong>此时，累计用户 Set 是空集，当天登录的用户 ID 会被记录到 key 为 user<code>:id:</code>20200803 的 Set 中。所以，user<code>:id:</code>20200803 这个 Set 中的用户就是当天的新增用户。</strong></p>
<p>然后，<strong>我们计算累计用户 Set 和 user<code>:id:</code>20200803  Set 的并集结果，结果保存在 user:id 这个累计用户 Set 中</strong>，如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SUNIONSTORE  user:<span class="built_in">id</span>  user:<span class="built_in">id</span>  user`:<span class="built_in">id</span>:`20200803 </span><br></pre></td></tr></table></figure>

<p><strong>此时，user:id 这个累计用户 Set 中就有了 8 月 3 日的用户 ID。等到 8 月 4 日再统计时，我们把 8 月 4 日登录的用户 ID 记录到 user<code>:id:</code>20200804 的 Set 中。接下来，我们执行 SDIFFSTORE 命令计算累计用户 Set 和 user<code>:id:</code>20200804 Set 的差集，结果保存在 key 为 user:new 的 Set 中，如下所示：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SDIFFSTORE  user:new  user`:<span class="built_in">id</span>:`20200804 user:<span class="built_in">id</span>  </span><br></pre></td></tr></table></figure>

<p>可以看到，<strong>这个差集中的用户 ID 在 user<code>:id:</code>20200804 的 Set 中存在，但是不在累计用户 Set 中。所以，user:new 这个 Set 中记录的就是 8 月 4 日的新增用户。</strong></p>
<p>当要计算 8 月 4 日的留存用户时，<strong>我们只需要再计算 user<code>:id:</code>20200803 和 user<code>:id:</code>20200804 两个 Set 的交集，就可以得到同时在这两个集合中的用户 ID 了，这些就是在 8 月 3 日登录，并且在 8 月 4 日留存的用户</strong>。执行的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SINTERSTORE user`:<span class="built_in">id</span>:`rem user`:<span class="built_in">id</span>:`20200803 user`:<span class="built_in">id</span>:`20200804</span><br></pre></td></tr></table></figure>

<p><strong>当你需要对多个集合进行聚合计算时，Set 类型会是一个非常不错的选择。</strong>不过，我要提醒你一下，这里有一个潜在的风险。</p>
<p><strong>Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下</strong>，如果直接执行这些计算，会导致 Redis 实例阻塞。所以，我给你分享一个小建议：<strong>你可以从主从集群中选择一个从库，让它专门负责聚合计算，或者是把数据读取到客户端，在客户端来完成聚合统计</strong>，这样就可以<strong>规避阻塞主库实例和其他从库实例的风险了。</strong></p>
<h2 id="排序统计"><a href="#排序统计" class="headerlink" title="排序统计"></a>排序统计</h2><p>接下来，<strong>我们再来聊一聊应对集合元素排序需求的方法。我以在电商网站上提供最新评论列表的场景为例</strong>，进行讲解。</p>
<p><strong>最新评论列表包含了所有评论中的最新留言</strong>，<strong>这就要求集合类型能对元素保序</strong>，也就是说，<strong>集合中的元素可以按序排列，这种对元素保序的集合类型叫作有序集合。</strong></p>
<p><strong>在 Redis 常用的 4 个集合类型中（List、Hash、Set、Sorted Set），List 和 Sorted Set 就属于有序集合。</strong></p>
<p><strong>List 是按照元素进入 List 的顺序进行排序的，而 Sorted Set 可以根据元素的权重来排序</strong>，<strong>我们可以自己来决定每个元素的权重值。比如说，我们可以根据元素插入 Sorted Set 的时间确定权重值，先插入的元素权重小，后插入的元素权重大。</strong></p>
<p>看起来好像都可以满足需求，我们该怎么选择呢？</p>
<p><strong>我先说说用 List 的情况。每个商品对应一个 List，这个 List 包含了对这个商品的所有评论，而且会按照评论时间保存这些评论，每来一个新评论，就用 LPUSH 命令把它插入 List 的队头。</strong></p>
<p><strong>在只有一页评论的时候，我们可以很清晰地看到最新的评论，但是，在实际应用中，网站一般会分页显示最新的评论列表，一旦涉及到分页操作，List 就可能会出现问题了。</strong></p>
<p>假设当前的评论 List 是{A, B, C, D, E, F}（其中，A 是最新的评论，以此类推，F 是最早的评论），在展示第一页的 3 个评论时，我们可以用下面的命令，得到最新的三条评论 A、B、C：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">LRANGE product1 0 2</span><br><span class="line"></span><br><span class="line">1) <span class="string">&quot;A&quot;</span></span><br><span class="line"></span><br><span class="line">2) <span class="string">&quot;B&quot;</span></span><br><span class="line"></span><br><span class="line">3) <span class="string">&quot;C&quot;</span></span><br></pre></td></tr></table></figure>

<p>然后，再用下面的命令获取第二页的 3 个评论，也就是 D、E、F。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">LRANGE product1 3 5</span><br><span class="line"></span><br><span class="line">1) <span class="string">&quot;D&quot;</span></span><br><span class="line"></span><br><span class="line">2) <span class="string">&quot;E&quot;</span></span><br><span class="line"></span><br><span class="line">3) <span class="string">&quot;F&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>但是，如果在展示第二页前，又产生了一个新评论 G，评论 G 就会被 LPUSH 命令插入到评论 List 的队头，评论 List 就变成了{G, A, B, C, D, E, F}。此时，再用刚才的命令获取第二页评论时，就会发现，评论 C 又被展示出来了，也就是 C、D、E。</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">LRANGE product1 3 5</span><br><span class="line"></span><br><span class="line">1) <span class="string">&quot;C&quot;</span></span><br><span class="line"></span><br><span class="line">2) <span class="string">&quot;D&quot;</span></span><br><span class="line"></span><br><span class="line">3) <span class="string">&quot;E&quot;</span></span><br></pre></td></tr></table></figure>

<p>之所以会这样，关键原因就在于，<strong>List 是通过元素在 List 中的位置来排序的，当有一个新元素插入时，原先的元素在 List 中的位置都后移了一位，比如说原来在第 1 位的元素现在排在了第 2 位。所以，对比新元素插入前后，List 相同位置上的元素就会发生变化，用 LRANGE 读取时，就会读到旧元素。（这个问题我之前写项目的时候遇到过）</strong></p>
<p>和 List 相比，<strong>Sorted Set 就不存在这个问题，因为它是根据元素的实际权重来排序和获取数据的。</strong></p>
<p><strong>我们可以按评论时间的先后给每条评论设置一个权重值，然后再把评论保存到 Sorted Set 中。Sorted Set 的 ZRANGEBYSCORE 命令就可以按权重排序后返回元素。这样的话，即使集合中的元素频繁更新，Sorted Set 也能通过 ZRANGEBYSCORE 命令准确地获取到按序排列的数据。</strong></p>
<p>假设越新的评论权重越大，<strong>目前最新评论的权重是 N，我们执行下面的命令时，就可以获得最新的 10 条评论：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ZRANGEBYSCORE comments N-9 N</span><br></pre></td></tr></table></figure>

<p><strong>所以，在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用 Sorted Set。</strong></p>
<h2 id="二值状态统计"><a href="#二值状态统计" class="headerlink" title="二值状态统计"></a>二值状态统计</h2><p>现在，我们再来分析下第三个场景：<strong>二值状态统计。这里的二值状态就是指集合元素的取值就只有 0 和 1 两种。在签到打卡的场景中，我们只用记录签到（1）或未签到（0），所以它就是非常典型的二值状态。</strong></p>
<p>在签到统计时，<strong>每个用户一天的签到用 1 个 bit 位就能表示，一个月（假设是 31 天）的签到情况用 31 个 bit 位就可以，而一年的签到也只需要用 365 个 bit 位，根本不用太复杂的集合类型。这个时候，我们就可以选择 Bitmap</strong>。这是 Redis 提供的扩展数据类型。我来给你解释一下它的实现原理。</p>
<p><strong>Bitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。String 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的每个 bit 位利用起来，用来表示一个元素的二值状态。你可以把 Bitmap 看作是一个 bit 数组。</strong></p>
<p><strong>Bitmap 提供了 GETBIT&#x2F;SETBIT 操作，使用一个偏移值 offset 对 bit 数组的某一个 bit 位进行读和写。不过，需要注意的是，Bitmap 的偏移量是从 0 开始算的，也就是说 offset 的最小值是 0。当使用 SETBIT 对一个 bit 位进行写操作时，这个 bit 位会被设置为 1。Bitmap 还提供了 BITCOUNT 操作，用来统计这个 bit 数组中所有“1”的个数。</strong></p>
<p>那么，具体该怎么用 Bitmap 进行签到统计呢？我还是借助一个具体的例子来说明。</p>
<p>假设我们要统计 ID 3000 的用户在 2020 年 8 月份的签到情况，就可以按照下面的步骤进行操作。</p>
<p>第一步，执行下面的命令，记录该用户 8 月 3 号已签到。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SETBIT uid:sign:3000:202008 2 1 </span><br></pre></td></tr></table></figure>

<p>第二步，检查该用户 8 月 3 日是否签到。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GETBIT uid:sign:3000:202008 2 </span><br></pre></td></tr></table></figure>

<p>第三步，统计该用户在 8 月份的签到次数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">BITCOUNT uid:sign:3000:202008</span><br></pre></td></tr></table></figure>

<p><strong>这样，我们就知道该用户在 8 月份的签到情况了，是不是很简单呢？接下来，你可以再思考一个问题：如果记录了 1 亿个用户 10 天的签到情况，你有办法统计出这 10 天连续签到的用户总数吗？</strong></p>
<p><strong>在介绍具体的方法之前，我们要先知道，Bitmap 支持用 BITOP 命令对多个 Bitmap 按位做“与”“或”“异或”的操作，操作的结果会保存到一个新的 Bitmap 中。</strong></p>
<p><strong>我以按位“与”操作为例来具体解释一下。从下图中，可以看到，三个 Bitmap bm1、bm2 和 bm3，对应 bit 位做“与”操作，结果保存到了一个新的 Bitmap 中（示例中，这个结果 Bitmap 的 key 被设为“resmap”）。</strong></p>
<p><img src="/2024/09/16/Redis12/4151af42513cf5f7996fe86c6064f97a.jpg" alt="img"></p>
<p>回到刚刚的问题，<strong>在统计 1 亿个用户连续 10 天的签到情况时，你可以把每天的日期作为 key，每个 key 对应一个 1 亿位的 Bitmap，每一个 bit 对应一个用户当天的签到情况。</strong></p>
<p><strong>接下来，我们对 10 个 Bitmap 做“与”操作，得到的结果也是一个 Bitmap。在这个 Bitmap 中，只有 10 天都签到的用户对应的 bit 位上的值才会是 1。最后，我们可以用 BITCOUNT 统计下 Bitmap 中的 1 的个数，这就是连续签到 10 天的用户总数了。</strong></p>
<p>现在，我们可以计算一下记录了 10 天签到情况后的内存开销。<strong>每天使用 1 个 1 亿位的 Bitmap，大约占 12MB 的内存（10^8&#x2F;8&#x2F;1024&#x2F;1024），10 天的 Bitmap 的内存开销约为 120MB，内存压力不算太大。不过，在实际应用时，最好对 Bitmap 设置过期时间，让 Redis 自动删除不再需要的签到记录，以节省内存开销。</strong></p>
<p>所以，<strong>如果只需要统计数据的二值状态</strong>，<strong>例如商品有没有、用户在不在等，就可以使用 Bitmap，因为它只用一个 bit 位就能表示 0 或 1</strong>。在记录海量数据时，<strong>Bitmap 能够有效地节省内存空间。</strong></p>
<h2 id="基数统计"><a href="#基数统计" class="headerlink" title="基数统计"></a>基数统计</h2><p>最后，我们再来看一个统计场景：基数统计。<strong>基数统计就是指统计一个集合中不重复的元素个数。对应到我们刚才介绍的场景中，就是统计网页的 UV。</strong></p>
<p><strong>网页 UV 的统计有个独特的地方，就是需要去重，一个用户一天内的多次访问只能算作一次。在 Redis 的集合类型中，Set 类型默认支持去重，所以看到有去重需求时，我们可能第一时间就会想到用 Set 类型。</strong></p>
<p>我们来结合一个例子看一看用 Set 的情况。</p>
<p>有一个用户 user1 访问 page1 时，你把这个信息加到 Set 中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SADD page1:uv user1</span><br></pre></td></tr></table></figure>

<p><strong>用户 1 再来访问时，Set 的去重功能就保证了不会重复记录用户 1 的访问次数，这样，用户 1 就算是一个独立访客。当你需要统计 UV 时，可以直接用 SCARD 命令，这个命令会返回一个集合中的元素个数。</strong></p>
<p>但是，如果 page1 非常火爆，UV 达到了千万，这个时候，<strong>一个 Set 就要记录千万个用户 ID。对于一个搞大促的电商网站而言，这样的页面可能有成千上万个，如果每个页面都用这样的一个 Set，就会消耗很大的内存空间</strong>。</p>
<p>当然，你也可以用 Hash 类型记录 UV。</p>
<p><strong>例如，你可以把用户 ID 作为 Hash 集合的 key，当用户访问页面时，就用 HSET 命令（用于设置 Hash 集合元素的值），对这个用户 ID 记录一个值“1”，表示一个独立访客，用户 1 访问 page1 后，我们就记录为 1 个独立访客</strong>，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HSET page1:uv user1 1</span><br></pre></td></tr></table></figure>

<p><strong>即使用户 1 多次访问页面，重复执行这个 HSET 命令，也只会把 user1 的值设置为 1，仍然只记为 1 个独立访客。当要统计 UV 时，我们可以用 HLEN 命令统计 Hash 集合中的所有元素个数。</strong></p>
<p>但是，和 Set 类型相似，<strong>当页面很多时，Hash 类型也会消耗很大的内存空间。那么，有什么办法既能完成统计，还能节省内存吗？</strong></p>
<p>这时候，就要用到 Redis 提供的 HyperLogLog 了。</p>
<p><strong>HyperLogLog 是一种用于统计基数的数据集合类型，它的最大优势就在于，当集合元素数量非常多时，它计算基数所需的空间总是固定的，而且还很小。</strong></p>
<p>在 Redis 中，<strong>每个 HyperLogLog 只需要花费 12 KB 内存，就可以计算接近 2^64 个元素的基数。你看，和元素越多就越耗费内存的 Set 和 Hash 类型相比，HyperLogLog 就非常节省空间。</strong></p>
<p>在统计 UV 时，<strong>你可以用 PFADD 命令（用于向 HyperLogLog 中添加新元素）把访问页面的每个用户都添加到 HyperLogLog 中。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PFADD page1:uv user1 user2 user3 user4 user5</span><br></pre></td></tr></table></figure>

<p><strong>接下来，就可以用 PFCOUNT 命令直接获得 page1 的 UV 值了，这个命令的作用就是返回 HyperLogLog 的统计结果。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PFCOUNT page1:uv</span><br></pre></td></tr></table></figure>

<p>关于 HyperLogLog 的具体实现原理，你不需要重点掌握，不会影响到你的日常使用，我就不多讲了。如果你想了解一下，课下可以看看这条链接。</p>
<p>不过，有一点需要你注意一下，<strong>HyperLogLog 的统计规则是基于概率完成的，所以它给出的统计结果是有一定误差的，标准误算率是 0.81%。这也就意味着，你使用 HyperLogLog 统计的 UV 是 100 万，但实际的 UV 可能是 101 万。虽然误差率不算大，但是，如果你需要精确统计结果的话，最好还是继续用 Set 或 Hash 类型。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课，<strong>我们结合统计新增用户数和留存用户数、最新评论列表、用户签到数以及网页独立访客量这 4 种典型场景，学习了集合类型的 4 种统计模式，分别是聚合统计、排序统计、二值状态统计和基数统计</strong>。为了方便你掌握，<strong>我把 Set、Sorted Set、Hash、List、Bitmap、HyperLogLog 的支持情况和优缺点汇总在了下面的表格里</strong>，希望你把这张表格保存下来，时不时地复习一下。</p>
<p><img src="/2024/09/16/Redis12/c0bb35d0d91a62ef4ca1bd939a9b136e.jpg" alt="img"></p>
<p><strong>可以看到，Set 和 Sorted Set 都支持多种聚合统计，不过，对于差集计算来说，只有 Set 支持</strong>。<strong>Bitmap 也能做多个 Bitmap 间的聚合计算，包括与、或和异或操作。</strong></p>
<p>当需要进行排序统计时，List 中的元素虽然有序，但是一旦有新元素插入，原来的元素在 List 中的位置就会移动，那么，按位置读取的排序结果可能就不准确了。<strong>而 Sorted Set 本身是按照集合元素的权重排序，可以准确地按序获取结果，所以建议你优先使用它。</strong></p>
<p>如果我们记录的数据只有 0 和 1 两个值的状态，<strong>Bitmap 会是一个很好的选择，这主要归功于 Bitmap 对于一个数据只用 1 个 bit 记录，可以节省内存。</strong></p>
<p>对于基数统计来说，<strong>如果集合元素量达到亿级别而且不需要精确统计时，我建议你使用 HyperLogLog。</strong></p>
<p>当然，Redis 的应用场景非常多，这张表中的总结不一定能覆盖到所有场景。<strong>我建议你也试着自己画一张表，把你遇到的其他场景添加进去。长久积累下来，你一定能够更加灵活地把集合类型应用到合适的实践项目中。</strong></p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>依照惯例，我给你留个小问题。这节课，我们学习了 4 种典型的统计模式，以及各种集合类型的支持情况和优缺点，我想请你聊一聊，你还遇到过其他的统计场景吗？用的是怎样的集合类型呢？</p>
<p><strong>使用Sorted Set可以实现统计一段时间内的在线用户数</strong>：用户上线时使用zadd online_users <code>$timestamp $user_id</code>把用户添加到Sorted Set中，使用zcount online_users <code>$start_timestamp $end_timestamp</code>就可以得出指定时间段内的在线用户数。</p>
<p>如果key是以天划分的，还可以执行zinterstore online_users_tmp 2 online_users<code>_&#123;date1&#125; online_users_</code>{date2} aggregate max，把结果存储到online_users_tmp中，然后通过zrange online_users_tmp 0 -1 withscores就可以得到这2天都在线过的用户，并且score就是这些用户最近一次的上线时间。</p>
<p><strong>还有一个有意思的方式，使用Set记录数据，再使用zunionstore命令求并集</strong>。例如sadd user1 apple orange banana、sadd user2 apple banana peach记录2个用户喜欢的水果，使用zunionstore fruits_union 2 user1 user2把结果存储到fruits_union这个key中，zrange fruits_union 0 -1 withscores可以得出每种水果被喜欢的次数。</p>
<p>使用HyperLogLog计算UV时，补充一点，还可以使用pfcount page1:uv page2:uv page3:uv或pfmerge page_union:uv page1:uv page2:uv page3:uv得出3个页面的UV总和。</p>
<p>另外，需要指出老师文章描述不严谨的地方：“Set数据类型，使用SUNIONSTORE、SDIFFSTORE、SINTERSTORE做并集、差集、交集时，选择一个从库进行聚合计算”。这3个命令都会在Redis中生成一个新key，而从库默认是readonly不可写的，所以这些命令只能在主库使用。想在从库上操作，可以使用SUNION、SDIFF、SINTER，这些命令可以计算出结果，但不会生成新key。</p>
<p>最后需要提醒一下：</p>
<ul>
<li>1、如果是在集群模式使用多个key聚合计算的命令，<strong>一定要注意，因为这些key可能分布在不同的实例上，多个实例之间是无法做聚合运算的，这样操作可能会直接报错或者得到的结果是错误的！</strong></li>
<li>2、<strong>当数据量非常大时，使用这些统计命令，因为复杂度较高，可能会有阻塞Redis的风险，建议把这些统计数据与在线业务数据拆分开，实例单独部署，防止在做统计操作时影响到在线业务。</strong></li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/dn3miy">数据结构 - 12 | 有一亿个keys要统计，应该用哪种集合？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>消息队列的考验：Redis有哪些解决方案？</title>
    <url>/2024/09/16/Redis15/</url>
    <content><![CDATA[<p>现在的互联网应用基本上都是采用分布式系统架构进行设计的，<strong>而很多分布式系统必备的一个基础软件就是消息队列。</strong></p>
<p><strong>消息队列要能支持组件通信消息的快速读写，而 Redis 本身支持数据的高速访问，正好可以满足消息队列的读写性能需求。不过，除了性能，消息队列还有其他的要求，所以，很多人都很关心一个问题：“Redis 适合做消息队列吗？”</strong></p>
<p>其实，这个问题的背后，隐含着两方面的核心问题：</p>
<ul>
<li>消息队列的消息存取需求是什么？</li>
<li>Redis 如何实现消息队列的需求？</li>
</ul>
<p>这节课，我们就来聊一聊消息队列的特征和 Redis 提供的消息队列方案。只有把这两方面的知识和实践经验串连起来，才能彻底理解基于 Redis 实现消息队列的技术实践。以后当你需要为分布式系统组件做消息队列选型时，就可以根据组件通信量和消息通信速度的要求，选择出适合的 Redis 消息队列方案了。</p>
<p>我们先来看下第一个问题：消息队列的消息读取有什么样的需求？</p>
<h2 id="消息队列的消息存取需求"><a href="#消息队列的消息存取需求" class="headerlink" title="消息队列的消息存取需求"></a>消息队列的消息存取需求</h2><p>我先介绍一下消息队列存取消息的过程。<strong>在分布式系统中，当两个组件要基于消息队列进行通信时，一个组件会把要处理的数据以消息的形式传递给消息队列，然后，这个组件就可以继续执行其他操作了；远端的另一个组件从消息队列中把消息读取出来，再在本地进行处理。</strong></p>
<p>为了方便你理解，我还是借助一个例子来解释一下。</p>
<p>假设组件 1 需要对采集到的数据进行求和计算，并写入数据库，但是，消息到达的速度很快，组件 1 没有办法及时地既做采集，又做计算，并且写入数据库。所以，我们可以使用基于消息队列的通信，<strong>让组件 1 把数据 x 和 y 保存为 JSON 格式的消息，再发到消息队列，这样它就可以继续接收新的数据了。组件 2 则异步地从消息队列中把数据读取出来，在服务器 2 上进行求和计算后，再写入数据库。这个过程如下图所示：</strong></p>
<p><img src="/2024/09/16/Redis15/d79d46ec4aa22bf46fde3ae1a99fc2bc.jpg" alt="img"></p>
<p>我们一般把消息队列中<strong>发送消息的组件称为生产者（例子中的组件 1），把接收消息的组件称为消费者（例子中的组件 2）</strong>，下图展示了一个通用的消息队列的架构模型：</p>
<p><img src="/2024/09/16/Redis15/f470bb957c1faff674c08b1fa65a3a62.jpg" alt="img"></p>
<p>在使用消息队列时，<strong>消费者可以异步读取生产者消息，然后再进行处理。这样一来，即使生产者发送消息的速度远远超过了消费者处理消息的速度，生产者已经发送的消息也可以缓存在消息队列中，避免阻塞生产者，这是消息队列作为分布式组件通信的一大优势。</strong></p>
<p><strong>不过，消息队列在存取消息时，必须要满足三个需求，分别是消息保序、处理重复的消息和保证消息可靠性。</strong></p>
<h3 id="需求一：消息保序"><a href="#需求一：消息保序" class="headerlink" title="需求一：消息保序"></a>需求一：消息保序</h3><p><strong>虽然消费者是异步处理消息，但是，消费者仍然需要按照生产者发送消息的顺序来处理消息，避免后发送的消息被先处理了。对于要求消息保序的场景来说，一旦出现这种消息被乱序处理的情况，就可能会导致业务逻辑被错误执行，从而给业务方造成损失。</strong></p>
<p>我们来看一个更新商品库存的场景。</p>
<p>假设生产者负责接收库存更新请求，消费者负责实际更新库存，现有库存量是 10。生产者先后发送了消息 1 和消息 2，消息 1 要把商品 X 的库存记录更新为 5，消息 2 是把商品 X 库存更新为 3。如果消息 1 和 2 在消息队列中无法保序，出现消息 2 早于消息 1 被处理的情况，<strong>那么，很显然，库存更新就出错了。这是业务应用无法接受的。</strong></p>
<p>面对这种情况，你可能会想到一种解决方案：<strong>不要把更新后的库存量作为生产者发送的消息</strong>，而是<strong>把库存扣除值作为消息的内容</strong>。这样一来，消息 1 是扣减库存量 5，消息 2 是扣减库存量 2。如果消息 1 和消息 2 之间没有库存查询请求的话，<strong>即使消费者先处理消息 2，再处理消息 1，这个方案也能够保证最终的库存量是正确的，也就是库存量为 3。</strong></p>
<p>但是，我们还需要考虑这样一种情况：假如消费者收到了这样三条消息：消息 1 是扣减库存量 5，消息 2 是读取库存量，消息 3 是扣减库存量 2，此时，如果消费者先处理了消息 3（把库存量扣减 2），那么库存量就变成了 8。然后，消费者处理了消息 2，读取当前的库存量是 8，这就会出现库存量查询不正确的情况。<strong>从业务应用层面看，消息 1、2、3 应该是顺序执行的，所以，消息 2 查询到的应该是扣减了 5 以后的库存量，而不是扣减了 2 以后的库存量。所以，用库存扣除值作为消息的方案，在消息中同时包含读写操作的场景下，会带来数据读取错误的问题。而且，这个方案还会面临一个问题，那就是重复消息处理。</strong></p>
<h3 id="需求二：重复消息处理"><a href="#需求二：重复消息处理" class="headerlink" title="需求二：重复消息处理"></a>需求二：重复消息处理</h3><p>消费者从消息队列读取消息时，<strong>有时会因为网络堵塞而出现消息重传的情况。此时，消费者可能会收到多条重复的消息。对于重复的消息，消费者如果多次处理的话，就可能造成一个业务逻辑被多次执行，如果业务逻辑正好是要修改数据，那就会出现数据被多次修改的问题了。</strong></p>
<p>还是以库存更新为例，<strong>假设消费者收到了一次消息 1，要扣减库存量 5，然后又收到了一次消息 1，那么，如果消费者无法识别这两条消息实际是一条相同消息的话，就会执行两次扣减库存量 5 的操作，此时，库存量就不对了。这当然也是无法接受的。</strong></p>
<h3 id="需求三：消息可靠性保证"><a href="#需求三：消息可靠性保证" class="headerlink" title="需求三：消息可靠性保证"></a>需求三：消息可靠性保证</h3><p>另外，消费者在处理消息的时候，<strong>还可能出现因为故障或宕机导致消息没有处理完成的情况。此时，消息队列需要能提供消息可靠性的保证，也就是说，当消费者重启后，可以重新读取消息再次进行处理，否则，就会出现消息漏处理的问题了。</strong></p>
<p><strong>Redis 的 List 和 Streams 两种数据类型，就可以满足消息队列的这三个需求。我们先来了解下基于 List 的消息队列实现方法。</strong></p>
<h2 id="基于-List-的消息队列解决方案"><a href="#基于-List-的消息队列解决方案" class="headerlink" title="基于 List 的消息队列解决方案"></a>基于 List 的消息队列解决方案</h2><p><strong>List 本身就是按先进先出的顺序对数据进行存取的</strong>，所以，如果使用 List 作为消息队列保存消息的话，<strong>就已经能满足消息保序的需求了。</strong></p>
<p>具体来说，<strong>生产者可以使用 LPUSH 命令把要发送的消息依次写入 List，而消费者则可以使用 RPOP 命令，从 List 的另一端按照消息的写入顺序，依次读取消息并进行处理。</strong></p>
<p>如下图所示，生产者先用 LPUSH 写入了两条库存消息，分别是 5 和 3，表示要把库存更新为 5 和 3；消费者则用 RPOP 把两条消息依次读出，然后进行相应的处理。</p>
<p><img src="/2024/09/16/Redis15/b0959216cbce7ac383ce206b8884777c.jpg" alt="img"></p>
<p>不过，在消费者读取数据时，有一个潜在的性能风险点。</p>
<p><strong>在生产者往 List 中写入数据时，List 并不会主动地通知消费者有新消息写入，如果消费者想要及时处理消息，就需要在程序中不停地调用 RPOP 命令（比如使用一个 while(1) 循环）。如果有新消息写入，RPOP 命令就会返回结果，否则，RPOP 命令返回空值，再继续循环。</strong></p>
<p><strong>所以，即使没有新消息写入 List，消费者也要不停地调用 RPOP 命令，这就会导致消费者程序的 CPU 一直消耗在执行 RPOP 命令上，带来不必要的性能损失。</strong></p>
<p>为了解决这个问题，Redis 提供了 BRPOP 命令。<strong>BRPOP 命令也称为阻塞式读取，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据</strong>。<strong>和消费者程序自己不停地调用 RPOP 命令相比，这种方式能节省 CPU 开销。</strong></p>
<p>消息保序的问题解决了，接下来，我们还需要考虑解决重复消息处理的问题，这里其实有一个要求：<strong>消费者程序本身能对重复消息进行判断。</strong></p>
<p><strong>一方面，消息队列要能给每一个消息提供全局唯一的 ID 号；另一方面，消费者程序要把已经处理过的消息的 ID 号记录下来。</strong></p>
<p><strong>当收到一条消息后，消费者程序就可以对比收到的消息 ID 和记录的已处理过的消息 ID，来判断当前收到的消息有没有经过处理。如果已经处理过，那么，消费者程序就不再进行处理了。</strong></p>
<p>这种处理特性也称为幂等性，<strong>幂等性就是指，对于同一条消息，消费者收到一次的处理结果和收到多次的处理结果是一致的</strong>。</p>
<p>不过，<strong>List 本身是不会为每个消息生成 ID 号的，所以，消息的全局唯一 ID 号就需要生产者程序在发送消息前自行生成。生成之后，我们在用 LPUSH 命令把消息插入 List 时，需要在消息中包含这个全局唯一 ID。</strong></p>
<p>例如，我们执行以下命令，就把一条全局 ID 为 101030001、库存量为 5 的消息插入了消息队列：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">LPUSH mq <span class="string">&quot;101030001:stock:5&quot;</span></span><br><span class="line"></span><br><span class="line">(<span class="built_in">integer</span>) 1</span><br></pre></td></tr></table></figure>

<p>最后，我们再来看下，<strong>List 类型是如何保证消息可靠性的。</strong></p>
<p>当消费者程序从 List 中读取一条消息后，<strong>List 就不会再留存这条消息了。所以，如果消费者程序在处理消息的过程出现了故障或宕机，就会导致消息没有处理完成，那么，消费者程序再次启动后，就没法再次从 List 中读取消息了。</strong></p>
<p>为了留存消息，<strong>List 类型提供了 BRPOPLPUSH 命令，这个命令的作用是让消费者程序从一个 List 中读取消息，同时，Redis 会把这个消息再插入到另一个 List（可以叫作备份 List）留存</strong>。<strong>这样一来，如果消费者程序读了消息但没能正常处理，等它重启后，就可以从备份 List 中重新读取消息并进行处理了</strong>。</p>
<p>我画了一张示意图，展示了使用 BRPOPLPUSH 命令留存消息，以及消费者再次读取消息的过程，你可以看下。</p>
<p><img src="/2024/09/16/Redis15/5045395da08317b546aab7eb698d013d.jpg" alt="img"></p>
<p>生产者先用 LPUSH 把消息“5”“3”插入到消息队列 mq 中。消费者程序使用 BRPOPLPUSH 命令读取消息“5”，<strong>同时，消息“5”还会被 Redis 插入到 mqback 队列中。如果消费者程序处理消息“5”时宕机了，等它重启后，可以从 mqback 中再次读取消息“5”，继续处理。</strong></p>
<p>好了，到这里，你可以看到，基于 List 类型，<strong>我们可以满足分布式组件对消息队列的三大需求</strong>。但是，在用 List 做消息队列时，我们还可能遇到过一个问题：<strong>生产者消息发送很快，而消费者处理消息的速度比较慢，这就导致 List 中的消息越积越多，给 Redis 的内存带来很大压力</strong>。</p>
<p>这个时候，<strong>我们希望启动多个消费者程序组成一个消费组，一起分担处理 List 中的消息</strong>。但是，<strong>List 类型并不支持消费组的实现。那么，还有没有更合适的解决方案呢？这就要说到 Redis 从 5.0 版本开始提供的 Streams 数据类型了。</strong></p>
<p>和 List 相比，Streams 同样能够满足消息队列的三大需求。<strong>而且，它还支持消费组形式的消息读取。接下来，我们就来了解下 Streams 的使用方法。</strong></p>
<h2 id="基于-Streams-的消息队列解决方案"><a href="#基于-Streams-的消息队列解决方案" class="headerlink" title="基于 Streams 的消息队列解决方案"></a>基于 Streams 的消息队列解决方案</h2><p>Streams 是 Redis 专门为消息队列设计的数据类型，它提供了丰富的消息队列操作命令。</p>
<ul>
<li><strong>XADD：插入消息，保证有序，可以自动生成全局唯一 ID；</strong></li>
<li><strong>XREAD：用于读取消息，可以按 ID 读取数据；</strong></li>
<li><strong>XREADGROUP：按消费组形式读取消息；</strong></li>
<li><strong>XPENDING 和 XACK：XPENDING 命令可以用来查询每个消费组内所有消费者已读取但尚未确认的消息，而 XACK 命令用于向消息队列确认消息处理已完成。</strong></li>
</ul>
<p>首先，我们来学习下 Streams 类型存取消息的操作 XADD。</p>
<p><strong>XADD 命令可以往消息队列中插入新消息，消息的格式是键 - 值对形式。对于插入的每一条消息，Streams 可以自动为其生成一个全局唯一的 ID。</strong></p>
<p>比如说，我们执行下面的命令，就可以往名称为 mqstream 的消息队列中插入一条消息，<strong>消息的键是 repo，值是 5</strong>。其中，消息队列名称后面的<code>*</code>，<strong>表示让 Redis 为插入的数据自动生成一个全局唯一的 ID，例如“1599203861727-0”。当然，我们也可以不用<code>*</code>，直接在消息队列名称后自行设定一个 ID 号，只要保证这个 ID 号是全局唯一的就行。不过，相比自行设定 ID 号，使用*会更加方便高效</strong>。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">XADD mqstream * repo 5</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;1599203861727-0&quot;</span></span><br></pre></td></tr></table></figure>

<p>可以看到，消息的全局唯一 ID 由两部分组成，<strong>第一部分“1599203861727”是数据插入时，以毫秒为单位计算的当前服务器时间，第二部分表示插入消息在当前毫秒内的消息序号，这是从 0 开始编号的。例如，“1599203861727-0”就表示在“1599203861727”毫秒内的第 1 条消息。</strong></p>
<p>当消费者需要读取消息时，可以直接使用 XREAD 命令从消息队列中读取。</p>
<p><strong>XREAD 在读取消息时，可以指定一个消息 ID，并从这个消息 ID 的下一条消息开始进行读取。</strong></p>
<p>例如，我们可以执行下面的命令，从 ID 号为 1599203861727-0 的消息开始，读取后续的所有消息（示例中一共 3 条）。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">XREAD BLOCK 100 STREAMS  mqstream 1599203861727-0</span><br><span class="line"></span><br><span class="line">1) 1) <span class="string">&quot;mqstream&quot;</span></span><br><span class="line"></span><br><span class="line">   2) 1) 1) <span class="string">&quot;1599274912765-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;3&quot;</span></span><br><span class="line"></span><br><span class="line">       2) 1) <span class="string">&quot;1599274925823-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;2&quot;</span></span><br><span class="line"></span><br><span class="line">       3) 1) <span class="string">&quot;1599274927910-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;1&quot;</span></span><br></pre></td></tr></table></figure>

<p>另外，<strong>消费者也可以在调用 XRAED 时设定 block 配置项，实现类似于 BRPOP 的阻塞读取操作。当消息队列中没有消息时，一旦设置了 block 配置项，XREAD 就会阻塞，阻塞的时长可以在 block 配置项进行设置。</strong></p>
<p>举个例子，我们来看一下下面的命令，<strong>其中，命令最后的“$”符号表示读取最新的消息，同时，我们设置了 block 10000 的配置项，10000 的单位是毫秒，表明 XREAD 在读取最新消息时，如果没有消息到来，XREAD 将阻塞 10000 毫秒（即 10 秒），然后再返回</strong>。下面命令中的 XREAD 执行后，消息队列 mqstream 中一直没有消息，所以，XREAD 在 10 秒后返回空值（nil）。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">XREAD block 10000 streams mqstream $</span><br><span class="line"></span><br><span class="line">(nil)</span><br><span class="line"></span><br><span class="line">(10.00s)</span><br></pre></td></tr></table></figure>

<p>刚刚讲到的这些操作是 List 也支持的，接下来，我们再来学习下 Streams 特有的功能。</p>
<p><strong>Streams 本身可以使用 XGROUP 创建消费组，创建消费组之后，Streams 可以使用 XREADGROUP 命令让消费组内的消费者读取消息。</strong></p>
<p>例如，我们执行下面的命令，创建一个名为 group1 的消费组，这个消费组消费的消息队列是 mqstream。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">XGROUP create mqstream group1 0</span><br><span class="line"></span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p>然后，我们再执行一段命令，<strong>让 group1 消费组里的消费者 consumer1 从 mqstream 中读取所有消息，其中，命令最后的参数“&gt;”，表示从第一条尚未被消费的消息开始读取。因为在 consumer1 读取消息前，group1 中没有其他消费者读取过消息，所以，consumer1 就得到 mqstream 消息队列中的所有消息了（一共 4 条）。</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">XREADGROUP group group1 consumer1 streams mqstream &gt;</span><br><span class="line"></span><br><span class="line">1) 1) <span class="string">&quot;mqstream&quot;</span></span><br><span class="line"></span><br><span class="line">   2) 1) 1) <span class="string">&quot;1599203861727-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;5&quot;</span></span><br><span class="line"></span><br><span class="line">       2) 1) <span class="string">&quot;1599274912765-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;3&quot;</span></span><br><span class="line"></span><br><span class="line">       3) 1) <span class="string">&quot;1599274925823-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;2&quot;</span></span><br><span class="line"></span><br><span class="line">       4) 1) <span class="string">&quot;1599274927910-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;1&quot;</span></span><br></pre></td></tr></table></figure>

<p>需要注意的是，<strong>消息队列中的消息一旦被消费组里的一个消费者读取了，就不能再被该消费组内的其他消费者读取了。比如说，我们执行完刚才的 XREADGROUP 命令后，再执行下面的命令，让 group1 内的 consumer2 读取消息时，consumer2 读到的就是空值，因为消息已经被 consumer1 读取完了，如下所示：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">XREADGROUP group group1 consumer2  streams mqstream 0</span><br><span class="line"></span><br><span class="line">1) 1) <span class="string">&quot;mqstream&quot;</span></span><br><span class="line"></span><br><span class="line">   2) (empty list or <span class="built_in">set</span>)</span><br></pre></td></tr></table></figure>

<p><strong>使用消费组的目的是让组内的多个消费者共同分担读取消息，所以，我们通常会让每个消费者读取部分消息，从而实现消息读取负载在多个消费者间是均衡分布的。</strong>例如，我们执行下列命令，让 group2 中的 consumer1、2、3 各自读取一条消息。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">XREADGROUP group group2 consumer1 count 1 streams mqstream &gt;</span><br><span class="line"></span><br><span class="line">1) 1) <span class="string">&quot;mqstream&quot;</span></span><br><span class="line"></span><br><span class="line">   2) 1) 1) <span class="string">&quot;1599203861727-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;5&quot;</span></span><br><span class="line"></span><br><span class="line">XREADGROUP group group2 consumer2 count 1 streams mqstream &gt;</span><br><span class="line"></span><br><span class="line">1) 1) <span class="string">&quot;mqstream&quot;</span></span><br><span class="line"></span><br><span class="line">   2) 1) 1) <span class="string">&quot;1599274912765-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;3&quot;</span></span><br><span class="line"></span><br><span class="line">XREADGROUP group group2 consumer3 count 1 streams mqstream &gt;</span><br><span class="line"></span><br><span class="line">1) 1) <span class="string">&quot;mqstream&quot;</span></span><br><span class="line"></span><br><span class="line">   2) 1) 1) <span class="string">&quot;1599274925823-0&quot;</span></span><br><span class="line"></span><br><span class="line">          2) 1) <span class="string">&quot;repo&quot;</span></span><br><span class="line"></span><br><span class="line">             2) <span class="string">&quot;2&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>为了保证消费者在发生故障或宕机再次重启后，仍然可以读取未处理完的消息，Streams 会自动使用内部队列（也称为 PENDING List）留存消费组里每个消费者读取的消息，直到消费者使用 XACK 命令通知 Streams“消息已经处理完成”。如果消费者没有成功处理消息，它就不会给 Streams 发送 XACK 命令，消息仍然会留存。此时，消费者可以在重启后，用 XPENDING 命令查看已读取、但尚未确认处理完成的消息。</strong></p>
<p>例如，我们来查看一下 group2 中各个消费者已读取、但尚未确认的消息个数。其中，XPENDING 返回结果的第二、三行分别表示 group2 中所有消费者读取的消息最小 ID 和最大 ID。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">XPENDING mqstream group2</span><br><span class="line"></span><br><span class="line">1) (<span class="built_in">integer</span>) 3</span><br><span class="line"></span><br><span class="line">2) <span class="string">&quot;1599203861727-0&quot;</span></span><br><span class="line"></span><br><span class="line">3) <span class="string">&quot;1599274925823-0&quot;</span></span><br><span class="line"></span><br><span class="line">4) 1) 1) <span class="string">&quot;consumer1&quot;</span></span><br><span class="line"></span><br><span class="line">       2) <span class="string">&quot;1&quot;</span></span><br><span class="line"></span><br><span class="line">   2) 1) <span class="string">&quot;consumer2&quot;</span></span><br><span class="line"></span><br><span class="line">       2) <span class="string">&quot;1&quot;</span></span><br><span class="line"></span><br><span class="line">   3) 1) <span class="string">&quot;consumer3&quot;</span></span><br><span class="line"></span><br><span class="line">       2) <span class="string">&quot;1&quot;</span></span><br></pre></td></tr></table></figure>

<p>如果我们还需要进一步查看某个消费者具体读取了哪些数据，可以执行下面的命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">XPENDING mqstream group2 - + 10 consumer2</span><br><span class="line"></span><br><span class="line">1) 1) <span class="string">&quot;1599274912765-0&quot;</span></span><br><span class="line"></span><br><span class="line">   2) <span class="string">&quot;consumer2&quot;</span></span><br><span class="line"></span><br><span class="line">   3) (<span class="built_in">integer</span>) 513336</span><br><span class="line"></span><br><span class="line">   4) (<span class="built_in">integer</span>) 1</span><br></pre></td></tr></table></figure>

<p>可以看到，consumer2 已读取的消息的 ID 是 1599274912765-0。</p>
<p><strong>一旦消息 1599274912765-0 被 consumer2 处理了，consumer2 就可以使用 XACK 命令通知 Streams，然后这条消息就会被删除。当我们再使用 XPENDING 命令查看时，就可以看到，consumer2 已经没有已读取、但尚未确认处理的消息了。</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> XACK mqstream group2 1599274912765-0</span><br><span class="line"></span><br><span class="line">(<span class="built_in">integer</span>) 1</span><br><span class="line"></span><br><span class="line">XPENDING mqstream group2 - + 10 consumer2</span><br><span class="line"></span><br><span class="line">(empty list or <span class="built_in">set</span>)</span><br></pre></td></tr></table></figure>

<p>现在，我们就知道了用 Streams 实现消息队列的方法，<strong>我还想再强调下，Streams 是 Redis 5.0 专门针对消息队列场景设计的数据类型，如果你的 Redis 是 5.0 及 5.0 以后的版本，就可以考虑把 Streams 用作消息队列了。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课，我们学习了分布式系统组件使用消息队列时的三大需求：<strong>消息保序、重复消息处理和消息可靠性保证</strong>，这三大需求可以进一步转换为对消息队列的三大要求：<strong>消息数据有序存取，消息数据具有全局唯一编号，以及消息数据在消费完成后被删除</strong>。</p>
<p>我画了一张表格，汇总了用 List 和 Streams 实现消息队列的特点和区别。当然，在实践的过程中，你也可以根据新的积累，进一步补充和完善这张表。</p>
<p><img src="/2024/09/16/Redis15/b2d6581e43f573da6218e790bb8c6814.jpg" alt="img"></p>
<p><strong>其实，关于 Redis 是否适合做消息队列，业界一直是有争论的。很多人认为，要使用消息队列，就应该采用 Kafka、RabbitMQ 这些专门面向消息队列场景的软件，而 Redis 更加适合做缓存。</strong></p>
<p>根据这些年做 Redis 研发工作的经验，我的看法是：<strong>Redis 是一个非常轻量级的键值数据库，部署一个 Redis 实例就是启动一个进程，部署 Redis 集群，也就是部署多个 Redis 实例。而 Kafka、RabbitMQ 部署时，涉及额外的组件，例如 Kafka 的运行就需要再部署 ZooKeeper。相比 Redis 来说，Kafka 和 RabbitMQ 一般被认为是重量级的消息队列。</strong></p>
<p>所以，关于是否用 Redis 做消息队列的问题，不能一概而论，我们需要考虑业务层面的数据体量，以及对性能、可靠性、可扩展性的需求。<strong>如果分布式系统中的组件消息通信量不大，那么，Redis 只需要使用有限的内存空间就能满足消息存储的需求，而且，Redis 的高性能特性能支持快速的消息读写，不失为消息队列的一个好的解决方案。</strong></p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提个小问题。<strong>如果一个生产者发送给消息队列的消息，需要被多个消费者进行读取和处理（例如，一个消息是一条从业务系统采集的数据，既要被消费者 1 读取进行实时计算，也要被消费者 2 读取并留存到分布式文件系统 HDFS 中，以便后续进行历史查询），你会使用 Redis 的什么数据类型来解决这个问题呢？</strong></p>
<p>如果一个生产者发送给消息队列的消息，需要被多个消费者进行读取和处理，你会使用Redis的什么数据类型来解决这个问题？</p>
<ul>
<li><strong>这种情况下，只能使用Streams数据类型来解决。使用Streams数据类型，创建多个消费者组，就可以实现同时消费生产者的数据。每个消费者组内可以再挂多个消费者分担读取消息进行消费，消费完成后，各自向Redis发送XACK，标记自己的消费组已经消费到了哪个位置，而且消费组之间互不影响。</strong></li>
<li>另外，老师在介绍使用List用作队列时，<strong>为了保证消息可靠性，使用BRPOPLPUSH命令把消息取出的同时，还把消息插入到备份队列中，从而防止消费者故障导致消息丢失。</strong></li>
<li>这种情况下，还需要额外做一些工作，<strong>也就是维护这个备份队列：每次执行BRPOPLPUSH命令后，因为都会把消息插入一份到备份队列中，所以当消费者成功消费取出的消息后，最好把备份队列中的消息删除，防止备份队列存储过多无用的数据，导致内存浪费。</strong></li>
</ul>
<p>这篇文章主要是讲消息队列的使用，借这个机会，也顺便总结一下使用消息队列时的注意点：</p>
<p>在使用消息队列时，<strong>重点需要关注的是如何保证不丢消息？</strong></p>
<p>那么下面就来分析一下，<strong>哪些情况下，会丢消息，以及如何解决？</strong></p>
<p>1、生产者在发布消息时异常：</p>
<ul>
<li>a) <strong>网络故障或其他问题导致发布失败（直接返回错误，消息根本没发出去）</strong></li>
<li>b) <strong>网络抖动导致发布超时（可能发送数据包成功，但读取响应结果超时了，不知道结果如何）</strong></li>
</ul>
<p><strong>情况a还好，消息根本没发出去，那么重新发一次就好了。但是情况b没办法知道到底有没有发布成功，所以也只能再发一次。所以这两种情况，生产者都需要重新发布消息，直到成功为止（一般设定一个最大重试次数，超过最大次数依旧失败的需要报警处理）</strong>。<strong>这就会导致消费者可能会收到重复消息的问题，所以消费者需要保证在收到重复消息时，依旧能保证业务的正确性（设计幂等逻辑），一般需要根据具体业务来做，例如使用消息的唯一ID，或者版本号配合业务逻辑来处理。</strong></p>
<p>2、消费者在处理消息时异常：</p>
<p><strong>也就是消费者把消息拿出来了，但是还没处理完，消费者就挂了。</strong>这种情况，需要消费者恢复时，依旧能处理之前没有消费成功的消息。<strong>使用List当作队列时，也就是利用老师文章所讲的备份队列来保证，代价是增加了维护这个备份队列的成本。而Streams则是采用ack的方式，消费成功后告知中间件，这种方式处理起来更优雅，成熟的队列中间件例如RabbitMQ、Kafka都是采用这种方式来保证消费者不丢消息的。</strong></p>
<p>3、消息队列中间件丢失消息</p>
<p>上面2个层面都比较好处理，只要客户端和服务端配合好，就能保证生产者和消费者都不丢消息。<strong>但是，如果消息队列中间件本身就不可靠，也有可能会丢失消息，毕竟生产者和消费这都依赖它，如果它不可靠，那么生产者和消费者无论怎么做，都无法保证数据不丢失。</strong></p>
<ul>
<li>a) 在用Redis当作队列或存储数据时，是有可能丢失数据的：一个场景是，<strong>如果打开AOF并且是每秒写盘，因为这个写盘过程是异步的，Redis宕机时会丢失1秒的数据。而如果AOF改为同步写盘，那么写入性能会下降。另一个场景是，如果采用主从集群，如果写入量比较大，从库同步存在延迟，此时进行主从切换，也存在丢失数据的可能（从库还未同步完成主库发来的数据就被提成主库）。总的来说，Redis不保证严格的数据完整性和主从切换时的一致性。我们在使用Redis时需要注意。</strong></li>
<li>b) 而采用RabbitMQ和Kafka这些专业的队列中间件时，就没有这个问题了。<strong>这些组件一般是部署一个集群，生产者在发布消息时，队列中间件一般会采用写多个节点+预写磁盘的方式保证消息的完整性，即便其中一个节点挂了，也能保证集群的数据不丢失。当然，为了做到这些，方案肯定比Redis设计的要复杂（毕竟是专们针对队列场景设计的）。</strong></li>
</ul>
<p>综上，<strong>Redis可以用作队列，而且性能很高，部署维护也很轻量，但缺点是无法严格保数据的完整性（个人认为这就是业界有争议要不要使用Redis当作队列的地方）</strong>。<strong>而使用专业的队列中间件，可以严格保证数据的完整性，但缺点是，部署维护成本高，用起来比较重。</strong></p>
<p>所以我们需要根据具体情况进行选择，<strong>如果对于丢数据不敏感的业务，例如发短信、发通知的场景，可以采用Redis作队列。如果是金融相关的业务场景，例如交易、支付这类，建议还是使用专业的队列中间件。</strong></p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/dlpvpk">数据结构 - 15 | 消息队列的考验：Redis有哪些解决方案？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>切片集群：数据增多了，是该加内存还是加实例？</title>
    <url>/2024/09/16/Redis09/</url>
    <content><![CDATA[<p>我曾遇到过这么一个需求：<strong>要用 Redis 保存 5000 万个键值对，每个键值对大约是 512B，为了能快速部署并对外提供服务，我们采用云主机来运行 Redis 实例，那么，该如何选择云主机的内存容量呢？</strong></p>
<p>*<em>我粗略地计算了一下，这些键值对所占的内存空间大约是 25GB（5000 万 <em>512B）。所以，当时，我想到的第一个方案就是：选择一台 32GB 内存的云主机来部署 Redis。因为 32GB 的内存能保存所有数据，而且还留有 7GB，可以保证系统的正常运行。同时，我还采用 RDB 对数据做持久化，以确保 Redis 实例故障后，还能从 RDB 恢复数据。</em></em></p>
<p>但是，在使用的过程中，我发现，Redis 的响应有时会非常慢。<strong>后来，我们使用 INFO 命令查看 Redis 的 latest_fork_usec 指标值（表示最近一次 fork 的耗时），结果显示这个指标值特别高，快到秒级别了。</strong></p>
<p>这跟 Redis 的持久化机制有关系。<strong>在使用 RDB 进行持久化时，Redis 会 fork 子进程来完成，fork 操作的用时和 Redis 的数据量是正相关的，而 fork 在执行时会阻塞主线程。数据量越大，fork 操作造成的主线程阻塞的时间越长。所以，在使用 RDB 对 25GB 的数据进行持久化时，数据量较大，后台运行的子进程在 fork 创建时阻塞了主线程，于是就导致 Redis 响应变慢了。</strong></p>
<p>看来，第一个方案显然是不可行的，我们必须要寻找其他的方案。<strong>这个时候，我们注意到了 Redis 的切片集群。虽然组建切片集群比较麻烦，但是它可以保存大量数据，而且对 Redis 主线程的阻塞影响较小。</strong></p>
<p><strong>切片集群，也叫分片集群，就是指启动多个 Redis 实例组成一个集群，然后按照一定的规则，把收到的数据划分成多份，每一份用一个实例来保存。</strong>回到我们刚刚的场景中，<strong>如果把 25GB 的数据平均分成 5 份（当然，也可以不做均分），使用 5 个实例来保存，每个实例只需要保存 5GB 数据。如下图所示：</strong></p>
<p><img src="/2024/09/16/Redis09/793251ca784yyf6ac37fe46389094b26.jpg" alt="img"></p>
<p>切片集群架构图</p>
<p><strong>那么，在切片集群中，实例在为 5GB 数据生成 RDB 时，数据量就小了很多，fork 子进程一般不会给主线程带来较长时间的阻塞。采用多个实例保存数据切片后，我们既能保存 25GB 数据，又避免了 fork 子进程阻塞主线程而导致的响应突然变慢。</strong></p>
<p>在实际应用 Redis 时，随着用户或业务规模的扩展，<strong>保存大量数据的情况通常是无法避免的。而切片集群，就是一个非常好的解决方案。这节课，我们就来学习一下。</strong></p>
<h2 id="如何保存更多数据？"><a href="#如何保存更多数据？" class="headerlink" title="如何保存更多数据？"></a>如何保存更多数据？</h2><p>在刚刚的案例里，<strong>为了保存大量数据，我们使用了大内存云主机和切片集群两种方法。实际上，这两种方法分别对应着 Redis 应对数据量增多的两种方案：纵向扩展（scale up）和横向扩展（scale out）。</strong></p>
<ul>
<li><strong>纵向扩展</strong>：<strong>升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU</strong>。就像下图中，原来的实例内存是 8GB，硬盘是 50GB，纵向扩展后，内存增加到 24GB，磁盘增加到 150GB。</li>
<li><strong>横向扩展</strong>：<strong>横向增加当前 Redis 实例的个数，就像下图中，原来使用 1 个 8GB 内存、50GB 磁盘的实例，现在使用三个相同配置的实例。</strong></li>
</ul>
<p><img src="/2024/09/16/Redis09/7a512fec7eba789c6d098b834929701a.jpg" alt="img"></p>
<p>纵向扩展和横向扩展对比图</p>
<p>那么，这两种方式的优缺点分别是什么呢？</p>
<p>首先，纵向扩展的好处是，<strong>实施起来简单、直接</strong>。不过，这个方案也面临两个潜在的问题。</p>
<p>第一个问题是，<strong>当使用 RDB 对数据进行持久化时，如果数据量增加，需要的内存也会增加，主线程 fork 子进程时就可能会阻塞（比如刚刚的例子中的情况）</strong>。不过，<strong>如果你不要求持久化保存 Redis 数据，那么，纵向扩展会是一个不错的选择。</strong></p>
<p>不过，这时，你还要面对第二个问题：<strong>纵向扩展会受到硬件和成本的限制</strong>。这很容易理解，<strong>毕竟，把内存从 32GB 扩展到 64GB 还算容易，但是，要想扩充到 1TB，就会面临硬件容量和成本上的限制了</strong>。</p>
<p>与纵向扩展相比，横向扩展是一个扩展性更好的方案。这是因为，要想保存更多的数据，采用这种方案的话，<strong>只用增加 Redis 的实例个数就行了</strong>，不用担心单个实例的硬件和成本限制。<strong>在面向百万、千万级别的用户规模时，横向扩展的 Redis 切片集群会是一个非常好的选择</strong>。</p>
<p>不过，在只使用单个实例的时候，<strong>数据存在哪儿，客户端访问哪儿，都是非常明确的，但是，切片集群不可避免地涉及到多个实例的分布式管理问题。要想把切片集群用起来，我们就需要解决两大问题：</strong></p>
<ul>
<li>数据切片后，<strong>在多个实例之间如何分布？</strong></li>
<li><strong>客户端怎么确定想要访问的数据在哪个实例上？</strong></li>
</ul>
<p>接下来，我们就一个个地解决。</p>
<h2 id="数据切片和实例的对应分布关系"><a href="#数据切片和实例的对应分布关系" class="headerlink" title="数据切片和实例的对应分布关系"></a>数据切片和实例的对应分布关系</h2><p>在切片集群中，<strong>数据需要分布在不同实例上，那么，数据和实例之间如何对应呢？这就和接下来我要讲的 Redis Cluster 方案有关了。不过，我们要先弄明白切片集群和 Redis Cluster 的联系与区别。</strong></p>
<p>实际上，<strong>切片集群是一种保存大量数据的通用机制，这个机制可以有不同的实现方案。在 Redis 3.0 之前，官方并没有针对切片集群提供具体的方案。从 3.0 开始，官方提供了一个名为 Redis Cluster 的方案，用于实现切片集群。Redis Cluster 方案中就规定了数据和实例的对应规则。</strong></p>
<p>具体来说，<strong>Redis Cluster 方案采用哈希槽（Hash Slot，接下来我会直接称之为 Slot），来处理数据和实例之间的映射关系</strong>。在 Redis Cluster 方案中，<strong>一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。</strong></p>
<p>具体的映射过程分为两大步：<strong>首先根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值；然后，再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。关于 CRC16 算法，不是这节课的重点，你简单看下链接中的资料就可以了。</strong></p>
<p>那么，这些哈希槽又是如何被映射到具体的 Redis 实例上的呢？</p>
<p>我们在部署 Redis Cluster 方案时，<strong>可以使用 cluster create 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384&#x2F;N 个。</strong></p>
<p>当然， <strong>我们也可以使用 cluster meet 命令手动建立实例间的连接，形成集群，再使用 cluster addslots 命令，指定每个实例上的哈希槽个数。</strong></p>
<p>举个例子，<strong>假设集群中不同 Redis 实例的内存大小配置不一，如果把哈希槽均分在各个实例上，在保存相同数量的键值对时，和内存大的实例相比，内存小的实例就会有更大的容量压力。遇到这种情况时，你可以根据不同实例的资源配置情况，使用 cluster addslots 命令手动分配哈希槽。</strong></p>
<p>为了便于你理解，我画一张示意图来解释一下，数据、哈希槽、实例这三者的映射分布情况。</p>
<p><img src="/2024/09/16/Redis09/7d070c8b19730b308bfaabbe82c2f1ab.jpg" alt="img"></p>
<p>示意图中的切片集群一共有 3 个实例，<strong>同时假设有 5 个哈希槽，我们首先可以通过下面的命令手动分配哈希槽：实例 1 保存哈希槽 0 和 1，实例 2 保存哈希槽 2 和 3，实例 3 保存哈希槽 4。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">redis-cli -h 172.16.19.3 –p 6379 cluster addslots 0,1</span><br><span class="line"></span><br><span class="line">redis-cli -h 172.16.19.4 –p 6379 cluster addslots 2,3</span><br><span class="line"></span><br><span class="line">redis-cli -h 172.16.19.5 –p 6379 cluster addslots 4</span><br></pre></td></tr></table></figure>

<p>在集群运行的过程中，<strong>key1 和 key2 计算完 CRC16 值后，对哈希槽总个数 5 取模，再根据各自的模数结果，就可以被映射到对应的实例 1 和实例 3 上了。</strong></p>
<p>另外，我再给你一个小提醒，<strong>在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作</strong>。</p>
<p><strong>好了，通过哈希槽，切片集群就实现了数据到哈希槽、哈希槽再到实例的分配</strong>。但是，即使实例有了哈希槽的映射信息，客户端又是怎么知道要访问的数据在哪个实例上呢？接下来，我就来和你聊聊。</p>
<h2 id="客户端如何定位数据？"><a href="#客户端如何定位数据？" class="headerlink" title="客户端如何定位数据？"></a>客户端如何定位数据？</h2><p>在定位键值对数据时，<strong>它所处的哈希槽是可以通过计算得到的，这个计算可以在客户端发送请求时来执行。但是，要进一步定位到实例，还需要知道哈希槽分布在哪个实例上。</strong></p>
<p>一般来说，<strong>客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端。但是，在集群刚刚创建的时候，每个实例只知道自己被分配了哪些哈希槽，是不知道其他实例拥有的哈希槽信息的。</strong></p>
<p>那么，客户端为什么可以在访问任何一个实例时，都能获得所有的哈希槽信息呢？这是因为，<strong>Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。</strong></p>
<p><strong>客户端收到哈希槽信息后，会把哈希槽信息缓存在本地</strong>。<strong>当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。</strong></p>
<p>但是，在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个：</p>
<ul>
<li>在集群中，<strong>实例有新增或删除，Redis 需要重新分配哈希槽</strong>；</li>
<li>为了负载均衡，<strong>Redis 需要把哈希槽在所有实例上重新分布一遍。</strong></li>
</ul>
<p><strong>此时，实例之间还可以通过相互传递消息，获得最新的哈希槽分配信息，但是，客户端是无法主动感知这些变化的</strong>。这就会导致，<strong>它缓存的分配信息和最新的分配信息就不一致了</strong>，那该怎么办呢？</p>
<p>Redis Cluster 方案提供了一种<strong>重定向机制，</strong>所谓的“重定向”，就是指，<strong>客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。</strong></p>
<p>那客户端又是怎么知道重定向时的新实例的访问地址呢？<strong>当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回下面的 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET hello:key</span><br><span class="line"></span><br><span class="line">(error) MOVED 13320 172.16.19.5:6379</span><br></pre></td></tr></table></figure>

<p>其中，MOVED 命令表示，<strong>客户端请求的键值对所在的哈希槽 13320，实际是在 172.16.19.5 这个实例上。通过返回的 MOVED 命令，就相当于把哈希槽所在的新实例的信息告诉给客户端了。这样一来，客户端就可以直接和 172.16.19.5 连接，并发送操作请求了。</strong></p>
<p>我画一张图来说明一下，<strong>MOVED 重定向命令的使用方法。可以看到，由于负载均衡，Slot  2 中的数据已经从实例 2 迁移到了实例 3</strong>，但是，<strong>客户端缓存仍然记录着“Slot 2 在实例 2”的信息，所以会给实例 2 发送命令。实例 2 给客户端返回一条 MOVED 命令，把 Slot  2 的最新位置（也就是在实例 3 上），返回给客户端，客户端就会再次向实例 3 发送请求，同时还会更新本地缓存，把 Slot  2 与实例的对应关系更新过来。</strong></p>
<p><img src="/2024/09/16/Redis09/350abedefcdbc39d6a8a8f1874eb0809.jpg" alt="img"></p>
<p>客户端MOVED重定向命令</p>
<p>需要注意的是，在上图中，<strong>当客户端给实例 2 发送命令时，Slot 2 中的数据已经全部迁移到了实例 3。在实际应用时，如果 Slot 2 中的数据比较多，就可能会出现一种情况：客户端向实例 2 发送请求，但此时，Slot 2 中的数据只有一部分迁移到了实例 3，还有部分数据没有迁移。在这种迁移部分完成的情况下，客户端就会收到一条 ASK 报错信息，如下所示：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET hello:key</span><br><span class="line"></span><br><span class="line">(error) ASK 13320 172.16.19.5:6379</span><br></pre></td></tr></table></figure>

<p>这个结果中的 ASK 命令就表示，<strong>客户端请求的键值对所在的哈希槽 13320，在 172.16.19.5 这个实例上，但是这个哈希槽正在迁移。此时，客户端需要先给 172.16.19.5 这个实例发送一个 ASKING 命令。这个命令的意思是，让这个实例允许执行客户端接下来发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据。</strong></p>
<p>看起来好像有点复杂，我再借助图片来解释一下。</p>
<p>在下图中，Slot 2 正在从实例 2 往实例 3 迁移，key1 和 key2 已经迁移过去，key3 和 key4 还在实例 2。<strong>客户端向实例 2 请求 key2 后，就会收到实例 2 返回的 ASK 命令。</strong></p>
<p>ASK 命令表示两层含义：</p>
<ul>
<li><strong>第一，表明 Slot 数据还在迁移中；</strong></li>
<li><strong>第二，ASK 命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 ASKING 命令，然后再发送操作命令。</strong></li>
</ul>
<p><img src="/2024/09/16/Redis09/e93ae7f4edf30724d58bf68yy714eeb0.jpg" alt="img"></p>
<p>客户端ASK重定向命令</p>
<p>和 MOVED 命令不同，<strong>ASK 命令并不会更新客户端缓存的哈希槽分配信息</strong>。所以，在上图中，<strong>如果客户端再次请求 Slot 2 中的数据，它还是会给实例 2 发送请求。这也就是说，ASK 命令的作用只是让客户端能给新实例发送一次请求，而不像 MOVED 命令那样，会更改本地缓存，让后续所有命令都发往新实例。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课，我们学习了<strong>切片集群在保存大量数据方面的优势，以及基于哈希槽的数据分布机制和客户端定位键值对的方法。</strong></p>
<p>在应对数据量扩容时，<strong>虽然增加内存这种纵向扩展的方法简单直接，但是会造成数据库的内存过大，导致性能变慢</strong>。</p>
<p><strong>Redis 切片集群提供了横向扩展的模式，也就是使用多个实例，并给每个实例配置一定数量的哈希槽，数据可以通过键的哈希值映射到哈希槽，再通过哈希槽分散保存到不同的实例上。这样做的好处是扩展性好，不管有多少数据，切片集群都能应对。</strong></p>
<p>另外，<strong>集群的实例增减，或者是为了实现负载均衡而进行的数据重新分布，会导致哈希槽和实例的映射关系发生变化，客户端发送请求时，会收到命令执行报错信息。了解了 MOVED 和 ASK 命令，你就不会为这类报错而头疼了。</strong></p>
<p>我刚刚说过，在 Redis 3.0 之前，Redis 官方并没有提供切片集群方案，<strong>但是，其实当时业界已经有了一些切片集群的方案，例如基于客户端分区的 ShardedJedis，基于代理的 Codis、Twemproxy 等</strong>。这些方案的应用早于 Redis Cluster 方案，<strong>在支撑的集群实例规模、集群稳定性、客户端友好性方面也都有着各自的优势</strong>，我会在后面的课程中，专门和你聊聊这些方案的实现机制，以及实践经验。这样一来，当你再碰到业务发展带来的数据量巨大的难题时，就可以根据这些方案的特点，选择合适的方案实现切片集群，以应对业务需求了。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，给你提一个小问题：<strong>Redis Cluster 方案通过哈希槽的方式把键值对分配到不同的实例上，这个过程需要对键值对的 key 做 CRC 计算</strong>，然后再和哈希槽做映射，这样做有什么好处吗？<strong>如果用一个表直接把键值对和实例的对应关系记录下来（例如键值对 1 在实例 2 上，键值对 2 在实例 1 上），这样就不用计算 key 和哈希槽的对应关系了，只用查表就行了</strong>，Redis 为什么不这么做呢？</p>
<p>Redis Cluster不采用把key直接映射到实例的方式，而采用哈希槽的方式原因：</p>
<ul>
<li><strong>1、整个集群存储key的数量是无法预估的，key的数量非常多时，直接记录每个key对应的实例映射关系，这个映射表会非常庞大，这个映射表无论是存储在服务端还是客户端都占用了非常大的内存空间。</strong></li>
<li><strong>2、Redis Cluster采用无中心化的模式（无proxy，客户端与服务端直连），客户端在某个节点访问一个key，如果这个key不在这个节点上，这个节点需要有纠正客户端路由到正确节点的能力（MOVED响应），这就需要节点之间互相交换路由表，每个节点拥有整个集群完整的路由关系。</strong>如果存储的都是key与实例的对应关系，节点之间交换信息也会变得非常庞大，消耗过多的网络资源，而且就算交换完成，相当于每个节点都需要额外存储其他节点的路由表，内存占用过大造成资源浪费。</li>
<li><strong>3、当集群在扩容、缩容、数据均衡时，节点之间会发生数据迁移，迁移时需要修改每个key的映射关系，维护成本高。</strong></li>
<li><strong>4、而在中间增加一层哈希槽，可以把数据和节点解耦，key通过Hash计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点，相当于消耗了很少的CPU资源，不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。</strong></li>
<li><strong>5、当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度</strong>，便于集群的维护和管理。</li>
</ul>
<p>另外，我想补充一下Redis集群相关的知识，以及我的理解：</p>
<p><strong>Redis使用集群方案就是为了解决单个节点数据量大、写入量大产生的性能瓶颈的问题。多个节点组成一个集群，可以提高集群的性能和可靠性，但随之而来的就是集群的管理问题，最核心问题有2个：请求路由、数据迁移（扩容&#x2F;缩容&#x2F;数据平衡）。</strong></p>
<p>1、请求路由：<strong>一般都是采用哈希槽的映射关系表找到指定节点，然后在这个节点上操作的方案。</strong></p>
<p><strong>Redis Cluster在每个节点记录完整的映射关系(便于纠正客户端的错误路由请求)，同时也发给客户端让客户端缓存一份，便于客户端直接找到指定节点，客户端与服务端配合完成数据的路由，这需要业务在使用Redis Cluster时，必须升级为集群版的SDK才支持客户端和服务端的协议交互。</strong></p>
<p>其他Redis集群化方案例如Twemproxy、Codis都是<strong>中心化模式（增加Proxy层）</strong>，<strong>客户端通过Proxy对整个集群进行操作，Proxy后面可以挂N多个Redis实例</strong>，Proxy层维护了路由的转发逻辑。<strong>操作Proxy就像是操作一个普通Redis一样，客户端也不需要更换SDK，而Redis Cluster是把这些路由逻辑做在了SDK中。当然，增加一层Proxy也会带来一定的性能损耗。</strong></p>
<p>2、数据迁移：<strong>当集群节点不足以支撑业务需求时，就需要扩容节点，扩容就意味着节点之间的数据需要做迁移，而迁移过程中是否会影响到业务，这也是判定一个集群方案是否成熟的标准。</strong></p>
<p><strong>Twemproxy不支持在线扩容，它只解决了请求路由的问题，扩容时需要停机做数据重新分配</strong>。<strong>而Redis Cluster和Codis都做到了在线扩容（不影响业务或对业务的影响非常小）</strong>，重点就是在数据迁移过程中，客户端对于正在迁移的key进行操作时，集群如何处理？还要保证响应正确的结果？</p>
<p><strong>Redis Cluster和Codis都需要服务端和客户端&#x2F;Proxy层互相配合，迁移过程中，服务端针对正在迁移的key，需要让客户端或Proxy去新节点访问（重定向），这个过程就是为了保证业务在访问这些key时依旧不受影响，而且可以得到正确的结果。</strong>由于重定向的存在，所以这个期间的访问延迟会变大。<strong>等迁移完成之后，Redis Cluster每个节点会更新路由映射表，同时也会让客户端感知到，更新客户端缓存。Codis会在Proxy层更新路由表，客户端在整个过程中无感知。</strong></p>
<p>除了访问正确的节点之外，<strong>数据迁移过程中还需要解决异常情况（迁移超时、迁移失败）、性能问题（如何让数据迁移更快、bigkey如何处理），这个过程中的细节也很多。</strong></p>
<p>Redis Cluster的数据迁移是同步的，<strong>迁移一个key会同时阻塞源节点和目标节点，迁移过程中会有性能问题。而Codis提供了异步迁移数据的方案，迁移速度更快，对性能影响最小，当然，实现方案也比较复杂。</strong></p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/re93oo">基础篇 - 09 | 切片集群：数据增多了，是该加内存还是加实例？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>旁路缓存：Redis是如何工作的？</title>
    <url>/2024/09/17/Redis23/</url>
    <content><![CDATA[<p>我们知道，<strong>Redis 提供了高性能的数据存取功能，所以广泛应用在缓存场景中，既能有效地提升业务应用的响应速度，还可以避免把高并发大压力的请求发送到数据库层。</strong></p>
<p>但是，<strong>如果 Redis 做缓存时出现了问题，比如说缓存失效，那么，大量请求就会直接积压到数据库层，必然会给数据库带来巨大的压力，很可能会导致数据库宕机或是故障，那么，业务应用就没有办法存取数据、响应用户请求了。这种生产事故，肯定不是我们希望看到的。</strong></p>
<p>正因为 Redis 用作缓存的普遍性以及它在业务应用中的重要作用，<strong>所以，我们需要系统地掌握缓存的一系列内容，包括工作原理、替换策略、异常处理和扩展机制。具体来说，我们需要解决四个关键问题：</strong></p>
<ul>
<li>Redis 缓存具体是怎么工作的？</li>
<li>Redis 缓存如果满了，该怎么办？</li>
<li>为什么会有缓存一致性、缓存穿透、缓存雪崩、缓存击穿等异常，该如何应对？</li>
<li>Redis 的内存毕竟有限，如果用快速的固态硬盘来保存数据，可以增加缓存的数据量，那么，Redis 缓存可以使用快速固态硬盘吗？</li>
</ul>
<p>这节课，我们来了解下缓存的特征和 Redis 适用于缓存的天然优势，以及 Redis 缓存的具体工作机制。</p>
<h2 id="缓存的特征"><a href="#缓存的特征" class="headerlink" title="缓存的特征"></a>缓存的特征</h2><p>要想弄明白 Redis 为什么适合用作缓存，我们得清楚缓存都有什么特征。</p>
<p>首先，你要知道，<strong>一个系统中的不同层之间的访问速度不一样，所以我们才需要缓存，这样就可以把一些需要频繁访问的数据放在缓存中，以加快它们的访问速度。</strong></p>
<p>为了让你能更好地理解，我以计算机系统为例，来解释一下。<strong>下图是计算机系统中的三层存储结构，以及它们各自的常用容量和访问性能。最上面是处理器，中间是内存，最下面是磁盘。</strong></p>
<p><img src="/2024/09/17/Redis23/ac80f6e1714f3e1e8eabcfd8da3d689c.jpg" alt="img"></p>
<p>从图上可以看到，<strong>CPU、内存和磁盘这三层的访问速度从几十 ns 到 100ns，再到几 ms，性能的差异很大</strong>。</p>
<p>想象一下，如果每次 CPU 处理数据时，<strong>都要从 ms 级别的慢速磁盘中读取数据，然后再进行处理，那么，CPU 只能等磁盘的数据传输完成。这样一来，高速的 CPU 就被慢速的磁盘拖累了，整个计算机系统的运行速度会变得非常慢。</strong></p>
<p>所以，计算机系统中，默认有两种缓存：</p>
<ul>
<li>CPU 里面的<strong>末级缓存，即 LLC，用来缓存内存中的数据，避免每次从内存中存取数据</strong>；</li>
<li><strong>内存中的高速页缓存，即 page cache，用来缓存磁盘中的数据，避免每次从磁盘中存取数据</strong>。</li>
</ul>
<p><img src="/2024/09/17/Redis23/7dyycf727f9396eb9788644474855a44.jpg" alt="img"></p>
<p><strong>跟内存相比，LLC 的访问速度更快，而跟磁盘相比，内存的访问是更快的</strong>。所以，我们可以看出来缓存的<strong>第一个特征</strong>：在一个层次化的系统中，<strong>缓存一定是一个快速子系统，数据存在缓存中时，能避免每次从慢速子系统中存取数据</strong>。对应到互联网应用来说，Redis 就是快速子系统，而数据库就是慢速子系统了。</p>
<p><strong>知道了这一点，你就能理解，为什么我们必须想尽办法让 Redis 提供高性能的访问，因为，如果访问速度很慢，Redis 作为缓存的价值就不大了。</strong></p>
<p>我们再看一下刚才的计算机分层结构。<strong>LLC 的大小是 MB 级别，page cache 的大小是 GB 级别，而磁盘的大小是 TB 级别</strong>。这其实包含了缓存的<strong>第二个特征：缓存系统的容量大小总是小于后端慢速系统的，我们不可能把所有数据都放在缓存系统中</strong>。</p>
<p>这个很有意思，它表明，<strong>缓存的容量终究是有限的，缓存中的数据量也是有限的，肯定是没法时刻都满足访问需求的。所以，缓存和后端慢速系统之间，必然存在数据写回和再读取的交互过程。简单来说，缓存中的数据需要按一定规则淘汰出去，写回后端系统，而新的数据又要从后端系统中读取进来，写入缓存。</strong></p>
<p>说到这儿，你肯定会想到，<strong>Redis 本身是支持按一定规则淘汰数据的，相当于实现了缓存的数据淘汰，其实，这也是 Redis 适合用作缓存的一个重要原因</strong>。</p>
<p>好了，我们现在了解了缓存的两个重要特征，那么，接下来，我们就来学习下，缓存是怎么处理请求的。实际上，业务应用在访问 Redis 缓存中的数据时，数据不一定存在，因此，处理的方式也不同。</p>
<h2 id="Redis-缓存处理请求的两种情况"><a href="#Redis-缓存处理请求的两种情况" class="headerlink" title="Redis 缓存处理请求的两种情况"></a>Redis 缓存处理请求的两种情况</h2><p>把 Redis 用作缓存时，<strong>我们会把 Redis 部署在数据库的前端，业务应用在访问数据时，会先查询 Redis 中是否保存了相应的数据。此时，根据数据是否存在缓存中，会有两种情况。</strong></p>
<ul>
<li><strong>缓存命中</strong>：Redis 中有相应数据，就直接读取 Redis，性能非常快。</li>
<li><strong>缓存缺失</strong>：Redis 中没有保存相应数据，就从后端数据库中读取数据，性能就会变慢。<strong>而且，一旦发生缓存缺失，为了让后续请求能从缓存中读取到数据，我们需要把缺失的数据写入 Redis，这个过程叫作缓存更新</strong>。<strong>缓存更新操作会涉及到保证缓存和数据库之间的数据一致性问题，关于这一点，我会在第 25 讲中再具体介绍。</strong></li>
</ul>
<p>我画了一张图，清晰地展示了发生缓存命中或缺失时，应用读取数据的情况，你可以看下这张图片。</p>
<p><img src="/2024/09/17/Redis23/6b0b489ec0c1c5049c8df84d77fa243d.jpg" alt="img"></p>
<p>假设我们在一个 Web 应用中，使用 Redis 作为缓存。用户请求发送给 Tomcat，Tomcat 负责处理业务逻辑。如果要访问数据，就需要从 MySQL 中读写数据。那么，我们可以把 Redis 部署在 MySQL 前端。如果访问的数据在 Redis 中，此时缓存命中，Tomcat 可以直接从 Redis 中读取数据，加速应用的访问。否则，Tomcat 就需要从慢速的数据库中读取数据了。</p>
<p>到这里，你可能已经发现了，使用 Redis 缓存时，我们基本有三个操作：</p>
<ul>
<li>应用读取数据时，需要先读取 Redis；</li>
<li>发生缓存缺失时，需要从数据库读取数据；</li>
<li>发生缓存缺失时，还需要更新缓存。</li>
</ul>
<p>那么，这些操作具体是由谁来做的呢？这和 Redis 缓存的使用方式相关。接下来，我就来和你聊聊 Redis 作为<strong>旁路缓存</strong>的使用操作方式。</p>
<h2 id="Redis-作为旁路缓存的使用操作"><a href="#Redis-作为旁路缓存的使用操作" class="headerlink" title="Redis 作为旁路缓存的使用操作"></a>Redis 作为旁路缓存的使用操作</h2><p><strong>Redis 是一个独立的系统软件，和业务应用程序是两个软件，当我们部署了 Redis 实例后，它只会被动地等待客户端发送请求，然后再进行处理。</strong>所以，<strong>如果应用程序想要使用 Redis 缓存，我们就要在程序中增加相应的缓存操作代码。所以，我们也把 Redis 称为旁路缓存，也就是说，读取缓存、读取数据库和更新缓存的操作都需要在应用程序中来完成。</strong></p>
<p>这和我刚才讲的计算机系统中的 LLC 和 page cache <strong>不一样</strong>。<strong>你可以回想下，平时在开发程序时，我们是没有专门在代码中显式地创建 LLC 或 page cache 的实例的，也没有显式调用过它们的 GET 接口</strong>。这是因为，<strong>我们在构建计算机硬件系统时，已经把 LLC 和 page cache 放在了应用程序的数据访问路径上，应用程序访问数据时直接就能用上缓存。</strong></p>
<p>那么，使用 Redis 缓存时，具体来说，我们需要在应用程序中增加三方面的代码：</p>
<ul>
<li>当应用程序需要读取数据时，<strong>我们需要在代码中显式调用 Redis 的 GET 操作接口，进行查询；</strong></li>
<li>如果缓存缺失了，<strong>应用程序需要再和数据库连接，从数据库中读取数据；</strong></li>
<li>当缓存中的数据需要更新时，<strong>我们也需要在应用程序中显式地调用 SET 操作接口，把更新的数据写入缓存。</strong></li>
</ul>
<p>那么，代码应该怎么加呢？我给你展示一段 Web 应用中使用 Redis 缓存的伪代码示例。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">cacheKey</span> <span class="operator">=</span> “productid_11010003”;</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> <span class="variable">cacheValue</span> <span class="operator">=</span> redisCache.get(cacheKey)；</span><br><span class="line"></span><br><span class="line"><span class="comment">//缓存命中</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ( cacheValue != NULL)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> cacheValue;</span><br><span class="line"></span><br><span class="line"><span class="comment">//缓存缺失</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"></span><br><span class="line">   cacheValue = getProductFromDB();</span><br><span class="line"></span><br><span class="line">   redisCache.put(cacheValue)  <span class="comment">//缓存更新</span></span><br></pre></td></tr></table></figure>

<p>可以看到，为了使用缓存，<strong>Web 应用程序需要有一个表示缓存系统的实例对象 redisCache，还需要主动调用 Redis 的 GET 接口，并且要处理缓存命中和缓存缺失时的逻辑，例如在缓存缺失时，需要更新缓存。</strong></p>
<p>了解了这一点，我们在使用 Redis 缓存时，有一个地方就需要注意了：<strong>因为需要新增程序代码来使用缓存，所以，Redis 并不适用于那些无法获得源码的应用，例如一些很早之前开发的应用程序，它们的源码已经没有再维护了，或者是第三方供应商开发的应用，没有提供源码，所以，我们就没有办法在这些应用中进行缓存操作。</strong></p>
<p>在使用旁路缓存时，<strong>我们需要在应用程序中增加操作代码，增加了使用 Redis 缓存的额外工作量，但是，也正因为 Redis 是旁路缓存，是一个独立的系统，我们可以单独对 Redis 缓存进行扩容或性能优化。而且，只要保持操作接口不变，我们在应用程序中增加的代码就不用再修改了。</strong></p>
<p>好了，到这里，我们知道了，<strong>通过在应用程序中加入 Redis 的操作代码，我们可以让应用程序使用 Redis 缓存数据了。不过，除了从 Redis 缓存中查询、读取数据以外，应用程序还可能会对数据进行修改，这时，我们既可以在缓存中修改，也可以在后端数据库中进行修改，我们该怎么选择呢</strong>？</p>
<p>其实，这就涉及到了 Redis 缓存的两种类型：<strong>只读缓存和读写缓存</strong>。<strong>只读缓存能加速读请求，而读写缓存可以同时加速读写请求。而且，读写缓存又有两种数据写回策略，可以让我们根据业务需求，在保证性能和保证数据可靠性之间进行选择。所以，接下来，我们来具体了解下 Redis 的缓存类型和相应的写回策略。</strong></p>
<h2 id="缓存的类型"><a href="#缓存的类型" class="headerlink" title="缓存的类型"></a>缓存的类型</h2><p>按照 Redis 缓存是否接受写请求，<strong>我们可以把它分成只读缓存和读写缓存</strong>。先来了解下只读缓存。</p>
<h3 id="只读缓存"><a href="#只读缓存" class="headerlink" title="只读缓存"></a>只读缓存</h3><p>当 Redis 用作只读缓存时，<strong>应用要读取数据的话，会先调用 Redis GET 接口，查询数据是否存在。而所有的数据写请求，会直接发往后端的数据库，在数据库中增删改。对于删改的数据来说，如果 Redis 已经缓存了相应的数据，应用需要把这些缓存的数据删除，Redis 中就没有这些数据了。</strong></p>
<p><strong>当应用再次读取这些数据时，会发生缓存缺失，应用会把这些数据从数据库中读出来，并写到缓存中。这样一来，这些数据后续再被读取时，就可以直接从缓存中获取了，能起到加速访问的效果。</strong></p>
<p>我给你举个例子。假设业务应用要修改数据 A，此时，数据 A 在 Redis 中也缓存了，那么，应用会先直接在数据库里修改 A，并把 Redis 中的 A 删除。<strong>等到应用需要读取数据 A 时，会发生缓存缺失，此时，应用从数据库中读取 A，并写入 Redis，以便后续请求从缓存中直接读取</strong>，如下图所示：</p>
<p><img src="/2024/09/17/Redis23/464ea24a098c87b9d292cf61a2b2fecd.jpg" alt="img"></p>
<p><strong>只读缓存直接在数据库中更新数据的好处是，所有最新的数据都在数据库中，而数据库是提供数据可靠性保障的，这些数据不会有丢失的风险。当我们需要缓存图片、短视频这些用户只读的数据时，就可以使用只读缓存这个类型了。</strong></p>
<h3 id="读写缓存"><a href="#读写缓存" class="headerlink" title="读写缓存"></a>读写缓存</h3><p>知道了只读缓存，读写缓存也就很容易理解了。</p>
<p>对于读写缓存来说，<strong>除了读请求会发送到缓存进行处理（直接在缓存中查询数据是否存在)，所有的写请求也会发送到缓存，在缓存中直接对数据进行增删改操作</strong>。此时，得益于 Redis 的高性能访问特性，<strong>数据的增删改操作可以在缓存中快速完成，处理结果也会快速返回给业务应用，这就可以提升业务应用的响应速度</strong>。</p>
<p>但是，和只读缓存不一样的是，<strong>在使用读写缓存时，最新的数据是在 Redis 中，而 Redis 是内存数据库，一旦出现掉电或宕机，内存中的数据就会丢失。这也就是说，应用的最新数据可能会丢失，给应用业务带来风险。</strong></p>
<p>所以，<strong>根据业务应用对数据可靠性和缓存性能的不同要求，我们会有同步直写和异步写回两种策略</strong>。其中<strong>，同步直写策略优先保证数据可靠性，而异步写回策略优先提供快速响应</strong>。学习了解这两种策略，可以帮助我们根据业务需求，做出正确的设计选择。</p>
<p>接下来，我们来具体看下这两种策略。</p>
<p>同步直写是指，<strong>写请求发给缓存的同时，也会发给后端数据库进行处理，等到缓存和数据库都写完数据，才给客户端返回</strong>。这样，<strong>即使缓存宕机或发生故障，最新的数据仍然保存在数据库中，这就提供了数据可靠性保证</strong>。</p>
<p>不过，同步直写会降低缓存的访问性能。<strong>这是因为缓存中处理写请求的速度是很快的，而数据库处理写请求的速度较慢。即使缓存很快地处理了写请求，也需要等待数据库处理完所有的写请求，才能给应用返回结果，这就增加了缓存的响应延迟。</strong></p>
<p>而异步写回策略，<strong>则是优先考虑了响应延迟</strong>。<strong>此时，所有写请求都先在缓存中处理。等到这些增改的数据要被从缓存中淘汰出来时，缓存将它们写回后端数据库。这样一来，处理这些数据的操作是在缓存中进行的，很快就能完成。只不过，如果发生了掉电，而它们还没有被写回数据库，就会有丢失的风险了。</strong></p>
<p>为了便于你理解，我也画了下面这张图，你可以看下。</p>
<p><img src="/2024/09/17/Redis23/009d055bb91d42c28b9316c649f87f66.jpg" alt="img"></p>
<p>关于是选择只读缓存，还是读写缓存，主要看我们对写请求是否有加速的需求。</p>
<ul>
<li>如果<strong>需要对写请求进行加速，我们选择读写缓存；</strong></li>
<li>如果<strong>写请求很少，或者是只需要提升读请求的响应速度的话</strong>，我们选择只读缓存。</li>
</ul>
<p>举个例子，在商品大促的场景中，商品的库存信息会一直被修改。<strong>如果每次修改都需到数据库中处理，就会拖慢整个应用，此时，我们通常会选择读写缓存的模式。</strong>而在短视频 App 的场景中，虽然视频的属性有很多，<strong>但是，一般确定后，修改并不频繁，此时，在数据库中进行修改对缓存影响不大，所以只读缓存模式是一个合适的选择。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>今天，我们学习了缓存的两个特征，<strong>分别是在分层系统中，数据暂存在快速子系统中有助于加速访问；缓存容量有限，缓存写满时，数据需要被淘汰。而 Redis 天然就具有高性能访问和数据淘汰机制，正好符合缓存的这两个特征的要求，所以非常适合用作缓存。</strong></p>
<p>另外，我们还学习了 Redis 作为旁路缓存的特性，<strong>旁路缓存就意味着需要在应用程序中新增缓存逻辑处理的代码。当然，如果是无法修改源码的应用场景，就不能使用 Redis 做缓存了。</strong></p>
<p><strong>Redis 做缓存时，还有两种模式，分别是只读缓存和读写缓存</strong>。<strong>其中，读写缓存还提供了同步直写和异步写回这两种模式，同步直写模式侧重于保证数据可靠性，而异步写回模式则侧重于提供低延迟访问，我们要根据实际的业务场景需求来进行选择。</strong></p>
<p>这节课，虽然我提到了 Redis 有数据淘汰机制，但是并没有展开讲具体的淘汰策略。那么，Redis 究竟是怎么淘汰数据的呢？我会在下节课给你具体介绍。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提一个小问题。这节课，我提到了 Redis 只读缓存和使用直写策略的读写缓存，这两种缓存都会把数据同步写到后端数据库中，你觉得，它们有什么区别吗？</p>
<p>Redis只读缓存和使用直写策略的读写缓存，这两种缓存都会把数据同步写到后端数据库中，它们的区别在于：</p>
<ul>
<li><strong>1、使用只读缓存时，是先把修改写到后端数据库中，再把缓存中的数据删除。</strong>当下次访问这个数据时，会以后端数据库中的值为准，重新加载到缓存中。<strong>这样做的优点是，数据库和缓存可以保证完全一致，并且缓存中永远保留的是经常访问的热点数据。缺点是每次修改操作都会把缓存中的数据删除，之后访问时都会先触发一次缓存缺失，然后从后端数据库加载数据到缓存中，这个过程访问延迟会变大。</strong></li>
<li><strong>2、使用读写缓存时，是同时修改数据库和缓存中的值。这样做的优点是，被修改后的数据永远在缓存中存在，下次访问时，能够直接命中缓存，不用再从后端数据库中查询，这个过程拥有比较好的性能，比较适合先修改又立即访问的业务场景。</strong>但缺点是在高并发场景下，如果存在多个操作同时修改同一个值的情况，可能会导致缓存和数据库的不一致。</li>
<li><strong>3、当使用只读缓存时，如果修改数据库失败了，那么缓存中的数据也不会被删除，此时数据库和缓存中的数据依旧保持一致。而使用读写缓存时，如果是先修改缓存，后修改数据库，如果缓存修改成功，而数据库修改失败了，那么此时数据库和缓存数据就不一致了。如果先修改数据库，再修改缓存，也会产生上面所说的并发场景下的不一致。</strong></li>
</ul>
<p>我个人总结，<strong>只读缓存是牺牲了一定的性能，优先保证数据库和缓存的一致性，它更适合对于一致性要求比较要高的业务场景。而如果对于数据库和缓存一致性要求不高，或者不存在并发修改同一个值的情况，那么使用读写缓存就比较合适</strong>，它可以保证更好的访问性能。</p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/gbcdbf">缓存 - 23 | 旁路缓存：Redis是如何工作的？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>如何在Redis中保存时间序列数据？</title>
    <url>/2024/09/16/Redis14/</url>
    <content><![CDATA[<p>我们现在做互联网产品的时候，都有这么一个需求：<strong>记录用户在网站或者 App 上的点击行为数据，来分析用户行为。这里的数据一般包括用户 ID、行为类型（例如浏览、登录、下单等）、行为发生的时间戳：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">UserID, Type, TimeStamp</span><br></pre></td></tr></table></figure>

<p>我之前做过的一个物联网项目的数据存取需求，和这个很相似。<strong>我们需要周期性地统计近万台设备的实时状态，包括设备 ID、压力、温度、湿度，以及对应的时间戳：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">DeviceID, Pressure, Temperature, Humidity, TimeStamp</span><br></pre></td></tr></table></figure>

<p>这些与发生时间相关的一组数据，就是时间序列数据。<strong>这些数据的特点是没有严格的关系模型，记录的信息可以表示成键和值的关系</strong>（例如，一个设备 ID 对应一条记录），所以，并不需要专门用关系型数据库（例如 MySQL）来保存。<strong>而 Redis 的键值数据模型，正好可以满足这里的数据存取需求。Redis 基于自身数据结构以及扩展模块，提供了两种解决方案。</strong></p>
<p><strong>这节课，我就以物联网场景中统计设备状态指标值为例，和你聊聊不同解决方案的做法和优缺点。</strong></p>
<p><strong>俗话说，“知己知彼，百战百胜”，我们就先从时间序列数据的读写特点开始，看看到底应该采用什么样的数据类型来保存吧。</strong></p>
<h2 id="时间序列数据的读写特点"><a href="#时间序列数据的读写特点" class="headerlink" title="时间序列数据的读写特点"></a>时间序列数据的读写特点</h2><p>在实际应用中，<strong>时间序列数据通常是持续高并发写入的，例如，需要连续记录数万个设备的实时状态值。同时，时间序列数据的写入主要就是插入新数据，而不是更新一个已存在的数据，也就是说，一个时间序列数据被记录后通常就不会变了，因为它就代表了一个设备在某个时刻的状态值（例如，一个设备在某个时刻的温度测量值，一旦记录下来，这个值本身就不会再变了）。</strong></p>
<p>所以，<strong>这种数据的写入特点很简单，就是插入数据快，这就要求我们选择的数据类型，在进行数据插入时，复杂度要低，尽量不要阻塞</strong>。看到这儿，<strong>你可能第一时间会想到用 Redis 的 String、Hash 类型来保存，因为它们的插入复杂度都是 O(1)，是个不错的选择。但是，我在第 11 讲中说过，String 类型在记录小数据时（例如刚才例子中的设备温度值），元数据的内存开销比较大，不太适合保存大量数据。</strong></p>
<p>那我们再看看，时间序列数据的“读”操作有什么特点。</p>
<p><strong>我们在查询时间序列数据时，既有对单条记录的查询（例如查询某个设备在某一个时刻的运行状态信息，对应的就是这个设备的一条记录），也有对某个时间范围内的数据的查询（例如每天早上 8 点到 10 点的所有设备的状态信息）。</strong></p>
<p>除此之外，还有一些更复杂的查询，<strong>比如对某个时间范围内的数据做聚合计算。这里的聚合计算，就是对符合查询条件的所有数据做计算，包括计算均值、最大 &#x2F; 最小值、求和等。例如，我们要计算某个时间段内的设备压力的最大值，来判断是否有故障发生。</strong></p>
<p>那用一个词概括<strong>时间序列数据的“读”，就是查询模式多。</strong></p>
<p>弄清楚了时间序列数据的读写特点，<strong>接下来我们就看看如何在 Redis 中保存这些数据。我们来分析下：针对时间序列数据的“写要快”，Redis 的高性能写特性直接就可以满足了；而针对“查询模式多”，也就是要支持单点查询、范围查询和聚合计算，Redis 提供了保存时间序列数据的两种方案，分别可以基于 Hash 和 Sorted Set 实现，以及基于 RedisTimeSeries 模块实现。</strong></p>
<p>接下来，我们先学习下第一种方案。</p>
<h2 id="基于-Hash-和-Sorted-Set-保存时间序列数据"><a href="#基于-Hash-和-Sorted-Set-保存时间序列数据" class="headerlink" title="基于 Hash 和 Sorted Set 保存时间序列数据"></a>基于 Hash 和 Sorted Set 保存时间序列数据</h2><p><strong>Hash 和 Sorted Set 组合的方式有一个明显的好处：它们是 Redis 内在的数据类型，代码成熟和性能稳定。所以，基于这两个数据类型保存时间序列数据，系统稳定性是可以预期的。</strong></p>
<p>不过，在前面学习的场景中，我们都是使用一个数据类型来存取数据，那么，<strong>为什么保存时间序列数据，要同时使用这两种类型？这是我们要回答的第一个问题。</strong></p>
<p>关于 Hash 类型，我们都知道，它有一个特点是，可以实现对单键的快速查询。这就满足了时间序列数据的单键查询需求。<strong>我们可以把时间戳作为 Hash 集合的 key，把记录的设备状态值作为 Hash 集合的 value。</strong></p>
<p>可以看下用 Hash 集合记录设备的温度值的示意图：</p>
<p><img src="/2024/09/16/Redis14/f2e7bc4586be59aa5e7e78a5599830be.jpg" alt="img"></p>
<p><strong>当我们想要查询某个时间点或者是多个时间点上的温度数据时，直接使用 HGET 命令或者 HMGET 命令，就可以分别获得 Hash 集合中的一个 key 和多个 key 的 value 值了。</strong></p>
<p><strong>举个例子。我们用 HGET 命令查询 202008030905 这个时刻的温度值，使用 HMGET 查询 202008030905、202008030907、202008030908 这三个时刻的温度值，如下所示：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">HGET device:temperature 202008030905</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;25.1&quot;</span></span><br><span class="line"></span><br><span class="line">HMGET device:temperature 202008030905 202008030907 202008030908</span><br><span class="line"></span><br><span class="line">1) <span class="string">&quot;25.1&quot;</span></span><br><span class="line"></span><br><span class="line">2) <span class="string">&quot;25.9&quot;</span></span><br><span class="line"></span><br><span class="line">3) <span class="string">&quot;24.9&quot;</span></span><br></pre></td></tr></table></figure>

<p>你看，用 Hash 类型来实现单键的查询很简单。但是，<strong>Hash 类型有个短板：它并不支持对数据进行范围查询。</strong></p>
<p>虽然<strong>时间序列数据是按时间递增顺序插入 Hash 集合中的，但 Hash 类型的底层结构是哈希表，并没有对数据进行有序索引。所以，如果要对 Hash 类型进行范围查询的话，就需要扫描 Hash 集合中的所有数据，再把这些数据取回到客户端进行排序，然后，才能在客户端得到所查询范围内的数据。显然，查询效率很低。</strong></p>
<p>为了能同时支持按时间戳范围的查询，<strong>可以用 Sorted Set 来保存时间序列数据，因为它能够根据元素的权重分数来排序。我们可以把时间戳作为 Sorted Set 集合的元素分数，把时间点上记录的数据作为元素本身。</strong></p>
<p>我还是以保存设备温度的时间序列数据为例，进行解释。下图显示了用 Sorted Set 集合保存的结果。</p>
<p><img src="/2024/09/16/Redis14/9e1214dbd5b42c5b3452ea73efc8c67a.jpg" alt="img"></p>
<p>使用 Sorted Set 保存数据后，<strong>我们就可以使用 ZRANGEBYSCORE 命令，按照输入的最大时间戳和最小时间戳来查询这个时间范围内的温度值了。如下所示，我们来查询一下在 2020 年 8 月 3 日 9 点 7 分到 9 点 10 分间的所有温度值：</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ZRANGEBYSCORE device:temperature 202008030907 202008030910</span><br><span class="line"></span><br><span class="line">1) <span class="string">&quot;25.9&quot;</span></span><br><span class="line"></span><br><span class="line">2) <span class="string">&quot;24.9&quot;</span></span><br><span class="line"></span><br><span class="line">3) <span class="string">&quot;25.3&quot;</span></span><br><span class="line"></span><br><span class="line">4) <span class="string">&quot;25.2&quot;</span></span><br></pre></td></tr></table></figure>

<p>现在我们知道了，同时使用 Hash 和 Sorted Set，可以满足单个时间点和一个时间范围内的数据查询需求了，但是我们又会面临一个新的问题，<strong>也就是我们要解答的第二个问题：如何保证写入 Hash 和 Sorted Set 是一个原子性的操作呢？</strong></p>
<p><strong>所谓“原子性的操作”，就是指我们执行多个写命令操作时（例如用 HSET 命令和 ZADD 命令分别把数据写入 Hash 和 Sorted Set），这些命令操作要么全部完成，要么都不完成。</strong></p>
<p><strong>只有保证了写操作的原子性，才能保证同一个时间序列数据，在 Hash 和 Sorted Set 中，要么都保存了，要么都没保存。否则，就可能出现 Hash 集合中有时间序列数据，而 Sorted Set 中没有，那么，在进行范围查询时，就没有办法满足查询需求了。</strong></p>
<p>那 Redis 是怎么保证原子性操作的呢？<strong>这里就涉及到了 Redis 用来实现简单的事务的 MULTI 和 EXEC 命令。当多个命令及其参数本身无误时，MULTI 和 EXEC 命令可以保证执行这些命令时的原子性。</strong>关于 Redis 的事务支持和原子性保证的异常情况，我会在第 30 讲中向你介绍，这节课，我们只要了解一下 MULTI 和 EXEC 这两个命令的使用方法就行了。</p>
<ul>
<li><strong>MULTI 命令：表示一系列原子性操作的开始。</strong>收到这个命令后，Redis 就知道，<strong>接下来再收到的命令需要放到一个内部队列中，后续一起执行，保证原子性。</strong></li>
<li><strong>EXEC 命令：表示一系列原子性操作的结束。</strong>一旦 Redis 收到了这个命令，<strong>就表示所有要保证原子性的命令操作都已经发送完成了</strong>。此时，Redis <strong>开始执行刚才放到内部队列中的所有命令操作</strong>。</li>
</ul>
<p>你可以看下下面这张示意图，命令 1 到命令 N 是在 MULTI 命令后、EXEC 命令前发送的，它们会被一起执行，保证原子性。</p>
<p><img src="/2024/09/16/Redis14/c0e2fd5834113cef92f2f68e7462a262.jpg" alt="img"></p>
<p>以保存设备状态信息的需求为例，我们执行下面的代码，把设备在 2020 年 8 月 3 日 9 时 5 分的温度，分别用 HSET 命令和 ZADD 命令写入 Hash 集合和 Sorted Set 集合。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">127.0.0.1:6379&gt; MULTI</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; HSET device:temperature 202008030911 26.8</span><br><span class="line"></span><br><span class="line">QUEUED</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; ZADD device:temperature 202008030911 26.8</span><br><span class="line"></span><br><span class="line">QUEUED</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; EXEC</span><br><span class="line"></span><br><span class="line">1) (<span class="built_in">integer</span>) 1</span><br><span class="line"></span><br><span class="line">2) (<span class="built_in">integer</span>) 1</span><br></pre></td></tr></table></figure>

<p>可以看到，首先，Redis 收到了客户端执行的 MULTI 命令。<strong>然后，客户端再执行 HSET 和 ZADD 命令后，Redis 返回的结果为“QUEUED”，表示这两个命令暂时入队，先不执行；执行了 EXEC 命令后，HSET 命令和 ZADD 命令才真正执行，并返回成功结果（结果值为 1）。</strong></p>
<p>到这里，我们就解决了时间序列数据的单点查询、范围查询问题，并使用 MUTLI 和 EXEC 命令保证了 Redis 能原子性地把数据保存到 Hash 和 Sorted Set 中。<strong>接下来，我们需要继续解决第三个问题：如何对时间序列数据进行聚合计算？</strong></p>
<p>聚合计算一般被用来周期性地统计时间窗口内的数据汇总状态，在实时监控与预警等场景下会频繁执行。</p>
<p>因为 Sorted Set 只支持范围查询，<strong>无法直接进行聚合计算，所以，我们只能先把时间范围内的数据取回到客户端，然后在客户端自行完成聚合计算。这个方法虽然能完成聚合计算，但是会带来一定的潜在风险</strong>，也就是<strong>大量数据在 Redis 实例和客户端间频繁传输，这会和其他操作命令竞争网络资源，导致其他操作变慢。</strong></p>
<p>在我们这个物联网项目中，就需要每 3 分钟统计一下各个设备的温度状态，一旦设备温度超出了设定的阈值，就要进行报警。这是一个典型的聚合计算场景，我们可以来看看这个过程中的数据体量。</p>
<p>假设我们需要每 3 分钟计算一次的所有设备各指标的最大值，每个设备每 15 秒记录一个指标值，1 分钟就会记录 4 个值，3 分钟就会有 12 个值。我们要统计的设备指标数量有 33 个，所以，单个设备每 3 分钟记录的指标数据有将近 400 个（33 * 12 &#x3D; 396），而设备总数量有 1 万台，这样一来，每 3 分钟就有将近 400 万条（396 * 1 万 &#x3D; 396 万）数据需要在客户端和 Redis 实例间进行传输。</p>
<p><strong>为了避免客户端和 Redis 实例间频繁的大量数据传输，我们可以使用 RedisTimeSeries 来保存时间序列数据。</strong></p>
<p>RedisTimeSeries <strong>支持直接在 Redis 实例上进行聚合计算</strong>。还是以刚才每 3 分钟算一次最大值为例。<strong>在 Redis 实例上直接聚合计算，那么，对于单个设备的一个指标值来说，每 3 分钟记录的 12 条数据可以聚合计算成一个值，单个设备每 3 分钟也就只有 33 个聚合值需要传输，1 万台设备也只有 33 万条数据。数据量大约是在客户端做聚合计算的十分之一，很显然，可以减少大量数据传输对 Redis 实例网络的性能影响。</strong></p>
<p>所以，<strong>如果我们只需要进行单个时间点查询或是对某个时间范围查询的话，适合使用 Hash 和 Sorted Set 的组合，它们都是 Redis 的内在数据结构，性能好，稳定性高。但是，如果我们需要进行大量的聚合计算，同时网络带宽条件不是太好时，Hash 和 Sorted Set 的组合就不太适合了。此时，使用 RedisTimeSeries 就更加合适一些。</strong></p>
<p>好了，接下来，我们就来具体学习下 RedisTimeSeries。</p>
<h2 id="基于-RedisTimeSeries-模块保存时间序列数据"><a href="#基于-RedisTimeSeries-模块保存时间序列数据" class="headerlink" title="基于 RedisTimeSeries 模块保存时间序列数据"></a>基于 RedisTimeSeries 模块保存时间序列数据</h2><p><strong>RedisTimeSeries 是 Redis 的一个扩展模块。它专门面向时间序列数据提供了数据类型和访问接口，并且支持在 Redis 实例上直接对数据进行按时间范围的聚合计算。</strong></p>
<p><strong>因为 RedisTimeSeries 不属于 Redis 的内建功能模块，在使用时，我们需要先把它的源码单独编译成动态链接库 redistimeseries.so，再使用 loadmodule 命令进行加载，如下所示：（有Nginx那味了）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">loadmodule redistimeseries.so</span><br></pre></td></tr></table></figure>

<p>当用于时间序列数据存取时，RedisTimeSeries 的操作主要有 5 个：</p>
<ul>
<li>用 TS.CREATE 命令创建时间序列数据集合；</li>
<li>用 TS.ADD 命令插入数据；</li>
<li>用 TS.GET 命令读取最新数据；</li>
<li>用 TS.MGET 命令按标签过滤查询数据集合；</li>
<li>用 TS.RANGE 支持聚合计算的范围查询。</li>
</ul>
<p>下面，我来介绍一下如何使用这 5 个操作。</p>
<p><strong>1. 用 TS.CREATE 命令创建一个时间序列数据集合</strong></p>
<p>在 TS.CREATE 命令中，我们需要设置时间序列数据集合的 key 和数据的过期时间（以毫秒为单位）。此外，我们还可以为数据集合设置标签，来表示数据集合的属性。</p>
<p>例如，我们执行下面的命令，创建一个 key 为 device:temperature、数据有效期为 600s 的时间序列数据集合。也就是说，这个集合中的数据创建了 600s 后，就会被自动删除。最后，我们给这个集合设置了一个标签属性{device_id:1}，表明这个数据集合中记录的是属于设备 ID 号为 1 的数据。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">TS.CREATE device:temperature RETENTION 600000 LABELS device_id 1</span><br><span class="line"></span><br><span class="line">OK</span><br></pre></td></tr></table></figure>

<p><strong>2. 用 TS.ADD 命令插入数据，用 TS.GET 命令读取最新数据</strong></p>
<p>我们可以用 TS.ADD 命令往时间序列集合中插入数据，包括时间戳和具体的数值，并使用 TS.GET 命令读取数据集合中的最新一条数据。</p>
<p>例如，我们执行下列 TS.ADD 命令时，就往 device:temperature 集合中插入了一条数据，记录的是设备在 2020 年 8 月 3 日 9 时 5 分的设备温度；再执行 TS.GET 命令时，就会把刚刚插入的最新数据读取出来。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">TS.ADD device:temperature 1596416700 25.1</span><br><span class="line"></span><br><span class="line">1596416700</span><br><span class="line"></span><br><span class="line">TS.GET device:temperature </span><br><span class="line"></span><br><span class="line">25.1</span><br></pre></td></tr></table></figure>

<p><strong>3. 用 TS.MGET 命令按标签过滤查询数据集合</strong></p>
<p>在保存多个设备的时间序列数据时，我们通常会把不同设备的数据保存到不同集合中。此时，我们就可以使用 TS.MGET 命令，按照标签查询部分集合中的最新数据。在使用 TS.CREATE 创建数据集合时，我们可以给集合设置标签属性。当我们进行查询时，就可以在查询条件中对集合标签属性进行匹配，最后的查询结果里只返回匹配上的集合中的最新数据。</p>
<p>举个例子。假设我们一共用 4 个集合为 4 个设备保存时间序列数据，设备的 ID 号是 1、2、3、4，我们在创建数据集合时，把 device_id 设置为每个集合的标签。此时，我们就可以使用下列 TS.MGET 命令，以及 FILTER 设置（这个配置项用来设置集合标签的过滤条件），查询 device_id 不等于 2 的所有其他设备的数据集合，并返回各自集合中的最新的一条数据。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">TS.MGET FILTER device_id!=2 </span><br><span class="line"></span><br><span class="line">1) 1) <span class="string">&quot;device:temperature:1&quot;</span></span><br><span class="line"></span><br><span class="line">   2) (empty list or <span class="built_in">set</span>)</span><br><span class="line"></span><br><span class="line">   3) 1) (<span class="built_in">integer</span>) 1596417000</span><br><span class="line"></span><br><span class="line">       2) <span class="string">&quot;25.3&quot;</span></span><br><span class="line"></span><br><span class="line">2) 1) <span class="string">&quot;device:temperature:3&quot;</span></span><br><span class="line"></span><br><span class="line">   2) (empty list or <span class="built_in">set</span>)</span><br><span class="line"></span><br><span class="line">   3) 1) (<span class="built_in">integer</span>) 1596417000</span><br><span class="line"></span><br><span class="line">       2) <span class="string">&quot;29.5&quot;</span></span><br><span class="line"></span><br><span class="line">3) 1) <span class="string">&quot;device:temperature:4&quot;</span></span><br><span class="line"></span><br><span class="line">   2) (empty list or <span class="built_in">set</span>)</span><br><span class="line"></span><br><span class="line">   3) 1) (<span class="built_in">integer</span>) 1596417000</span><br><span class="line"></span><br><span class="line">       2) <span class="string">&quot;30.1&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>4. 用 TS.RANGE 支持需要聚合计算的范围查询</strong></p>
<p>最后，在对时间序列数据进行聚合计算时，我们可以使用 TS.RANGE 命令指定要查询的数据的时间范围，同时用 AGGREGATION 参数指定要执行的聚合计算类型。RedisTimeSeries 支持的聚合计算类型很丰富，包括求均值（avg）、求最大 &#x2F; 最小值（max&#x2F;min），求和（sum）等。</p>
<p>例如，在执行下列命令时，我们就可以按照每 180s 的时间窗口，对 2020 年 8 月 3 日 9 时 5 分和 2020 年 8 月 3 日 9 时 12 分这段时间内的数据进行均值计算了。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">TS.RANGE device:temperature 1596416700 1596417120 AGGREGATION avg 180000</span><br><span class="line"></span><br><span class="line">1) 1) (<span class="built_in">integer</span>) 1596416700</span><br><span class="line"></span><br><span class="line">   2) <span class="string">&quot;25.6&quot;</span></span><br><span class="line"></span><br><span class="line">2) 1) (<span class="built_in">integer</span>) 1596416880</span><br><span class="line"></span><br><span class="line">   2) <span class="string">&quot;25.8&quot;</span></span><br><span class="line"></span><br><span class="line">3) 1) (<span class="built_in">integer</span>) 1596417060</span><br><span class="line"></span><br><span class="line">   2) <span class="string">&quot;26.1&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>与使用 Hash 和 Sorted Set 来保存时间序列数据相比，RedisTimeSeries 是专门为时间序列数据访问设计的扩展模块，能支持在 Redis 实例上直接进行聚合计算，以及按标签属性过滤查询数据集合，当我们需要频繁进行聚合计算，以及从大量集合中筛选出特定设备或用户的数据集合时，RedisTimeSeries 就可以发挥优势了。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>在这节课，我们一起学习了如何用 Redis 保存时间序列数据。时间序列数据的写入特点是要能快速写入，而查询的特点有三个：</p>
<ul>
<li><strong>点查询，根据一个时间戳，查询相应时间的数据；</strong></li>
<li><strong>范围查询，查询起始和截止时间戳范围内的数据；</strong></li>
<li><strong>聚合计算，针对起始和截止时间戳范围内的所有数据进行计算，例如求最大 &#x2F; 最小值，求均值等。</strong></li>
</ul>
<p>关于快速写入的要求，Redis 的高性能写特性足以应对了；而针对多样化的查询需求，Redis 提供了两种方案。</p>
<p>第一种方案是，<strong>组合使用 Redis 内置的 Hash 和 Sorted Set 类型</strong>，把数据同时保存在 Hash 集合和 Sorted Set 集合中。这种方案既可以利用 Hash 类型实现对单键的快速查询，还能利用 Sorted Set 实现对范围查询的高效支持，一下子满足了时间序列数据的两大查询需求。</p>
<p>不过，第一种方案也有两个不足：</p>
<ul>
<li>一个是，在执行聚合计算时，<strong>我们需要把数据读取到客户端再进行聚合，当有大量数据要聚合时，数据传输开销大；</strong></li>
<li>另一个是，<strong>所有的数据会在两个数据类型中各保存一份，内存开销不小。不过，我们可以通过设置适当的数据过期时间，释放内存，减小内存压力。</strong></li>
</ul>
<p>我们学习的第二种实现方案是使用 RedisTimeSeries 模块。这是专门为存取时间序列数据而设计的扩展模块。<strong>和第一种方案相比，RedisTimeSeries 能支持直接在 Redis 实例上进行多种数据聚合计算，避免了大量数据在实例和客户端间传输</strong>。不过，RedisTimeSeries 的底层数据结构使用了链表，它的范围查询的复杂度是 O(N) 级别的，同时，它的 TS.GET 查询只能返回最新的数据，没有办法像第一种方案的 Hash 类型一样，可以返回任一时间点的数据。</p>
<p>所以，组合使用 Hash 和 Sorted Set，或者使用 RedisTimeSeries，在支持时间序列数据存取上各有优劣势。我给你的建议是：</p>
<ul>
<li>如果你的部署环境中<strong>网络带宽高、Redis 实例内存大，可以优先考虑第一种方案；</strong></li>
<li>如果你的<strong>部署环境中网络、内存资源有限，而且数据量大，聚合计算频繁，需要按数据集合属性查询，可以优先考虑第二种方案。</strong></li>
</ul>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提个小问题。</p>
<p>在这节课上，我提到，我们可以使用 Sorted Set 保存时间序列数据，把时间戳作为 score，把实际的数据作为 member，你觉得这样保存数据有没有潜在的风险？另外，如果你是 Redis 的开发维护者，你会把聚合计算也设计为 Sorted Set 的一个内在功能吗？</p>
<p>使用Sorted Set保存时序数据，把时间戳作为score，把实际的数据作为member，有什么潜在的风险？</p>
<p>我目前能想到的风险是，<strong>如果对某一个对象的时序数据记录很频繁的话，那么这个key很容易变成一个bigkey，在key过期释放内存时可能引发阻塞风险。所以不能把这个对象的所有时序数据存储在一个key上，而是需要拆分存储，例如可以按天&#x2F;周&#x2F;月拆分（根据具体查询需求来定）</strong>。当然，拆分key的缺点是，<strong>在查询时，可能需要客户端查询多个key后再做聚合才能得到结果。</strong></p>
<p>如果你是Redis的开发维护者，你会把聚合计算也设计为Sorted Set的内在功能吗？</p>
<p>不会。<strong>因为聚合计算是CPU密集型任务，Redis在处理请求时是单线程的，也就是它在做聚合计算时无法利用到多核CPU来提升计算速度，如果计算量太大，这也会导致Redis的响应延迟变长，影响Redis的性能</strong>。Redis的定位就是高性能的内存数据库，要求访问速度极快。<strong>所以对于时序数据的存储和聚合计算，我觉得更好的方式是交给时序数据库去做，时序数据库会针对这些存储和计算的场景做针对性优化。</strong></p>
<p>另外，在使用MULTI和EXEC命令时，<strong>建议客户端使用pipeline，当使用pipeline时，客户端会把命令一次性批量发送给服务端，然后让服务端执行。</strong></p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/ucgg9e">数据结构 - 14 | 如何在Redis中保存时间序列数据？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>替换策略：缓存满了怎么办？</title>
    <url>/2024/09/17/Redis24/</url>
    <content><![CDATA[<p><strong>Redis 缓存使用内存来保存数据，避免业务应用从后端数据库中读取数据，可以提升应用的响应速度。那么，如果我们把所有要访问的数据都放入缓存，是不是一个很好的设计选择呢？其实，这样做的性价比反而不高。</strong></p>
<p>举个例子吧。MySQL 中有 1TB 的数据，如果我们使用 Redis 把这 1TB 的数据都缓存起来，虽然应用都能在内存中访问数据了，<strong>但是，这样配置并不合理，因为性价比很低</strong>。一方面，1TB 内存的价格大约是 3.5 万元，<strong>而 1TB 磁盘的价格大约是 1000 元。另一方面，数据访问都是有局部性的，也就是我们通常所说的“八二原理”，80% 的请求实际只访问了 20% 的数据。所以，用 1TB 的内存做缓存，并没有必要。</strong></p>
<p>为了保证较高的性价比，<strong>缓存的空间容量必然要小于后端数据库的数据总量。不过，内存大小毕竟有限，随着要缓存的数据量越来越大，有限的缓存空间不可避免地会被写满。此时，该怎么办呢？</strong></p>
<p>解决这个问题就涉及到缓存系统的一个重要机制，即<strong>缓存数据的淘汰机制</strong>。简单来说，数据淘汰机制包括两步：</p>
<ul>
<li>第一，根据一定的策略，<strong>筛选出对应用访问来说“不重要”的数据；</strong></li>
<li>第二，<strong>将这些数据从缓存中删除，为新来的数据腾出空间；</strong></li>
</ul>
<p>这节课上，我就来和你聊聊缓存满了之后的数据淘汰机制。<strong>通常，我们也把它叫作缓存替换机制，同时还会讲到一系列选择淘汰数据的具体策略。了解了数据淘汰机制和相应策略，我们才可以选择合理的 Redis 配置，提高缓存命中率，提升应用的访问性能。</strong></p>
<p>不过，在学习淘汰策略之前，我们首先要知道设置缓存容量的依据和方法。毕竟，在实际使用缓存时，我们需要决定用多大的空间来缓存数据。</p>
<h2 id="设置多大的缓存容量合适？"><a href="#设置多大的缓存容量合适？" class="headerlink" title="设置多大的缓存容量合适？"></a>设置多大的缓存容量合适？</h2><p><strong>缓存容量设置得是否合理，会直接影响到使用缓存的性价比。我们通常希望以最小的代价去获得最大的收益，所以，把昂贵的内存资源用在关键地方就非常重要了。</strong></p>
<p>就像我刚才说的，<strong>实际应用中的数据访问是具有局部性的</strong>。下面有一张图，图里有红、蓝两条线，显示了不同比例数据贡献的访问量情况。<strong>蓝线代表了“八二原理”表示的数据局部性，而红线则表示在当前应用负载下，数据局部性的变化</strong>。</p>
<p>我们先看看蓝线。<strong>它表示的就是“八二原理”，有 20% 的数据贡献了 80% 的访问了，而剩余的数据虽然体量很大，但只贡献了 20% 的访问量。这 80% 的数据在访问量上就形成了一条长长的尾巴，我们也称为“长尾效应”。</strong></p>
<p><img src="/2024/09/17/Redis24/986ed247a4353524f387f0bbf76586e4.jpg" alt="img"></p>
<p>所以，<strong>如果按照“八二原理”来设置缓存空间容量，也就是把缓存空间容量设置为总数据量的 20% 的话，就有可能拦截到 80% 的访问。</strong></p>
<p>为什么说是“有可能”呢？</p>
<p>这是因为，<strong>“八二原理”是对大量实际应用的数据访问情况做了统计后，得出的一个统计学意义上的数据量和访问量的比例。具体到某一个应用来说，数据访问的规律会和具体的业务场景有关。对于最常被访问的 20% 的数据来说，它们贡献的访问量，既有可能超过 80%，也有可能不到 80%。</strong></p>
<p>我们再通过一个电商商品的场景，来说明下“有可能”这件事儿。<strong>一方面，在商品促销时，热门商品的信息可能只占到总商品数据信息量的 5%，而这些商品信息承载的可能是超过 90% 的访问请求。这时，我们只要缓存这 5% 的数据，就能获得很好的性能收益。另一方面，如果业务应用要对所有商品信息进行查询统计，这时候，即使按照“八二原理”缓存了 20% 的商品数据，也不能获得很好的访问性能，因为 80% 的数据仍然需要从后端数据库中获取。</strong></p>
<p>接下来，我们再看看数据访问局部性示意图中的红线。<strong>近年来，有些研究人员专门对互联网应用（例如视频播放网站）中，用户请求访问内容的分布情况做过分析，得到了这张图中的红线。</strong></p>
<p>在这条红线上，<strong>80% 的数据贡献的访问量，超过了传统的长尾效应中 80% 数据能贡献的访问量。原因在于，用户的个性化需求越来越多，在一个业务应用中，不同用户访问的内容可能差别很大，所以，用户请求的数据和它们贡献的访问量比例，不再具备长尾效应中的“八二原理”分布特征了。也就是说，20% 的数据可能贡献不了 80% 的访问，而剩余的 80% 数据反而贡献了更多的访问量，我们称之为重尾效应。</strong></p>
<p>正是因为 20% 的数据不一定能贡献 80% 的访问量，我们不能简单地按照“总数据量的 20%”来设置缓存最大空间容量。<strong>在实践过程中，我看到过的缓存容量占总数据量的比例，从 5% 到 40% 的都有。这个容量规划不能一概而论，是需要结合应用数据实际访问特征和成本开销来综合考虑的。</strong></p>
<p>这其实也是我一直在和你分享的经验，<strong>系统的设计选择是一个权衡的过程：大容量缓存是能带来性能加速的收益，但是成本也会更高，而小容量缓存不一定就起不到加速访问的效果。</strong>一般来说，<strong>我会建议把缓存容量设置为总数据量的 15% 到 30%，兼顾访问性能和内存空间开销</strong>。</p>
<p>对于 Redis 来说，一旦确定了缓存最大容量，比如 4GB，你就可以使用下面这个命令来设定缓存的大小了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CONFIG SET maxmemory 4gb</span><br></pre></td></tr></table></figure>

<p>不过，<strong>缓存被写满是不可避免的</strong>。即使你精挑细选，<strong>确定了缓存容量，还是要面对缓存写满时的替换操作</strong>。<strong>缓存替换需要解决两个问题：决定淘汰哪些数据，如何处理那些被淘汰的数据。</strong></p>
<p>接下来，我们就来学习下，Redis 中的数据淘汰策略。</p>
<h2 id="Redis-缓存有哪些淘汰策略？"><a href="#Redis-缓存有哪些淘汰策略？" class="headerlink" title="Redis 缓存有哪些淘汰策略？"></a>Redis 缓存有哪些淘汰策略？</h2><p>Redis 4.0 之前一共实现了 6 种内存淘汰策略，在 4.0 之后，又增加了 2 种策略。我们可以按照是否会进行数据淘汰把它们分成两类：</p>
<ul>
<li><strong>不进行数据淘汰的策略，只有 noeviction 这一种。</strong></li>
<li><strong>会进行淘汰的 7 种其他策略。</strong></li>
</ul>
<p>会进行淘汰的 7 种策略，我们可以再进一步根据淘汰候选数据集的范围把它们分成两类：</p>
<ul>
<li><strong>在设置了过期时间的数据中进行淘汰</strong>，包括 volatile-random、volatile-ttl、volatile-lru、volatile-lfu（Redis  4.0 后新增）四种。</li>
<li><strong>在所有数据范围内进行淘汰</strong>，包括 allkeys-lru、allkeys-random、allkeys-lfu（Redis 4.0 后新增）三种。</li>
</ul>
<p>我把这 8 种策略的分类，画到了一张图里：</p>
<p><img src="/2024/09/17/Redis24/04bdd13b760016ec3b30f4b02e133df6.jpg" alt="img"></p>
<p>下面我就来具体解释下各个策略。</p>
<p>默认情况下，<strong>Redis 在使用的内存空间超过 maxmemory 值时，并不会淘汰数据</strong>，也就是设定的 <strong>noeviction 策略</strong>。对应到 Redis 缓存，<strong>也就是指，一旦缓存被写满了，再有写请求来时，Redis 不再提供服务，而是直接返回错误。Redis 用作缓存时，实际的数据集通常都是大于缓存容量的，总会有新的数据要写入缓存，这个策略本身不淘汰数据，也就不会腾出新的缓存空间，我们不把它用在 Redis 缓存中。</strong></p>
<p>我们再分析下 volatile-random、volatile-ttl、volatile-lru 和 volatile-lfu 这四种淘汰策略。它们筛选的候选数据范围，被限制在已经设置了过期时间的键值对上。<strong>也正因为此，即使缓存没有写满，这些数据如果过期了，也会被删除。</strong></p>
<p>例如，<strong>我们使用 EXPIRE 命令对一批键值对设置了过期时间后，无论是这些键值对的过期时间是快到了，还是 Redis 的内存使用量达到了 maxmemory 阈值，Redis 都会进一步按照 volatile-ttl、volatile-random、volatile-lru、volatile-lfu 这四种策略的具体筛选规则进行淘汰。</strong></p>
<p>volatile-ttl 在筛选时，<strong>会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除</strong>。</p>
<p>volatile-random 就像它的名称一样，<strong>在设置了过期时间的键值对中，进行随机删除。</strong></p>
<ul>
<li><strong>volatile-lru 会使用 LRU 算法筛选设置了过期时间的键值对。</strong></li>
<li><strong>volatile-lfu 会使用 LFU 算法选择设置了过期时间的键值对。</strong></li>
</ul>
<p>可以看到，volatile-ttl 和 volatile-random 筛选规则比较简单，而 volatile-lru 因为涉及了 LRU 算法，所以我会在分析 allkeys-lru 策略时再详细解释。volatile-lfu 使用了 LFU 算法，我会在第 27 讲中具体解释，现在你只需要知道，它是在 LRU 算法的基础上，同时考虑了数据的访问时效性和数据的访问次数，可以看作是对淘汰策略的优化。</p>
<p>相对于 volatile-ttl、volatile-random、volatile-lru、volatile-lfu 这四种策略淘汰的是设置了过期时间的数据，<strong>allkeys-lru、allkeys-random、allkeys-lfu 这三种淘汰策略的备选淘汰数据范围，就扩大到了所有键值对，无论这些键值对是否设置了过期时间</strong>。它们筛选数据进行淘汰的规则是：</p>
<ul>
<li><strong>allkeys-random 策略，从所有键值对中随机选择并删除数据；</strong></li>
<li><strong>allkeys-lru 策略，使用 LRU 算法在所有数据中进行筛选。</strong></li>
<li><strong>allkeys-lfu 策略，使用 LFU 算法在所有数据中进行筛选。</strong></li>
</ul>
<p><strong>这也就是说，如果一个键值对被删除策略选中了，即使它的过期时间还没到，也需要被删除。当然，如果它的过期时间到了但未被策略选中，同样也会被删除。</strong></p>
<p>接下来，我们就看看 volatile-lru 和 allkeys-lru 策略都用到的 LRU 算法吧。LRU 算法工作机制并不复杂，我们一起学习下。</p>
<p><strong>LRU 算法的全称是 Least Recently Used，从名字上就可以看出，这是按照最近最少使用的原则来筛选数据，最不常用的数据会被筛选出来，而最近频繁使用的数据会留在缓存中。</strong></p>
<p>那具体是怎么筛选的呢？<strong>LRU 会把所有的数据组织成一个链表，链表的头和尾分别表示 MRU 端和 LRU 端，分别代表最近最常使用的数据和最近最不常用的数据。我们看一个例子。</strong></p>
<p><img src="/2024/09/17/Redis24/0201f85c84203300ae4085c60e955yy5.jpg" alt="img"></p>
<p><strong>我们现在有数据 6、3、9、20、5。如果数据 20 和 3 被先后访问，它们都会从现有的链表位置移到 MRU 端，而链表中在它们之前的数据则相应地往后移一位。因为，LRU 算法选择删除数据时，都是从 LRU 端开始，所以把刚刚被访问的数据移到 MRU 端，就可以让它们尽可能地留在缓存中。</strong></p>
<p>如果有一个新数据 15 要被写入缓存，但此时已经没有缓存空间了，也就是链表没有空余位置了，那么，LRU 算法做两件事：</p>
<ul>
<li><strong>数据 15 是刚被访问的，所以它会被放到 MRU 端；</strong></li>
<li><strong>算法把 LRU 端的数据 5 从缓存中删除，相应的链表中就没有数据 5 的记录了。</strong></li>
</ul>
<p>其实，LRU 算法背后的想法非常朴素：<strong>它认为刚刚被访问的数据，肯定还会被再次访问，所以就把它放在 MRU 端；长久不访问的数据，肯定就不会再被访问了，所以就让它逐渐后移到 LRU 端，在缓存满时，就优先删除它。</strong></p>
<p>不过，LRU 算法在实际实现时，需要用链表管理所有的缓存数据，这会<strong>带来额外的空间开销</strong>。而且，当有数据被访问时，需要在链表上把该数据移动到 MRU 端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。</p>
<p><strong>所以，在 Redis 中，LRU 算法被做了简化，以减轻数据淘汰对缓存性能的影响。</strong>具体来说，<strong>Redis 默认会记录每个数据的最近一次访问的时间戳（由键值对数据结构 RedisObject 中的 lru 字段记录）。然后，Redis 在决定淘汰的数据时，第一次会随机选出 N 个数据，把它们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 lru 字段，把 lru 字段值最小的数据从缓存中淘汰出去。</strong></p>
<p>Redis 提供了一个配置参数 maxmemory-samples，这个参数就是 Redis 选出的数据个数 N。例如，我们执行如下命令，可以让 Redis 选出 100 个数据作为候选数据集：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CONFIG SET maxmemory-samples 100</span><br></pre></td></tr></table></figure>

<p>当需要再次淘汰数据时，<strong>Redis 需要挑选数据进入第一次淘汰时创建的候选集合</strong>。这儿的挑选标准是：<strong>能进入候选集合的数据的 lru 字段值必须小于候选集合中最小的 lru 值</strong>。当有新数据进入候选数据集后，<strong>如果候选数据集中的数据个数达到了 maxmemory-samples，Redis 就把候选数据集中 lru 字段值最小的数据淘汰出去。（有点小顶堆的味道了）</strong></p>
<p><strong>这样一来，Redis 缓存不用为所有的数据维护一个大链表，也不用在每次数据访问时都移动链表项，提升了缓存的性能。</strong></p>
<p>好了，到这里，我们就学完了除了使用 LFU 算法以外的 5 种缓存淘汰策略，我再给你三个使用建议。</p>
<p><strong>优先使用 allkeys-lru 策略</strong>。这样，<strong>可以充分利用 LRU 这一经典缓存算法的优势，把最近最常访问的数据留在缓存中，提升应用的访问性能。如果你的业务数据中有明显的冷热数据区分，我建议你使用 allkeys-lru 策略。</strong></p>
<p><strong>如果业务应用中的数据访问频率相差不大，没有明显的冷热数据区分，建议使用 allkeys-random 策略，随机选择淘汰的数据就行。</strong></p>
<p><strong>如果你的业务中有置顶的需求</strong>，比如置顶新闻、置顶视频，那么，<strong>可以使用 volatile-lru 策略，同时不给这些置顶数据设置过期时间。这样一来，这些需要置顶的数据一直不会被删除，而其他数据会在过期时根据 LRU 规则进行筛选。</strong></p>
<p><strong>一旦被淘汰的数据被选定后，Redis 怎么处理这些数据呢？这就要说到缓存替换时的具体操作了。</strong></p>
<h2 id="如何处理被淘汰的数据？"><a href="#如何处理被淘汰的数据？" class="headerlink" title="如何处理被淘汰的数据？"></a>如何处理被淘汰的数据？</h2><p>一般来说，一旦被淘汰的数据选定后，<strong>如果这个数据是干净数据，那么我们就直接删除；如果这个数据是脏数据，我们需要把它写回数据库</strong>，如下图所示：</p>
<p><img src="/2024/09/17/Redis24/953e48912yy9515abf9db588d447cc5e.jpg" alt="img"></p>
<p>那怎么判断一个数据到底是干净的还是脏的呢？</p>
<p><strong>干净数据和脏数据的区别就在于，和最初从后端数据库里读取时的值相比，有没有被修改过。干净数据一直没有被修改，所以后端数据库里的数据也是最新值。在替换时，它可以被直接删除。</strong></p>
<p><strong>而脏数据就是曾经被修改过的，已经和后端数据库中保存的数据不一致了。此时，如果不把脏数据写回到数据库中，这个数据的最新值就丢失了，就会影响应用的正常使用。</strong></p>
<p><strong>这么一来，缓存替换既腾出了缓存空间，用来缓存新的数据，同时，将脏数据写回数据库，也保证了最新数据不会丢失。</strong></p>
<p>不过，对于 Redis 来说，它决定了被淘汰的数据后，会把它们删除。<strong>即使淘汰的数据是脏数据，Redis 也不会把它们写回数据库。所以，我们在使用 Redis 缓存时，如果数据被修改了，需要在数据修改时就将它写回数据库。否则，这个脏数据被淘汰时，会被 Redis 删除，而数据库里也没有最新的数据了。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>在这节课上，我围绕着“缓存满了该怎么办”这一问题，向你介绍了缓存替换时的数据淘汰策略，以及被淘汰数据的处理方法。</p>
<p><strong>Redis 4.0 版本以后一共提供了 8 种数据淘汰策略，从淘汰数据的候选集范围来看，我们有两种候选范围：一种是所有数据都是候选集，一种是设置了过期时间的数据是候选集。</strong>另外，无论是面向哪种候选数据集进行淘汰数据选择，我们都有三种策略，<strong>分别是随机选择，根据 LRU 算法选择，以及根据 LFU 算法选择。</strong>当然，<strong>当面向设置了过期时间的数据集选择淘汰数据时，我们还可以根据数据离过期时间的远近来决定</strong>。</p>
<p>一般来说，缓存系统对于选定的被淘汰数据，会根据其是干净数据还是脏数据，选择直接删除还是写回数据库。<strong>但是，在 Redis 中，被淘汰数据无论干净与否都会被删除，所以，这是我们在使用 Redis 缓存时要特别注意的：当数据修改成为脏数据时，需要在数据库中也把数据修改过来。</strong></p>
<p>选择哪种缓存策略是值得我们多加琢磨的，<strong>它在筛选数据方面是否能筛选出可能被再次访问的数据，直接决定了缓存效率的高与低</strong>。</p>
<p>很简单的一个对比，<strong>如果我们使用随机策略，刚筛选出来的要被删除的数据可能正好又被访问了，此时应用就只能花费几毫秒从数据库中读取数据了。而如果使用 LRU 策略，被筛选出来的数据往往是经过时间验证了，如果在一段时间内一直没有访问，本身被再次访问的概率也很低了。</strong></p>
<p>所以，我给你的建议是，<strong>先根据是否有始终会被频繁访问的数据（例如置顶消息），来选择淘汰数据的候选集，也就是决定是针对所有数据进行淘汰，还是针对设置了过期时间的数据进行淘汰。候选数据集范围选定后，建议优先使用 LRU 算法，也就是，allkeys-lru 或 volatile-lru 策略。</strong></p>
<p>当然，<strong>设置缓存容量的大小也很重要，我的建议是：结合实际应用的数据总量、热数据的体量，以及成本预算，把缓存空间大小设置在总数据量的 15% 到 30% 这个区间就可以。</strong></p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提一个小问题。<strong>这节课，我向你介绍了 Redis 缓存在应对脏数据时，需要在数据修改的同时，也把它写回数据库</strong>，针对我们上节课介绍的缓存读写模式：<strong>只读缓存，以及读写缓存中的两种写回策略</strong>，请你思考下，Redis 缓存对应哪一种或哪几种模式？</p>
<p>Redis在用作缓存时，使用只读缓存或读写缓存的哪种模式？</p>
<ul>
<li><strong>1、只读缓存模式：每次修改直接写入后端数据库，如果Redis缓存不命中，则什么都不用操作，如果Redis缓存命中，则删除缓存中的数据，待下次读取时从后端数据库中加载最新值到缓存中。</strong></li>
<li><strong>2、读写缓存模式+同步直写策略：由于Redis在淘汰数据时，直接在内部删除键值对，外部无法介入处理脏数据写回数据库，所以使用Redis作读写缓存时，只能采用同步直写策略，修改缓存的同时也要写入到后端数据库中，从而保证修改操作不被丢失。</strong>但这种方案在并发场景下会导致数据库和缓存的不一致，<strong>需要在特定业务场景下或者配合分布式锁使用。</strong></li>
</ul>
<p><strong>当一个系统引入缓存时，需要面临最大的问题就是，如何保证缓存和后端数据库的一致性问题，最常见的3个解决方案分别是Cache Aside、Read&#x2F;Write Throught和Write Back缓存更新策略</strong>。</p>
<ul>
<li><strong>1、Cache Aside策略：就是文章所讲的只读缓存模式。读操作命中缓存直接返回，否则从后端数据库加载到缓存再返回。</strong>写操作直接更新数据库，然后删除缓存。<strong>这种策略的优点是一切以后端数据库为准，可以保证缓存和数据库的一致性。缺点是写操作会让缓存失效，再次读取时需要从数据库中加载。这种策略是我们在开发软件时最常用的，在使用Memcached或Redis时一般都采用这种方案。</strong></li>
<li><strong>2、Read&#x2F;Write Throught策略：应用层读写只需要操作缓存，不需要关心后端数据库。</strong>应用层在操作缓存时，缓存层会自动从数据库中加载或写回到数据库中，这种策略的优点是，<strong>对于应用层的使用非常友好，只需要操作缓存即可，缺点是需要缓存层支持和后端数据库的联动。</strong></li>
<li><strong>3、Write Back策略：类似于文章所讲的读写缓存模式+异步写回策略。写操作只写缓存，比较简单。</strong>而读操作如果命中缓存则直接返回，否则需要从数据库中加载到缓存中，在加载之前，如果缓存已满，则先把需要淘汰的缓存数据写回到后端数据库中，再把对应的数据放入到缓存中。<strong>这种策略的优点是，写操作飞快（只写缓存），缺点是如果数据还未来得及写入后端数据库，系统发生异常会导致缓存和数据库的不一致。</strong>这种策略经常使用在操作系统Page Cache中，或者应对大量写操作的数据库引擎中。</li>
</ul>
<p>除了以上提到的缓存和数据库的更新策略之外，<strong>还有一个问题就是操作缓存或数据库发生异常时如何处理？例如缓存操作成功，数据库操作失败，或者反过来，还是有可能会产生不一致的情况。</strong></p>
<p>比较简单的解决方案是，<strong>根据业务设计好更新缓存和数据库的先后顺序来降低影响，或者给缓存设置较短的有效期来降低不一致的时间。如果需要严格保证缓存和数据库的一致性，即保证两者操作的原子性，这就涉及到分布式事务问题了</strong>，常见的解决方案就是我们经常听到的<strong>两阶段提交（2PC）、三阶段提交（3PC）、TCC、消息队列等方式来保证了，方案也会比较复杂，一般用在对于一致性要求较高的业务场景中。</strong></p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/ez140u">缓存 - 24 | 替换策略：缓存满了怎么办？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>缓存异常（上）：如何解决缓存和数据库的数据不一致问题？</title>
    <url>/2024/09/17/Redis25/</url>
    <content><![CDATA[<p>在实际应用 Redis 缓存时，我们经常会遇到一些异常问题，概括来说有 4 个方面：<strong>缓存中的数据和数据库中的不一致；缓存雪崩；缓存击穿和缓存穿透。</strong></p>
<p>只要我们使用 Redis 缓存，<strong>就必然会面对缓存和数据库间的一致性保证问题</strong>，这也算是 Redis 缓存应用中的“必答题”了。<strong>最重要的是，如果数据不一致，那么业务应用从缓存中读取的数据就不是最新数据，这会导致严重的错误。比如说，我们把电商商品的库存信息缓存在 Redis 中，如果库存信息不对，那么业务层下单操作就可能出错，这当然是不能接受的。</strong>所以，这节课我就重点和你聊聊这个问题。关于缓存雪崩、穿透和击穿等问题，我会在下一节课向你介绍。</p>
<p>接下来，我们就来看看，缓存和数据库之间的数据不一致是怎么引起的。</p>
<h2 id="缓存和数据库的数据不一致是如何发生的？"><a href="#缓存和数据库的数据不一致是如何发生的？" class="headerlink" title="缓存和数据库的数据不一致是如何发生的？"></a>缓存和数据库的数据不一致是如何发生的？</h2><p>首先，我们得清楚“数据的一致性”具体是啥意思。其实，这里的“一致性”包含了两种情况：</p>
<ul>
<li><strong>缓存中有数据，那么，缓存的数据值需要和数据库中的值相同；</strong></li>
<li><strong>缓存中本身没有数据，那么，数据库中的值必须是最新值。</strong></li>
</ul>
<p><strong>不符合这两种情况的，就属于缓存和数据库的数据不一致问题了</strong>。不过，当缓存的读写模式不同时，缓存数据不一致的发生情况不一样，我们的应对方法也会有所不同，<strong>所以，我们先按照缓存读写模式，来分别了解下不同模式下的缓存不一致情况</strong>。我在第 23 讲中讲过<strong>，根据是否接收写请求，我们可以把缓存分成读写缓存和只读缓存</strong>。</p>
<p>对于读写缓存来说，<strong>如果要对数据进行增删改，就需要在缓存中进行，同时还要根据采取的写回策略，决定是否同步写回到数据库中。</strong></p>
<ul>
<li>同步直写策略：写缓存时，也同步写数据库，缓存和数据库中的数据一致；</li>
<li>异步写回策略：写缓存时不同步写数据库，等到数据从缓存中淘汰时，再写回数据库。使用这种策略时，如果数据还没有写回数据库，缓存就发生了故障，那么，此时，数据库就没有最新的数据了。</li>
</ul>
<p>所以，对于读写缓存来说，要想保证缓存和数据库中的数据一致，<strong>就要采用同步直写策略</strong>。不过，需要注意的是，<strong>如果采用这种策略，就需要同时更新缓存和数据库</strong>。所以，<strong>我们要在业务应用中使用事务机制，来保证缓存和数据库的更新具有原子性，也就是说，两者要不一起更新，要不都不更新，返回错误信息，进行重试。否则，我们就无法实现同步直写</strong>。</p>
<p>当然，在有些场景下，我们对数据一致性的要求可能不是那么高，比如说缓存的是电商商品的非关键属性或者短视频的创建或修改时间等，<strong>那么，我们可以使用异步写回策略</strong>。</p>
<p>下面我们再来说说<strong>只读缓存</strong>。对于只读缓存来说，<strong>如果有数据新增，会直接写入数据库；而有数据删改时，就需要把只读缓存中的数据标记为无效</strong>。这样一来，应用后续再访问这些增删改的数据时，因为缓存中没有相应的数据，就会发生缓存缺失。<strong>此时，应用再从数据库中把数据读入缓存，这样后续再访问数据时，就能够直接从缓存中读取了</strong>。</p>
<p>接下来，我以 Tomcat 向 MySQL 中写入和删改数据为例，来给你解释一下，数据的增删改操作具体是如何进行的，如下图所示：</p>
<p><img src="/2024/09/17/Redis25/15ae0147459ecc46436f35a0f3e5yydc.jpg" alt="img"></p>
<p>从图中可以看到，<strong>Tomcat 上运行的应用，无论是新增（Insert 操作）、修改（Update 操作）、还是删除（Delete 操作）数据 X，都会直接在数据库中增改删。当然，如果应用执行的是修改或删除操作，还会删除缓存的数据 X</strong>。</p>
<p>那么，这个过程中会不会出现数据不一致的情况呢？考虑到新增数据和删改数据的情况不一样，所以我们分开来看。</p>
<p><strong>1. 新增数据</strong></p>
<p><strong>如果是新增数据，数据会直接写到数据库中，不用对缓存做任何操作，此时，缓存中本身就没有新增数据，而数据库中是最新值，这种情况符合我们刚刚所说的一致性的第 2 种情况，所以，此时，缓存和数据库的数据是一致的</strong>。</p>
<p><strong>2. 删改数据</strong></p>
<p><strong>如果发生删改操作，应用既要更新数据库，也要在缓存中删除数据</strong>。这两个操作如果无法保证原子性，也就是说，要不都完成，要不都没完成，此时，就会出现数据不一致问题了。这个问题比较复杂，我们来分析一下。</p>
<p><strong>我们假设应用先删除缓存，再更新数据库，如果缓存删除成功，但是数据库更新失败，那么，应用再访问数据时，缓存中没有数据，就会发生缓存缺失。然后，应用再访问数据库，但是数据库中的值为旧值，应用就访问到旧值了</strong>。</p>
<p>我来举个例子说明一下，可以先看看下面的图片。</p>
<p><img src="/2024/09/17/Redis25/b305a6355c9da145e4d1f86d23f4f0ae.jpg" alt="img"></p>
<p><strong>应用要把数据 X 的值从 10 更新为 3，先在 Redis 缓存中删除了 X 的缓存值，但是更新数据库却失败了。如果此时有其他并发的请求访问 X，会发现 Redis 中缓存缺失，紧接着，请求就会访问数据库，读到的却是旧值 10。</strong></p>
<p>你可能会问，如果我们先更新数据库，再删除缓存中的值，是不是就可以解决这个问题呢？我们再来分析下。</p>
<p><strong>如果应用先完成了数据库的更新，但是，在删除缓存时失败了，那么，数据库中的值是新值，而缓存中的是旧值，这肯定是不一致的。这个时候，如果有其他的并发请求来访问数据，按照正常的缓存访问流程，就会先在缓存中查询，但此时，就会读到旧值了</strong>。</p>
<p>我还是借助一个例子来说明一下。</p>
<p><img src="/2024/09/17/Redis25/767b4b2b1bafffd9a4b6368f05930a77.jpg" alt="img"></p>
<p>应用要把数据 X 的值从 10 更新为 3，先成功更新了数据库，然后在 Redis 缓存中删除 X 的缓存，但是这个操作却失败了，这个时候，数据库中 X 的新值为 3，Redis 中的 X 的缓存值为 10，这肯定是不一致的。<strong>如果刚好此时有其他客户端也发送请求访问 X，会先在 Redis 中查询，该客户端会发现缓存命中，但是读到的却是旧值 10</strong>。</p>
<p><strong>好了，到这里，我们可以看到，在更新数据库和删除缓存值的过程中，无论这两个操作的执行顺序谁先谁后，只要有一个操作失败了，就会导致客户端读取到旧值</strong>。我画了下面这张表，总结了刚刚所说的这两种情况。</p>
<p><img src="/2024/09/17/Redis25/2c376b536aff9d14d8606499f401cdac.jpg" alt="img"></p>
<p>问题发生的原因我们知道了，那该怎么解决呢？</p>
<h2 id="如何解决数据不一致问题？"><a href="#如何解决数据不一致问题？" class="headerlink" title="如何解决数据不一致问题？"></a>如何解决数据不一致问题？</h2><p>首先，我给你介绍一种方法：<strong>重试机制</strong>。</p>
<p><strong>具体来说，可以把要删除的缓存值或者是要更新的数据库值暂存到消息队列中（例如使用 Kafka 消息队列）</strong>。<strong>当应用没有能够成功地删除缓存值或者是更新数据库值时，可以从消息队列中重新读取这些值，然后再次进行删除或更新</strong>。</p>
<p><strong>如果能够成功地删除或更新，我们就要把这些值从消息队列中去除，以免重复操作，此时，我们也可以保证数据库和缓存的数据一致了</strong>。否则的话，我们还需要再次进行重试。<strong>如果重试超过的一定次数，还是没有成功，我们就需要向业务层发送报错信息了</strong>。</p>
<p><strong>下图显示了先更新数据库，再删除缓存值时，如果缓存删除失败，再次重试后删除成功的情况</strong>，你可以看下。</p>
<p><img src="/2024/09/17/Redis25/74a66b9ce185d7c5b53986fc522dfcab.jpg" alt="img"></p>
<p><strong>刚刚说的是在更新数据库和删除缓存值的过程中，其中一个操作失败的情况，实际上，即使这两个操作第一次执行时都没有失败，当有大量并发请求时，应用还是有可能读到不一致的数据</strong>。</p>
<p>同样，我们按照不同的删除和更新顺序，分成两种情况来看。在这两种情况下，我们的解决方法也有所不同。</p>
<p><strong>情况一：先删除缓存，再更新数据库。</strong></p>
<p><strong>假设线程 A 删除缓存值后，还没有来得及更新数据库（比如说有网络延迟），线程 B 就开始读取数据了，那么这个时候，线程 B 会发现缓存缺失，就只能去数据库读取</strong>。这会带来两个问题：</p>
<p>线程 B 读取到了旧值；</p>
<p><strong>线程 B 是在缓存缺失的情况下读取的数据库，所以，它还会把旧值写入缓存，这可能会导致其他线程从缓存中读到旧值</strong>。</p>
<p><strong>等到线程 B 从数据库读取完数据、更新了缓存后，线程 A 才开始更新数据库，此时，缓存中的数据是旧值，而数据库中的是最新值，两者就不一致了</strong>。</p>
<p>我用一张表来汇总下这种情况。</p>
<p><img src="/2024/09/17/Redis25/857c2b5449d9a04de6fe93yy1e355c12.jpg" alt="img"></p>
<p>这该怎么办呢？我来给你提供一种解决方案。</p>
<p><strong>在线程 A 更新完数据库值以后，我们可以让它先 sleep 一小段时间，再进行一次缓存删除操作。</strong></p>
<p><strong>之所以要加上 sleep 的这段时间，就是为了让线程 B 能够先从数据库读取数据，再把缺失的数据写入缓存，然后，线程 A 再进行删除</strong>。所以，线程 A sleep 的时间，就需要大于线程 B 读取数据再写入缓存的时间。这个时间怎么确定呢？<strong>建议你在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，以此为基础来进行估算。</strong></p>
<p><strong>这样一来，其它线程读取数据时，会发现缓存缺失，所以会从数据库中读取最新值。因为这个方案会在第一次删除缓存值后，延迟一段时间再次进行删除，所以我们也把它叫做“延迟双删”</strong>。</p>
<p>下面的这段伪代码就是<strong>“延迟双删”</strong>方案的示例，你可以看下。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">redis.delKey(X)</span><br><span class="line"></span><br><span class="line">db.update(X)</span><br><span class="line"></span><br><span class="line">Thread.sleep(N)</span><br><span class="line"></span><br><span class="line">redis.delKey(X)</span><br></pre></td></tr></table></figure>

<p><strong>情况二：先更新数据库值，再删除缓存值。</strong></p>
<p><strong>如果线程 A 删除了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了，那么此时，线程 B 查询缓存时，发现缓存命中，就会直接从缓存中读取旧值</strong>。不过，在这种情况下，<strong>如果其他线程并发读缓存的请求不多，那么，就不会有很多请求读取到旧值。而且，线程 A 一般也会很快删除缓存值，这样一来，其他线程再次读取时，就会发生缓存缺失，进而从数据库中读取最新值</strong>。所以，这种情况对业务的影响较小。</p>
<p>我再画一张表，带你总结下先更新数据库、再删除缓存值的情况。</p>
<p><img src="/2024/09/17/Redis25/a1c66ee114yyc9f37f2a35f21b46010b.jpg" alt="img"></p>
<p>好了，到这里，我们了解到了，<strong>缓存和数据库的数据不一致一般是由两个原因导致的，我给你提供了相应的解决方案</strong>。</p>
<p>删除缓存值或更新数据库失败而导致数据不一致，<strong>你可以使用重试机制确保删除或更新操作成功</strong>。</p>
<p>在删除缓存值、更新数据库的这两步操作中，<strong>有其他线程的并发读操作，导致其他线程读取到旧值，应对方案是延迟双删</strong>。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><strong>在这节课，我们学习了在使用 Redis 缓存时，最常遇见的一个问题，也就是缓存和数据库不一致的问题</strong>。针对这个问题，<strong>我们可以分成读写缓存和只读缓存两种情况进行分析</strong>。</p>
<p>对于读写缓存来说，如果我们采用<strong>同步写回</strong>策略，<strong>那么可以保证缓存和数据库中的数据一致</strong>。只读缓存的情况比较复杂，我总结了一张表，以便于你更加清晰地了解数据不一致的问题原因、现象和应对方案。</p>
<p><img src="/2024/09/17/Redis25/11ae5e620c63de76448bc658fe6a496f.jpg" alt="img"></p>
<p>希望你能把我总结的这张表格放入到你的学习笔记中，时不时复习一下。</p>
<p>最后，我还想再多说几句。<strong>在大多数业务场景下，我们会把 Redis 作为只读缓存使用。针对只读缓存来说，我们既可以先删除缓存值再更新数据库，也可以先更新数据库再删除缓存</strong>。<strong>我的建议是，优先使用先更新数据库再删除缓存的方法</strong>，原因主要有两个：</p>
<ul>
<li><strong>先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力；</strong></li>
<li><strong>如果业务应用中读取数据库和写缓存的时间不好估算，那么，延迟双删中的等待时间就不好设置。</strong></li>
</ul>
<p>不过，<strong>当使用先更新数据库再删除缓存时，也有个地方需要注意，如果业务层要求必须读取一致的数据，那么，我们就需要在更新数据库时，先在 Redis 缓存客户端暂存并发读请求，等数据库更新完、缓存值删除后，再读取数据，从而保证数据一致性。</strong></p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提个小问题。这节课，我提到，在只读缓存中进行数据的删改操作时，需要在缓存中删除相应的缓存值。我想请你思考一下，如果在这个过程中，我们不是删除缓存值，而是直接更新缓存的值，你觉得和删除缓存值相比，有什么好处和不足吗？</p>
<p>数据在删改操作时，如果不是删除缓存值，而是直接更新缓存的值，你觉得和删除缓存值相比，有什么好处和不足？</p>
<p><strong>这种情况相当于把Redis当做读写缓存使用，删改操作同时操作数据库和缓存。</strong></p>
<ul>
<li><strong>1、先更新数据库，再更新缓存：如果更新数据库成功，但缓存更新失败，此时数据库中是最新值，但缓存中是旧值，后续的读请求会直接命中缓存，得到的是旧值。</strong></li>
<li><strong>2、先更新缓存，再更新数据库：如果更新缓存成功，但数据库更新失败，此时缓存中是最新值，数据库中是旧值，后续读请求会直接命中缓存，但得到的是最新值，短期对业务影响不大</strong>。但是，一旦缓存过期或者满容后被淘汰，读请求就会从数据库中重新加载旧值到缓存中，之后的读请求会从缓存中得到旧值，对业务产生影响。</li>
</ul>
<p>同样地，<strong>针对这种其中一个操作可能失败的情况，也可以使用重试机制解决，把第二步操作放入到消息队列中，消费者从消息队列取出消息，再更新缓存或数据库，成功后把消息从消息队列删除，否则进行重试，以此达到数据库和缓存的最终一致。</strong></p>
<p>以上是没有并发请求的情况。如果存在并发读写，也会产生不一致，分为以下4种场景。</p>
<ul>
<li><strong>1、先更新数据库，再更新缓存，写+读并发</strong>：线程A先更新数据库，之后线程B读取数据，此时线程B会命中缓存，读取到旧值，之后线程A更新缓存成功，后续的读请求会命中缓存得到最新值。这种场景下，线程A未更新完缓存之前，在这期间的读请求会短暂读到旧值，对业务短暂影响。</li>
<li><strong>2、先更新缓存，再更新数据库，写+读并发</strong>：线程A先更新缓存成功，之后线程B读取数据，此时线程B命中缓存，读取到最新值后返回，之后线程A更新数据库成功。这种场景下，虽然线程A还未更新完数据库，数据库会与缓存存在短暂不一致，但在这之前进来的读请求都能直接命中缓存，获取到最新值，所以对业务没影响。</li>
<li><strong>3、先更新数据库，再更新缓存，写+写并发</strong>：线程A和线程B同时更新同一条数据，更新数据库的顺序是先A后B，但更新缓存时顺序是先B后A，这会导致数据库和缓存的不一致。</li>
<li><strong>4、先更新缓存，再更新数据库，写+写并发</strong>：与场景3类似，线程A和线程B同时更新同一条数据，更新缓存的顺序是先A后B，但是更新数据库的顺序是先B后A，这也会导致数据库和缓存的不一致。</li>
</ul>
<p>场景1和2对业务影响较小，场景3和4会造成数据库和缓存不一致，影响较大。<strong>也就是说，在读写缓存模式下，写+读并发对业务的影响较小，而写+写并发时，会造成数据库和缓存的不一致。</strong></p>
<p><strong>针对场景3和4的解决方案是，对于写请求，需要配合分布式锁使用。写请求进来时，针对同一个资源的修改操作，先加分布式锁，这样同一时间只允许一个线程去更新数据库和缓存，没有拿到锁的线程把操作放入到队列中，延时处理。用这种方式保证多个线程操作同一资源的顺序性，以此保证一致性。</strong></p>
<p>综上，<strong>使用读写缓存同时操作数据库和缓存时，因为其中一个操作失败导致不一致的问题，同样可以通过消息队列重试来解决。而在并发的场景下，读+写并发对业务没有影响或者影响较小，而写+写并发时需要配合分布式锁的使用，才能保证缓存和数据库的一致性</strong>。</p>
<p>另外，<strong>读写缓存模式由于会同时更新数据库和缓存：</strong></p>
<ul>
<li><strong>优点是，缓存中一直会有数据，如果更新操作后会立即再次访问，可以直接命中缓存，能够降低读请求对于数据库的压力（没有了只读缓存的删除缓存导致缓存缺失和再加载的过程）</strong>。</li>
<li>缺点是，<strong>如果更新后的数据，之后很少再被访问到，会导致缓存中保留的不是最热的数据，缓存利用率不高（只读缓存中保留的都是热数据），所以读写缓存比较适合用于读写相当的业务场景</strong>。</li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/hr8ud5">缓存 - 25 | 缓存异常（上）：如何解决缓存和数据库的数据不一致问题？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>无锁的原子操作：Redis如何应对并发访问？</title>
    <url>/2024/09/17/Redis29/</url>
    <content><![CDATA[<p>我们在使用 Redis 时，<strong>不可避免地会遇到并发访问的问题</strong>，<strong>比如说如果多个用户同时下单，就会对缓存在 Redis 中的商品库存并发更新</strong>。一旦有了并发写操作，数据就会被修改，<strong>如果我们没有对并发写请求做好控制，就可能导致数据被改错，影响到业务的正常使用（例如库存数据错误，导致下单异常）</strong>。</p>
<p>为了保证并发访问的正确性，Redis 提供了两种方法，分别是<strong>加锁和原子操作</strong>。</p>
<p><strong>加锁是一种常用的方法，在读取数据前，客户端需要先获得锁，否则就无法进行操作。当一个客户端获得锁后，就会一直持有这把锁，直到客户端完成数据更新，才释放这把锁。</strong></p>
<p>看上去好像是一种很好的方案，但是，其实这里会有两个问题：</p>
<ul>
<li>一个是，<strong>如果加锁操作多，会降低系统的并发访问性能；</strong></li>
<li>第二个是，<strong>Redis 客户端要加锁时，需要用到分布式锁，而分布式锁实现复杂，需要用额外的存储系统来提供加解锁操作，我会在下节课向你介绍。</strong></li>
</ul>
<p><strong>原子操作是另一种提供并发访问控制的方法</strong>。<strong>原子操作是指执行过程保持原子性的操作，而且原子操作执行时并不需要再加锁，实现了无锁操作。这样一来，既能保证并发控制，还能减少对系统并发性能的影响</strong>。</p>
<p>这节课，我就来和你聊聊 Redis 中的原子操作。原子操作的目标是实现并发访问控制，那么当有并发访问请求时，我们具体需要控制什么呢？接下来，我就先向你介绍下并发控制的内容。</p>
<h2 id="并发访问中需要对什么进行控制？"><a href="#并发访问中需要对什么进行控制？" class="headerlink" title="并发访问中需要对什么进行控制？"></a>并发访问中需要对什么进行控制？</h2><p>我们说的并发访问控制，<strong>是指对多个客户端访问操作同一份数据的过程进行控制，以保证任何一个客户端发送的操作在 Redis 实例上执行时具有互斥性</strong>。例如，客户端 A 的访问操作在执行时，客户端 B 的操作不能执行，需要等到 A 的操作结束后，才能执行。</p>
<p>并发访问控制对应的操作<strong>主要是数据修改操作</strong>。当客户端需要修改数据时，基本流程分成两步：</p>
<ul>
<li><strong>客户端先把数据读取到本地，在本地进行修改；</strong></li>
<li><strong>客户端修改完数据后，再写回 Redis。</strong></li>
</ul>
<p>我们把这个流程叫做<strong>“读取 - 修改 - 写回”操作（Read-Modify-Write，简称为 RMW 操作）</strong>。当有多个客户端对同一份数据执行 RMW 操作的话，<strong>我们就需要让 RMW 操作涉及的代码以原子性方式执行。访问同一份数据的 RMW 操作代码，就叫做临界区代码。</strong></p>
<p>不过，当有多个客户端并发执行临界区代码时，就会存在一些潜在问题，接下来，我用一个多客户端更新商品库存的例子来解释一下。</p>
<p>我们先看下临界区代码。假设客户端要对商品库存执行扣减 1 的操作，伪代码如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">current = GET(id)</span><br><span class="line"></span><br><span class="line">current--</span><br><span class="line"></span><br><span class="line">SET(id, current)</span><br></pre></td></tr></table></figure>

<p><strong>可以看到，客户端首先会根据商品 id，从 Redis 中读取商品当前的库存值 current（对应 Read)，然后，客户端对库存值减 1（对应 Modify），再把库存值写回 Redis（对应 Write）。当有多个客户端执行这段代码时，这就是一份临界区代码。</strong></p>
<p>如果我们对临界区代码的执行没有控制机制，就会出现数据更新错误。在刚才的例子中，假设现在有两个客户端 A 和 B，同时执行刚才的临界区代码，就会出现错误，你可以看下下面这张图。</p>
<p><img src="/2024/09/17/Redis29/dce821cd00c1937b4aab1f130424335c.jpg" alt="img"></p>
<p>可以看到，客户端 A 在 t1 时读取库存值 10 并扣减 1，在 t2 时，客户端 A 还没有把扣减后的库存值 9 写回 Redis，而在此时，客户端 B 读到库存值 10，也扣减了 1，B 记录的库存值也为 9 了。等到 t3 时，A 往 Redis 写回了库存值 9，而到 t4 时，B 也写回了库存值 9。</p>
<p>如果按正确的逻辑处理，<strong>客户端 A 和 B 对库存值各做了一次扣减，库存值应该为 8。所以，这里的库存值明显更新错了。</strong></p>
<p>出现这个现象的原因是，<strong>临界区代码中的客户端读取数据、更新数据、再写回数据涉及了三个操作，而这三个操作在执行时并不具有互斥性，多个客户端基于相同的初始值进行修改，而不是基于前一个客户端修改后的值再修改</strong>。</p>
<p>为了保证数据并发修改的正确性，<strong>我们可以用锁把并行操作变成串行操作，串行操作就具有互斥性。一个客户端持有锁后，其他客户端只能等到锁释放，才能拿锁再进行修改。</strong></p>
<p>下面的伪代码显示了使用锁来控制临界区代码的执行情况，你可以看下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">LOCK()</span><br><span class="line"></span><br><span class="line">current = GET(id)</span><br><span class="line"></span><br><span class="line">current--</span><br><span class="line"></span><br><span class="line">SET(id, current)</span><br><span class="line"></span><br><span class="line">UNLOCK()</span><br></pre></td></tr></table></figure>

<p>虽然加锁保证了互斥性，但是<strong>加锁也会导致系统并发性能降低</strong>。</p>
<p>如下图所示，<strong>当客户端 A 加锁执行操作时，客户端 B、C 就需要等待。A 释放锁后，假设 B 拿到锁，那么 C 还需要继续等待，所以，t1 时段内只有 A 能访问共享数据，t2 时段内只有 B 能访问共享数据，系统的并发性能当然就下降了</strong>。</p>
<p><img src="/2024/09/17/Redis29/845b4694700264482d64a3dbb7a36525.jpg" alt="img"></p>
<p>和加锁类似，<strong>原子操作也能实现并发控制，但是原子操作对系统并发性能的影响较小，接下来，我们就来了解下 Redis 中的原子操作。</strong></p>
<h2 id="Redis-的两种原子操作方法"><a href="#Redis-的两种原子操作方法" class="headerlink" title="Redis 的两种原子操作方法"></a>Redis 的两种原子操作方法</h2><p>为了实现并发控制要求的临界区代码互斥执行，Redis 的原子操作采用了两种方法：</p>
<ul>
<li>把多个操作在 <strong>Redis 中实现成一个操作，也就是单命令操作；</strong></li>
<li>把多个操作<strong>写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本。</strong></li>
</ul>
<p>我们先来看下 Redis 本身的单命令操作。</p>
<p><strong>Redis 是使用单线程来串行处理客户端的请求操作命令的，所以，当 Redis 执行某个命令操作时，其他命令是无法执行的，这相当于命令操作是互斥执行的</strong>。当然，<strong>Redis 的快照生成、AOF 重写这些操作，可以使用后台线程或者是子进程执行，也就是和主线程的操作并行执行</strong>。不过，<strong>这些操作只是读取数据，不会修改数据，所以，我们并不需要对它们做并发控制</strong>。</p>
<p>你可能也注意到了，<strong>虽然 Redis 的单个命令操作可以原子性地执行，但是在实际应用中，数据修改时可能包含多个操作，至少包括读数据、数据增减、写回数据三个操作，这显然就不是单个命令操作了，那该怎么办呢</strong>？</p>
<p>别担心，<strong>Redis 提供了 INCR&#x2F;DECR 命令，把这三个操作转变为一个原子操作了。INCR&#x2F;DECR 命令可以对数据进行增值 &#x2F; 减值操作，而且它们本身就是单个命令操作，Redis 在执行它们时，本身就具有互斥性</strong>。</p>
<p>比如说，在刚才的库存扣减例子中，<strong>客户端可以使用下面的代码，直接完成对商品 id 的库存值减 1 操作。即使有多个客户端执行下面的代码，也不用担心出现库存值扣减错误的问题</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">DECR id </span><br></pre></td></tr></table></figure>

<p>所以，<strong>如果我们执行的 RMW 操作是对数据进行增减值的话，Redis 提供的原子操作 INCR 和 DECR 可以直接帮助我们进行并发控制</strong>。</p>
<p>但是，如果我们要执行的操作不是简单地增减数据，<strong>而是有更加复杂的判断逻辑或者是其他操作，那么，Redis 的单命令操作已经无法保证多个操作的互斥执行了。所以，这个时候，我们需要使用第二个方法，也就是 Lua 脚本</strong>。</p>
<p><strong>Redis 会把整个 Lua 脚本作为一个整体执行，在执行的过程中不会被其他命令打断，从而保证了 Lua 脚本中操作的原子性。如果我们有多个操作要执行，但是又无法用 INCR&#x2F;DECR 这种命令操作来实现，就可以把这些要执行的操作编写到一个 Lua 脚本中</strong>。然后，<strong>我们可以使用 Redis 的 EVAL 命令来执行脚本。这样一来，这些操作在执行时就具有了互斥性</strong>。</p>
<p>我再给你举个例子，来具体解释下 Lua 的使用。</p>
<p>当一个业务应用的访问用户增加时，我们有时需要限制某个客户端在一定时间范围内的访问次数，比如爆款商品的购买限流、社交网络中的每分钟点赞次数限制等。</p>
<p>那该怎么限制呢？<strong>我们可以把客户端 IP 作为 key，把客户端的访问次数作为 value，保存到 Redis 中。客户端每访问一次后，我们就用 INCR 增加访问次数</strong>。</p>
<p>不过，在这种场景下，<strong>客户端限流其实同时包含了对访问次数和时间范围的限制，例如每分钟的访问次数不能超过 20。所以，我们可以在客户端第一次访问时，给对应键值对设置过期时间，例如设置为 60s 后过期。同时，在客户端每次访问时，我们读取客户端当前的访问次数，如果次数超过阈值，就报错，限制客户端再次访问。你可以看下下面的这段代码，它实现了对客户端每分钟访问次数不超过 20 次的限制。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//获取ip对应的访问次数</span><br><span class="line"></span><br><span class="line">current = GET(ip)</span><br><span class="line"></span><br><span class="line">//如果超过访问次数超过20次，则报错</span><br><span class="line"></span><br><span class="line">IF current != NULL AND current &gt; 20 THEN</span><br><span class="line"></span><br><span class="line">     ERROR &quot;exceed 20 accesses per second&quot;</span><br><span class="line"></span><br><span class="line">ELSE</span><br><span class="line"></span><br><span class="line">     //如果访问次数不足20次，增加一次访问计数</span><br><span class="line"></span><br><span class="line">     value = INCR(ip)</span><br><span class="line"></span><br><span class="line">     //如果是第一次访问，将键值对的过期时间设置为60s后</span><br><span class="line"></span><br><span class="line">     IF value == 1 THEN</span><br><span class="line"></span><br><span class="line">         EXPIRE(ip,60)</span><br><span class="line"></span><br><span class="line">     END</span><br><span class="line"></span><br><span class="line">     //执行其他操作</span><br><span class="line"></span><br><span class="line">     DO THINGS</span><br><span class="line"></span><br><span class="line">END</span><br></pre></td></tr></table></figure>

<p>可以看到，在这个例子中，我们已经使用了 INCR 来原子性地增加计数。但是，客户端限流的逻辑不只有计数，还包括<strong>访问次数判断和过期时间设置</strong>。</p>
<p>对于这些操作，<strong>我们同样需要保证它们的原子性。否则，如果客户端使用多线程访问，访问次数初始值为 0，第一个线程执行了 INCR(ip) 操作后，第二个线程紧接着也执行了 INCR(ip)，此时，ip 对应的访问次数就被增加到了 2，我们就无法再对这个 ip 设置过期时间了。这样就会导致，这个 ip 对应的客户端访问次数达到 20 次之后，就无法再进行访问了。即使过了 60s，也不能再继续访问，显然不符合业务要求。</strong></p>
<p>所以，这个例子中的操作无法用 Redis 单个命令来实现，<strong>此时，我们就可以使用 Lua 脚本来保证并发控制。我们可以把访问次数加 1、判断访问次数是否为 1，以及设置过期时间这三个操作写入一个 Lua 脚本，如下所示：</strong></p>
<figure class="highlight lua"><table><tr><td class="code"><pre><span class="line"><span class="keyword">local</span> current</span><br><span class="line"></span><br><span class="line">current = redis.call(<span class="string">&quot;incr&quot;</span>,KEYS[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">tonumber</span>(current) == <span class="number">1</span> <span class="keyword">then</span></span><br><span class="line"></span><br><span class="line">     redis.call(<span class="string">&quot;expire&quot;</span>,KEYS[<span class="number">1</span>],<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>

<p>假设我们编写的脚本名称为 lua.script，我们接着就可以使用 Redis 客户端，带上 eval 选项，来执行该脚本。脚本所需的参数将通过以下命令中的 keys 和 args 进行传递。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">redis-cli  --eval lua.script  keys , args</span><br></pre></td></tr></table></figure>

<p>这样一来，访问次数加 1、判断访问次数是否为 1，以及设置过期时间这三个操作就可以原子性地执行了。<strong>即使客户端有多个线程同时执行这个脚本，Redis 也会依次串行执行脚本代码，避免了并发操作带来的数据错误。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><strong>在并发访问时，并发的 RMW 操作会导致数据错误，所以需要进行并发控制。所谓并发控制，就是要保证临界区代码的互斥执行。</strong></p>
<p><strong>Redis 提供了两种原子操作的方法来实现并发控制，分别是单命令操作和 Lua 脚本</strong>。因为原子操作本身<strong>不会对太多的资源限制访问</strong>，可以维持较高的系统并发性能。</p>
<p>但是，单命令原子操作的适用范围较小，并不是所有的 RMW 操作都能转变成单命令的原子操作（例如 INCR&#x2F;DECR 命令只能在读取数据后做原子增减），<strong>当我们需要对读取的数据做更多判断，或者是我们对数据的修改不是简单的增减时，单命令操作就不适用了</strong>。</p>
<p>而 Redis 的 Lua 脚本可以包含多个操作，这些操作都会以原子性的方式执行，绕开了单命令操作的限制。<strong>不过，如果把很多操作都放在 Lua 脚本中原子执行，会导致 Redis 执行脚本的时间增加，同样也会降低 Redis 的并发性能</strong>。所以，我给你一个小建议：<strong>在编写 Lua 脚本时，你要避免把不需要做并发控制的操作写入脚本中</strong>。</p>
<p>当然，<strong>加锁也能实现临界区代码的互斥执行，只是如果有多个客户端加锁时，就需要分布式锁的支持了</strong>。所以，下节课，我就来和你聊聊分布式锁的实现。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我向你提个小问题，<strong>Redis 在执行 Lua 脚本时，是可以保证原子性的，那么，在我举的 Lua 脚本例子（lua.script）中，你觉得是否需要把读取客户端 ip 的访问次数，也就是 GET(ip)，以及判断访问次数是否超过 20 的判断逻辑，也加到 Lua 脚本中吗？</strong></p>
<p>是否需要把读取客户端 ip 的访问次数 GET(ip)，以及判断访问次数是否超过 20 的判断逻辑，也加到 Lua 脚本中？</p>
<p>我觉得不需要，理由主要有2个。</p>
<ul>
<li>1、这2个逻辑都是读操作，<strong>不会对资源临界区产生修改，所以不需要做并发控制</strong>。</li>
<li>2、<strong>减少 lua 脚本中的命令，可以降低Redis执行脚本的时间，避免阻塞 Redis</strong>。</li>
</ul>
<p>另外使用lua脚本时，还有一些注意点：</p>
<ul>
<li>1、<strong>lua 脚本尽量只编写通用的逻辑代码，避免直接写死变量。变量通过外部调用方传递进来，这样 lua 脚本的可复用度更高。</strong></li>
<li>2、<strong>建议先使用SCRIPT LOAD命令把 lua 脚本加载到 Redis 中，然后得到一个脚本唯一摘要值，再通过EVALSHA命令 + 脚本摘要值来执行脚本，这样可以避免每次发送脚本内容到 Redis，减少网络开销。</strong></li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/gpwp4h">锁 - 29 | 无锁的原子操作：Redis如何应对并发访问？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>如何使用Redis实现分布式锁？</title>
    <url>/2024/09/17/Redis30/</url>
    <content><![CDATA[<p>上节课，我提到，<strong>在应对并发问题时，除了原子操作，Redis 客户端还可以通过加锁的方式，来控制并发写操作对共享数据的修改，从而保证数据的正确性</strong>。</p>
<p>但是，Redis 属于分布式系统，当有多个客户端需要争抢锁时，我们必须要保证，<strong>这把锁不能是某个客户端本地的锁</strong>。否则的话，其它客户端是无法访问这把锁的，当然也就不能获取这把锁了。</p>
<p>所以，在分布式系统中，<strong>当有多个客户端需要获取锁时，我们需要分布式锁。此时，锁是保存在一个共享存储系统中的，可以被多个客户端共享访问和获取</strong>。</p>
<p><strong>Redis 本身可以被多个客户端共享访问，正好就是一个共享存储系统，可以用来保存分布式锁。而且 Redis 的读写性能高，可以应对高并发的锁操作场景。所以，这节课，我就来和你聊聊如何基于 Redis 实现分布式锁</strong>。</p>
<p>我们日常在写程序的时候，<strong>经常会用到单机上的锁，你应该也比较熟悉了。而分布式锁和单机上的锁既有相似性，但也因为分布式锁是用在分布式场景中，所以又具有一些特殊的要求。</strong></p>
<p>所以，接下来，我就先带你对比下分布式锁和单机上的锁，找出它们的联系与区别，这样就可以加深你对分布式锁的概念和实现要求的理解。</p>
<h2 id="单机上的锁和分布式锁的联系与区别"><a href="#单机上的锁和分布式锁的联系与区别" class="headerlink" title="单机上的锁和分布式锁的联系与区别"></a>单机上的锁和分布式锁的联系与区别</h2><p>我们先来看下单机上的锁。</p>
<p><strong>对于在单机上运行的多线程程序来说，锁本身可以用一个变量表示。</strong></p>
<ul>
<li>变量值为 0 时，<strong>表示没有线程获取锁；</strong></li>
<li>变量值为 1 时，<strong>表示已经有线程获取到锁了。</strong></li>
</ul>
<p>我们通常说的线程调用加锁和释放锁的操作，到底是啥意思呢？我来解释一下。<strong>实际上，一个线程调用加锁操作，其实就是检查锁变量值是否为 0。如果是 0，就把锁的变量值设置为 1，表示获取到锁，如果不是 0，就返回错误信息，表示加锁失败，已经有别的线程获取到锁了。而一个线程调用释放锁操作，其实就是将锁变量的值置为 0，以便其它线程可以来获取锁。</strong></p>
<p>我用一段代码来展示下加锁和释放锁的操作，其中，lock 为锁变量。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">acquire_lock()&#123;</span><br><span class="line"></span><br><span class="line">  if lock == 0</span><br><span class="line"></span><br><span class="line">      lock = 1</span><br><span class="line"></span><br><span class="line">      return 1</span><br><span class="line"></span><br><span class="line">  else</span><br><span class="line"></span><br><span class="line">      return 0</span><br><span class="line"></span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">release_lock()&#123;</span><br><span class="line"></span><br><span class="line">  lock = 0</span><br><span class="line"></span><br><span class="line">  return 1</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>和单机上的锁类似，分布式锁同样可以<strong>用一个变量来实现</strong>。客户端加锁和释放锁的操作逻辑，也和单机上的加锁和释放锁操作逻辑一致：<strong>加锁时同样需要判断锁变量的值，根据锁变量值来判断能否加锁成功；释放锁时需要把锁变量值设置为 0，表明客户端不再持有锁</strong>。</p>
<p>但是，和线程在单机上操作锁不同的是，在分布式场景下，<strong>锁变量需要由一个共享存储系统来维护</strong>，只有这样，多个客户端才可以通过访问共享存储系统来访问锁变量。相应的，<strong>加锁和释放锁的操作就变成了读取、判断和设置共享存储系统中的锁变量值</strong>。</p>
<p>这样一来，我们就可以得出实现分布式锁的两个要求。</p>
<ul>
<li>要求一：<strong>分布式锁的加锁和释放锁的过程，涉及多个操作。所以，在实现分布式锁时，我们需要保证这些锁操作的原子性；</strong></li>
<li>要求二：<strong>共享存储系统保存了锁变量，如果共享存储系统发生故障或宕机，那么客户端也就无法进行锁操作了。在实现分布式锁时，我们需要考虑保证共享存储系统的可靠性，进而保证锁的可靠性。</strong></li>
</ul>
<p>好了，知道了具体的要求，接下来，我们就来学习下 Redis 是怎么实现分布式锁的。</p>
<p>其实，<strong>我们既可以基于单个 Redis 节点来实现，也可以使用多个 Redis 节点实现。在这两种情况下，锁的可靠性是不一样的。我们先来看基于单个 Redis 节点的实现方法。</strong></p>
<h2 id="基于单个-Redis-节点实现分布式锁"><a href="#基于单个-Redis-节点实现分布式锁" class="headerlink" title="基于单个 Redis 节点实现分布式锁"></a>基于单个 Redis 节点实现分布式锁</h2><p>作为分布式锁实现过程中的共享存储系统，<strong>Redis 可以使用键值对来保存锁变量，再接收和处理不同客户端发送的加锁和释放锁的操作请求。那么，键值对的键和值具体是怎么定的呢？</strong></p>
<p>我们要赋予锁变量一个变量名，<strong>把这个变量名作为键值对的键，而锁变量的值，则是键值对的值，这样一来，Redis 就能保存锁变量了，客户端也就可以通过 Redis 的命令操作来实现锁操作。</strong></p>
<p>为了帮助你理解，我画了一张图片，它展示 Redis 使用键值对保存锁变量，以及两个客户端同时请求加锁的操作过程。</p>
<p><img src="/2024/09/17/Redis30/1d18742c1e5fc88835ec27f1becfc145.jpg" alt="img"></p>
<p>可以看到，<strong>Redis 可以使用一个键值对 lock_key:0 来保存锁变量，其中，键是 lock_key，也是锁变量的名称，锁变量的初始值是 0。</strong></p>
<p>我们再来分析下加锁操作。</p>
<p>在图中，客户端 A 和 C 同时请求加锁。<strong>因为 Redis 使用单线程处理请求，所以，即使客户端 A 和 C 同时把加锁请求发给了 Redis，Redis 也会串行处理它们的请求。</strong></p>
<p><strong>我们假设 Redis 先处理客户端 A 的请求，读取 lock_key 的值，发现 lock_key 为 0，所以，Redis 就把 lock_key 的 value 置为 1，表示已经加锁了。紧接着，Redis 处理客户端 C 的请求，此时，Redis 会发现 lock_key 的值已经为 1 了，所以就返回加锁失败的信息。</strong></p>
<p>刚刚说的是加锁的操作，那释放锁该怎么操作呢？其实，释放锁就是直接把锁变量值设置为 0。</p>
<p>我还是借助一张图片来解释一下。这张图片展示了客户端 A 请求释放锁的过程。当客户端 A 持有锁时，锁变量 lock_key 的值为 1。<strong>客户端 A 执行释放锁操作后，Redis 将 lock_key 的值置为 0，表明已经没有客户端持有锁了。</strong></p>
<p><img src="/2024/09/17/Redis30/c7c413b47d42f06f08fce92404f31e82.jpg" alt="img"></p>
<p><strong>因为加锁包含了三个操作（读取锁变量、判断锁变量值以及把锁变量值设置为 1），而这三个操作在执行时需要保证原子性。</strong>那怎么保证原子性呢？</p>
<p>上节课，我们学过，要想保证操作的原子性，有两种通用的方法，分别是<strong>使用 Redis 的单命令操作和使用 Lua 脚本</strong>。那么，在分布式加锁场景下，该怎么应用这两个方法呢？</p>
<p>我们先来看下，Redis 可以用哪些单命令操作实现加锁操作。</p>
<p><strong>首先是 SETNX 命令，它用于设置键值对的值。具体来说，就是这个命令在执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置。</strong></p>
<p>举个例子，如果执行下面的命令时，key 不存在，那么 key 会被创建，并且值会被设置为 value；如果 key 已经存在，SETNX 不做任何赋值操作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SETNX key value</span><br></pre></td></tr></table></figure>

<p>对于释放锁操作来说，<strong>我们可以在执行完业务逻辑后，使用 DEL 命令删除锁变量。不过，你不用担心锁变量被删除后，其他客户端无法请求加锁了。因为 SETNX 命令在执行时，如果要设置的键值对（也就是锁变量）不存在，SETNX 命令会先创建键值对，然后设置它的值。</strong>所以，释放锁之后，再有客户端请求加锁时，<strong>SETNX 命令会创建保存锁变量的键值对，并设置锁变量的值，完成加锁</strong>。</p>
<p>总结来说，<strong>我们就可以用 SETNX 和 DEL 命令组合来实现加锁和释放锁操作</strong>。下面的伪代码示例显示了锁操作的过程，你可以看下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 加锁</span><br><span class="line"></span><br><span class="line">SETNX lock_key 1</span><br><span class="line"></span><br><span class="line">// 业务逻辑</span><br><span class="line"></span><br><span class="line">DO THINGS</span><br><span class="line"></span><br><span class="line">// 释放锁</span><br><span class="line"></span><br><span class="line">DEL lock_key</span><br></pre></td></tr></table></figure>

<p>不过，使用 SETNX 和 DEL 命令组合实现分布锁，存在两个潜在的风险。</p>
<p>第一个风险是，<strong>假如某个客户端在执行了 SETNX 命令、加锁之后，紧接着却在操作共享数据时发生了异常，结果一直没有执行最后的 DEL 命令释放锁。因此，锁就一直被这个客户端持有，其它客户端无法拿到锁，也无法访问共享数据和执行后续操作，这会给业务应用带来影响。</strong></p>
<p>针对这个问题，一个有效的解决方法是，<strong>给锁变量设置一个过期时间</strong>。这样一来，即使持有锁的客户端发生了异常，无法主动地释放锁，<strong>Redis 也会根据锁变量的过期时间，在锁变量过期后，把它删除</strong>。其它客户端在锁变量过期后，就可以重新请求加锁，这就不会出现无法加锁的问题了。</p>
<p>我们再来看第二个风险。如果客户端 A 执行了 SETNX 命令加锁后，假设客户端 B 执行了 DEL 命令释放锁，<strong>此时，客户端 A 的锁就被误释放了。如果客户端 C 正好也在申请加锁，就可以成功获得锁，进而开始操作共享数据</strong>。这样一来，客户端 A 和 C 同时在对共享数据进行操作，<strong>数据就会被修改错误，这也是业务层不能接受的。</strong></p>
<p>为了应对这个问题，我们需要<strong>能区分来自不同客户端的锁操作</strong>，具体咋做呢？其实，我们可以在锁变量的值上想想办法。</p>
<p>在使用 SETNX 命令进行加锁的方法中，我们通过把锁变量值设置为 1 或 0，表示是否加锁成功。1 和 0 只有两种状态，无法表示究竟是哪个客户端进行的锁操作。<strong>所以，我们在加锁操作时，可以让每个客户端给锁变量设置一个唯一值，这里的唯一值就可以用来标识当前操作的客户端。在释放锁操作时，客户端需要判断，当前锁变量的值是否和自己的唯一标识相等，只有在相等的情况下，才能释放锁。这样一来，就不会出现误释放锁的问题了。</strong></p>
<p>知道了解决方案，那么，在 Redis 中，具体是怎么实现的呢？我们再来了解下。</p>
<p>在查看具体的代码前，我要先带你学习下 Redis 的 SET 命令。</p>
<p>我们刚刚在说 SETNX 命令的时候提到，对于不存在的键值对，它会先创建再设置值（也就是“不存在即设置”），为了能达到和 SETNX 命令一样的效果，<strong>Redis 给 SET 命令提供了类似的选项 NX，用来实现“不存在即设置”。如果使用了 NX 选项，SET 命令只有在键值对不存在时，才会进行设置，否则不做赋值操作。此外，SET 命令在执行时还可以带上 EX 或 PX 选项，用来设置键值对的过期时间</strong>。</p>
<p>举个例子，执行下面的命令时，只有 key 不存在时，SET 才会创建 key，并对 key 进行赋值。另外，<strong>key 的存活时间由 seconds 或者 milliseconds 选项值来决定</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SET key value [EX seconds | PX milliseconds]  [NX]</span><br></pre></td></tr></table></figure>

<p>有了 SET 命令的 NX 和 EX&#x2F;PX 选项后，我们就可以用下面的命令来实现加锁操作了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 加锁, unique_value作为客户端唯一性的标识</span><br><span class="line"></span><br><span class="line">SET lock_key unique_value NX PX 10000</span><br></pre></td></tr></table></figure>

<p><strong>其中，unique_value 是客户端的唯一标识，可以用一个随机生成的字符串来表示，PX 10000 则表示 lock_key 会在 10s 后过期，以免客户端在这期间发生异常而无法释放锁。</strong></p>
<p><strong>因为在加锁操作中，每个客户端都使用了一个唯一标识，所以在释放锁操作时，我们需要判断锁变量的值，是否等于执行释放锁操作的客户端的唯一标识</strong>，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//释放锁 比较unique_value是否相等，避免误释放</span><br><span class="line">if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then</span><br><span class="line"></span><br><span class="line">     return redis.call(&quot;del&quot;,KEYS[1])</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line"></span><br><span class="line">     return 0</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p><strong>这是使用 Lua 脚本（unlock.script）实现的释放锁操作的伪代码，其中，KEYS[1]表示 lock_key，ARGV[1]是当前客户端的唯一标识，这两个值都是我们在执行 Lua 脚本时作为参数传入的。</strong></p>
<p>最后，我们执行下面的命令，就可以完成锁释放操作了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">redis-cli  --eval  unlock.script lock_key , unique_value </span><br></pre></td></tr></table></figure>

<p>你可能也注意到了，在释放锁操作中，我们使用了 Lua 脚本，<strong>这是因为，释放锁操作的逻辑也包含了读取锁变量、判断值、删除锁变量的多个操作，而 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，从而保证了锁释放操作的原子性。</strong></p>
<p>好了，到这里，<strong>你了解了如何使用 SET 命令和 Lua 脚本在 Redis 单节点上实现分布式锁。但是，我们现在只用了一个 Redis 实例来保存锁变量，如果这个 Redis 实例发生故障宕机了，那么锁变量就没有了。此时，客户端也无法进行锁操作了，这就会影响到业务的正常执行。所以，我们在实现分布式锁时，还需要保证锁的可靠性。那怎么提高呢？这就要提到基于多个 Redis 节点实现分布式锁的方式了。</strong></p>
<h2 id="基于多个-Redis-节点实现高可靠的分布式锁"><a href="#基于多个-Redis-节点实现高可靠的分布式锁" class="headerlink" title="基于多个 Redis 节点实现高可靠的分布式锁"></a>基于多个 Redis 节点实现高可靠的分布式锁</h2><p><strong>当我们要实现高可靠的分布式锁时，就不能只依赖单个的命令操作了，我们需要按照一定的步骤和规则进行加解锁操作，否则，就可能会出现锁无法工作的情况</strong>。“一定的步骤和规则”是指啥呢？其实就是分布式锁的算法。</p>
<p><strong>为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了分布式锁算法 Redlock</strong>。</p>
<p>Redlock 算法的基本思路，<strong>是让客户端和多个独立的 Redis 实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。</strong></p>
<p>我们来具体看下 Redlock 算法的执行步骤。<strong>Redlock 算法的实现需要有 N 个独立的 Redis 实例。接下来，我们可以分成 3 步来完成加锁操作</strong>。</p>
<p><strong>第一步是，客户端获取当前时间。</strong></p>
<p><strong>第二步是，客户端按顺序依次向 N 个 Redis 实例执行加锁操作。</strong></p>
<p><strong>这里的加锁操作和在单实例上执行的加锁操作一样，使用 SET 命令，带上 NX，EX&#x2F;PX 选项，以及带上客户端的唯一标识。当然，如果某个 Redis 实例发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给加锁操作设置一个超时时间。</strong></p>
<p><strong>如果客户端在和一个 Redis 实例请求加锁时，一直到超时都没有成功，那么此时，客户端会和下一个 Redis 实例继续请求加锁。加锁操作的超时时间需要远远地小于锁的有效时间，一般也就是设置为几十毫秒。</strong></p>
<p><strong>第三步是，一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。</strong></p>
<p>客户端只有在满足下面的这两个条件时，才能认为是加锁成功。</p>
<ul>
<li>条件一：<strong>客户端从超过半数（大于等于 N&#x2F;2+1）的 Redis 实例上成功获取到了锁；</strong></li>
<li>条件二：<strong>客户端获取锁的总耗时没有超过锁的有效时间。</strong></li>
</ul>
<p>在满足了这两个条件后，<strong>我们需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。</strong></p>
<p>当然，<strong>如果客户端在和所有实例执行完加锁操作后，没能同时满足这两个条件，那么，客户端向所有 Redis 节点发起释放锁的操作。</strong></p>
<p>在 Redlock 算法中，<strong>释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。这样一来，只要 N 个 Redis 实例中的半数以上实例能正常工作，就能保证分布式锁的正常工作了。</strong></p>
<p>所以，在实际的业务应用中，<strong>如果你想要提升分布式锁的可靠性，就可以通过 Redlock 算法来实现。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><strong>分布式锁是由共享存储系统维护的变量，多个客户端可以向共享存储系统发送命令进行加锁或释放锁操作。Redis 作为一个共享存储系统，可以用来实现分布式锁。</strong></p>
<p>在基于单个 Redis 实例实现分布式锁时，对于加锁操作，我们需要满足三个条件：</p>
<ul>
<li><strong>加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁；</strong></li>
<li><strong>锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX&#x2F;PX 选项，设置其过期时间；</strong></li>
<li><strong>锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端。</strong></li>
</ul>
<p>和加锁类似，<strong>释放锁也包含了读取锁变量值、判断锁变量值和删除锁变量三个操作，不过，我们无法使用单个命令来实现，所以，我们可以采用 Lua 脚本执行释放锁操作，通过 Redis 原子性地执行 Lua 脚本，来保证释放锁操作的原子性。</strong></p>
<p>不过，<strong>基于单个 Redis 实例实现分布式锁时，会面临实例异常或崩溃的情况，这会导致实例无法提供锁操作</strong>，正因为此，Redis 也提供了 Redlock 算法，用来实现基于多个实例的分布式锁。这样一来，<strong>锁变量由多个实例维护，即使有实例发生了故障，锁变量仍然是存在的，客户端还是可以完成锁操作。Redlock 算法是实现高可靠分布式锁的一种有效解决方案，你可以在实际应用中把它用起来</strong>。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提个小问题。这节课，我提到，我们可以使用 SET 命令带上 NX 和 EX&#x2F;PX 选项进行加锁操作，那么，我想请你再思考一下，我们是否可以用下面的方式来实现加锁操作呢？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 加锁</span><br><span class="line"></span><br><span class="line">SETNX lock_key unique_value</span><br><span class="line"></span><br><span class="line">EXPIRE lock_key 10S</span><br><span class="line"></span><br><span class="line">// 业务逻辑</span><br><span class="line"></span><br><span class="line">DO THINGS</span><br></pre></td></tr></table></figure>

<p>是否可以使用 SETNX + EXPIRE 来完成加锁操作？</p>
<p>不可以这么使用。<strong>使用 2 个命令无法保证操作的原子性，在异常情况下，加锁结果会不符合预期</strong>。异常情况主要分为以下几种情况：</p>
<ul>
<li>1、SETNX 执行成功，执行 EXPIRE 时由于网络问题设置过期失败</li>
<li>2、SETNX 执行成功，此时 Redis 实例宕机，EXPIRE 没有机会执行</li>
<li>3、SETNX 执行成功，客户端异常崩溃，EXPIRE 没有机会执行</li>
</ul>
<p>如果发生以上情况，<strong>并且客户端在释放锁时发生异常，没有正常释放锁，那么这把锁就会一直无法释放，其他线程都无法再获得锁</strong>。</p>
<p>下面说一下关于 Redis 分布式锁可靠性的问题。</p>
<p><strong>使用单个 Redis 节点（只有一个master）使用分布锁，如果实例宕机，那么无法进行锁操作了。那么采用主从集群模式部署是否可以保证锁的可靠性？</strong></p>
<p>答案是也很难保证。<strong>如果在 master 上加锁成功，此时 master 宕机，由于主从复制是异步的，加锁操作的命令还未同步到 slave，此时主从切换，新 master 节点依旧会丢失该锁，对业务来说相当于锁失效了。</strong></p>
<p><strong>所以 Redis 作者才提出基于多个 Redis 节点（master节点）的 Redlock 算法，但这个算法涉及的细节很多，作者在提出这个算法时，业界的分布式系统专家还与 Redis 作者发生过一场争论，来评估这个算法的可靠性，争论的细节都是关于异常情况可能导致 Redlock 失效的场景，例如加锁过程中客户端发生了阻塞、机器时钟发生跳跃等等。</strong></p>
<p>感兴趣的可以看下这篇文章，详细介绍了争论的细节，以及 Redis 分布式锁在各种异常情况是否安全的分析，收益会非常大：<a href="http://zhangtielei.com/posts/blog-redlock-reasoning.html%E3%80%82">http://zhangtielei.com/posts/blog-redlock-reasoning.html。</a></p>
<p>简单总结，基于 Redis 使用分布锁的注意点：</p>
<ul>
<li>1、使用 SET <code>$lock_key $unique_val EX $second NX</code> 命令<strong>保证加锁原子性，并为锁设置过期时间</strong></li>
<li>2、<strong>锁的过期时间要提前评估好，要大于操作共享资源的时间</strong></li>
<li>3、<strong>每个线程加锁时设置随机值，释放锁时判断是否和加锁设置的值一致，防止自己的锁被别人释放</strong></li>
<li>4、<strong>释放锁时使用 Lua 脚本，保证操作的原子性</strong></li>
<li>5、<strong>基于多个节点的 Redlock，加锁时超过半数节点操作成功，并且获取锁的耗时没有超过锁的有效时间才算加锁成功</strong></li>
<li>6、Redlock 释放锁时，<strong>要对所有节点释放（即使某个节点加锁失败了），因为加锁时可能发生服务端加锁成功，由于网络问题，给客户端回复网络包失败的情况，所以需要把所有节点可能存的锁都释放掉</strong></li>
<li>7、使用 Redlock 时<strong>要避免机器时钟发生跳跃</strong>，需要运维来保证，对运维有一定要求，否则可能会导致 Redlock 失效。<strong>例如共 3 个节点，线程 A 操作 2 个节点加锁成功，但其中 1 个节点机器时钟发生跳跃，锁提前过期，线程 B 正好在另外 2 个节点也加锁成功，此时 Redlock 相当于失效了（Redis 作者和分布式系统专家争论的重要点就在这）</strong></li>
<li>8、如果为了效率，<strong>使用基于单个 Redis 节点的分布式锁即可，此方案缺点是允许锁偶尔失效，优点是简单效率高</strong></li>
<li>9、如果是为了正确性，<strong>业务对于结果要求非常严格，建议使用 Redlock，但缺点是使用比较重，部署成本高</strong></li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/cgcm2v">锁 - 30 | 如何使用Redis实现分布式锁？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>缓存异常（下）：如何解决缓存雪崩、击穿、穿透难题？</title>
    <url>/2024/09/17/Redis26/</url>
    <content><![CDATA[<p>上节课，<strong>我们学习了缓存和数据库的数据不一致问题和应对方法</strong>。除了数据不一致问题，<strong>我们常常还会面临缓存异常的三个问题，分别是缓存雪崩、缓存击穿和缓存穿透</strong>。这三个问题一旦发生，<strong>会导致大量的请求积压到数据库层</strong>。如果请求的并发量很大，<strong>就会导致数据库宕机或是故障，这就是很严重的生产事故了</strong>。</p>
<p>这节课，我就来和你聊聊这三个问题的表现、诱发原因以及解决方法。俗话说，知己知彼，百战不殆。了解了问题的成因，我们就能够在应用 Redis 缓存时，进行合理的缓存设置，以及相应的业务应用前端设置，提前做好准备。</p>
<p>接下来，我们就先看下缓存雪崩的问题和应对方案。</p>
<h2 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h2><p><strong>缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。</strong></p>
<p>缓存雪崩一般是由两个原因导致的，应对方案也有所不同，我们一个个来看。</p>
<p>第一个原因是：<strong>缓存中有大量数据同时过期，导致大量请求无法得到处理</strong>。</p>
<p>具体来说，当数据保存在缓存中，<strong>并且设置了过期时间时，如果在某一个时刻，大量数据同时过期，此时，应用再访问这些数据的话，就会发生缓存缺失</strong>。紧接着，<strong>应用就会把请求发送给数据库，从数据库中读取数据。如果应用的并发请求量很大，那么数据库的压力也就很大，这会进一步影响到数据库的其他正常业务请求处理</strong>。我们来看一个简单的例子，如下图所示：</p>
<p><img src="/2024/09/17/Redis26/74bb1aa4b2213e3ff29e2ee701e8f72e.jpg" alt="img"></p>
<p>针对大量数据同时失效带来的缓存雪崩问题，我给你提供两种解决方案。</p>
<p>首先，<strong>我们可以避免给大量的数据设置相同的过期时间。如果业务层的确要求有些数据同时失效，你可以在用 EXPIRE 命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加 1~3 分钟）</strong>，这样一来，<strong>不同数据的过期时间有所差别，但差别又不会太大，既避免了大量数据同时过期，同时也保证了这些数据基本在相近的时间失效，仍然能满足业务需求</strong>。</p>
<p>除了<strong>微调过期时间</strong>，我们还可以通过<strong>服务降级</strong>，来应对缓存雪崩。</p>
<p>所谓的服务降级，是指发生缓存雪崩时，针对不同的数据采取不同的处理方式。</p>
<p><strong>当业务应用访问的是非核心数据（例如电商商品属性）时，暂时停止从缓存中查询这些数据，而是直接返回预定义信息、空值或是错误信息；</strong></p>
<p><strong>当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。</strong></p>
<p>这样一来，<strong>只有部分过期数据的请求会发送到数据库，数据库的压力就没有那么大了</strong>。下面这张图显示的是服务降级时数据请求的执行情况，你可以看下。</p>
<p><img src="/2024/09/17/Redis26/4ab3be5ba24cf172879e6b2cff649ca8.jpg" alt="img"></p>
<p><strong>除了大量数据同时失效会导致缓存雪崩，还有一种情况也会发生缓存雪崩，那就是，Redis 缓存实例发生故障宕机了，无法处理请求，这就会导致大量请求一下子积压到数据库层，从而发生缓存雪崩。</strong></p>
<p>一般来说，<strong>一个 Redis 实例可以支持数万级别的请求处理吞吐量，而单个数据库可能只能支持数千级别的请求处理吞吐量，它们两个的处理能力可能相差了近十倍。由于缓存雪崩，Redis 缓存失效，所以，数据库就可能要承受近十倍的请求压力，从而因为压力过大而崩溃</strong>。</p>
<p>此时，<strong>因为 Redis 实例发生了宕机，我们需要通过其他方法来应对缓存雪崩了</strong>。我给你提供两个建议。</p>
<p><strong>第一个建议，是在业务系统中实现服务熔断或请求限流机制。</strong></p>
<p><strong>所谓的服务熔断，是指在发生缓存雪崩时，为了防止引发连锁的数据库雪崩，甚至是整个系统的崩溃，我们暂停业务应用对缓存系统的接口访问</strong>。再具体点说，<strong>就是业务应用调用缓存接口时，缓存客户端并不把请求发给 Redis 缓存实例，而是直接返回</strong>，等到 Redis 缓存实例重新恢复服务后，<strong>再允许应用请求发送到缓存系统</strong>。</p>
<p>这样一来，我们就<strong>避免了大量请求因缓存缺失，而积压到数据库系统，保证了数据库系统的正常运行</strong>。</p>
<p>在业务系统运行时，<strong>我们可以监测 Redis 缓存所在机器和数据库所在机器的负载指标，例如每秒请求数、CPU 利用率、内存利用率等</strong>。<strong>如果我们发现 Redis 缓存实例宕机了，而数据库所在机器的负载压力突然增加（例如每秒请求数激增），此时，就发生缓存雪崩了</strong>。大量请求被发送到数据库进行处理。<strong>我们可以启动服务熔断机制，暂停业务应用对缓存服务的访问，从而降低对数据库的访问压力</strong>，如下图所示：</p>
<p><img src="/2024/09/17/Redis26/17d39f6233c3332161c588b42eccaeb5.jpg" alt="img"></p>
<p><strong>服务熔断虽然可以保证数据库的正常运行，但是暂停了整个缓存系统的访问，对业务应用的影响范围大</strong>。为了尽可能减少这种影响，<strong>我们也可以进行请求限流</strong>。这里说的请求限流，<strong>就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库</strong>。</p>
<p>我给你举个例子<strong>。假设业务系统正常运行时，请求入口前端允许每秒进入系统的请求是 1 万个，其中，9000 个请求都能在缓存系统中进行处理，只有 1000 个请求会被应用发送到数据库进行处理</strong>。</p>
<p><strong>一旦发生了缓存雪崩，数据库的每秒请求数突然增加到每秒 1 万个，此时，我们就可以启动请求限流机制，在请求入口前端只允许每秒进入系统的请求数为 1000 个，再多的请求就会在入口前端被直接拒绝服务。所以，使用了请求限流，就可以避免大量并发请求压力传递到数据库层。</strong></p>
<p><img src="/2024/09/17/Redis26/d5a0928e1d97cae2f4a4fb5b93e5c854.jpg" alt="img"></p>
<p><strong>使用服务熔断或是请求限流机制，来应对 Redis 实例宕机导致的缓存雪崩问题，是属于“事后诸葛亮”，也就是已经发生缓存雪崩了，我们使用这两个机制，来降低雪崩对数据库和整个业务系统的影响</strong>。</p>
<p><strong>我给你的第二个建议就是事前预防。</strong></p>
<p><strong>通过主从节点的方式构建 Redis 缓存高可靠集群</strong>。<strong>如果 Redis 缓存的主节点故障宕机了，从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓存雪崩问题</strong>。</p>
<p><strong>缓存雪崩是发生在大量数据同时失效的场景下</strong>，而接下来我要向你介绍的缓存击穿，<strong>是发生在某个热点数据失效的场景下</strong>。和缓存雪崩相比，<strong>缓存击穿失效的数据数量要小很多</strong>，应对方法也不一样，我们来看下。</p>
<h2 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h2><p>缓存击穿是指，<strong>针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求</strong>。缓存击穿的情况，经常发生在热点数据过期失效时，如下图所示：</p>
<p><img src="/2024/09/17/Redis26/d4c77da4yy7d6e34aca460642923ab4b.jpg" alt="img"></p>
<p><strong>为了避免缓存击穿给数据库带来的激增压力，我们的解决方法也比较直接，对于访问特别频繁的热点数据，我们就不设置过期时间了</strong>。这样一来，<strong>对热点数据的访问请求，都可以在缓存中进行处理，而 Redis 数万级别的高吞吐量可以很好地应对大量的并发请求访问</strong>。</p>
<p>好了，到这里，你了解了缓存雪崩和缓存击穿问题，以及它们的应对方案。<strong>当发生缓存雪崩或击穿时，数据库中还是保存了应用要访问的数据。接下来，我向你介绍的缓存穿透问题，和雪崩、击穿问题不一样，缓存穿透发生时，数据也不在数据库中，这会同时给缓存和数据库带来访问压力</strong>，那该怎么办呢？我们来具体看下。</p>
<h2 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h2><p><strong>缓存穿透是指要访问的数据既不在 Redis 缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据</strong>。此时，应用也无法从数据库中读取数据再写入缓存，来服务后续请求，这样一来，缓存也就成了“摆设”，<strong>如果应用持续有大量请求访问数据，就会同时给缓存和数据库带来巨大压力</strong>，如下图所示：</p>
<p><img src="/2024/09/17/Redis26/46c49dd155665579c5204a66da8ffc2e.jpg" alt="img"></p>
<p>那么，缓存穿透会发生在什么时候呢？一般来说，有两种情况。</p>
<ul>
<li><p>业务层误操作：<strong>缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据</strong>；</p>
</li>
<li><p>恶意攻击：<strong>专门访问数据库中没有的数据</strong>。</p>
</li>
</ul>
<p>为了避免缓存穿透的影响，我来给你提供三种应对方案。</p>
<p><strong>第一种方案是，缓存空值或缺省值。</strong></p>
<p>一旦发生缓存穿透，<strong>我们就可以针对查询的数据，在 Redis 中缓存一个空值或是和业务层协商确定的缺省值（例如，库存的缺省值可以设为 0）。紧接着，应用发送的后续请求再进行查询时，就可以直接从 Redis 中读取空值或缺省值，返回给业务应用了，避免了把大量请求发送给数据库处理，保持了数据库的正常运行</strong>。</p>
<p><strong>第二种方案是，使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力。</strong></p>
<p>我们先来看下，布隆过滤器是如何工作的。</p>
<p>布隆过滤器由一个<strong>初值都为 0 的 bit 数组和 N 个哈希函数组成，可以用来快速判断某个数据是否存在</strong>。当我们想标记某个数据存在时（例如，数据已被写入数据库），布隆过滤器会通过三个操作完成标记：</p>
<ul>
<li><p>首先，<strong>使用 N 个哈希函数，分别计算这个数据的哈希值，得到 N 个哈希值</strong>。</p>
</li>
<li><p>然后，<strong>我们把这 N 个哈希值对 bit 数组的长度取模，得到每个哈希值在数组中的对应位置</strong>。</p>
</li>
<li><p>最后，<strong>我们把对应位置的 bit 位设置为 1，这就完成了在布隆过滤器中标记数据的操作</strong>。</p>
</li>
</ul>
<p><strong>如果数据不存在（例如，数据库里没有写入数据），我们也就没有用布隆过滤器标记过数据，那么，bit 数组对应 bit 位的值仍然为 0。</strong></p>
<p>当需要查询某个数据时，我们就执行刚刚说的计算过程，<strong>先得到这个数据在 bit 数组中对应的 N 个位置。紧接着，我们查看 bit 数组中这 N 个位置上的 bit 值。只要这 N 个 bit 值有一个不为 1，这就表明布隆过滤器没有对该数据做过标记，所以，查询的数据一定没有在数据库中保存</strong>。为了便于你理解，我画了一张图，你可以看下。</p>
<p><img src="/2024/09/17/Redis26/98f7d32499e4386b40aebc3622aa7268.jpg" alt="img"></p>
<p>图中布隆过滤器是一个包含 10 个 bit 位的数组，使用了 3 个哈希函数，<strong>当在布隆过滤器中标记数据 X 时，X 会被计算 3 次哈希值，并对 10 取模，取模结果分别是 1、3、7。所以，bit 数组的第 1、3、7 位被设置为 1。当应用想要查询 X 时，只要查看数组的第 1、3、7 位是否为 1，只要有一个为 0，那么，X 就肯定不在数据库中</strong>。</p>
<p>正是基于布隆过滤器的快速检测特性，<strong>我们可以在把数据写入数据库时，使用布隆过滤器做个标记。当缓存缺失后，应用查询数据库时，可以通过查询布隆过滤器快速判断数据是否存在。如果不存在，就不用再去数据库中查询了。这样一来，即使发生缓存穿透了，大量请求只会查询 Redis 和布隆过滤器，而不会积压到数据库，也就不会影响数据库的正常运行。</strong>布隆过滤器<strong>可以使用 Redis 实现，本身就能承担较大的并发访问压力</strong>。</p>
<p>最后一种方案是，在请求入口的<strong>前端进行请求检测。</strong>缓存穿透的一个原因是有大量的恶意请求访问不存在的数据，<strong>所以，一个有效的应对方案是在请求入口前端，对业务系统接收到的请求进行合法性检测，把恶意的请求（例如请求参数不合理、请求参数是非法值、请求字段不存在）直接过滤掉，不让它们访问后端缓存和数据库。这样一来，也就不会出现缓存穿透问题了</strong>。</p>
<p><strong>跟缓存雪崩、缓存击穿这两类问题相比，缓存穿透的影响更大一些，希望你能重点关注一下</strong>。从预防的角度来说，<strong>我们需要避免误删除数据库和缓存中的数据；从应对角度来说，我们可以在业务系统中使用缓存空值或缺省值、使用布隆过滤器，以及进行恶意请求检测等方法</strong>。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>这节课，我们学习了缓存雪崩、击穿和穿透这三类异常问题。<strong>从问题成因来看，缓存雪崩和击穿主要是因为数据不在缓存中了，而缓存穿透则是因为数据既不在缓存中，也不在数据库中</strong>。所以，缓存雪崩或击穿时，<strong>一旦数据库中的数据被再次写入到缓存后，应用又可以在缓存中快速访问数据了，数据库的压力也会相应地降低下来，而缓存穿透发生时，Redis 缓存和数据库会同时持续承受请求压力</strong>。</p>
<p>为了方便你掌握，我把这三大问题的原因和应对方案总结到了一张表格，你可以再复习一下。</p>
<p><img src="/2024/09/17/Redis26/b5bd931239be18bef24b2ef36c70e9e1.jpg" alt="img"></p>
<p>最后，我想强调一下，<strong>服务熔断、服务降级、请求限流这些方法都是属于“有损”方案，在保证数据库和整体系统稳定的同时，会对业务应用带来负面影响</strong>。例如使用服务降级时，<strong>有部分数据的请求就只能得到错误返回信息，无法正常处理。如果使用了服务熔断，那么，整个缓存系统的服务都被暂停了，影响的业务范围更大</strong>。而使用了请求限流机制后，<strong>整个业务系统的吞吐率会降低，能并发处理的用户请求会减少，会影响到用户体验</strong>。</p>
<p>所以，我给你的建议是，尽量使用预防式方案：</p>
<ul>
<li><p>针对缓存雪崩，<strong>合理地设置数据过期时间，以及搭建高可靠缓存集群；</strong></p>
</li>
<li><p>针对缓存击穿，<strong>在缓存访问非常频繁的热点数据时，不要设置过期时间；</strong></p>
</li>
<li><p>针对缓存穿透，<strong>提前在入口前端实现恶意请求检测，或者规范数据库的数据删除操作，避免误删除。</strong></p>
</li>
</ul>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提个小问题。在讲到缓存雪崩时，我提到，可以采用服务熔断、服务降级、请求限流的方法来应对。请你思考下，这三个机制可以用来应对缓存穿透问题吗？</p>
<p>是否可以采用服务熔断、服务降级、请求限流的方法来应对缓存穿透问题？</p>
<p>我觉得需要区分场景来看：</p>
<ul>
<li>如果缓存穿透的原因是恶意攻击，<strong>攻击者故意访问数据库中不存在的数据</strong>。<strong>这种情况可以先使用服务熔断、服务降级、请求限流的方式，对缓存和数据库层增加保护，防止大量恶意请求把缓存和数据库压垮。在这期间可以对攻击者进行防护，例如封禁IP等操作</strong>。</li>
<li>如果缓存穿透的原因是，<strong>业务层误操作把数据从缓存和数据库都删除了，如果误删除的数据很少，不会导致大量请求压到数据库的情况，那么快速恢复误删的数据就好了</strong>，不需要使用服务熔断、服务降级、请求限流。<strong>如果误操作删除的数据范围比较广，导致大量请求压到数据库层，此时使用服务熔断、服务降级、请求限流的方法来应对是有帮助的，使用这些方法先把缓存和数据库保护起来，然后使用备份库快速恢复数据</strong>，在数据恢复期间，这些保护方法可以为数据库恢复提供保障。</li>
</ul>
<p>还有一种缓存穿透的场景，我们平时会遇到的，和大家分享一下。</p>
<p><strong>对于一个刚上线的新业务模块，如果还没有用户在这个模块内产生业务数据，当用户需要查询这个业务模块自己的数据时，由于缓存和数据库都没有这个用户的数据，此时也会产生缓存穿透，但这种场景不像误删数据和恶意攻击那样，而是属于正常的用户行为</strong>。</p>
<p>这种场景采用服务熔断、服务降级、请求限流的方式就没有任何意义了，反而会影响正常用户的访问。<strong>这种场景只能使用缓存回种空值、布隆过滤器来解决。</strong></p>
<p>可见，<strong>服务熔断、服务降级、请求限流的作用是，当系统内部发生故障或潜在问题时，为了防止系统内部的问题进一步恶化，所以会采用这些方式对系统增加保护，待系统内部故障恢复后，可以依旧继续对外提供服务，这些方法属于服务治理的范畴，在任何可能导致系统故障的场景下，都可以选择性配合使用</strong>。</p>
<p>另外，关于文章所讲的由于“Redis缓存实例发生故障宕机”导致<strong>缓存雪崩</strong>的问题，我觉得一个可以优化的方案是，<strong>当Redis实例故障宕机后，业务请求可以直接返回错误，没必要再去请求数据库了，这样就不会导致数据库层压力变大。当然，最好的方式还是Redis部署主从集群+哨兵，主节点宕机后，哨兵可以及时把从节点提升为主，继续提供服务</strong>。</p>
<p>关于布隆过滤器的使用，还有几点和大家分享。</p>
<ul>
<li><strong>1、布隆过滤器会有误判：由于采用固定bit的数组，使用多个哈希函数映射到多个bit上，有可能会导致两个不同的值都映射到相同的一组bit上。虽然有误判，但对于业务没有影响，无非就是还存在一些穿透而已，但整体上已经过滤了大多数无效穿透请求。</strong></li>
<li><strong>2、布隆过滤器误判率和空间使用的计算：误判本质是因为哈希冲突，降低误判的方法是增加哈希函数 + 扩大整个bit数组的长度，但增加哈希函数意味着影响性能，扩大数组长度意味着空间占用变大，所以使用布隆过滤器，需要在误判率和性能、空间作一个平衡，具体的误判率是有一个计算公式可以推导出来的（比较复杂）</strong>。<strong>但我们在使用开源的布隆过滤器时比较简单，通常会提供2个参数：预估存入的数据量大小、要求的误判率，输入这些参数后，布隆过滤器会有自动计算出最佳的哈希函数数量和数组占用的空间大小，直接使用即可。</strong></li>
<li><strong>3、布隆过滤器可以放在缓存和数据库的最前面：把Redis当作布隆过滤器时（4.0提供了布隆过滤器模块，4.0以下需要引入第三方库）</strong>，当用户产生业务数据写入缓存和数据库后，<strong>同时也写入布隆过滤器，之后当用户访问自己的业务数据时，先检查布隆过滤器，如果过滤器不存在，就不需要查询缓存和数据库了，可以同时降低缓存和数据库的压力</strong>。</li>
<li><strong>4、Redis实现的布隆过滤器bigkey问题：Redis布隆过滤器是使用String类型实现的，存储的方式是一个bigkey，建议使用时单独部署一个实例，专门存放布隆过滤器的数据，不要和业务数据混用，否则在集群环境下，数据迁移时会导致Redis阻塞问题。</strong></li>
</ul>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/ocxcuc">缓存 - 26 | 缓存异常（下）：如何解决缓存雪崩、击穿、穿透难题？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>事务机制：Redis能实现ACID属性吗？</title>
    <url>/2024/09/17/Redis31/</url>
    <content><![CDATA[<p>事务是数据库的一个重要功能。<strong>所谓的事务，就是指对数据进行读写的一系列操作。事务在执行时，会提供专门的属性保证，包括原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），也就是 ACID 属性</strong>。这些属性既包括了<strong>对事务执行结果的要求，也有对数据库在事务执行前后的数据状态变化的要求</strong>。</p>
<p>那么，Redis 可以完全保证 ACID 属性吗？毕竟，如果有些属性在一些场景下不能保证的话，很可能会导致数据出错，所以，我们必须要掌握 Redis 对这些属性的支持情况，并且提前准备应对策略。</p>
<p>接下来，我们就先了解 ACID 属性对事务执行的具体要求，有了这个知识基础后，我们才能准确地判断 Redis 的事务机制能否保证 ACID 属性。</p>
<h2 id="事务-ACID-属性的要求"><a href="#事务-ACID-属性的要求" class="headerlink" title="事务 ACID 属性的要求"></a>事务 ACID 属性的要求</h2><p>首先来看原子性。<strong>原子性的要求很明确，就是一个事务中的多个操作必须都完成，或者都不完成。业务应用使用事务时，原子性也是最被看重的一个属性。</strong></p>
<p>我给你举个例子。<strong>假如用户在一个订单中购买了两个商品 A 和 B，那么，数据库就需要把这两个商品的库存都进行扣减。如果只扣减了一个商品的库存，那么，这个订单完成后，另一个商品的库存肯定就错了。</strong></p>
<p>第二个属性是一致性。<strong>这个很容易理解，就是指数据库中的数据在事务执行前后是一致的。</strong></p>
<p>第三个属性是隔离性。<strong>它要求数据库在执行一个事务时，其它操作无法存取到正在执行事务访问的数据。</strong></p>
<p>我还是借助用户下单的例子给你解释下。<strong>假设商品 A 和 B 的现有库存分别是 5 和 10，用户 X 对 A、B 下单的数量分别是 3、6。如果事务不具备隔离性，在用户 X 下单事务执行的过程中，用户 Y 一下子也购买了 5 件 B，这和 X 购买的 6 件 B 累加后，就超过 B 的总库存值了，这就不符合业务要求了</strong>。</p>
<p>最后一个属性是持久性。<strong>数据库执行事务后，数据的修改要被持久化保存下来。当数据库重启后，数据的值需要是被修改后的值。</strong></p>
<p>了解了 ACID 属性的具体要求后，我们再来看下 Redis 是如何实现事务机制的。</p>
<h2 id="Redis-如何实现事务？"><a href="#Redis-如何实现事务？" class="headerlink" title="Redis 如何实现事务？"></a>Redis 如何实现事务？</h2><p>事务的执行过程包含三个步骤，<strong>Redis 提供了 MULTI、EXEC 两个命令来完成这三个步骤。下面我们来分析下。</strong></p>
<p><strong>第一步，客户端要使用一个命令显式地表示一个事务的开启。在 Redis 中，这个命令就是 MULTI。</strong></p>
<p><strong>第二步，客户端把事务中本身要执行的具体操作（例如增删改数据）发送给服务器端。这些操作就是 Redis 本身提供的数据读写命令，例如 GET、SET 等。不过，这些命令虽然被客户端发送到了服务器端，但 Redis 实例只是把这些命令暂存到一个命令队列中，并不会立即执行。</strong></p>
<p><strong>第三步，客户端向服务器端发送提交事务的命令，让数据库实际执行第二步中发送的具体操作</strong>。Redis 提供的 <strong>EXEC 命令</strong>就是执行事务提交的。<strong>当服务器端收到 EXEC 命令后，才会实际执行命令队列中的所有命令</strong>。</p>
<p>下面的代码就显示了使用 MULTI 和 EXEC 执行一个事务的过程，你可以看下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\#开启事务</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; MULTI</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">\#将a:stock减1，</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; DECR a:stock</span><br><span class="line"></span><br><span class="line">QUEUED</span><br><span class="line"></span><br><span class="line">\#将b:stock减1</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; DECR b:stock</span><br><span class="line"></span><br><span class="line">QUEUED</span><br><span class="line"></span><br><span class="line">\#实际执行事务</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; EXEC</span><br><span class="line"></span><br><span class="line">1) (integer) 4</span><br><span class="line"></span><br><span class="line">2) (integer) 9</span><br></pre></td></tr></table></figure>

<p>我们假设 a:stock、b:stock 两个键的初始值是 5 和 10。在 MULTI 命令后执行的两个 DECR 命令，是把 a:stock、b:stock 两个键的值分别减 1，它们执行后的返回结果都是 QUEUED，<strong>这就表示，这些操作都被暂存到了命令队列，还没有实际执行。等到执行了 EXEC 命令后，可以看到返回了 4、9，这就表明，两个 DECR 命令已经成功地执行了。</strong></p>
<p>好了，通过使用 MULTI 和 EXEC 命令，我们可以实现多个操作的共同执行，但是这符合事务要求的 ACID 属性吗？接下来，我们就来具体分析下。</p>
<h2 id="Redis-的事务机制能保证哪些属性？"><a href="#Redis-的事务机制能保证哪些属性？" class="headerlink" title="Redis 的事务机制能保证哪些属性？"></a>Redis 的事务机制能保证哪些属性？</h2><p>原子性是事务操作最重要的一个属性，所以，我们先来分析下 Redis 事务机制能否保证原子性。</p>
<h3 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h3><p><strong>如果事务正常执行，没有发生任何错误，那么，MULTI 和 EXEC 配合使用，就可以保证多个操作都完成。但是，如果事务执行发生错误了，原子性还能保证吗？我们需要分三种情况来看。</strong></p>
<p>第一种情况是，<strong>在执行 EXEC 命令前，客户端发送的操作命令本身就有错误</strong>（比如语法错误，使用了不存在的命令），在命令入队时就被 Redis 实例判断出来了。</p>
<p>对于这种情况，在命令入队时，<strong>Redis 就会报错并且记录下这个错误。此时，我们还能继续提交命令操作。等到执行了 EXEC 命令之后，Redis 就会拒绝执行所有提交的命令操作，返回事务失败的结果。这样一来，事务中的所有命令都不会再被执行了，保证了原子性。</strong></p>
<p>我们来看一个因为事务操作入队时发生错误，而导致事务失败的小例子。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\#开启事务</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; MULTI</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">\#发送事务中的第一个操作，但是Redis不支持该命令，返回报错信息</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; PUT a:stock 5</span><br><span class="line"></span><br><span class="line">(error) ERR unknown command `PUT`, with args beginning with: `a:stock`, `5`, </span><br><span class="line"></span><br><span class="line">\#发送事务中的第二个操作，这个操作是正确的命令，Redis把该命令入队</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; DECR b:stock</span><br><span class="line"></span><br><span class="line">QUEUED</span><br><span class="line"></span><br><span class="line">\#实际执行事务，但是之前命令有错误，所以Redis拒绝执行</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; EXEC</span><br><span class="line"></span><br><span class="line">(error) EXECABORT Transaction discarded because of previous errors.</span><br></pre></td></tr></table></figure>

<p>在这个例子中，<strong>事务里包含了一个 Redis 本身就不支持的 PUT 命令，所以，在 PUT 命令入队时，Redis 就报错了。虽然，事务里还有一个正确的 DECR 命令，但是，在最后执行 EXEC 命令后，整个事务被放弃执行了。</strong></p>
<p>我们再来看第二种情况。</p>
<p>和第一种情况不同的是，<strong>事务操作入队时，命令和操作的数据类型不匹配，但 Redis 实例没有检查出错误</strong>。但是，在执行完 EXEC 命令以后，<strong>Redis 实际执行这些事务操作时，就会报错。不过，需要注意的是，虽然 Redis 会对错误命令报错，但还是会把正确的命令执行完。在这种情况下，事务的原子性就无法得到保证了。</strong></p>
<p>举个小例子。<strong>事务中的 LPOP 命令对 String 类型数据进行操作，入队时没有报错，但是，在 EXEC 执行时报错了。LPOP 命令本身没有执行成功，但是事务中的 DECR 命令却成功执行了。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\#开启事务</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; MULTI</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">\#发送事务中的第一个操作，LPOP命令操作的数据类型不匹配，此时并不报错</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; LPOP a:stock</span><br><span class="line"></span><br><span class="line">QUEUED</span><br><span class="line"></span><br><span class="line">\#发送事务中的第二个操作</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; DECR b:stock</span><br><span class="line"></span><br><span class="line">QUEUED</span><br><span class="line"></span><br><span class="line">\#实际执行事务，事务第一个操作执行报错</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; EXEC</span><br><span class="line"></span><br><span class="line">1) (error) WRONGTYPE Operation against a key holding the wrong kind of value</span><br><span class="line"></span><br><span class="line">2) (integer) 8</span><br></pre></td></tr></table></figure>

<p>看到这里，你可能有个疑问，<strong>传统数据库（例如 MySQL）在执行事务时，会提供回滚机制，当事务执行发生错误时，事务中的所有操作都会撤销，已经修改的数据也会被恢复到事务执行前的状态，那么，在刚才的例子中，如果命令实际执行时报错了，是不是可以用回滚机制恢复原来的数据呢？</strong></p>
<p>其实，<strong>Redis 中并没有提供回滚机制</strong>。虽然 Redis 提供了 DISCARD 命令，<strong>但是，这个命令只能用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。</strong></p>
<p>DISCARD 命令具体怎么用呢？我们来看下下面的代码。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\#读取a:stock的值4</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; GET a:stock</span><br><span class="line"></span><br><span class="line">&quot;4&quot;</span><br><span class="line"></span><br><span class="line">\#开启事务</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; MULTI </span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">\#发送事务的第一个操作，对a:stock减1</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; DECR a:stock</span><br><span class="line"></span><br><span class="line">QUEUED</span><br><span class="line"></span><br><span class="line">\#执行DISCARD命令，主动放弃事务</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; DISCARD</span><br><span class="line"></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">\#再次读取a:stock的值，值没有被修改</span><br><span class="line"></span><br><span class="line">127.0.0.1:6379&gt; GET a:stock</span><br><span class="line"></span><br><span class="line">&quot;4&quot;</span><br></pre></td></tr></table></figure>

<p>这个例子中，a:stock 键的值一开始为 4，然后，我们执行一个事务，想对 a:stock 的值减 1。但是，在事务的最后，我们执行的是 DISCARD 命令，所以事务就被放弃了。我们再次查看 a:stock 的值，会发现仍然为 4。</p>
<p>最后，我们再来看下第三种情况：<strong>在执行事务的 EXEC 命令时，Redis 实例发生了故障，导致事务执行失败</strong>。</p>
<p>在这种情况下，<strong>如果 Redis 开启了 AOF 日志，那么，只会有部分的事务操作被记录到 AOF 日志中。我们需要使用 redis-check-aof 工具检查 AOF 日志文件，这个工具可以把未完成的事务操作从 AOF 文件中去除。这样一来，我们使用 AOF 恢复实例后，事务操作不会再被执行，从而保证了原子性。</strong></p>
<p>当然，<strong>如果 AOF 日志并没有开启，那么实例重启后，数据也都没法恢复了，此时，也就谈不上原子性了。</strong></p>
<p>好了，到这里，你了解了 Redis 对事务原子性属性的保证情况，我们来简单小结下：</p>
<ul>
<li><strong>命令入队时就报错，会放弃事务执行，保证原子性；</strong></li>
<li><strong>命令入队时没报错，实际执行时报错，不保证原子性；</strong></li>
<li><strong>EXEC 命令执行时实例故障，如果开启了 AOF 日志，可以保证原子性。</strong></li>
</ul>
<p>接下来，我们再来学习下一致性属性的保证情况。</p>
<h3 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h3><p>事务的一致性保证会受到错误命令、实例故障的影响。所以，我们按照命令出错和实例故障的发生时机，分成三种情况来看。</p>
<p><strong>情况一：命令入队时就报错</strong></p>
<p>在这种情况下，<strong>事务本身就会被放弃执行，所以可以保证数据库的一致性。</strong></p>
<p><strong>情况二：命令入队时没报错，实际执行时报错</strong></p>
<p>在这种情况下，<strong>有错误的命令不会被执行，正确的命令可以正常执行，也不会改变数据库的一致性。</strong></p>
<p><strong>情况三：EXEC 命令执行时实例发生故障</strong></p>
<p>在这种情况下，<strong>实例故障后会进行重启，这就和数据恢复的方式有关了，我们要根据实例是否开启了 RDB 或 AOF 来分情况讨论下。</strong></p>
<ul>
<li>如果我们没有开启 RDB 或 AOF，<strong>那么，实例故障重启后，数据都没有了，数据库是一致的。</strong></li>
<li>如果我们使用了 RDB 快照，<strong>因为 RDB 快照不会在事务执行时执行，所以，事务命令操作的结果不会被保存到 RDB 快照中，使用 RDB 快照进行恢复时，数据库里的数据也是一致的。</strong></li>
<li>如果我们使用了 AOF 日志，<strong>而事务操作还没有被记录到 AOF 日志时，实例就发生了故障，那么，使用 AOF 日志恢复的数据库数据是一致的。如果只有部分操作被记录到了 AOF 日志，我们可以使用 redis-check-aof 清除事务中已经完成的操作，数据库恢复后也是一致的。</strong></li>
</ul>
<p>所以，总结来说，<strong>在命令执行错误或 Redis 发生故障的情况下，Redis 事务机制对一致性属性是有保证的。接下来，我们再继续分析下隔离性。</strong></p>
<h3 id="隔离性"><a href="#隔离性" class="headerlink" title="隔离性"></a>隔离性</h3><p>事务的隔离性保证，<strong>会受到和事务一起执行的并发操作的影响</strong>。而<strong>事务执行又可以分成命令入队（EXEC 命令执行前）和命令实际执行（EXEC 命令执行后）两个阶段，所以，我们就针对这两个阶段，分成两种情况来分析：</strong></p>
<p>并发操作在 EXEC 命令前执行，<strong>此时，隔离性的保证要使用 WATCH 机制来实现，否则隔离性无法保证；</strong></p>
<p>并发操作在 EXEC 命令后执行，<strong>此时，隔离性可以保证。</strong></p>
<p>我们先来看第一种情况。一个事务的 EXEC 命令还没有执行时，事务的命令操作是暂存在命令队列中的。<strong>此时，如果有其它的并发操作，我们就需要看事务是否使用了 WATCH 机制。</strong></p>
<p><strong>WATCH 机制的作用是，在事务执行前，监控一个或多个键的值变化情况，当事务调用 EXEC 命令执行时，WATCH 机制会先检查监控的键是否被其它客户端修改了。如果修改了，就放弃事务执行，避免事务的隔离性被破坏。</strong>然后，客户端可以再次执行事务，<strong>此时，如果没有并发修改事务数据的操作了，事务就能正常执行，隔离性也得到了保证。</strong></p>
<p>WATCH 机制的具体实现是由 WATCH 命令实现的，我给你举个例子，你可以看下下面的图，进一步理解下 WATCH 命令的使用。</p>
<p><img src="/2024/09/17/Redis31/4f8589410f77df16311dd29131676373.jpg" alt="img"></p>
<p>我来给你具体解释下图中的内容。</p>
<p>在 t1 时，客户端 X 向实例发送了 WATCH 命令。实例收到 WATCH 命令后，开始监测 a:stock 的值的变化情况。</p>
<p>紧接着，在 t2 时，客户端 X 把 MULTI 命令和 DECR 命令发送给实例，实例把 DECR 命令暂存入命令队列。</p>
<p>在 t3 时，客户端 Y 也给实例发送了一个 DECR 命令，要修改 a:stock 的值，实例收到命令后就直接执行了。</p>
<p>等到 t4 时，实例收到客户端 X 发送的 EXEC 命令，但是，实例的 WATCH 机制发现 a:stock 已经被修改了，就会放弃事务执行。这样一来，事务的隔离性就可以得到保证了。</p>
<p><strong>当然，如果没有使用 WATCH 机制，在 EXEC 命令前执行的并发操作是会对数据进行读写的。而且，在执行 EXEC 命令的时候，事务要操作的数据已经改变了，在这种情况下，Redis 并没有做到让事务对其它操作隔离，隔离性也就没有得到保障。下面这张图显示了没有 WATCH 机制时的情况，你可以看下。</strong></p>
<p><img src="/2024/09/17/Redis31/8ca37debfff91282b9c62a25fd7e9a57.jpg" alt="img"></p>
<p>在 t2 时刻，客户端 X 发送的 EXEC 命令还没有执行，但是客户端 Y 的 DECR 命令就执行了，此时，a:stock 的值会被修改，这就无法保证 X 发起的事务的隔离性了。</p>
<p>刚刚说的是并发操作在 EXEC 命令前执行的情况，下面我再来说一说第二种情况：<strong>并发操作在 EXEC 命令之后被服务器端接收并执行</strong>。</p>
<p>因为 Redis 是用单线程执行命令，<strong>而且，EXEC 命令执行后，Redis 会保证先把命令队列中的所有命令执行完。所以，在这种情况下，并发操作不会破坏事务的隔离性，如下图所示：</strong></p>
<p><img src="/2024/09/17/Redis31/11a1eff930920a0b423a6e46c23f44ae.jpg" alt="img"></p>
<p>最后，我们来分析一下 Redis 事务的持久性属性保证情况。</p>
<h3 id="持久性"><a href="#持久性" class="headerlink" title="持久性"></a>持久性</h3><p><strong>因为 Redis 是内存数据库，所以，数据是否持久化保存完全取决于 Redis 的持久化配置模式。</strong></p>
<p><strong>如果 Redis 没有使用 RDB 或 AOF，那么事务的持久化属性肯定得不到保证。如果 Redis 使用了 RDB 模式，那么，在一个事务执行后，而下一次的 RDB 快照还未执行前，如果发生了实例宕机，这种情况下，事务修改的数据也是不能保证持久化的。</strong></p>
<p><strong>如果 Redis 采用了 AOF 模式，因为 AOF 模式的三种配置选项 no、everysec 和 always 都会存在数据丢失的情况，所以，事务的持久性属性也还是得不到保证。</strong></p>
<p><strong>所以，不管 Redis 采用什么持久化模式，事务的持久性属性是得不到保证的。</strong></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>在这节课上，我们学习了 Redis 中的事务实现。<strong>Redis 通过 MULTI、EXEC、DISCARD 和 WATCH 四个命令来支持事务机制，这 4 个命令的作用，我总结在下面的表中，你可以再看下。</strong></p>
<p><img src="/2024/09/17/Redis31/9571308df0620214d7ccb2f2cc73a250.jpg" alt="img"></p>
<p><strong>事务的 ACID 属性是我们使用事务进行正确操作的基本要求。通过这节课的分析，我们了解到了，Redis 的事务机制可以保证一致性和隔离性，但是无法保证持久性。不过，因为 Redis 本身是内存数据库，持久性并不是一个必须的属性，我们更加关注的还是原子性、一致性和隔离性这三个属性。</strong></p>
<p>原子性的情况比较复杂，<strong>只有当事务中使用的命令语法有误时，原子性得不到保证，在其它情况下，事务都可以原子性执行。</strong></p>
<p>所以，我给你一个小建议：<strong>严格按照 Redis 的命令规范进行程序开发，并且通过 code review 确保命令的正确性</strong>。这样一来，Redis 的事务机制就能被应用在实践中，保证多操作的正确执行。</p>
<h2 id="每课一问"><a href="#每课一问" class="headerlink" title="每课一问"></a>每课一问</h2><p>按照惯例，我给你提个小问题，在执行事务时，如果 Redis 实例发生故障，而 Redis 使用了 RDB 机制，那么，事务的原子性还能得到保证吗？</p>
<p>在执行事务时，如果 Redis 实例发生故障，而 Redis 使用的 RDB 机制，事务的原子性还能否得到保证？</p>
<p>我觉得是可以保证原子性的。</p>
<p>如果一个事务只执行了一半，<strong>然后 Redis 实例故障宕机了，由于 RDB 不会在事务执行时执行，所以 RDB 文件中不会记录只执行了一部分的结果数据。之后用 RDB 恢复实例数据，恢复的还是事务之前的数据。但 RDB 本身是快照持久化，所以会存在数据丢失，丢失的是距离上一次 RDB 之间的所有更改操作。</strong></p>
<p>关于 Redis 事务的使用，有几个细节我觉得有必要补充下，关于 Pipeline 和 WATCH 命令的使用。</p>
<p><strong>1、在使用事务时，建议配合 Pipeline 使用。</strong></p>
<ul>
<li>a) 如果不使用 Pipeline，<strong>客户端是先发一个 MULTI 命令到服务端，客户端收到 OK，然后客户端再发送一个个操作命令，客户端依次收到 QUEUED，最后客户端发送 EXEC 执行整个事务（文章例子就是这样演示的），这样消息每次都是一来一回，效率比较低，而且在这多次操作之间，别的客户端可能就把原本准备修改的值给修改了，所以无法保证隔离性。</strong></li>
<li>b) <strong>而使用 Pipeline 是一次性把所有命令打包好全部发送到服务端，服务端全部处理完成后返回。这么做好的好处，一是减少了来回网络 IO 次数，提高操作性能。二是一次性发送所有命令到服务端，服务端在处理过程中，是不会被别的请求打断的（Redis单线程特性，此时别的请求进不来），这本身就保证了隔离性</strong>。我们平时使用的 Redis SDK 在使用开启事务时，<strong>一般都会默认开启 Pipeline 的</strong>，可以留意观察一下。</li>
</ul>
<p>2、关于 WATCH 命令的使用场景。</p>
<ul>
<li>a) 在上面 1-a 场景中，也就是使用了事务命令，但没有配合 Pipeline 使用，<strong>如果想要保证隔离性，需要使用 WATCH 命令保证，也就是文章中讲 WATCH 的例子。但如果是 1-b 场景，使用了 Pipeline 一次发送所有命令到服务端，那么就不需要使用 WATCH 了，因为服务端本身就保证了隔离性。</strong></li>
<li>b) 如果事务 + Pipeline 就可以保证隔离性，那 WATCH 还有没有使用的必要？答案是有的。<strong>对于一个资源操作为读取、修改、写回这种场景，如果需要保证事物的原子性，此时就需要用到 WATCH 了。例如想要修改某个资源，但需要事先读取它的值，再基于这个值进行计算后写回，如果在这期间担心这个资源被其他客户端修改了，那么可以先 WATCH 这个资源，再读取、修改、写回，如果写回成功，说明其他客户端在这期间没有修改这个资源。如果其他客户端修改了这个资源，那么这个事务操作会返回失败，不会执行，从而保证了原子性。</strong></li>
</ul>
<p>细节比较多，如果不太好理解，最好亲自动手试一下。</p>
<p>参考文章：<a href="https://geekdaxue.co/read/haofeiyu@redis/mx9e53">锁 - 31 | 事务机制：Redis能实现ACID属性吗？ - 《Redis 读书笔记》 - 极客文档 (geekdaxue.co)</a></p>
<p>官网解释：<a href="https://redis.io/docs/latest/develop/interact/transactions/">Transactions | Docs (redis.io)</a></p>
]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
</search>
